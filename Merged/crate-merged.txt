

package io.crate.analyze;

import com.google.common.base.MoreObjects;
import io.crate.metadata.ColumnIdent;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.common.Base64;
import org.elasticsearch.common.Nullable;
import org.elasticsearch.common.Strings;
import org.elasticsearch.common.bytes.BytesReference;
import org.elasticsearch.common.io.stream.BytesStreamOutput;
import org.elasticsearch.common.io.stream.StreamOutput;
import org.elasticsearch.common.lucene.BytesRefs;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

public class Id {

    private final List<BytesRef> values;
    private String routingValue;


    private String singleStringValue = null;

    public Id(List<ColumnIdent> primaryKeys, List<BytesRef> primaryKeyValues,
              ColumnIdent clusteredBy) {
        this(primaryKeys, primaryKeyValues, clusteredBy, true);
    }

    public Id(List<ColumnIdent> primaryKeys, List<BytesRef> primaryKeyValues,
              ColumnIdent clusteredBy, boolean create) {
        values = new ArrayList<>(primaryKeys.size());
        if (primaryKeys.size() == 1 && primaryKeys.get(0).name().equals("_id") && create) {
            singleStringValue = Strings.base64UUID();
        } else {
            singleStringValue = null;
            if (primaryKeys.size() != primaryKeyValues.size()) {

                if (create) {
                    throw new UnsupportedOperationException("Missing required primary key values");
                }
                return;
            }
            for (int i=0; i<primaryKeys.size(); i++)  {
                BytesRef primaryKeyValue = primaryKeyValues.get(i);
                if (primaryKeyValue == null) {

                    return;
                }
                if (primaryKeys.get(i).equals(clusteredBy)) {

                    values.add(0, primaryKeyValue);
                    routingValue = primaryKeyValue.utf8ToString();
                } else {
                    values.add(primaryKeyValue);
                }
            }
        }

    }

    public String routingValue(){
        return MoreObjects.firstNonNull(routingValue, stringValue());
    }

    public boolean isValid() {
        return values.size() > 0 || singleStringValue != null;
    }

    private void encodeValues(StreamOutput out) throws IOException {
        out.writeVInt(values.size());
        for (BytesRef value : values) {
            out.writeBytesRef(value);
        }
    }

    private BytesReference bytes() {
        assert values.size() > 0;
        BytesStreamOutput out = new BytesStreamOutput(estimateSize(values));
        try {
            encodeValues(out);
            out.close();
        } catch (IOException e) {

        }
        return out.bytes();
    }


    private int estimateSize(List<BytesRef> values) {
        int expectedEncodedSize = 0;
        for (BytesRef value : values) {

            expectedEncodedSize += 5 + (value != null ? value.length : 0);
        }
        return expectedEncodedSize;
    }

    @Nullable
    public String stringValue() {
        if (singleStringValue != null) {
            return singleStringValue;
        }
        if (values.size() == 0) {
            return null;
        } else if (values.size() == 1) {
            return BytesRefs.toString(values.get(0));
        }
        return Base64.encodeBytes(bytes().toBytes());
    }

    @Nullable
    public String toString() {
        return stringValue();
    }

    public List<BytesRef> values() {
        if (singleStringValue != null && values.size() == 0) {

            values.add(new BytesRef(singleStringValue));
        }
        return values;
    }
}

<code block>


package io.crate.analyze;

import com.google.common.base.Preconditions;
import io.crate.Constants;
import io.crate.exceptions.InvalidColumnNameException;
import io.crate.metadata.ColumnIdent;
import io.crate.metadata.ReferenceIdent;
import io.crate.metadata.ReferenceInfo;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.symbol.Reference;
import io.crate.sql.tree.DefaultTraversalVisitor;
import io.crate.sql.tree.Insert;
import io.crate.sql.tree.Node;

import java.util.ArrayList;

public abstract class AbstractInsertAnalyzer extends DefaultTraversalVisitor<AbstractInsertAnalyzedStatement, Analysis> {

    protected final AnalysisMetaData analysisMetaData;

    protected AbstractInsertAnalyzer(AnalysisMetaData analysisMetaData) {
        this.analysisMetaData = analysisMetaData;
    }

    protected void handleInsertColumns(Insert node, int maxInsertValues, AbstractInsertAnalyzedStatement context) {

        int numColumns;

        if (node.columns().size() == 0) { 
            numColumns = context.tableInfo().columns().size();
            if (maxInsertValues > numColumns) {
                throw new IllegalArgumentException("too many values");
            }
            context.columns(new ArrayList<Reference>(numColumns));

            int i = 0;
            for (ReferenceInfo columnInfo : context.tableInfo().columns()) {
                if (i >= maxInsertValues) {
                    break;
                }
                addColumn(columnInfo.ident().columnIdent().name(), context, i);
                i++;
            }

        } else {
            numColumns = node.columns().size();
            context.columns(new ArrayList<Reference>(numColumns));
            if (maxInsertValues > node.columns().size()) {
                throw new IllegalArgumentException("too few values");
            }
            for (int i = 0; i < node.columns().size(); i++) {
                addColumn(node.columns().get(i), context, i);
            }
        }

        if (!context.tableInfo().hasAutoGeneratedPrimaryKey() && context.primaryKeyColumnIndices().size() == 0) {
            throw new IllegalArgumentException("Primary key is required but is missing from the insert statement");
        }
        ColumnIdent clusteredBy = context.tableInfo().clusteredBy();
        if (clusteredBy != null && !clusteredBy.name().equalsIgnoreCase("_id") && context.routingColumnIndex() < 0) {
            throw new IllegalArgumentException("Clustered by value is required but is missing from the insert statement");
        }
    }


    protected Reference addColumn(String column, AbstractInsertAnalyzedStatement context, int i) {
        assert context.tableInfo() != null;
        return addColumn(new ReferenceIdent(context.tableInfo().ident(), column), context, i);
    }


    protected Reference addColumn(ReferenceIdent ident, AbstractInsertAnalyzedStatement context, int i) {
        final ColumnIdent columnIdent = ident.columnIdent();
        Preconditions.checkArgument(!columnIdent.name().startsWith("_"), "Inserting system columns is not allowed");
        if (Constants.INVALID_COLUMN_NAME_PREDICATE.apply(columnIdent.name())) {
            throw new InvalidColumnNameException(columnIdent.name());
        }


        for (ColumnIdent pkIdent : context.tableInfo().primaryKey()) {
            if (pkIdent.getRoot().equals(columnIdent)) {
                context.addPrimaryKeyColumnIdx(i);
            }
        }


        for (ColumnIdent partitionIdent : context.tableInfo().partitionedBy()) {
            if (partitionIdent.getRoot().equals(columnIdent)) {
                context.addPartitionedByIndex(i);
            }
        }


        ColumnIdent routing = context.tableInfo().clusteredBy();
        if (routing != null && (routing.equals(columnIdent) || routing.isChildOf(columnIdent))) {
            context.routingColumnIndex(i);
        }


        Reference columnReference = context.allocateUniqueReference(ident);
        context.columns().add(columnReference);
        return columnReference;
    }

    public AnalyzedStatement analyze(Node node, Analysis analysis) {
        analysis.expectsAffectedRows(true);
        return super.process(node, analysis);
    }
}

<code block>


package io.crate.metadata.doc;

import com.google.common.base.MoreObjects;
import com.google.common.collect.*;
import io.crate.Constants;
import io.crate.analyze.TableParameter;
import io.crate.analyze.TableParameterInfo;
import io.crate.core.NumberOfReplicas;
import io.crate.exceptions.TableAliasSchemaException;
import io.crate.metadata.*;
import io.crate.metadata.settings.CrateTableSettings;
import io.crate.metadata.table.ColumnPolicy;
import io.crate.planner.RowGranularity;
import io.crate.types.ArrayType;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.action.admin.indices.alias.Alias;
import org.elasticsearch.action.admin.indices.template.put.PutIndexTemplateRequest;
import org.elasticsearch.action.admin.indices.template.put.TransportPutIndexTemplateAction;
import org.elasticsearch.cluster.metadata.IndexMetaData;
import org.elasticsearch.cluster.metadata.MappingMetaData;
import org.elasticsearch.common.Booleans;
import org.elasticsearch.common.collect.Tuple;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.common.xcontent.XContentHelper;

import java.io.IOException;
import java.util.*;

public class DocIndexMetaData {

    private static final String ID = "_id";
    public static final ColumnIdent ID_IDENT = new ColumnIdent(ID);
    private final IndexMetaData metaData;

    private final MappingMetaData defaultMappingMetaData;
    private final Map<String, Object> defaultMappingMap;

    private final Map<ColumnIdent, IndexReferenceInfo.Builder> indicesBuilder = new HashMap<>();

    private final ImmutableSortedSet.Builder<ReferenceInfo> columnsBuilder = ImmutableSortedSet.orderedBy(new Comparator<ReferenceInfo>() {
        @Override
        public int compare(ReferenceInfo o1, ReferenceInfo o2) {
            return o1.ident().columnIdent().fqn().compareTo(o2.ident().columnIdent().fqn());
        }
    });


    private final ImmutableMap.Builder<ColumnIdent, ReferenceInfo> referencesBuilder = ImmutableSortedMap.naturalOrder();
    private final ImmutableList.Builder<ReferenceInfo> partitionedByColumnsBuilder = ImmutableList.builder();

    private final TableIdent ident;
    private final int numberOfShards;
    private final BytesRef numberOfReplicas;
    private final ImmutableMap<String, Object> tableParameters;
    private Map<String, Object> metaMap;
    private Map<String, Object> metaColumnsMap;
    private Map<String, Object> indicesMap;
    private List<List<String>> partitionedByList;
    private ImmutableList<ReferenceInfo> columns;
    private ImmutableMap<ColumnIdent, IndexReferenceInfo> indices;
    private ImmutableList<ReferenceInfo> partitionedByColumns;
    private ImmutableMap<ColumnIdent, ReferenceInfo> references;
    private ImmutableList<ColumnIdent> primaryKey;
    private ColumnIdent routingCol;
    private ImmutableList<ColumnIdent> partitionedBy;
    private final boolean isAlias;
    private final Set<String> aliases;
    private boolean hasAutoGeneratedPrimaryKey = false;

    private ColumnPolicy columnPolicy = ColumnPolicy.DYNAMIC;

    private final static ImmutableMap<String, DataType> dataTypeMap = ImmutableMap.<String, DataType>builder()
            .put("date", DataTypes.TIMESTAMP)
            .put("string", DataTypes.STRING)
            .put("boolean", DataTypes.BOOLEAN)
            .put("byte", DataTypes.BYTE)
            .put("short", DataTypes.SHORT)
            .put("integer", DataTypes.INTEGER)
            .put("long", DataTypes.LONG)
            .put("float", DataTypes.FLOAT)
            .put("double", DataTypes.DOUBLE)
            .put("ip", DataTypes.IP)
            .put("geo_point", DataTypes.GEO_POINT)
            .put("object", DataTypes.OBJECT)
            .put("nested", DataTypes.OBJECT).build();

    public DocIndexMetaData(IndexMetaData metaData, TableIdent ident) throws IOException {
        this.ident = ident;
        this.metaData = metaData;
        this.isAlias = !metaData.getIndex().equals(ident.esName());
        this.numberOfShards = metaData.numberOfShards();
        final Settings settings = metaData.getSettings();
        this.numberOfReplicas = NumberOfReplicas.fromSettings(settings);
        this.aliases = ImmutableSet.copyOf(metaData.aliases().keys().toArray(String.class));
        this.defaultMappingMetaData = this.metaData.mappingOrDefault(Constants.DEFAULT_MAPPING_TYPE);
        if (defaultMappingMetaData == null) {
            this.defaultMappingMap = new HashMap<>();
        } else {
            this.defaultMappingMap = this.defaultMappingMetaData.sourceAsMap();
        }
        this.tableParameters = TableParameterInfo.tableParametersFromIndexMetaData(metaData);

        prepareCrateMeta();
    }

    @SuppressWarnings("unchecked")
    private static <T> T getNested(Map map, String key) {
        return (T) map.get(key);
    }

    private void prepareCrateMeta() {
        metaMap = getNested(defaultMappingMap, "_meta");
        if (metaMap != null) {
            indicesMap = getNested(metaMap, "indices");
            if (indicesMap == null) {
                indicesMap = ImmutableMap.of();
            }
            metaColumnsMap = getNested(metaMap, "columns");
            if (metaColumnsMap == null) {
                metaColumnsMap = ImmutableMap.of();
            }

            partitionedByList = getNested(metaMap, "partitioned_by");
            if (partitionedByList == null) {
                partitionedByList = ImmutableList.of();
            }
        } else {
            metaMap = new HashMap<>();
            indicesMap = new HashMap<>();
            metaColumnsMap = new HashMap<>();
            partitionedByList = ImmutableList.of();
        }
    }

    private void addPartitioned(ColumnIdent column, DataType type) {
        add(column, type, ColumnPolicy.DYNAMIC, ReferenceInfo.IndexType.NOT_ANALYZED, true);
    }

    private void add(ColumnIdent column, DataType type, ReferenceInfo.IndexType indexType) {
        add(column, type, ColumnPolicy.DYNAMIC, indexType, false);
    }

    private void add(ColumnIdent column, DataType type, ColumnPolicy columnPolicy,
                     ReferenceInfo.IndexType indexType, boolean partitioned) {
        ReferenceInfo info = newInfo(column, type, columnPolicy, indexType);

        if (partitioned || !(partitionedBy != null && partitionedBy.contains(column))) {
            if (info.ident().isColumn()) {
                columnsBuilder.add(info);
            }
            referencesBuilder.put(info.ident().columnIdent(), info);
        }
        if (partitioned) {
            partitionedByColumnsBuilder.add(info);
        }
    }

    private ReferenceInfo newInfo(ColumnIdent column,
                                  DataType type,
                                  ColumnPolicy columnPolicy,
                                  ReferenceInfo.IndexType indexType) {
        RowGranularity granularity = RowGranularity.DOC;
        if (partitionedBy.contains(column)) {
            granularity = RowGranularity.PARTITION;
        }
        return new ReferenceInfo(new ReferenceIdent(ident, column), granularity, type,
                columnPolicy, indexType);
    }


    public static DataType getColumnDataType(Map<String, Object> columnProperties) {
        DataType type;
        String typeName = (String) columnProperties.get("type");

        if (typeName == null) {
            if (columnProperties.containsKey("properties")) {
                type = DataTypes.OBJECT;
            } else {
                return DataTypes.NOT_SUPPORTED;
            }
        } else if (typeName.equalsIgnoreCase("array")) {

            Map<String, Object> innerProperties = getNested(columnProperties, "inner");
            DataType innerType = getColumnDataType(innerProperties);
            type = new ArrayType(innerType);
        } else {
            typeName = typeName.toLowerCase(Locale.ENGLISH);
            type = MoreObjects.firstNonNull(dataTypeMap.get(typeName), DataTypes.NOT_SUPPORTED);
        }
        return type;
    }

    private ReferenceInfo.IndexType getColumnIndexType(Map<String, Object> columnProperties) {
        String indexType = (String) columnProperties.get("index");
        String analyzerName = (String) columnProperties.get("analyzer");
        if (indexType != null) {
            if (indexType.equals(ReferenceInfo.IndexType.NOT_ANALYZED.toString())) {
                return ReferenceInfo.IndexType.NOT_ANALYZED;
            } else if (indexType.equals(ReferenceInfo.IndexType.NO.toString())) {
                return ReferenceInfo.IndexType.NO;
            } else if (indexType.equals(ReferenceInfo.IndexType.ANALYZED.toString())
                    && analyzerName != null && !analyzerName.equals("keyword")) {
                return ReferenceInfo.IndexType.ANALYZED;
            }
        } 
        else if (analyzerName != null && !analyzerName.equals("keyword")) {
            return ReferenceInfo.IndexType.ANALYZED;
        }
        return ReferenceInfo.IndexType.NOT_ANALYZED;
    }

    private ColumnIdent childIdent(ColumnIdent ident, String name) {
        if (ident == null) {
            return new ColumnIdent(name);
        }
        if (ident.isColumn()) {
            return new ColumnIdent(ident.name(), name);
        } else {
            ImmutableList.Builder<String> builder = ImmutableList.builder();
            for (String s : ident.path()) {
                builder.add(s);
            }
            builder.add(name);
            return new ColumnIdent(ident.name(), builder.build());
        }
    }


    @SuppressWarnings("unchecked")
    private void internalExtractColumnDefinitions(ColumnIdent columnIdent,
                                                  Map<String, Object> propertiesMap) {
        if (propertiesMap == null) {
            return;
        }

        for (Map.Entry<String, Object> columnEntry : propertiesMap.entrySet()) {
            Map<String, Object> columnProperties = (Map) columnEntry.getValue();
            DataType columnDataType = getColumnDataType(columnProperties);
            ColumnIdent newIdent = childIdent(columnIdent, columnEntry.getKey());

            columnProperties = furtherColumnProperties(columnProperties);
            ReferenceInfo.IndexType columnIndexType = getColumnIndexType(columnProperties);
            if (columnDataType == DataTypes.OBJECT
                    || (columnDataType.id() == ArrayType.ID
                    && ((ArrayType) columnDataType).innerType() == DataTypes.OBJECT)) {
                ColumnPolicy columnPolicy =
                        ColumnPolicy.of(columnProperties.get("dynamic"));
                add(newIdent, columnDataType, columnPolicy, ReferenceInfo.IndexType.NO, false);

                if (columnProperties.get("properties") != null) {

                    internalExtractColumnDefinitions(newIdent, (Map<String, Object>) columnProperties.get("properties"));
                }
            } else if (columnDataType != DataTypes.NOT_SUPPORTED) {
                List<String> copyToColumns = getNested(columnProperties, "copy_to");


                if (copyToColumns != null) {
                    for (String copyToColumn : copyToColumns) {
                        ColumnIdent targetIdent = ColumnIdent.fromPath(copyToColumn);
                        IndexReferenceInfo.Builder builder = getOrCreateIndexBuilder(targetIdent);
                        builder.addColumn(newInfo(newIdent, columnDataType, ColumnPolicy.DYNAMIC, columnIndexType));
                    }
                }

                if (indicesMap.containsKey(newIdent.fqn())) {
                    IndexReferenceInfo.Builder builder = getOrCreateIndexBuilder(newIdent);
                    builder.indexType(columnIndexType)
                            .ident(new ReferenceIdent(ident, newIdent));
                } else {
                    add(newIdent, columnDataType, columnIndexType);
                }
            }
        }
    }


    private Map<String, Object> furtherColumnProperties(Map<String, Object> columnProperties) {
        if (columnProperties.get("inner") != null) {
            return (Map<String, Object>) columnProperties.get("inner");
        } else {
            return columnProperties;
        }
    }

    private IndexReferenceInfo.Builder getOrCreateIndexBuilder(ColumnIdent ident) {
        IndexReferenceInfo.Builder builder = indicesBuilder.get(ident);
        if (builder == null) {
            builder = new IndexReferenceInfo.Builder();
            indicesBuilder.put(ident, builder);
        }
        return builder;
    }

    private ImmutableList<ColumnIdent> getPrimaryKey() {
        Map<String, Object> metaMap = getNested(defaultMappingMap, "_meta");
        if (metaMap == null) {
            hasAutoGeneratedPrimaryKey = true;
            return ImmutableList.of(ID_IDENT);
        }

        ImmutableList.Builder<ColumnIdent> builder = ImmutableList.builder();
        Object pKeys = metaMap.get("primary_keys");
        if (pKeys == null) {
            hasAutoGeneratedPrimaryKey = true;
            return ImmutableList.of(ID_IDENT);
        }

        if (pKeys instanceof String) {
            builder.add(ColumnIdent.fromPath((String) pKeys));
        } else if (pKeys instanceof Collection) {
            Collection keys = (Collection) pKeys;
            if (keys.isEmpty()) {
                hasAutoGeneratedPrimaryKey = true;
                return ImmutableList.of(ID_IDENT);
            }
            for (Object pkey : keys) {
                builder.add(ColumnIdent.fromPath(pkey.toString()));
            }
        }
        return builder.build();
    }

    private ImmutableList<ColumnIdent> getPartitionedBy() {
        ImmutableList.Builder<ColumnIdent> builder = ImmutableList.builder();
        for (List<String> partitionedByInfo : partitionedByList) {
            builder.add(ColumnIdent.fromPath(partitionedByInfo.get(0)));
        }
        return builder.build();
    }

    private ColumnPolicy getColumnPolicy() {
        Object dynamic = getNested(defaultMappingMap, "dynamic");
        if (ColumnPolicy.STRICT.value().equals(String.valueOf(dynamic).toLowerCase(Locale.ENGLISH))) {
            return ColumnPolicy.STRICT;
        } else if (Booleans.isExplicitFalse(String.valueOf(dynamic))) {
            return ColumnPolicy.IGNORED;
        } else {
            return ColumnPolicy.DYNAMIC;
        }
    }

    private void createColumnDefinitions() {
        Map<String, Object> propertiesMap = getNested(defaultMappingMap, "properties");
        internalExtractColumnDefinitions(null, propertiesMap);
        extractPartitionedByColumns();
    }

    private ImmutableMap<ColumnIdent, IndexReferenceInfo> createIndexDefinitions() {
        ImmutableMap.Builder<ColumnIdent, IndexReferenceInfo> builder = ImmutableMap.builder();
        for (Map.Entry<ColumnIdent, IndexReferenceInfo.Builder> entry : indicesBuilder.entrySet()) {
            builder.put(entry.getKey(), entry.getValue().build());
        }
        indices = builder.build();
        return indices;
    }

    private void extractPartitionedByColumns() {
        for (Tuple<ColumnIdent, DataType> partitioned : PartitionedByMappingExtractor.extractPartitionedByColumns(partitionedByList)) {
            addPartitioned(partitioned.v1(), partitioned.v2());
        }
    }

    private ColumnIdent getRoutingCol() {
        if (defaultMappingMetaData != null) {
            Map<String, Object> metaMap = getNested(defaultMappingMap, "_meta");
            if (metaMap != null) {
                String routingPath = (String) metaMap.get("routing");
                if (routingPath != null) {
                    return ColumnIdent.fromPath(routingPath);
                }
            }
        }
        if (primaryKey.size() == 1) {
            return primaryKey.get(0);
        }
        return ID_IDENT;
    }

    public DocIndexMetaData build() {
        partitionedBy = getPartitionedBy();
        columnPolicy = getColumnPolicy();
        createColumnDefinitions();
        indices = createIndexDefinitions();
        columns = ImmutableList.copyOf(columnsBuilder.build());
        partitionedByColumns = partitionedByColumnsBuilder.build();

        for (Tuple<ColumnIdent, ReferenceInfo> sysColumn : DocSysColumns.forTable(ident)) {
            referencesBuilder.put(sysColumn.v1(), sysColumn.v2());
        }
        references = referencesBuilder.build();
        primaryKey = getPrimaryKey();
        routingCol = getRoutingCol();
        return this;
    }

    public ImmutableMap<ColumnIdent, ReferenceInfo> references() {
        return references;
    }

    public ImmutableList<ReferenceInfo> columns() {
        return columns;
    }

    public ImmutableMap<ColumnIdent, IndexReferenceInfo> indices() {
        return indices;
    }

    public ImmutableList<ReferenceInfo> partitionedByColumns() {
        return partitionedByColumns;
    }

    public ImmutableList<ColumnIdent> primaryKey() {
        return primaryKey;
    }

    public ColumnIdent routingCol() {
        return routingCol;
    }


    public boolean schemaEquals(DocIndexMetaData other) {
        if (this == other) return true;
        if (other == null) return false;



        if (columns != null ? !columns.equals(other.columns) : other.columns != null) return false;
        if (primaryKey != null ? !primaryKey.equals(other.primaryKey) : other.primaryKey != null) return false;
        if (indices != null ? !indices.equals(other.indices) : other.indices != null) return false;
        if (references != null ? !references.equals(other.references) : other.references != null) return false;
        if (routingCol != null ? !routingCol.equals(other.routingCol) : other.routingCol != null) return false;

        return true;
    }

    protected DocIndexMetaData merge(DocIndexMetaData other,
                                     TransportPutIndexTemplateAction transportPutIndexTemplateAction,
                                     boolean thisIsCreatedFromTemplate) throws IOException {
        if (schemaEquals(other)) {
            return this;
        } else if (thisIsCreatedFromTemplate) {
            if (this.references.size() < other.references.size()) {



                updateTemplate(other, transportPutIndexTemplateAction, this.metaData.settings());

                return new DocIndexMetaData(IndexMetaData.builder(other.metaData).settings(this.metaData.settings()).build(), other.ident).build();
            } else if (references().size() == other.references().size() &&
                    !references().keySet().equals(other.references().keySet())) {
                XContentHelper.update(defaultMappingMap, other.defaultMappingMap, false);

                updateTemplate(this, transportPutIndexTemplateAction, this.metaData.settings());
                return this;
            }

            return this;
        } else {
            throw new TableAliasSchemaException(other.ident.name());
        }
    }

    private void updateTemplate(DocIndexMetaData md,
                                TransportPutIndexTemplateAction transportPutIndexTemplateAction,
                                Settings updateSettings) {
        String templateName = PartitionName.templateName(ident.schema(), ident.name());
        PutIndexTemplateRequest request = new PutIndexTemplateRequest(templateName)
                .mapping(Constants.DEFAULT_MAPPING_TYPE, md.defaultMappingMap)
                .create(false)
                .settings(updateSettings)
                .template(templateName + "*");
        for (String alias : md.aliases()) {
            request = request.alias(new Alias(alias));
        }
        transportPutIndexTemplateAction.execute(request);
    }


    public String concreteIndexName() {
        return metaData.index();
    }

    public boolean isAlias() {
        return isAlias;
    }

    public Set<String> aliases() {
        return aliases;
    }

    public boolean hasAutoGeneratedPrimaryKey() {
        return hasAutoGeneratedPrimaryKey;
    }

    public int numberOfShards() {
        return numberOfShards;
    }

    public BytesRef numberOfReplicas() {
        return numberOfReplicas;
    }

    public ImmutableList<ColumnIdent> partitionedBy() {
        return partitionedBy;
    }

    public ColumnPolicy columnPolicy() {
        return columnPolicy;
    }

    public ImmutableMap<String, Object> tableParameters() {
        return tableParameters;
    }
}

<code block>
package io.crate.metadata.doc;

import com.google.common.collect.ImmutableMap;
import io.crate.metadata.ColumnIdent;
import io.crate.metadata.ReferenceIdent;
import io.crate.metadata.ReferenceInfo;
import io.crate.metadata.TableIdent;
import io.crate.planner.RowGranularity;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.elasticsearch.common.collect.Tuple;

import java.util.ArrayList;
import java.util.List;
import java.util.Map;

public class DocSysColumns {


    public static final ColumnIdent ID = new ColumnIdent("_id");
    public static final ColumnIdent VERSION = new ColumnIdent("_version");
    public static final ColumnIdent SCORE = new ColumnIdent("_score");
    public static final ColumnIdent UID = new ColumnIdent("_uid");
    public static final ColumnIdent DOC = new ColumnIdent("_doc");
    public static final ColumnIdent RAW = new ColumnIdent("_raw");
    public static final ColumnIdent DOCID = new ColumnIdent("_docid");

    public static final ImmutableMap<ColumnIdent, DataType> COLUMN_IDENTS = ImmutableMap.<ColumnIdent, DataType>builder()
            .put(ID, DataTypes.STRING)
            .put(VERSION, DataTypes.LONG)
            .put(SCORE, DataTypes.FLOAT)
            .put(UID, DataTypes.STRING)
            .put(DOC, DataTypes.OBJECT)
            .put(RAW, DataTypes.STRING)
            .put(DOCID, DataTypes.LONG)
            .build();

    private static ReferenceInfo newInfo(TableIdent table, ColumnIdent column, DataType dataType) {
        return new ReferenceInfo(new ReferenceIdent(table, column), RowGranularity.DOC, dataType);
    }

    public static List<Tuple<ColumnIdent, ReferenceInfo>> forTable(TableIdent tableIdent) {
        List<Tuple<ColumnIdent, ReferenceInfo>> columns = new ArrayList<>(COLUMN_IDENTS.size());

        for (Map.Entry<ColumnIdent, DataType> entry : COLUMN_IDENTS.entrySet()) {
            columns.add(new Tuple<>(entry.getKey(), newInfo(tableIdent, entry.getKey(), entry.getValue())));
        }

        return columns;
    }
}

<code block>


package io.crate.lucene;

import com.google.common.base.Preconditions;
import com.google.common.base.Predicates;
import com.google.common.base.Throwables;
import com.google.common.collect.ImmutableMap;
import com.google.common.collect.Iterables;
import com.spatial4j.core.context.jts.JtsSpatialContext;
import com.spatial4j.core.shape.Rectangle;
import com.spatial4j.core.shape.Shape;
import com.vividsolutions.jts.geom.Coordinate;
import com.vividsolutions.jts.geom.Geometry;
import io.crate.analyze.WhereClause;
import io.crate.exceptions.UnsupportedFeatureException;
import io.crate.lucene.match.MatchQueryBuilder;
import io.crate.lucene.match.MultiMatchQueryBuilder;
import io.crate.metadata.DocReferenceConverter;
import io.crate.metadata.Functions;
import io.crate.operation.Input;
import io.crate.operation.collect.CollectInputSymbolVisitor;
import io.crate.operation.collect.LuceneDocCollector;
import io.crate.operation.operator.*;
import io.crate.operation.operator.any.*;
import io.crate.operation.predicate.IsNullPredicate;
import io.crate.operation.predicate.MatchPredicate;
import io.crate.operation.predicate.NotPredicate;
import io.crate.operation.reference.doc.lucene.CollectorContext;
import io.crate.operation.reference.doc.lucene.LuceneCollectorExpression;
import io.crate.operation.reference.doc.lucene.LuceneDocLevelReferenceResolver;
import io.crate.operation.scalar.geo.DistanceFunction;
import io.crate.operation.scalar.geo.WithinFunction;
import io.crate.planner.symbol.*;
import io.crate.types.CollectionType;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.apache.lucene.index.AtomicReader;
import org.apache.lucene.index.AtomicReaderContext;
import org.apache.lucene.index.Term;
import org.apache.lucene.queries.BooleanFilter;
import org.apache.lucene.queries.TermsFilter;
import org.apache.lucene.sandbox.queries.regex.JavaUtilRegexCapabilities;
import org.apache.lucene.sandbox.queries.regex.RegexQuery;
import org.apache.lucene.search.*;
import org.apache.lucene.util.Bits;
import org.apache.lucene.util.BytesRef;
import org.apache.lucene.util.automaton.RegExp;
import org.elasticsearch.ExceptionsHelper;
import org.elasticsearch.common.Nullable;
import org.elasticsearch.common.collect.Tuple;
import org.elasticsearch.common.geo.GeoDistance;
import org.elasticsearch.common.geo.GeoPoint;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.inject.Singleton;
import org.elasticsearch.common.lucene.BytesRefs;
import org.elasticsearch.common.lucene.docset.MatchDocIdSet;
import org.elasticsearch.common.lucene.search.Queries;
import org.elasticsearch.index.cache.IndexCache;
import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;
import org.elasticsearch.index.mapper.FieldMapper;
import org.elasticsearch.index.mapper.MapperService;
import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
import org.elasticsearch.index.query.RegexpFlag;
import org.elasticsearch.index.search.geo.GeoDistanceRangeFilter;
import org.elasticsearch.index.search.geo.GeoPolygonFilter;
import org.elasticsearch.index.search.geo.InMemoryGeoBoundingBoxFilter;
import org.elasticsearch.search.internal.SearchContext;

import java.io.IOException;
import java.util.*;

import static com.google.common.base.Preconditions.checkArgument;
import static io.crate.operation.scalar.regex.RegexMatcher.isPcrePattern;

@Singleton
public class LuceneQueryBuilder {

    private final static Visitor VISITOR = new Visitor();
    private final CollectInputSymbolVisitor<LuceneCollectorExpression<?>> inputSymbolVisitor;

    @Inject
    public LuceneQueryBuilder(Functions functions) {
        inputSymbolVisitor = new CollectInputSymbolVisitor<>(functions, new LuceneDocLevelReferenceResolver(null));
    }

    public Context convert(WhereClause whereClause, SearchContext searchContext, IndexCache indexCache) throws UnsupportedFeatureException {
        Context ctx = new Context(inputSymbolVisitor, searchContext, indexCache);
        if (whereClause.noMatch()) {
            ctx.query = Queries.newMatchNoDocsQuery();
        } else if (!whereClause.hasQuery()) {
            ctx.query = Queries.newMatchAllQuery();
        } else {
            ctx.query = VISITOR.process(whereClause.query(), ctx);
        }
        return ctx;
    }

    public static class Context {
        Query query;

        final Map<String, Object> filteredFieldValues = new HashMap<>();

        final SearchContext searchContext;
        final CollectInputSymbolVisitor<LuceneCollectorExpression<?>> inputSymbolVisitor;
        final IndexCache indexCache;

        Context(CollectInputSymbolVisitor<LuceneCollectorExpression<?>> inputSymbolVisitor,
                SearchContext searchContext,
                IndexCache indexCache) {
            this.inputSymbolVisitor = inputSymbolVisitor;
            this.searchContext = searchContext;
            this.indexCache = indexCache;
        }

        public Query query() {
            return this.query;
        }

        @Nullable
        public Float minScore() {
            Object score = filteredFieldValues.get("_score");
            if (score == null) {
                return null;
            }
            return ((Number) score).floatValue();
        }

        @Nullable
        public String unsupportedMessage(String field){
            return UNSUPPORTED_FIELDS.get(field);
        }


        final static Set<String> FILTERED_FIELDS = new HashSet<String>(){{ add("_score"); }};


        final static Map<String, String> UNSUPPORTED_FIELDS = ImmutableMap.<String, String>builder()
                .put("_version", "\"_version\" column is not valid in the WHERE clause")
                .build();
    }

    public static String convertWildcardToRegex(String wildcardString) {



        wildcardString = wildcardString.replaceAll("(?<!\\\\)\\*", "\\\\*");
        wildcardString = wildcardString.replaceAll("(?<!\\\\)%", ".*");
        wildcardString = wildcardString.replaceAll("\\\\%", "%");

        wildcardString = wildcardString.replaceAll("(?<!\\\\)\\?", "\\\\?");
        wildcardString = wildcardString.replaceAll("(?<!\\\\)_", ".");
        return wildcardString.replaceAll("\\\\_", "_");
    }

    public static String convertWildcard(String wildcardString) {



        wildcardString = wildcardString.replaceAll("(?<!\\\\)\\*", "\\\\*");
        wildcardString = wildcardString.replaceAll("(?<!\\\\)%", "*");
        wildcardString = wildcardString.replaceAll("\\\\%", "%");

        wildcardString = wildcardString.replaceAll("(?<!\\\\)\\?", "\\\\?");
        wildcardString = wildcardString.replaceAll("(?<!\\\\)_", "?");
        return wildcardString.replaceAll("\\\\_", "_");
    }

    public static String negateWildcard(String wildCard) {
        return String.format(Locale.ENGLISH, "~(%s)", wildCard);
    }

    static class Visitor extends SymbolVisitor<Context, Query> {

        interface FunctionToQuery {

            @Nullable
            Query apply (Function input, Context context) throws IOException;
        }

        static abstract class CmpQuery implements FunctionToQuery {

            @Nullable
            protected Tuple<Reference, Literal> prepare(Function input) {
                assert input != null;
                assert input.arguments().size() == 2;

                Symbol left = input.arguments().get(0);
                Symbol right = input.arguments().get(1);

                if (!(left instanceof Reference) || !(right.symbolType().isValueSymbol())) {
                    return null;
                }
                assert right.symbolType() == SymbolType.LITERAL;
                return new Tuple<>((Reference)left, (Literal)right);
            }
        }

        static abstract class AbstractAnyQuery implements FunctionToQuery {

            @Override
            public Query apply(Function function, Context context) throws IOException {
                Symbol left = function.arguments().get(0);
                Symbol collectionSymbol = function.arguments().get(1);
                Preconditions.checkArgument(DataTypes.isCollectionType(collectionSymbol.valueType()),
                        "invalid argument for ANY expression");
                if (left.symbolType().isValueSymbol()) {

                    assert collectionSymbol.symbolType().isReference() : "no reference found in ANY expression";
                    return applyArrayReference((Reference)collectionSymbol, (Literal)left, context);
                } else if (collectionSymbol.symbolType().isValueSymbol()) {
                    assert left.symbolType().isReference() : "no reference found in ANY expression";
                    return applyArrayLiteral((Reference)left, (Literal)collectionSymbol, context);
                } else {

                    return null;
                }
            }


            public static Iterable<?> toIterable(Object value) {
                return Iterables.transform(AnyOperator.collectionValueToIterable(value), new com.google.common.base.Function<Object, Object>() {
                    @javax.annotation.Nullable
                    @Override
                    public Object apply(@javax.annotation.Nullable Object input) {
                        if (input != null && input instanceof String) {
                            input = new BytesRef((String)input);
                        }
                        return input;
                    }
                });
            }

            protected abstract Query applyArrayReference(Reference arrayReference, Literal literal, Context context) throws IOException;
            protected abstract Query applyArrayLiteral(Reference reference, Literal arrayLiteral, Context context) throws IOException;
        }

        static class AnyEqQuery extends AbstractAnyQuery {

            private TermsFilter termsFilter(String columnName, Literal arrayLiteral) {
                TermsFilter termsFilter;
                Object values = arrayLiteral.value();
                if (values instanceof Collection) {
                    termsFilter = new TermsFilter(
                            columnName,
                            getBytesRefs((Collection)values,
                                    TermBuilder.forType(arrayLiteral.valueType())));
                } else  {
                    termsFilter = new TermsFilter(
                            columnName,
                            getBytesRefs((Object[])values,
                                    TermBuilder.forType(arrayLiteral.valueType())));
                }
                return termsFilter;
            }

            @Override
            protected Query applyArrayReference(Reference arrayReference, Literal literal, Context context) throws IOException {
                QueryBuilderHelper builder = QueryBuilderHelper.forType(((CollectionType)arrayReference.valueType()).innerType());
                return builder.eq(arrayReference.ident().columnIdent().fqn(), literal.value());
            }

            @Override
            protected Query applyArrayLiteral(Reference reference, Literal arrayLiteral, Context context) throws IOException {
                String columnName = reference.ident().columnIdent().fqn();
                return new FilteredQuery(Queries.newMatchAllQuery(), termsFilter(columnName, arrayLiteral));
            }
        }

        static class AnyNeqQuery extends AbstractAnyQuery {

            @Override
            protected Query applyArrayReference(Reference arrayReference, Literal literal, Context context) throws IOException {

                String columnName = arrayReference.info().ident().columnIdent().fqn();
                Object value = literal.value();

                QueryBuilderHelper builder = QueryBuilderHelper.forType(arrayReference.valueType());
                BooleanQuery query = new BooleanQuery();
                query.setMinimumNumberShouldMatch(1);
                query.add(
                        builder.rangeQuery(columnName, value, null, false, false),
                        BooleanClause.Occur.SHOULD
                );
                query.add(
                        builder.rangeQuery(columnName, null, value, false, false),
                        BooleanClause.Occur.SHOULD
                );
                return query;
            }

            @Override
            protected Query applyArrayLiteral(Reference reference, Literal arrayLiteral, Context context) throws IOException {

                String columnName = reference.info().ident().columnIdent().fqn();
                QueryBuilderHelper builder = QueryBuilderHelper.forType(reference.valueType());
                BooleanFilter filter = new BooleanFilter();

                BooleanFilter notFilter = new BooleanFilter();
                for (Object value : toIterable(arrayLiteral.value())) {
                    notFilter.add(builder.eqFilter(columnName, value), BooleanClause.Occur.MUST);
                }
                filter.add(notFilter, BooleanClause.Occur.MUST_NOT);

                return new FilteredQuery(Queries.newMatchAllQuery(), filter);
            }
        }

        static class AnyNotLikeQuery extends AbstractAnyQuery {

            @Override
            protected Query applyArrayReference(Reference arrayReference, Literal literal, Context context) throws IOException {
                String notLike = negateWildcard(
                        convertWildcardToRegex(BytesRefs.toString(literal.value())));
                return new RegexpQuery(new Term(
                        arrayReference.info().ident().columnIdent().fqn(),
                        notLike),
                        RegexpFlag.COMPLEMENT.value()
                );
            }

            @Override
            protected Query applyArrayLiteral(Reference reference, Literal arrayLiteral, Context context) throws IOException {

                BooleanQuery query = new BooleanQuery();
                BooleanQuery notQuery = new BooleanQuery();

                String columnName = reference.ident().columnIdent().fqn();
                QueryBuilderHelper builder = QueryBuilderHelper.forType(reference.valueType());
                for (Object value : toIterable(arrayLiteral.value())) {
                    notQuery.add(builder.like(columnName, value), BooleanClause.Occur.MUST);
                }
                query.add(notQuery, BooleanClause.Occur.MUST_NOT);
                return query;
            }
        }

        static class AnyLikeQuery extends AbstractAnyQuery {

            private final LikeQuery likeQuery = new LikeQuery();

            @Override
            protected Query applyArrayReference(Reference arrayReference, Literal literal, Context context) throws IOException {
                return likeQuery.toQuery(arrayReference, literal.value());
            }

            @Override
            protected Query applyArrayLiteral(Reference reference, Literal arrayLiteral, Context context) throws IOException {

                BooleanQuery booleanQuery = new BooleanQuery();
                booleanQuery.setMinimumNumberShouldMatch(1);
                for (Object value : toIterable(arrayLiteral.value())) {
                    booleanQuery.add(likeQuery.toQuery(reference, value), BooleanClause.Occur.SHOULD);
                }
                return booleanQuery;
            }
        }

        static class LikeQuery extends CmpQuery {

            @Override
            public Query apply(Function input, Context context) {
                Tuple<Reference, Literal> tuple = prepare(input);
                if (tuple == null) {
                    return null;
                }
                return toQuery(tuple.v1(), tuple.v2().value());
            }

            public Query toQuery(Reference reference, Object value) {
                String columnName = reference.info().ident().columnIdent().fqn();
                QueryBuilderHelper builder = QueryBuilderHelper.forType(reference.valueType());
                return builder.like(columnName, value);
            }
        }

        static class InQuery extends CmpQuery {

            @Override
            public Query apply(Function input, Context context) throws IOException {
                Tuple<Reference, Literal> tuple = prepare(input);
                if (tuple == null) {
                    return null;
                }
                String field = tuple.v1().info().ident().columnIdent().fqn();
                Literal literal = tuple.v2();
                CollectionType dataType = ((CollectionType) literal.valueType());

                Set values = (Set) literal.value();
                DataType innerType = dataType.innerType();
                BytesRef[] terms = getBytesRefs(values, TermBuilder.forType(innerType));
                TermsFilter termsFilter = new TermsFilter(field, terms);
                return new FilteredQuery(Queries.newMatchAllQuery(), termsFilter);
            }
        }

        class NotQuery implements FunctionToQuery {

            @Override
            public Query apply(Function input, Context context) {
                assert input != null;
                assert input.arguments().size() == 1;
                BooleanQuery query = new BooleanQuery();

                query.add(process(input.arguments().get(0), context), BooleanClause.Occur.MUST_NOT);
                query.add(Queries.newMatchAllQuery(), BooleanClause.Occur.MUST);

                return query;
            }
        }

        static class IsNullQuery implements FunctionToQuery {

            @Override
            public Query apply(Function input, Context context) {
                assert input != null;
                assert input.arguments().size() == 1;
                Symbol arg = input.arguments().get(0);
                if (arg.symbolType() != SymbolType.REFERENCE) {
                    return null;
                }
                Reference reference = (Reference)arg;

                String columnName = reference.info().ident().columnIdent().fqn();
                QueryBuilderHelper builderHelper = QueryBuilderHelper.forType(reference.valueType());
                BooleanFilter boolFilter = new BooleanFilter();
                boolFilter.add(builderHelper.rangeFilter(columnName, null, null, true, true),
                        BooleanClause.Occur.MUST_NOT);
                return new FilteredQuery(Queries.newMatchAllQuery(), boolFilter);
            }
        }

        static class EqQuery extends CmpQuery {

            @Override
            public Query apply(Function input, Context context) {
                Tuple<Reference, Literal> tuple = super.prepare(input);
                if (tuple == null) {
                    return null;
                }
                Reference reference = tuple.v1();
                Literal literal = tuple.v2();
                String columnName = reference.info().ident().columnIdent().fqn();
                if (DataTypes.isCollectionType(reference.valueType()) && DataTypes.isCollectionType(literal.valueType())) {


                    BooleanFilter boolTermsFilter = new BooleanFilter();
                    DataType type = literal.valueType();
                    while (DataTypes.isCollectionType(type)) {
                        type = ((CollectionType) type).innerType();
                    }
                    QueryBuilderHelper builder = QueryBuilderHelper.forType(type);
                    Object value = literal.value();
                    buildTermsQuery(boolTermsFilter, value, columnName, builder);

                    if (boolTermsFilter.clauses().isEmpty()) {

                        return genericFunctionQuery(input, context.inputSymbolVisitor, context.searchContext);
                    }




                    BooleanFilter filterClauses = new BooleanFilter();
                    filterClauses.add(boolTermsFilter, BooleanClause.Occur.MUST);
                    filterClauses.add(
                            genericFunctionFilter(input, context.inputSymbolVisitor, context.searchContext),
                            BooleanClause.Occur.MUST);
                    return new FilteredQuery(Queries.newMatchAllQuery(), filterClauses);
                }
                QueryBuilderHelper builder = QueryBuilderHelper.forType(tuple.v1().valueType());
                return builder.eq(columnName, tuple.v2().value());
            }

            private void buildTermsQuery(BooleanFilter booleanFilter,
                                            Object value,
                                            String columnName,
                                            QueryBuilderHelper builder) {
                if (value == null) {
                    return;
                }
                if (value.getClass().isArray()) {
                    Object[] array = (Object[]) value;
                    for (Object o : array) {
                        buildTermsQuery(booleanFilter, o, columnName, builder);
                    }
                } else {
                    booleanFilter.add(builder.eqFilter(columnName, value), BooleanClause.Occur.MUST);
                }
            }
        }

        class AndQuery implements FunctionToQuery {
            @Override
            public Query apply(Function input, Context context) {
                assert input != null;
                BooleanQuery query = new BooleanQuery();
                for (Symbol symbol : input.arguments()) {
                    query.add(process(symbol, context), BooleanClause.Occur.MUST);
                }
                return query;
            }
        }

        class OrQuery implements FunctionToQuery {
            @Override
            public Query apply(Function input, Context context) {
                assert input != null;
                BooleanQuery query = new BooleanQuery();
                query.setMinimumNumberShouldMatch(1);
                for (Symbol symbol : input.arguments()) {
                    query.add(process(symbol, context), BooleanClause.Occur.SHOULD);
                }
                return query;
            }
        }

        static class AnyRangeQuery extends AbstractAnyQuery {

            private final RangeQuery rangeQuery;
            private final RangeQuery inverseRangeQuery;

            AnyRangeQuery(String comparison, String inverseComparison) {
                rangeQuery = new RangeQuery(comparison);
                inverseRangeQuery = new RangeQuery(inverseComparison);
            }

            @Override
            protected Query applyArrayReference(Reference arrayReference, Literal literal, Context context) throws IOException {

                return rangeQuery.toQuery(
                        arrayReference,
                        ((CollectionType)arrayReference.valueType()).innerType(),
                        literal.value()
                );
            }

            @Override
            protected Query applyArrayLiteral(Reference reference, Literal arrayLiteral, Context context) throws IOException {

                BooleanQuery booleanQuery = new BooleanQuery();
                booleanQuery.setMinimumNumberShouldMatch(1);
                for (Object value : toIterable(arrayLiteral.value())) {
                    booleanQuery.add(inverseRangeQuery.toQuery(reference, reference.valueType(), value), BooleanClause.Occur.SHOULD);
                }
                return booleanQuery;
            }
        }

        static class RangeQuery extends CmpQuery {

            private final boolean includeLower;
            private final boolean includeUpper;
            private final com.google.common.base.Function<Object, Tuple<?, ?>> boundsFunction;

            private static final com.google.common.base.Function<Object, Tuple<?, ?>> LOWER_BOUND = new com.google.common.base.Function<Object, Tuple<?,?>>() {
                @javax.annotation.Nullable
                @Override
                public Tuple<?, ?> apply(@Nullable Object input) {
                    return new Tuple<>(input, null);
                }
            };

            private static final com.google.common.base.Function<Object, Tuple<?, ?>> UPPER_BOUND = new com.google.common.base.Function<Object, Tuple<?,?>>() {
                @Override
                public Tuple<?, ?> apply(Object input) {
                    return new Tuple<>(null, input);
                }
            };

            public RangeQuery(String comparison) {
                switch (comparison) {
                    case "lt":
                        boundsFunction = UPPER_BOUND;
                        includeLower = false;
                        includeUpper = false;
                        break;
                    case "gt":
                        boundsFunction = LOWER_BOUND;
                        includeLower = false;
                        includeUpper = false;
                        break;
                    case "lte":
                        boundsFunction = UPPER_BOUND;
                        includeLower = false;
                        includeUpper = true;
                        break;
                    case "gte":
                        boundsFunction = LOWER_BOUND;
                        includeLower = true;
                        includeUpper = false;
                        break;
                    default:
                        throw new IllegalArgumentException("invalid comparison");
                }
            }

            @Override
            public Query apply(Function input, Context context) throws IOException {
                Tuple<Reference, Literal> tuple = super.prepare(input);
                if (tuple == null) {
                    return null;
                }
                return toQuery(tuple.v1(), tuple.v1().valueType(), tuple.v2().value());
            }

            public Query toQuery(Reference reference, DataType type, Object value) {
                String columnName = reference.info().ident().columnIdent().fqn();
                QueryBuilderHelper builder = QueryBuilderHelper.forType(type);
                Tuple<?, ?> bounds = boundsFunction.apply(value);
                assert bounds != null;
                return builder.rangeQuery(columnName, bounds.v1(), bounds.v2(), includeLower, includeUpper);
            }
        }

        static class ToMatchQuery implements FunctionToQuery {

            @Override
            public Query apply(Function input, Context context) throws IOException {
                List<Symbol> arguments = input.arguments();
                assert arguments.size() == 4 : "invalid number of arguments";
                assert Symbol.isLiteral(arguments.get(0), DataTypes.OBJECT);
                assert Symbol.isLiteral(arguments.get(1), DataTypes.STRING);
                assert Symbol.isLiteral(arguments.get(2), DataTypes.STRING);
                assert Symbol.isLiteral(arguments.get(3), DataTypes.OBJECT);

                @SuppressWarnings("unchecked")
                Map<String, Object> fields = (Map) ((Literal) arguments.get(0)).value();
                BytesRef queryString = (BytesRef) ((Literal) arguments.get(1)).value();
                BytesRef matchType = (BytesRef) ((Literal) arguments.get(2)).value();
                Map options = (Map) ((Literal) arguments.get(3)).value();

                checkArgument(queryString != null, "cannot use NULL as query term in match predicate");

                MatchQueryBuilder queryBuilder;
                if (fields.size() == 1) {
                    queryBuilder = new MatchQueryBuilder(context.searchContext, context.indexCache, matchType, options);
                } else {
                    queryBuilder = new MultiMatchQueryBuilder(context.searchContext, context.indexCache, matchType, options);
                }
                return queryBuilder.query(fields, queryString);
            }
        }

        static class RegexpMatchQuery extends CmpQuery {

            @Override
            public Query apply(Function input, Context context) throws IOException {
                Tuple<Reference, Literal> prepare = prepare(input);
                if (prepare == null) { return null; }
                String fieldName = prepare.v1().info().ident().columnIdent().fqn();
                Object value = prepare.v2().value();


                if (value instanceof String) {
                    if (isPcrePattern(value)) {
                        return new RegexQuery(new Term(fieldName, (String) value));
                    } else {
                        return new RegexpQuery(new Term(fieldName, (String) value), RegExp.ALL);
                    }
                }

                if (value instanceof BytesRef) {
                    if (isPcrePattern(value)) {
                        return new RegexQuery(new Term(fieldName, (BytesRef) value));
                    } else {
                        return new RegexpQuery(new Term(fieldName, (BytesRef) value), RegExp.ALL);
                    }
                }

                throw new IllegalArgumentException("Can only use ~ with patterns of type string");
            }
        }

        static class RegexMatchQueryCaseInsensitive extends CmpQuery {

            @Override
            public Query apply(Function input, Context context) throws IOException {
                Tuple<Reference, Literal> prepare = prepare(input);
                if (prepare == null) { return null; }
                String fieldName = prepare.v1().info().ident().columnIdent().fqn();
                Object value = prepare.v2().value();


                if (value instanceof String) {
                    RegexQuery query = new RegexQuery(new Term(fieldName, (String) value));
                    query.setRegexImplementation(new JavaUtilRegexCapabilities(
                            JavaUtilRegexCapabilities.FLAG_CASE_INSENSITIVE |
                            JavaUtilRegexCapabilities.FLAG_UNICODE_CASE));
                    return query;
                }

                if (value instanceof BytesRef) {
                    RegexQuery query = new RegexQuery(new Term(fieldName, (BytesRef) value));
                    query.setRegexImplementation(new JavaUtilRegexCapabilities(
                            JavaUtilRegexCapabilities.FLAG_CASE_INSENSITIVE |
                            JavaUtilRegexCapabilities.FLAG_UNICODE_CASE));
                    return query;
                }

                throw new IllegalArgumentException("Can only use ~* with patterns of type string");
            }
        }


         interface InnerFunctionToQuery {


            @Nullable
            Query apply(Function parent, Function inner, Context context) throws IOException;
        }


        static class WithinQuery implements FunctionToQuery, InnerFunctionToQuery {

            @Override
            public Query apply(Function parent, Function inner, Context context) throws IOException {
                FunctionLiteralPair outerPair = new FunctionLiteralPair(parent);
                if (!outerPair.isValid()) {
                    return null;
                }
                Query query = getQuery(inner, context);
                if (query == null) return null;
                Boolean negate = !(Boolean) outerPair.input().value();
                if (negate) {
                    BooleanQuery booleanQuery = new BooleanQuery();
                    booleanQuery.add(query, BooleanClause.Occur.MUST_NOT);
                    return booleanQuery;
                } else {
                    return query;
                }
            }

            private Query getQuery(Function inner, Context context) {
                RefLiteralPair innerPair = new RefLiteralPair(inner);
                if (!innerPair.isValid()) {
                    return null;
                }
                GeoPointFieldMapper mapper = getGeoPointFieldMapper(
                        innerPair.reference().info().ident().columnIdent().fqn(),
                        context.searchContext
                );
                Shape shape = (Shape) innerPair.input().value();
                Geometry geometry = JtsSpatialContext.GEO.getGeometryFrom(shape);
                IndexGeoPointFieldData fieldData = context.searchContext.fieldData().getForField(mapper);
                Filter filter;
                if (geometry.isRectangle()) {
                    Rectangle boundingBox = shape.getBoundingBox();
                    filter = new InMemoryGeoBoundingBoxFilter(
                            new GeoPoint(boundingBox.getMaxY(), boundingBox.getMinX()),
                            new GeoPoint(boundingBox.getMinY(), boundingBox.getMaxX()),
                            fieldData
                    );
                } else {
                    Coordinate[] coordinates = geometry.getCoordinates();
                    GeoPoint[] points = new GeoPoint[coordinates.length];
                    for (int i = 0; i < coordinates.length; i++) {
                        Coordinate coordinate = coordinates[i];
                        points[i] = new GeoPoint(coordinate.y, coordinate.x);
                    }
                    filter = new GeoPolygonFilter(fieldData, points);
                }
                return new FilteredQuery(Queries.newMatchAllQuery(), context.indexCache.filter().cache(filter));
            }

            @Override
            public Query apply(Function input, Context context) throws IOException {
                return getQuery(input, context);
            }
        }

        class DistanceQuery implements InnerFunctionToQuery {

            final GeoDistance geoDistance = GeoDistance.DEFAULT;
            final String optimizeBox = "memory";


            @Override
            public Query apply(Function parent, Function inner, Context context) {
                assert inner.info().ident().name().equals(DistanceFunction.NAME);

                RefLiteralPair distanceRefLiteral = new RefLiteralPair(inner);
                if (!distanceRefLiteral.isValid()) {

                    return null;
                }
                FunctionLiteralPair functionLiteralPair = new FunctionLiteralPair(parent);
                if (!functionLiteralPair.isValid()) {

                    return null;
                }
                Double distance = DataTypes.DOUBLE.value(functionLiteralPair.input().value());

                String fieldName = distanceRefLiteral.reference().info().ident().columnIdent().fqn();
                FieldMapper mapper = getGeoPointFieldMapper(fieldName, context.searchContext);
                GeoPointFieldMapper geoMapper = ((GeoPointFieldMapper) mapper);
                IndexGeoPointFieldData fieldData = context.searchContext.fieldData().getForField(mapper);

                Input geoPointInput = distanceRefLiteral.input();
                Double[] pointValue = (Double[]) geoPointInput.value();
                double lat = pointValue[1];
                double lon = pointValue[0];

                String parentName = functionLiteralPair.functionName();

                Double from = null;
                Double to = null;
                boolean includeLower = false;
                boolean includeUpper = false;

                switch (parentName) {
                    case EqOperator.NAME:
                        includeLower = true;
                        includeUpper = true;
                        from = distance;
                        to = distance;
                        break;
                    case LteOperator.NAME:
                        includeUpper = true;
                        to = distance;
                        break;
                    case LtOperator.NAME:
                        to = distance;
                        break;
                    case GteOperator.NAME:
                        from = distance;
                        includeLower = true;
                        break;
                    case GtOperator.NAME:
                        from = distance;
                        break;
                    default:

                        return null;
                }
                GeoPoint geoPoint = new GeoPoint(lat, lon);
                Filter filter = new GeoDistanceRangeFilter(
                        geoPoint,
                        from,
                        to,
                        includeLower,
                        includeUpper,
                        geoDistance,
                        geoMapper,
                        fieldData,
                        optimizeBox
                );
                return new FilteredQuery(Queries.newMatchAllQuery(), context.indexCache.filter().cache(filter));
            }
        }

        private static GeoPointFieldMapper getGeoPointFieldMapper(String fieldName, SearchContext searchContext) {
            MapperService.SmartNameFieldMappers smartMappers = searchContext.smartFieldMappers(fieldName);
            if (smartMappers == null || !smartMappers.hasMapper()) {
                throw new IllegalArgumentException(String.format("column \"%s\" doesn't exist", fieldName));
            }
            FieldMapper mapper = smartMappers.mapper();
            if (!(mapper instanceof GeoPointFieldMapper)) {
                throw new IllegalArgumentException(String.format("column \"%s\" isn't of type geo_point", fieldName));
            }
            return (GeoPointFieldMapper) mapper;
        }

        private static final EqQuery eqQuery = new EqQuery();
        private static final RangeQuery ltQuery = new RangeQuery("lt");
        private static final RangeQuery lteQuery = new RangeQuery("lte");
        private static final RangeQuery gtQuery = new RangeQuery("gt");
        private static final RangeQuery gteQuery = new RangeQuery("gte");
        private static final WithinQuery withinQuery = new WithinQuery();
        private final ImmutableMap<String, FunctionToQuery> functions =
                ImmutableMap.<String, FunctionToQuery>builder()
                        .put(WithinFunction.NAME, withinQuery)
                        .put(AndOperator.NAME, new AndQuery())
                        .put(OrOperator.NAME, new OrQuery())
                        .put(EqOperator.NAME, eqQuery)
                        .put(LtOperator.NAME, ltQuery)
                        .put(LteOperator.NAME, lteQuery)
                        .put(GteOperator.NAME, gteQuery)
                        .put(GtOperator.NAME, gtQuery)
                        .put(LikeOperator.NAME, new LikeQuery())
                        .put(InOperator.NAME, new InQuery())
                        .put(NotPredicate.NAME, new NotQuery())
                        .put(IsNullPredicate.NAME, new IsNullQuery())
                        .put(MatchPredicate.NAME, new ToMatchQuery())
                        .put(AnyEqOperator.NAME, new AnyEqQuery())
                        .put(AnyNeqOperator.NAME, new AnyNeqQuery())
                        .put(AnyLtOperator.NAME, new AnyRangeQuery("gt", "lt"))
                        .put(AnyLteOperator.NAME, new AnyRangeQuery("gte", "lte"))
                        .put(AnyGteOperator.NAME, new AnyRangeQuery("lte", "gte"))
                        .put(AnyGtOperator.NAME, new AnyRangeQuery("lt", "gt"))
                        .put(AnyLikeOperator.NAME, new AnyLikeQuery())
                        .put(AnyNotLikeOperator.NAME, new AnyNotLikeQuery())
                        .put(RegexpMatchOperator.NAME, new RegexpMatchQuery())
                        .put(RegexpMatchCaseInsensitiveOperator.NAME, new RegexMatchQueryCaseInsensitive())
                        .build();

        private final ImmutableMap<String, InnerFunctionToQuery> innerFunctions =
                ImmutableMap.<String, InnerFunctionToQuery>builder()
                        .put(DistanceFunction.NAME, new DistanceQuery())
                        .put(WithinFunction.NAME, withinQuery)
                        .build();

        @Override
        public Query visitFunction(Function function, Context context) {
            assert function != null;
            if (fieldIgnored(function, context)) {
                return Queries.newMatchAllQuery();
            }
            validateNoUnsupportedFields(function, context);

            FunctionToQuery toQuery = functions.get(function.info().ident().name());
            if (toQuery == null) {
                return genericFunctionQuery(function, context.inputSymbolVisitor, context.searchContext);
            }

            Query query;
            try {
                query = toQuery.apply(function, context);
            } catch (IOException e) {
                throw ExceptionsHelper.convertToRuntime(e);
            } catch (UnsupportedOperationException e) {
                return genericFunctionQuery(function, context.inputSymbolVisitor, context.searchContext);
            }
            if (query == null) {
                query = queryFromInnerFunction(function, context);
                if (query == null) {
                    return genericFunctionQuery(function, context.inputSymbolVisitor, context.searchContext);
                }
            }
            return query;
        }

        private Query queryFromInnerFunction(Function function, Context context) {
            for (Symbol symbol : function.arguments()) {
                if (symbol.symbolType() == SymbolType.FUNCTION) {
                    String functionName = ((Function) symbol).info().ident().name();
                    InnerFunctionToQuery functionToQuery = innerFunctions.get(functionName);
                    if (functionToQuery != null) {
                        try {
                            Query query = functionToQuery.apply(function, (Function)symbol, context);
                            if (query != null) {
                                return query;
                            }
                        } catch (IOException e) {
                            throw ExceptionsHelper.convertToRuntime(e);
                        }
                    }
                }
            }
            return null;
        }

        private boolean fieldIgnored(Function function, Context context) {
            if (function.arguments().size() != 2) {
                return false;
            }

            Symbol left = function.arguments().get(0);
            Symbol right = function.arguments().get(1);
            if (left.symbolType() == SymbolType.REFERENCE && right.symbolType().isValueSymbol()) {
                String columnName = ((Reference) left).info().ident().columnIdent().name();
                if (Context.FILTERED_FIELDS.contains(columnName)) {
                    context.filteredFieldValues.put(columnName, ((Input) right).value());
                    return true;
                }
                String unsupportedMessage = Context.UNSUPPORTED_FIELDS.get(columnName);
                if (unsupportedMessage != null) {
                    throw new UnsupportedFeatureException(unsupportedMessage);
                }
            }
            return false;
        }

        @Nullable
        private String validateNoUnsupportedFields(Function function, Context context){
            if(function.arguments().size() != 2){
                return null;
            }
            Symbol left = function.arguments().get(0);
            Symbol right = function.arguments().get(1);
            if (left.symbolType() == SymbolType.REFERENCE && right.symbolType().isValueSymbol()) {
                String columnName = ((Reference) left).info().ident().columnIdent().name();
                String unsupportedMessage = context.unsupportedMessage(columnName);
                if(unsupportedMessage != null){
                    throw new UnsupportedFeatureException(unsupportedMessage);
                }
            }
            return null;
        }

        private static Filter genericFunctionFilter(Function function,
                                                    CollectInputSymbolVisitor<LuceneCollectorExpression<?>> inputSymbolVisitor,
                                                    SearchContext searchContext) {
            if (function.valueType() != DataTypes.BOOLEAN) {
                raiseUnsupported(function);
            }




            function = (Function)DocReferenceConverter.convertIf(function, Predicates.<Reference>alwaysTrue());

            final CollectInputSymbolVisitor.Context ctx = inputSymbolVisitor.extractImplementations(function);
            assert ctx.topLevelInputs().size() == 1;
            @SuppressWarnings("unchecked")
            final Input<Boolean> condition = (Input<Boolean>) ctx.topLevelInputs().get(0);
            @SuppressWarnings("unchecked")
            final List<LuceneCollectorExpression> expressions = ctx.docLevelExpressions();
            final CollectorContext collectorContext = new CollectorContext();
            collectorContext.searchContext(searchContext);
            collectorContext.visitor(new LuceneDocCollector.CollectorFieldsVisitor(expressions.size()));

            for (LuceneCollectorExpression expression : expressions) {
                expression.startCollect(collectorContext);
            }
            return new Filter() {
                @Override
                public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
                    for (LuceneCollectorExpression expression : expressions) {
                        expression.setNextReader(context.reader().getContext());
                    }
                    return BitsFilteredDocIdSet.wrap(
                            new FunctionDocSet(
                                    context.reader(),
                                    collectorContext.visitor(),
                                    condition,
                                    expressions,
                                    context.reader().maxDoc(),
                                    acceptDocs
                            ),
                            acceptDocs
                    );
                }
            };
        }

        private static Query genericFunctionQuery(Function function,
                                                  CollectInputSymbolVisitor<LuceneCollectorExpression<?>> inputSymbolVisitor,
                                                  SearchContext searchContext) {
            return new FilteredQuery(
                    Queries.newMatchAllQuery(),
                    genericFunctionFilter(function, inputSymbolVisitor, searchContext));
        }

        static class FunctionDocSet extends MatchDocIdSet {

            private final AtomicReader reader;
            private final LuceneDocCollector.CollectorFieldsVisitor fieldsVisitor;
            private final Input<Boolean> condition;
            private final List<LuceneCollectorExpression> expressions;
            private final boolean fieldsVisitorEnabled;

            protected FunctionDocSet(AtomicReader reader,
                                     @Nullable LuceneDocCollector.CollectorFieldsVisitor fieldsVisitor,
                                     Input<Boolean> condition,
                                     List<LuceneCollectorExpression> expressions,
                                     int maxDoc,
                                     @Nullable Bits acceptDocs) {
                super(maxDoc, acceptDocs);
                this.reader = reader;
                this.fieldsVisitor = fieldsVisitor;

                this.fieldsVisitorEnabled = fieldsVisitor == null ? false : fieldsVisitor.required();
                this.condition = condition;
                this.expressions = expressions;
            }

            @Override
            protected boolean matchDoc(int doc) {
                if (fieldsVisitorEnabled) {
                    fieldsVisitor.reset();
                    try {
                        reader.document(doc, fieldsVisitor);
                    } catch (IOException e) {
                        throw Throwables.propagate(e);
                    }
                }
                for (LuceneCollectorExpression expression : expressions) {
                    expression.setNextDocId(doc);
                }
                Boolean value = condition.value();
                if (value == null) {
                    return false;
                }
                return value;
            }
        }

        private static Query raiseUnsupported(Function function) {
            throw new UnsupportedOperationException(
                    SymbolFormatter.format("Cannot convert function %s into a query", function));
        }

        @Override
        public Query visitReference(Reference symbol, Context context) {

            if (symbol.valueType() == DataTypes.BOOLEAN) {
                return QueryBuilderHelper.forType(DataTypes.BOOLEAN).eq(symbol.info().ident().columnIdent().fqn(), true);
            }
            return super.visitReference(symbol, context);
        }

        @Override
        protected Query visitSymbol(Symbol symbol, Context context) {
            throw new UnsupportedOperationException(
                    SymbolFormatter.format("Can't build query from symbol %s", symbol));
        }
    }

    static class FunctionLiteralPair {

        private final String functionName;
        private final Function function;
        private final Input input;

        FunctionLiteralPair(Function outerFunction) {
            assert outerFunction.arguments().size() == 2 : "function requires 2 arguments";
            Symbol left = outerFunction.arguments().get(0);
            Symbol right = outerFunction.arguments().get(1);

            functionName = outerFunction.info().ident().name();

            if (left instanceof Function) {
                function = (Function) left;
            } else if (right instanceof Function) {
                function = (Function) right;
            } else {
                function = null;
            }

            if (left.symbolType().isValueSymbol()) {
                input = (Input) left;
            } else if (right.symbolType().isValueSymbol()) {
                input = (Input) right;
            } else {
                input = null;
            }
        }

        public boolean isValid() {
            return input != null && function != null;
        }

        public Input input() {
            return input;
        }

        public String functionName() {
            return functionName;
        }
    }

    static class RefLiteralPair {

        private final Reference reference;
        private final Input input;

        RefLiteralPair(Function function) {
            assert function.arguments().size() == 2 : "function requires 2 arguments";
            Symbol left = function.arguments().get(0);
            Symbol right = function.arguments().get(1);

            if (left instanceof Reference) {
                reference = (Reference) left;
            } else if (right instanceof Reference) {
                reference = (Reference) right;
            } else {
                reference = null;
            }

            if (left.symbolType().isValueSymbol()) {
                input = (Input) left;
            } else if (right.symbolType().isValueSymbol()) {
                input = (Input) right;
            } else {
                input = null;
            }
        }

        public boolean isValid() {
            return input != null && reference != null;
        }

        public Reference reference() {
            return reference;
        }

        public Input input() {
            return input;
        }
    }

    @SuppressWarnings("unchecked")
    private static BytesRef[] getBytesRefs(Object[] values, TermBuilder termBuilder) {
        BytesRef[] terms = new BytesRef[values.length];
        int i = 0;
        for (Object value : values) {
            terms[i] = termBuilder.term(value);
            i++;
        }
        return terms;
    }

    @SuppressWarnings("unchecked")
    private static BytesRef[] getBytesRefs(Collection values, TermBuilder termBuilder) {
        BytesRef[] terms = new BytesRef[values.size()];
        int i = 0;
        for (Object value : values) {
            terms[i] = termBuilder.term(value);
            i++;
        }
        return terms;
    }
}

<code block>


package io.crate.integrationtests;

import io.crate.testing.TestingHelpers;
import org.elasticsearch.common.collect.MapBuilder;
import org.hamcrest.Matchers;
import org.junit.Test;

import java.util.Arrays;

import static org.hamcrest.core.Is.is;

public class WherePKIntegrationTest extends SQLTransportIntegrationTest {

    @Test
    public void testWherePkColInWithLimit() throws Exception {
        execute("create table users (" +
                "   id int primary key," +
                "   name string" +
                ") clustered into 2 shards with (number_of_replicas = 0)");
        ensureYellow();
        execute("insert into users (id, name) values (?, ?)", new Object[][] {
                new Object[] { 1, "Arthur" },
                new Object[] { 2, "Trillian" },
                new Object[] { 3, "Marvin" },
                new Object[] { 4, "Slartibartfast" },
        });
        execute("refresh table users");

        execute("select name from users where id in (1, 3, 4) order by name desc limit 2");
        assertThat(response.rowCount(), is(2L));
        assertThat((String) response.rows()[0][0], is("Slartibartfast"));
        assertThat((String) response.rows()[1][0], is("Marvin"));
    }

    @Test
    public void testWherePKWithFunctionInOutputsAndOrderBy() throws Exception {
        execute("create table users (" +
                "   id int primary key," +
                "   name string" +
                ") clustered into 2 shards with (number_of_replicas = 0)");
        ensureYellow();
        execute("insert into users (id, name) values (?, ?)", new Object[][]{
                new Object[]{1, "Arthur"},
                new Object[]{2, "Trillian"},
                new Object[]{3, "Marvin"},
                new Object[]{4, "Slartibartfast"},
        });
        execute("refresh table users");
        execute("select substr(name, 1, 1) from users where id in (1, 2, 3) order by substr(name, 1, 1) desc");
        assertThat(response.rowCount(), is(3L));
        assertThat((String) response.rows()[0][0], is("T"));
        assertThat((String) response.rows()[1][0], is("M"));
        assertThat((String) response.rows()[2][0], is("A"));
    }

    @Test
    public void testWherePKWithOrderBySymbolThatIsMissingInSelectList() throws Exception {
        execute("create table users (" +
                "   id int primary key," +
                "   name string" +
                ") clustered into 2 shards with (number_of_replicas = 0)");
        ensureYellow();
        execute("insert into users (id, name) values (?, ?)", new Object[][]{
                new Object[]{1, "Arthur"},
                new Object[]{2, "Trillian"},
                new Object[]{3, "Marvin"},
                new Object[]{4, "Slartibartfast"},
        });
        execute("refresh table users");
        execute("select name from users where id in (1, 2, 3) order by id desc");
        assertThat(response.rowCount(), is(3L));
        assertThat((String) response.rows()[0][0], is("Marvin"));
        assertThat((String) response.rows()[1][0], is("Trillian"));
        assertThat((String) response.rows()[2][0], is("Arthur"));
    }

    @Test
    public void testWherePkColLimit0() throws Exception {
        execute("create table users (id int primary key, name string) " +
                "clustered into 1 shards with (number_of_replicas = 0)");
        ensureYellow();
        execute("insert into users (id, name) values (1, 'Arthur')");
        execute("refresh table users");
        execute("select name from users where id = 1 limit 0");
        assertThat(response.rowCount(), is(0L));
    }

    @Test
    public void testSelectNestedObjectWherePk() throws Exception {
        execute("create table items (id string primary key, details object as (tags array(string)) )" +
            "clustered into 3 shards with (number_of_replicas = '0-1')");
        ensureYellow();

        execute("insert into items (id, details) values (?, ?)", new Object[]{
            "123", MapBuilder.newMapBuilder().put("tags", Arrays.asList("small", "blue")).map()
        });
        execute("refresh table items");

        execute("select id, details['tags'] from items where id = '123'");
        assertThat(response.rowCount(), is(1L));
        assertThat((String) response.rows()[0][0], is("123"));

        String[] tags = Arrays.copyOf((Object[])response.rows()[0][1], 2, String[].class);
        assertThat(tags, Matchers.arrayContaining("small", "blue"));
    }

    @Test
    public void testEmptyClusteredByUnderId() throws Exception {

        execute("create table auto_id (" +
                "  name string," +
                "  location geo_point" +
                ") with (number_of_replicas=0)");
        ensureYellow();

        execute("insert into auto_id (name, location) values (',', [36.567, 52.998]), ('Dornbirn', [54.45, 4.567])");
        execute("refresh table auto_id");


        execute("select * from auto_id where _id=''");
        assertThat(response.cols(), is(Matchers.arrayContaining("location", "name")));
        assertThat(response.rowCount(), is(0L));
    }

    @Test
    public void testEmptyClusteredByExplicit() throws Exception {

        execute("create table explicit_routing (" +
                "  name string," +
                "  location geo_point" +
                ") clustered by (name) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into explicit_routing (name, location) values (',', [36.567, 52.998]), ('Dornbirn', [54.45, 4.567])");
        execute("refresh table explicit_routing");
        execute("select * from explicit_routing where name=''");
        assertThat(response.cols(), is(Matchers.arrayContaining("location", "name")));
        assertThat(response.rowCount(), is(0L));

        execute("select * from explicit_routing where name=','");
        assertThat(response.cols(), is(Matchers.arrayContaining("location", "name")));
        assertThat(response.rowCount(), is(1L));
        assertThat(TestingHelpers.printedTable(response.rows()), is("[36.567, 52.998]| ,\n"));
    }

    @Test
    public void testQueryOnEmptyClusteredByColumn() throws Exception {
        execute("create table expl_routing (id int primary key, name string primary key) " +
                "clustered by (name) with (number_of_replicas = 0)");
        ensureYellow();

        if (randomInt(1) == 0) {
            execute("insert into expl_routing (id, name) values (?, ?)", new Object[][]{
                    new Object[]{1, ""},
                    new Object[]{2, ""},
                    new Object[]{1, "1"}
            });
        } else {
            execute("insert into expl_routing (id, name) values (?, ?)", new Object[]{1, ""});
            execute("insert into expl_routing (id, name) values (?, ?)", new Object[]{2, ""});
            execute("insert into expl_routing (id, name) values (?, ?)", new Object[]{1, "1"});
        }
        execute("refresh table expl_routing");

        execute("select count(*) from expl_routing where name = ''");
        assertThat((Long)response.rows()[0][0], is(2L));

        execute("select * from expl_routing where name = '' order by id");
        assertThat(response.rowCount(), is(2L));
        assertThat((Integer) response.rows()[0][0], is(1));
        assertThat((Integer)response.rows()[1][0], is(2));

        execute("delete from expl_routing where name = ''");
        execute("select count(*) from expl_routing");
        assertThat((Long) response.rows()[0][0], is(1L));
    }

    @Test
    public void testDeleteByQueryCommaRouting() throws Exception {
        execute("create table explicit_routing (" +
                "  name string," +
                "  location geo_point" +
                ") clustered by (name) into 3 shards with (number_of_replicas=0)");
        ensureYellow();




        execute("insert into explicit_routing (name, location) values ('A', [36.567, 52.998]), ('W', [54.45, 4.567])");
        execute("refresh table explicit_routing");


        execute("delete from explicit_routing where name='A,W'");
        assertThat(response.rowCount(), is(-1L));
        execute("refresh table explicit_routing");

        execute("select * from explicit_routing");
        assertThat(response.rowCount(), is(2L));

    }
}


<code block>


package io.crate.analyze.where;

import com.google.common.collect.ImmutableList;
import io.crate.analyze.*;
import io.crate.analyze.relations.TableRelation;
import io.crate.core.collections.TreeMapBuilder;
import io.crate.metadata.*;
import io.crate.metadata.sys.MetaDataSysModule;
import io.crate.metadata.table.ColumnPolicy;
import io.crate.metadata.table.SchemaInfo;
import io.crate.metadata.table.TestingTableInfo;
import io.crate.operation.operator.OperatorModule;
import io.crate.operation.operator.any.AnyEqOperator;
import io.crate.operation.operator.any.AnyLikeOperator;
import io.crate.operation.predicate.PredicateModule;
import io.crate.operation.scalar.ScalarFunctionModule;
import io.crate.planner.RowGranularity;
import io.crate.sql.parser.SqlParser;
import io.crate.test.integration.CrateUnitTest;
import io.crate.testing.MockedClusterServiceModule;
import io.crate.types.ArrayType;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import io.crate.types.SetType;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.common.inject.Injector;
import org.elasticsearch.common.inject.ModulesBuilder;
import org.elasticsearch.threadpool.ThreadPool;
import org.hamcrest.Matchers;
import org.junit.After;
import org.junit.Before;
import org.junit.Rule;
import org.junit.Test;
import org.junit.rules.ExpectedException;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.Map;
import java.util.concurrent.TimeUnit;

import static io.crate.testing.TestingHelpers.*;
import static org.hamcrest.Matchers.*;
import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;

@SuppressWarnings("unchecked")
public class WhereClauseAnalyzerTest extends CrateUnitTest {

    @Rule
    public ExpectedException expectedException = ExpectedException.none();

    private Analyzer analyzer;
    private AnalysisMetaData ctxMetaData;
    private ThreadPool threadPool;

    @Before
    public void setUp() throws Exception {
        super.setUp();
        threadPool = newMockedThreadPool();
        Injector injector = new ModulesBuilder()
                .add(new MockedClusterServiceModule())
                .add(new PredicateModule())
                .add(new OperatorModule())
                .add(new ScalarFunctionModule())
                .add(new MetaDataSysModule())
                .add(new TestMetaDataModule()).createInjector();
        analyzer = injector.getInstance(Analyzer.class);
        ctxMetaData = injector.getInstance(AnalysisMetaData.class);
    }

    @After
    public void after() throws Exception {
        threadPool.shutdown();
        threadPool.awaitTermination(1, TimeUnit.SECONDS);
    }

    static final Routing twoNodeRouting = new Routing(TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder()
            .put("nodeOne", TreeMapBuilder.<String, List<Integer>>newMapBuilder().put("t1", Arrays.asList(1, 2)).map())
            .put("nodeTow", TreeMapBuilder.<String, List<Integer>>newMapBuilder().put("t1", Arrays.asList(3, 4)).map())
            .map());

    class TestMetaDataModule extends MetaDataModule {
        @Override
        protected void bindSchemas() {
            super.bindSchemas();
            bind(ThreadPool.class).toInstance(threadPool);
            SchemaInfo schemaInfo = mock(SchemaInfo.class);
            when(schemaInfo.name()).thenReturn(ReferenceInfos.DEFAULT_SCHEMA_NAME);
            when(schemaInfo.getTableInfo("users")).thenReturn(
                    TestingTableInfo.builder(new TableIdent("doc", "users"), RowGranularity.DOC, twoNodeRouting)
                            .add("id", DataTypes.STRING, null)
                            .add("name", DataTypes.STRING, null)
                            .add("tags", new ArrayType(DataTypes.STRING), null)
                            .addPrimaryKey("id")
                            .clusteredBy("id")
                            .build());
            when(schemaInfo.getTableInfo("parted")).thenReturn(
                    TestingTableInfo.builder(new TableIdent("doc", "parted"), RowGranularity.DOC, twoNodeRouting)
                            .add("id", DataTypes.INTEGER, null)
                            .add("name", DataTypes.STRING, null)
                            .add("date", DataTypes.TIMESTAMP, null, true)
                            .add("obj", DataTypes.OBJECT, null, ColumnPolicy.IGNORED)
                            .addPartitions(
                                    new PartitionName("parted", Arrays.asList(new BytesRef("1395874800000"))).stringValue(),
                                    new PartitionName("parted", Arrays.asList(new BytesRef("1395961200000"))).stringValue(),
                                    new PartitionName("parted", new ArrayList<BytesRef>() {{
                                        add(null);
                                    }}).stringValue())
                            .build());
            when(schemaInfo.getTableInfo("parted_pk")).thenReturn(
                    TestingTableInfo.builder(new TableIdent("doc", "parted"), RowGranularity.DOC, twoNodeRouting)
                            .addPrimaryKey("id").addPrimaryKey("date")
                            .add("id", DataTypes.INTEGER, null)
                            .add("name", DataTypes.STRING, null)
                            .add("date", DataTypes.TIMESTAMP, null, true)
                            .add("obj", DataTypes.OBJECT, null, ColumnPolicy.IGNORED)
                            .addPartitions(
                                    new PartitionName("parted_pk", Arrays.asList(new BytesRef("1395874800000"))).stringValue(),
                                    new PartitionName("parted_pk", Arrays.asList(new BytesRef("1395961200000"))).stringValue(),
                                    new PartitionName("parted_pk", new ArrayList<BytesRef>() {{
                                        add(null);
                                    }}).stringValue())
                            .build());
            when(schemaInfo.getTableInfo("bystring")).thenReturn(
                    TestingTableInfo.builder(new TableIdent("doc", "bystring"), RowGranularity.DOC, twoNodeRouting)
                            .add("name", DataTypes.STRING, null)
                            .add("score", DataTypes.DOUBLE, null)
                            .addPrimaryKey("name")
                            .clusteredBy("name")
                            .build());
            when(schemaInfo.getTableInfo("users_multi_pk")).thenReturn(
                    TestingTableInfo.builder(new TableIdent("doc", "users_multi_pk"), RowGranularity.DOC, twoNodeRouting)
                            .add("id", DataTypes.LONG, null)
                            .add("name", DataTypes.STRING, null)
                            .add("details", DataTypes.OBJECT, null)
                            .add("awesome", DataTypes.BOOLEAN, null)
                            .add("friends", new ArrayType(DataTypes.OBJECT), null, ColumnPolicy.DYNAMIC)
                            .addPrimaryKey("id")
                            .addPrimaryKey("name")
                            .clusteredBy("id")
                            .build());
            when(schemaInfo.getTableInfo("users_clustered_by_only")).thenReturn(
                    TestingTableInfo.builder(new TableIdent("doc", "users_clustered_by_only"), RowGranularity.DOC, twoNodeRouting)
                            .add("id", DataTypes.LONG, null)
                            .add("name", DataTypes.STRING, null)
                            .add("details", DataTypes.OBJECT, null)
                            .add("awesome", DataTypes.BOOLEAN, null)
                            .add("friends", new ArrayType(DataTypes.OBJECT), null, ColumnPolicy.DYNAMIC)
                            .clusteredBy("id")
                            .build());
            schemaBinder.addBinding(ReferenceInfos.DEFAULT_SCHEMA_NAME).toInstance(schemaInfo);
        }
    }

    private DeleteAnalyzedStatement analyzeDelete(String stmt, Object[][] bulkArgs) {
        return (DeleteAnalyzedStatement) analyzer.analyze(SqlParser.createStatement(stmt),
                new ParameterContext(new Object[0], bulkArgs, ReferenceInfos.DEFAULT_SCHEMA_NAME)).analyzedStatement();
    }

    private DeleteAnalyzedStatement analyzeDelete(String stmt) {
        return analyzeDelete(stmt, new Object[0][]);
    }

    private UpdateAnalyzedStatement analyzeUpdate(String stmt) {
        return (UpdateAnalyzedStatement) analyzer.analyze(SqlParser.createStatement(stmt),
                new ParameterContext(new Object[0], new Object[0][], ReferenceInfos.DEFAULT_SCHEMA_NAME)).analyzedStatement();
    }

    private WhereClause analyzeSelect(String stmt, Object... args) {
        SelectAnalyzedStatement statement = (SelectAnalyzedStatement) analyzer.analyze(SqlParser.createStatement(stmt),
                new ParameterContext(args, new Object[0][], ReferenceInfos.DEFAULT_SCHEMA_NAME)).analyzedStatement();
        return statement.relation().querySpec().where();
    }

    private WhereClause analyzeSelectWhere(String stmt) {
        return analyzeSelect(stmt);
    }

    @Test
    public void testWhereSinglePKColumnEq() throws Exception {
        DeleteAnalyzedStatement statement = analyzeDelete("delete from users where id = ?", new Object[][]{
                new Object[]{1},
                new Object[]{2},
                new Object[]{3},
        });
        TableRelation tableRelation = statement.analyzedRelation();
        WhereClauseAnalyzer whereClauseAnalyzer = new WhereClauseAnalyzer(ctxMetaData, tableRelation);
        assertThat(whereClauseAnalyzer.analyze(statement.whereClauses().get(0)).docKeys().get(), contains(isDocKey("1")));
        assertThat(whereClauseAnalyzer.analyze(statement.whereClauses().get(1)).docKeys().get(), contains(isDocKey("2")));
        assertThat(whereClauseAnalyzer.analyze(statement.whereClauses().get(2)).docKeys().get(), contains(isDocKey("3")));
    }




    @Test
    public void testSelectById() throws Exception {
        WhereClause whereClause = analyzeSelect("select name from users_clustered_by_only where _id=1");
        assertTrue(whereClause.docKeys().isPresent());
        assertThat(whereClause.docKeys().get().getOnlyKey(), isDocKey("1"));
    }

    @Test
    public void testSelectWherePartitionedByColumn() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select id from parted where date = 1395874800000");
        assertThat(whereClause.hasQuery(), is(false));
        assertThat(whereClause.noMatch(), is(false));
        assertThat(whereClause.partitions(),
                Matchers.contains(new PartitionName("parted", Arrays.asList(new BytesRef("1395874800000"))).stringValue()));
    }

    @Test
    public void testSelectPartitionedByPK() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select id from parted_pk where id = 1 and date = 1395874800000");
        assertThat(whereClause.docKeys().get(), contains(isDocKey(1, 1395874800000L)));

        assertThat(whereClause.partitions(), empty());

    }

    @Test
    public void testWherePartitionedByColumn() throws Exception {
        DeleteAnalyzedStatement statement = analyzeDelete("delete from parted where date = 1395874800000");
        WhereClause whereClause = statement.whereClauses().get(0);

        assertThat(whereClause.hasQuery(), is(false));
        assertThat(whereClause.noMatch(), is(false));
        assertThat(whereClause.partitions(),
                Matchers.contains(new PartitionName("parted", Arrays.asList(new BytesRef("1395874800000"))).stringValue()));
    }

    @Test
    public void testUpdateWherePartitionedByColumn() throws Exception {
        UpdateAnalyzedStatement updateAnalyzedStatement = analyzeUpdate("update parted set id = 2 where date = 1395874800000");
        UpdateAnalyzedStatement.NestedAnalyzedStatement nestedAnalyzedStatement = updateAnalyzedStatement.nestedStatements().get(0);

        assertThat(nestedAnalyzedStatement.whereClause().hasQuery(), is(false));
        assertThat(nestedAnalyzedStatement.whereClause().noMatch(), is(false));

        assertEquals(ImmutableList.of(
                        new PartitionName("parted", Arrays.asList(new BytesRef("1395874800000"))).stringValue()),
                nestedAnalyzedStatement.whereClause().partitions()
        );
    }

    @Test
    public void testClusteredByValueContainsComma() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select * from bystring where name = 'a,b,c'");
        assertThat(whereClause.clusteredBy().get(), contains(isLiteral("a,b,c")));
        assertThat(whereClause.docKeys().get().size(), is(1));
        assertThat(whereClause.docKeys().get().getOnlyKey(), isDocKey("a,b,c"));
    }

    @Test
    public void testEmptyClusteredByValue() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select * from bystring where name = ''");
        assertThat(whereClause.clusteredBy().get(), contains(isLiteral("")));
        assertThat(whereClause.docKeys().get().getOnlyKey(), isDocKey(""));
    }

    @Test
    public void testClusteredBy() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select name from users where id=1");
        assertThat(whereClause.clusteredBy().get(), contains(isLiteral("1")));
        assertThat(whereClause.docKeys().get().getOnlyKey(), isDocKey("1"));

        whereClause = analyzeSelectWhere("select name from users where id=1 or id=2");

        assertThat(whereClause.docKeys().get().size(), is(2));
        assertThat(whereClause.docKeys().get(), containsInAnyOrder(isDocKey("1"), isDocKey("2")));

        assertThat(whereClause.clusteredBy().get(), containsInAnyOrder(isLiteral("1"), isLiteral("2")));
    }


    @Test
    public void testClusteredByOnly() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select name from users_clustered_by_only where id=1");
        assertFalse(whereClause.docKeys().isPresent());
        assertThat(whereClause.clusteredBy().get(),  contains(isLiteral(1L)));

        whereClause = analyzeSelectWhere("select name from users_clustered_by_only where id=1 or id=2");
        assertFalse(whereClause.docKeys().isPresent());
        assertThat(whereClause.clusteredBy().get(), containsInAnyOrder(isLiteral(1L), isLiteral(2L)));

        whereClause = analyzeSelectWhere("select name from users_clustered_by_only where id in (3,4,5)");
        assertFalse(whereClause.docKeys().isPresent());
        assertThat(whereClause.clusteredBy().get(), containsInAnyOrder(
                isLiteral(3L), isLiteral(4L), isLiteral(5L)));



        whereClause = analyzeSelectWhere("select name from users_clustered_by_only where id=1 and id=2");
        assertFalse(whereClause.docKeys().isPresent());
        assertFalse(whereClause.clusteredBy().isPresent());
    }

    @Test
    public void testCompositePrimaryKey() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select name from users_multi_pk where id=1");
        assertFalse(whereClause.docKeys().isPresent());
        assertThat(whereClause.clusteredBy().get(), contains(isLiteral(1L)));

        whereClause = analyzeSelectWhere("select name from users_multi_pk where id=1 and name='Douglas'");
        assertThat(whereClause.docKeys().get(), contains(isDocKey(1L, "Douglas")));
        assertThat(whereClause.clusteredBy().get(), contains(isLiteral(1L)));

        whereClause = analyzeSelectWhere("select name from users_multi_pk where id=1 or id=2 and name='Douglas'");
        assertFalse(whereClause.docKeys().isPresent());
        assertThat(whereClause.clusteredBy().get(), containsInAnyOrder(
                isLiteral(1L), isLiteral(2L)));

        whereClause = analyzeSelectWhere("select name from users_multi_pk where id=1 and name='Douglas' or name='Arthur'");
        assertFalse(whereClause.docKeys().isPresent());
        assertFalse(whereClause.clusteredBy().isPresent());
    }

    @Test
    public void testPrimaryKeyAndVersion() throws Exception {
        WhereClause whereClause = analyzeSelectWhere(
                "select name from users where id = 2 and \"_version\" = 1");
        assertThat(whereClause.docKeys().get().getOnlyKey(), isDocKey("2", 1L));
    }

    @Test
    public void testMultiplePrimaryKeys() throws Exception {
        WhereClause whereClause = analyzeSelectWhere(
                "select name from users where id = 2 or id = 1");
        assertThat(whereClause.docKeys().get(), containsInAnyOrder(isDocKey("1"), isDocKey("2")));
        assertThat(whereClause.clusteredBy().get(), containsInAnyOrder(isLiteral("1"), isLiteral("2")));
    }

    @Test
    public void testMultiplePrimaryKeysAndInvalidColumn() throws Exception {
        WhereClause whereClause = analyzeSelectWhere(
                "select name from users where id = 2 or id = 1 and name = 'foo'");
        assertFalse(whereClause.docKeys().isPresent());
    }

    @Test
    public void testNotEqualsDoesntMatchPrimaryKey() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select name from users where id != 1");
        assertFalse(whereClause.docKeys().isPresent());
        assertFalse(whereClause.clusteredBy().isPresent());
    }

    @Test
    public void testMultipleCompoundPrimaryKeys() throws Exception {
        WhereClause whereClause = analyzeSelectWhere(
                "select * from sys.shards where (schema_name='doc' and id = 1 and table_name = 'foo' and partition_ident='') " +
                        "or (schema_name='doc' and id = 2 and table_name = 'bla' and partition_ident='')");

        assertThat(whereClause.docKeys().get(), containsInAnyOrder(
                isDocKey("doc", "foo", 1, ""), isDocKey("doc", "bla", 2, "")
        ));
        assertFalse(whereClause.clusteredBy().isPresent());

        whereClause = analyzeSelectWhere(
                "select * from sys.shards where (schema_name='doc' and id = 1 and table_name = 'foo') " +
                        "or (schema_name='doc' and id = 2 and table_name = 'bla') or id = 1");
        assertFalse(whereClause.docKeys().isPresent());
        assertFalse(whereClause.clusteredBy().isPresent());
    }

    @Test
    public void test1ColPrimaryKey() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select name from sys.nodes where id='jalla'");

        assertThat(whereClause.docKeys().get(), contains(isDocKey("jalla")));

        whereClause = analyzeSelectWhere("select name from sys.nodes where 'jalla'=id");
        assertThat(whereClause.docKeys().get(), contains(isDocKey("jalla")));

        whereClause = analyzeSelectWhere("select name from sys.nodes where id='jalla' and id='jalla'");
        assertThat(whereClause.docKeys().get(), contains(isDocKey("jalla")));

        whereClause = analyzeSelectWhere("select name from sys.nodes where id='jalla' and (id='jalla' or 1=1)");
        assertThat(whereClause.docKeys().get(), contains(isDocKey("jalla")));



        whereClause = analyzeSelectWhere("select name from sys.nodes where id='jalla' and id='kelle'");
        assertFalse(whereClause.docKeys().isPresent());


        whereClause = analyzeSelectWhere("select name from sys.nodes where id='jalla' or name = 'something'");
        assertFalse(whereClause.docKeys().isPresent());
        assertFalse(whereClause.noMatch());

        whereClause = analyzeSelectWhere("select name from sys.nodes where name = 'something'");
        assertFalse(whereClause.docKeys().isPresent());
        assertFalse(whereClause.noMatch());

    }

    @Test
    public void test3ColPrimaryKey() throws Exception {
        WhereClause whereClause = analyzeSelectWhere(
                "select id from sys.shards where id=1 and table_name='jalla' and schema_name='doc' and partition_ident=''");

        assertThat(whereClause.docKeys().get(), contains(isDocKey("doc", "jalla", 1, "")));
        assertFalse(whereClause.noMatch());

        whereClause = analyzeSelectWhere(
                "select id from sys.shards where id=1 and table_name='jalla' and id=1 and schema_name='doc' and partition_ident=''");
        assertThat(whereClause.docKeys().get(), contains(isDocKey("doc", "jalla", 1, "")));
        assertFalse(whereClause.noMatch());


        whereClause = analyzeSelectWhere("select id from sys.shards where id=1");
        assertFalse(whereClause.docKeys().isPresent());
        assertFalse(whereClause.noMatch());


        whereClause = analyzeSelectWhere(
                "select id from sys.shards where id=1 and schema_name='doc' and table_name='jalla' and id=2 and partition_ident=''");
        assertFalse(whereClause.docKeys().isPresent());


    }

    @Test
    public void test1ColPrimaryKeySetLiteralDiffMatches() throws Exception {
        WhereClause whereClause = analyzeSelectWhere(
                "select name from sys.nodes where id in ('jalla', 'kelle') and id in ('jalla', 'something')");
        assertFalse(whereClause.noMatch());
        assertThat(whereClause.docKeys().get(), contains(isDocKey("jalla")));
    }

    @Test
    public void test1ColPrimaryKeySetLiteral() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select name from sys.nodes where id in ('jalla', 'kelle')");
        assertFalse(whereClause.noMatch());
        assertThat(whereClause.docKeys().get(), containsInAnyOrder(isDocKey("jalla"), isDocKey("kelle")));
    }

    @Test
    public void test1ColPrimaryKeyNotSetLiteral() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select name from sys.nodes where id not in ('jalla', 'kelle')");
        assertFalse(whereClause.noMatch());
        assertFalse(whereClause.docKeys().isPresent());
        assertFalse(whereClause.clusteredBy().isPresent());
    }

    @Test
    public void test3ColPrimaryKeySetLiteral() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select id from sys.shards where id=1 and schema_name='doc' and" +
                " table_name in ('jalla', 'kelle') and partition_ident=''");
        assertThat(whereClause.docKeys().get(), containsInAnyOrder(
                isDocKey("doc", "jalla", 1, ""), isDocKey("doc", "kelle", 1, "")));
    }

    @Test
    public void test3ColPrimaryKeyWithOr() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select id from sys.shards where id=1 and schema_name='doc' and " +
                "(table_name = 'jalla' or table_name='kelle') and partition_ident=''");
        assertEquals(2, whereClause.docKeys().get().size());
        assertThat(whereClause.docKeys().get(), containsInAnyOrder(
                isDocKey("doc", "jalla", 1, ""), isDocKey("doc", "kelle", 1, "")));
    }

    @Test
    public void testSelectFromPartitionedTable() throws Exception {
        String partition1 = new PartitionName("parted", Arrays.asList(new BytesRef("1395874800000"))).stringValue();
        String partition2 = new PartitionName("parted", Arrays.asList(new BytesRef("1395961200000"))).stringValue();
        String partition3 = new PartitionName("parted", new ArrayList<BytesRef>() {{
            add(null);
        }}).stringValue();

        WhereClause whereClause = analyzeSelectWhere("select id, name from parted where date = 1395874800000");
        assertEquals(ImmutableList.of(partition1), whereClause.partitions());
        assertFalse(whereClause.hasQuery());
        assertFalse(whereClause.noMatch());

        whereClause = analyzeSelectWhere("select id, name from parted where date = 1395874800000 " +
                "and substr(name, 0, 4) = 'this'");
        assertEquals(ImmutableList.of(partition1), whereClause.partitions());
        assertThat(whereClause.hasQuery(), is(true));
        assertThat(whereClause.noMatch(), is(false));

        whereClause = analyzeSelectWhere("select id, name from parted where date >= 1395874800000");
        assertThat(whereClause.partitions(), containsInAnyOrder(partition1, partition2));
        assertFalse(whereClause.hasQuery());
        assertFalse(whereClause.noMatch());

        whereClause = analyzeSelectWhere("select id, name from parted where date < 1395874800000");
        assertEquals(ImmutableList.of(), whereClause.partitions());
        assertTrue(whereClause.noMatch());

        whereClause = analyzeSelectWhere("select id, name from parted where date = 1395874800000 and date = 1395961200000");
        assertEquals(ImmutableList.of(), whereClause.partitions());
        assertTrue(whereClause.noMatch());

        whereClause = analyzeSelectWhere("select id, name from parted where date = 1395874800000 or date = 1395961200000");
        assertThat(whereClause.partitions(), containsInAnyOrder(partition1, partition2));
        assertFalse(whereClause.hasQuery());
        assertFalse(whereClause.noMatch());

        whereClause = analyzeSelectWhere("select id, name from parted where date < 1395874800000 or date > 1395874800000");
        assertEquals(ImmutableList.of(partition2), whereClause.partitions());
        assertFalse(whereClause.hasQuery());
        assertFalse(whereClause.noMatch());

        whereClause = analyzeSelectWhere("select id, name from parted where date in (1395874800000, 1395961200000)");
        assertThat(whereClause.partitions(), containsInAnyOrder(partition1, partition2));
        assertFalse(whereClause.hasQuery());
        assertFalse(whereClause.noMatch());

        whereClause = analyzeSelectWhere("select id, name from parted where date in (1395874800000, 1395961200000) and id = 1");
        assertThat(whereClause.partitions(), containsInAnyOrder(partition1, partition2));
        assertTrue(whereClause.hasQuery());
        assertFalse(whereClause.noMatch());


        whereClause = analyzeSelectWhere("select id, name from parted where not (date = 1395874800000 and obj['col'] = 'undefined')");
        assertThat(whereClause.partitions(), containsInAnyOrder(partition1, partition2, partition3));
        assertThat(whereClause.hasQuery(), is(false));
        assertThat(whereClause.noMatch(), is(false));

        whereClause = analyzeSelectWhere("select id, name from parted where date in (1395874800000) or date in (1395961200000)");
        assertThat(whereClause.partitions(), containsInAnyOrder(partition1, partition2));
        assertFalse(whereClause.hasQuery());
        assertFalse(whereClause.noMatch());

        whereClause = analyzeSelectWhere("select id, name from parted where date = 1395961200000 and id = 1");
        assertEquals(ImmutableList.of(partition2), whereClause.partitions());
        assertTrue(whereClause.hasQuery());
        assertFalse(whereClause.noMatch());

        whereClause = analyzeSelectWhere("select id, name from parted where (date =1395874800000 or date = 1395961200000) and id = 1");
        assertThat(whereClause.partitions(), containsInAnyOrder(partition1, partition2));
        assertTrue(whereClause.hasQuery());
        assertFalse(whereClause.noMatch());

        whereClause = analyzeSelectWhere("select id, name from parted where date = 1395874800000 and id is null");
        assertEquals(ImmutableList.of(partition1), whereClause.partitions());
        assertTrue(whereClause.hasQuery());
        assertFalse(whereClause.noMatch());

        whereClause = analyzeSelectWhere("select id, name from parted where date is null and id = 1");
        assertEquals(ImmutableList.of(partition3), whereClause.partitions());
        assertTrue(whereClause.hasQuery());
        assertFalse(whereClause.noMatch());

        whereClause = analyzeSelectWhere("select id, name from parted where 1395874700000 < date and date < 1395961200001");
        assertThat(whereClause.partitions(), containsInAnyOrder(partition1, partition2));
        assertFalse(whereClause.hasQuery());
        assertFalse(whereClause.noMatch());

        whereClause = analyzeSelectWhere("select id, name from parted where '2014-03-16T22:58:20' < date and date < '2014-03-27T23:00:01'");
        assertThat(whereClause.partitions(), containsInAnyOrder(partition1, partition2));
        assertFalse(whereClause.hasQuery());
        assertFalse(whereClause.noMatch());
    }

    @Test
    public void testSelectFromPartitionedTableUnsupported() throws Exception {


        try {
            analyzeSelectWhere("select id, name from parted where date = 1395961200000 or id = 1");
            fail("Expected UnsupportedOperationException");
        } catch (UnsupportedOperationException e) {
            assertThat(e.getMessage(),
                    is("logical conjunction of the conditions in the WHERE clause which involve " +
                            "partitioned columns led to a query that can't be executed."));
        }

        try {
            analyzeSelectWhere("select id, name from parted where id = 1 or date = 1395961200000");
            fail("Expected UnsupportedOperationException");
        } catch (UnsupportedOperationException e) {
            assertThat(e.getMessage(),
                    is("logical conjunction of the conditions in the WHERE clause which involve " +
                            "partitioned columns led to a query that can't be executed."));
        }
    }

    @Test
    public void testAnyInvalidArrayType() throws Exception {
        expectedException.expect(IllegalArgumentException.class);
        expectedException.expectMessage("array expression of invalid type array(string)");
        analyzeSelectWhere("select * from users_multi_pk where awesome = any(['foo', 'bar', 'baz'])");
    }

    @Test
    public void testInConvertedToAnyIfOnlyLiterals() throws Exception {
        StringBuilder sb = new StringBuilder("select id from sys.shards where id in (");
        int i=0;
        for (; i < 1500; i++) {
            sb.append(i);
            sb.append(',');
        }
        sb.append(i++);
        sb.append(')');
        String s = sb.toString();

        WhereClause whereClause = analyzeSelectWhere(s);
        assertThat(whereClause.query(), isFunction(AnyEqOperator.NAME, ImmutableList.<DataType>of(DataTypes.INTEGER, new SetType(DataTypes.INTEGER))));
    }

    @Test
    public void testInNormalizedToAnyWithScalars() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select * from users where id in (null, 1+2, 3+4, abs(-99))");
        assertThat(whereClause.query(), isFunction(AnyEqOperator.NAME));
        assertThat(whereClause.docKeys().isPresent(), is(true));
        assertThat(whereClause.docKeys().get(), containsInAnyOrder(isNullDocKey(), isDocKey("3"), isDocKey("7"), isDocKey("99")));
    }

    @Test
    public void testAnyEqConvertableArrayTypeLiterals() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select * from users where name = any([1, 2, 3])");
        assertThat(whereClause.query(), isFunction(AnyEqOperator.NAME, ImmutableList.<DataType>of(DataTypes.STRING, new ArrayType(DataTypes.STRING))));
    }

    @Test
    public void testAnyLikeConvertableArrayTypeLiterals() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select * from users where name like any([1, 2, 3])");
        assertThat(whereClause.query(), isFunction(AnyLikeOperator.NAME, ImmutableList.<DataType>of(DataTypes.STRING, new ArrayType(DataTypes.STRING))));
    }

    @Test
    public void testAnyLikeArrayLiteral() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select * from users where name like any(['a', 'b', 'c'])");
        assertThat(whereClause.query(), isFunction(AnyLikeOperator.NAME, ImmutableList.<DataType>of(DataTypes.STRING, new ArrayType(DataTypes.STRING))));
    }
}

<code block>


package io.crate.metadata.table;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableMap;
import io.crate.analyze.AlterPartitionedTableParameterInfo;
import io.crate.analyze.TableParameterInfo;
import io.crate.analyze.WhereClause;
import io.crate.exceptions.ColumnUnknownException;
import io.crate.metadata.*;
import io.crate.metadata.doc.DocIndexMetaData;
import io.crate.metadata.doc.DocSysColumns;
import io.crate.planner.RowGranularity;
import io.crate.planner.symbol.DynamicReference;
import io.crate.types.DataType;
import org.mockito.Answers;

import javax.annotation.Nullable;
import java.util.Collection;
import java.util.Iterator;
import java.util.List;
import java.util.Map;

import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;

public class TestingTableInfo extends AbstractDynamicTableInfo {

    private final Routing routing;
    private final ColumnIdent clusteredBy;

    public static Builder builder(TableIdent ident, RowGranularity granularity, Routing routing) {
        return new Builder(ident, granularity, routing);
    }

    public static class Builder {

        private final ImmutableList.Builder<ReferenceInfo> columns = ImmutableList.builder();
        private final ImmutableMap.Builder<ColumnIdent, ReferenceInfo> references = ImmutableMap.builder();
        private final ImmutableList.Builder<ReferenceInfo> partitionedByColumns = ImmutableList.builder();
        private final ImmutableList.Builder<ColumnIdent> primaryKey = ImmutableList.builder();
        private final ImmutableList.Builder<ColumnIdent> partitionedBy = ImmutableList.builder();
        private final ImmutableList.Builder<PartitionName> partitions = ImmutableList.builder();
        private final ImmutableMap.Builder<ColumnIdent, IndexReferenceInfo> indexColumns = ImmutableMap.builder();
        private ColumnIdent clusteredBy;


        private final RowGranularity granularity;
        private final TableIdent ident;
        private final Routing routing;
        private boolean isAlias = false;
        private ColumnPolicy columnPolicy = ColumnPolicy.DYNAMIC;

        private SchemaInfo schemaInfo = mock(SchemaInfo.class, Answers.RETURNS_MOCKS.get());

        public Builder(TableIdent ident, RowGranularity granularity, Routing routing) {
            this.granularity = granularity;
            this.routing = routing;
            this.ident = ident;
        }

        private ReferenceInfo genInfo(ColumnIdent columnIdent, DataType type) {
            return new ReferenceInfo(
                    new ReferenceIdent(ident, columnIdent.name(), columnIdent.path()),
                    RowGranularity.DOC, type
            );
        }

        private void addDocSysColumns() {
            for (Map.Entry<ColumnIdent, DataType> entry : DocSysColumns.COLUMN_IDENTS.entrySet()) {
                references.put(
                        entry.getKey(),
                        genInfo(entry.getKey(), entry.getValue())
                );
            }
        }

        public Builder add(String column, DataType type, List<String> path) {
            return add(column, type, path, ColumnPolicy.DYNAMIC);
        }
        public Builder add(String column, DataType type, List<String> path, ColumnPolicy columnPolicy) {
            return add(column, type, path, columnPolicy, ReferenceInfo.IndexType.NOT_ANALYZED, false);
        }
        public Builder add(String column, DataType type, List<String> path, ReferenceInfo.IndexType indexType) {
            return add(column, type, path, ColumnPolicy.DYNAMIC, indexType, false);
        }
        public Builder add(String column, DataType type, List<String> path,
                           boolean partitionBy) {
            return add(column, type, path, ColumnPolicy.DYNAMIC,
                    ReferenceInfo.IndexType.NOT_ANALYZED, partitionBy);
        }

        public Builder add(String column, DataType type, List<String> path,
                           ColumnPolicy columnPolicy, ReferenceInfo.IndexType indexType,
                           boolean partitionBy) {
            RowGranularity rowGranularity = granularity;
            if (partitionBy) {
                rowGranularity = RowGranularity.PARTITION;
            }
            ReferenceInfo info = new ReferenceInfo(new ReferenceIdent(ident, column, path),
                    rowGranularity, type, columnPolicy, indexType);
            if (info.ident().isColumn()) {
                columns.add(info);
            }
            references.put(info.ident().columnIdent(), info);
            if (partitionBy) {
                partitionedByColumns.add(info);
                partitionedBy.add(info.ident().columnIdent());
            }
            return this;
        }

        public Builder addIndex(ColumnIdent columnIdent, ReferenceInfo.IndexType indexType) {
            IndexReferenceInfo.Builder builder = new IndexReferenceInfo.Builder()
                    .ident(new ReferenceIdent(ident, columnIdent))
                    .indexType(indexType);
            indexColumns.put(columnIdent, builder.build());
            return this;
        }

        public Builder addPrimaryKey(String column) {
            primaryKey.add(ColumnIdent.fromPath(column));
            return this;
        }

        public Builder clusteredBy(String clusteredBy) {
            this.clusteredBy = ColumnIdent.fromPath(clusteredBy);
            return this;
        }

        public Builder isAlias(boolean isAlias) {
            this.isAlias = isAlias;
            return this;
        }

        public Builder schemaInfo(SchemaInfo schemaInfo) {
            this.schemaInfo = schemaInfo;
            return this;
        }

        public Builder addPartitions(String... partitionNames) {
            for (String partitionName : partitionNames) {
                PartitionName partition = PartitionName.fromString(partitionName, ident.schema(), ident.name());
                partitions.add(partition);
            }
            return this;
        }

        public TableInfo build() {
            addDocSysColumns();
            return new TestingTableInfo(
                    columns.build(),
                    partitionedByColumns.build(),
                    indexColumns.build(),
                    references.build(),
                    ident,
                    granularity,
                    routing,
                    primaryKey.build(),
                    clusteredBy,
                    isAlias,
                    partitionedBy.build(),
                    partitions.build(),
                    columnPolicy,
                    schemaInfo == null ? mock(SchemaInfo.class, Answers.RETURNS_MOCKS.get()) : schemaInfo);
        }

    }


    private final List<ReferenceInfo> columns;
    private final List<ReferenceInfo> partitionedByColumns;
    private final Map<ColumnIdent, IndexReferenceInfo> indexColumns;
    private final Map<ColumnIdent, ReferenceInfo> references;
    private final TableIdent ident;
    private final RowGranularity granularity;
    private final List<ColumnIdent> primaryKey;
    private final boolean isAlias;
    private final boolean hasAutoGeneratedPrimaryKey;
    private final List<ColumnIdent> partitionedBy;
    private final List<PartitionName> partitions;
    private final ColumnPolicy columnPolicy;
    private final TableParameterInfo tableParameterInfo;


    public TestingTableInfo(List<ReferenceInfo> columns,
                            List<ReferenceInfo> partitionedByColumns,
                            Map<ColumnIdent, IndexReferenceInfo> indexColumns,
                            Map<ColumnIdent, ReferenceInfo> references,
                            TableIdent ident, RowGranularity granularity,
                            Routing routing,
                            List<ColumnIdent> primaryKey,
                            ColumnIdent clusteredBy,
                            boolean isAlias,
                            List<ColumnIdent> partitionedBy,
                            List<PartitionName> partitions,
                            ColumnPolicy columnPolicy,
                            SchemaInfo schemaInfo
                            ) {
        super(schemaInfo);
        this.columns = columns;
        this.partitionedByColumns = partitionedByColumns;
        this.indexColumns = indexColumns;
        this.references = references;
        this.ident = ident;
        this.granularity = granularity;
        this.routing = routing;
        if (primaryKey == null || primaryKey.isEmpty()){
            this.primaryKey = ImmutableList.of(DocIndexMetaData.ID_IDENT);
            this.hasAutoGeneratedPrimaryKey = true;
        } else {
            this.primaryKey = primaryKey;
            this.hasAutoGeneratedPrimaryKey = false;
        }
        this.clusteredBy = clusteredBy;
        this.isAlias = isAlias;
        this.columnPolicy = columnPolicy;
        this.partitionedBy = partitionedBy;
        this.partitions = partitions;
        if (partitionedByColumns.isEmpty()) {
            tableParameterInfo = new TableParameterInfo();
        } else {
            tableParameterInfo = new AlterPartitionedTableParameterInfo();
        }
    }

    @Override
    public ReferenceInfo getReferenceInfo(ColumnIdent columnIdent) {
        return references.get(columnIdent);
    }

    @Override
    public Collection<ReferenceInfo> columns() {
        return columns;
    }


    @Override
    public List<ReferenceInfo> partitionedByColumns() {
        return partitionedByColumns;
    }

    @Override
    public IndexReferenceInfo indexColumn(ColumnIdent ident) {
        return indexColumns.get(ident);
    }

    @Override
    public boolean isPartitioned() {
        return !partitionedByColumns.isEmpty();
    }

    @Override
    public RowGranularity rowGranularity() {
        return granularity;
    }

    @Override
    public TableIdent ident() {
        return ident;
    }

    @Override
    public Routing getRouting(WhereClause whereClause, @Nullable String preference) {
        return routing;
    }

    @Override
    public List<ColumnIdent> primaryKey() {
        return primaryKey;
    }

    @Override
    public boolean hasAutoGeneratedPrimaryKey() {
        return hasAutoGeneratedPrimaryKey;
    }

    @Override
    public ColumnIdent clusteredBy() {
        return clusteredBy;
    }

    @Override
    public boolean isAlias() {
        return isAlias;
    }

    @Override
    public String[] concreteIndices() {
        return new String[]{ident.esName()};
    }

    @Override
    public DynamicReference getDynamic(ColumnIdent ident) {
        if (!ident.isColumn()) {
            ColumnIdent parentIdent = ident.getParent();
            ReferenceInfo parentInfo = getReferenceInfo(parentIdent);
            if (parentInfo != null && parentInfo.columnPolicy() == ColumnPolicy.STRICT) {
                throw new ColumnUnknownException(ident.sqlFqn());
            }
        }
        return new DynamicReference(new ReferenceIdent(ident(), ident), rowGranularity());
    }

    @Override
    public Iterator<ReferenceInfo> iterator() {
        return references.values().iterator();
    }

    @Override
    public List<ColumnIdent> partitionedBy() {
        return partitionedBy;
    }

    @Override
    public List<PartitionName> partitions() {
        return partitions;
    }

    @Override
    public ColumnPolicy columnPolicy() {
        return columnPolicy;
    }

    @Override
    public TableParameterInfo tableParameterInfo () {
        return tableParameterInfo;
    }

    @Override
    public SchemaInfo schemaInfo() {
        final SchemaInfo schemaInfo = super.schemaInfo();
        when(schemaInfo.name()).thenReturn(ident.schema());
        return schemaInfo;
    }
}

<code block>


package io.crate.lucene;

import com.google.common.collect.Sets;
import io.crate.analyze.WhereClause;
import io.crate.metadata.Functions;
import io.crate.operation.operator.*;
import io.crate.operation.operator.any.*;
import io.crate.planner.symbol.Literal;
import io.crate.planner.symbol.Reference;
import io.crate.test.integration.CrateUnitTest;
import io.crate.types.ArrayType;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import io.crate.types.SetType;
import org.apache.lucene.queries.BooleanFilter;
import org.apache.lucene.queries.TermsFilter;
import org.apache.lucene.sandbox.queries.regex.RegexQuery;
import org.apache.lucene.search.*;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.common.inject.ModulesBuilder;
import org.elasticsearch.common.lucene.search.MatchNoDocsQuery;
import org.elasticsearch.common.lucene.search.XConstantScoreQuery;
import org.elasticsearch.index.cache.IndexCache;
import org.elasticsearch.search.internal.SearchContext;
import org.junit.Before;
import org.junit.Test;
import org.mockito.Answers;

import java.util.Arrays;

import static io.crate.testing.TestingHelpers.*;
import static org.hamcrest.Matchers.instanceOf;
import static org.hamcrest.Matchers.is;
import static org.mockito.Mockito.mock;

public class LuceneQueryBuilderTest extends CrateUnitTest {

    private LuceneQueryBuilder builder;
    private SearchContext searchContext;
    private IndexCache indexCache;

    @Before
    public void prepare() throws Exception {
        Functions functions = new ModulesBuilder()
                .add(new OperatorModule()).createInjector().getInstance(Functions.class);
        builder = new LuceneQueryBuilder(functions);
        searchContext = mock(SearchContext.class, Answers.RETURNS_MOCKS.get());
        indexCache = mock(IndexCache.class, Answers.RETURNS_MOCKS.get());
    }

    @Test
    public void testNoMatchWhereClause() throws Exception {
        Query query = convert(WhereClause.NO_MATCH);
        assertThat(query, instanceOf(MatchNoDocsQuery.class));
    }

    @Test
    public void testWhereRefEqNullWithDifferentTypes() throws Exception {
        for (DataType type : DataTypes.PRIMITIVE_TYPES) {
            Reference foo = createReference("foo", type);
            Query query = convert(whereClause(EqOperator.NAME, foo, Literal.newLiteral(type, null)));





            assertThat(query, instanceOf(MatchNoDocsQuery.class));
        }
    }

    @Test
    public void testWhereRefEqRef() throws Exception {
        Reference foo = createReference("foo", DataTypes.STRING);
        Query query = convert(whereClause(EqOperator.NAME, foo, foo));
        assertThat(query, instanceOf(FilteredQuery.class));
    }

    @Test
    public void testLteQuery() throws Exception {
        Query query = convert(new WhereClause(createFunction(LteOperator.NAME,
                DataTypes.BOOLEAN,
                createReference("x", DataTypes.INTEGER),
                Literal.newLiteral(10))));
        assertThat(query, instanceOf(NumericRangeQuery.class));
        assertThat(query.toString(), is("x:{* TO 10]"));
    }

    @Test
    public void testEqOnTwoArraysBecomesGenericFunctionQuery() throws Exception {
        DataType longArray = new ArrayType(DataTypes.LONG);
        Query query = convert(new WhereClause(createFunction(EqOperator.NAME,
                DataTypes.BOOLEAN,
                createReference("x", longArray),
                Literal.newLiteral(longArray, new Object[] { 10L, null, 20L }))));
        assertThat(query, instanceOf(FilteredQuery.class));
        FilteredQuery filteredQuery = (FilteredQuery) query;

        assertThat(filteredQuery.getFilter(), instanceOf(BooleanFilter.class));
        assertThat(filteredQuery.getQuery(), instanceOf(XConstantScoreQuery.class));

        BooleanFilter filter = (BooleanFilter) filteredQuery.getFilter();
        assertThat(filter.clauses().get(0).getFilter(), instanceOf(BooleanFilter.class)); 
        assertThat(filter.clauses().get(1).getFilter(), instanceOf(Filter.class)); 
    }

    @Test
    public void testEqOnTwoArraysBecomesGenericFunctionQueryAllValuesNull() throws Exception {
        DataType longArray = new ArrayType(DataTypes.LONG);
        Query query = convert(new WhereClause(createFunction(EqOperator.NAME,
                DataTypes.BOOLEAN,
                createReference("x", longArray),
                Literal.newLiteral(longArray, new Object[] { null, null, null }))));
        assertThat(query, instanceOf(FilteredQuery.class));
    }

    @Test
    public void testEqOnArrayWithTooManyClauses() throws Exception {
        Object[] values = new Object[2000]; 
        Arrays.fill(values, 10L);
        DataType longArray = new ArrayType(DataTypes.LONG);
        Query query = convert(new WhereClause(createFunction(EqOperator.NAME,
                DataTypes.BOOLEAN,
                createReference("x", longArray),
                Literal.newLiteral(longArray, values))));
        assertThat(query, instanceOf(FilteredQuery.class));
    }

    @Test
    public void testGteQuery() throws Exception {
        Query query = convert(new WhereClause(createFunction(GteOperator.NAME,
                DataTypes.BOOLEAN,
                createReference("x", DataTypes.INTEGER),
                Literal.newLiteral(10))));
        assertThat(query, instanceOf(NumericRangeQuery.class));
        assertThat(query.toString(), is("x:[10 TO *}"));
    }

    @Test
    public void testWhereRefInSetLiteralIsConvertedToBooleanQuery() throws Exception {
        DataType dataType = new SetType(DataTypes.INTEGER);
        Reference foo = createReference("foo", DataTypes.INTEGER);
        WhereClause whereClause = new WhereClause(
                createFunction(InOperator.NAME, DataTypes.BOOLEAN,
                        foo,
                        Literal.newLiteral(dataType, Sets.newHashSet(1, 3))));
        Query query = convert(whereClause);
        assertThat(query, instanceOf(FilteredQuery.class));
        assertThat(((FilteredQuery)query).getFilter(), instanceOf(TermsFilter.class));
    }

    @Test
    public void testWhereStringRefInSetLiteralIsConvertedToBooleanQuery() throws Exception {
        DataType dataType = new SetType(DataTypes.STRING);
        Reference foo = createReference("foo", DataTypes.STRING);
        WhereClause whereClause = new WhereClause(
                createFunction(InOperator.NAME, DataTypes.BOOLEAN,
                        foo,
                        Literal.newLiteral(dataType, Sets.newHashSet(new BytesRef("foo"), new BytesRef("bar")))
                ));
        Query query = convert(whereClause);
        assertThat(query, instanceOf(FilteredQuery.class));
        assertThat(((FilteredQuery)query).getFilter(), instanceOf(TermsFilter.class));
    }


    @Test
    public void testRegexQueryFast() throws Exception {
        Reference value = createReference("foo", DataTypes.STRING);
        Literal pattern = Literal.newLiteral(new BytesRef("[a-z]"));
        Query query = convert(whereClause(RegexpMatchOperator.NAME, value, pattern));
        assertThat(query, instanceOf(RegexpQuery.class));
    }


    @Test
    public void testRegexQueryPcre() throws Exception {
        Reference value = createReference("foo", DataTypes.STRING);
        Literal pattern = Literal.newLiteral(new BytesRef("\\D"));
        Query query = convert(whereClause(RegexpMatchOperator.NAME, value, pattern));
        assertThat(query, instanceOf(RegexQuery.class));
    }

    @Test
    public void testAnyEqArrayLiteral() throws Exception {
        Reference ref = createReference("d", DataTypes.DOUBLE);
        Literal doubleArrayLiteral = Literal.newLiteral(new Object[]{-1.5d, 0.0d, 1.5d}, new ArrayType(DataTypes.DOUBLE));
        Query query = convert(whereClause(AnyEqOperator.NAME, ref, doubleArrayLiteral));
        assertThat(query, instanceOf(FilteredQuery.class));
        assertThat(((FilteredQuery)query).getFilter(), instanceOf(TermsFilter.class));
    }

    @Test
    public void testAnyEqArrayReference() throws Exception {
        Reference ref = createReference("d_array", new ArrayType(DataTypes.DOUBLE));
        Literal doubleLiteral = Literal.newLiteral(1.5d);
        Query query = convert(whereClause(AnyEqOperator.NAME, doubleLiteral, ref));
        assertThat(query.toString(), is("d_array:[1.5 TO 1.5]"));
    }

    @Test
    public void testAnyGreaterAndSmaller() throws Exception {

        Reference arrayRef = createReference("d_array", new ArrayType(DataTypes.DOUBLE));
        Literal doubleLiteral = Literal.newLiteral(1.5d);

        Reference ref = createReference("d", DataTypes.DOUBLE);
        Literal arrayLiteral = Literal.newLiteral(new Object[]{1.2d, 3.5d}, new ArrayType(DataTypes.DOUBLE));


        Query ltQuery = convert(whereClause(AnyLtOperator.NAME, doubleLiteral, arrayRef));
        assertThat(ltQuery.toString(), is("d_array:{1.5 TO *}"));


        Query ltQuery2 = convert(whereClause(AnyLtOperator.NAME, ref, arrayLiteral));
        assertThat(ltQuery2.toString(), is("(d:{* TO 1.2} d:{* TO 3.5})~1"));


        Query lteQuery = convert(whereClause(AnyLteOperator.NAME, doubleLiteral, arrayRef));
        assertThat(lteQuery.toString(), is("d_array:[1.5 TO *}"));


        Query lteQuery2 = convert(whereClause(AnyLteOperator.NAME, ref, arrayLiteral));
        assertThat(lteQuery2.toString(), is("(d:{* TO 1.2] d:{* TO 3.5])~1"));


        Query gtQuery = convert(whereClause(AnyGtOperator.NAME, doubleLiteral, arrayRef));
        assertThat(gtQuery.toString(), is("d_array:{* TO 1.5}"));


        Query gtQuery2 = convert(whereClause(AnyGtOperator.NAME, ref, arrayLiteral));
        assertThat(gtQuery2.toString(), is("(d:{1.2 TO *} d:{3.5 TO *})~1"));


        Query gteQuery = convert(whereClause(AnyGteOperator.NAME, doubleLiteral, arrayRef));
        assertThat(gteQuery.toString(), is("d_array:{* TO 1.5]"));


        Query gteQuery2 = convert(whereClause(AnyGteOperator.NAME, ref, arrayLiteral));
        assertThat(gteQuery2.toString(), is("(d:[1.2 TO *} d:[3.5 TO *})~1"));
    }

    @Test
    public void testAnyOnArrayLiteral() throws Exception {
        Reference ref = createReference("d", DataTypes.STRING);
        Literal stringArrayLiteral = Literal.newLiteral(new Object[]{new BytesRef("a"), new BytesRef("b"), new BytesRef("c")}, new ArrayType(DataTypes.STRING));


        Query neqQuery = convert(whereClause(AnyNeqOperator.NAME, ref, stringArrayLiteral));
        assertThat(neqQuery, instanceOf(FilteredQuery.class));
        assertThat(((FilteredQuery)neqQuery).getFilter(), instanceOf(BooleanFilter.class));
        BooleanFilter filter = (BooleanFilter)((FilteredQuery) neqQuery).getFilter();
        assertThat(filter.toString(), is("BooleanFilter(-BooleanFilter(+d:a +d:b +d:c))"));


        Query likeQuery = convert(whereClause(AnyLikeOperator.NAME, ref, stringArrayLiteral));
        assertThat(likeQuery, instanceOf(BooleanQuery.class));
        BooleanQuery likeBQuery = (BooleanQuery)likeQuery;
        assertThat(likeBQuery.clauses().size(), is(3));
        for (int i = 0; i < 2; i++) {
            assertThat(likeBQuery.clauses().get(i).getQuery(), instanceOf(WildcardQuery.class));
        }


        Query notLikeQuery = convert(whereClause(AnyNotLikeOperator.NAME, ref, stringArrayLiteral));
        assertThat(notLikeQuery, instanceOf(BooleanQuery.class));
        BooleanQuery notLikeBQuery = (BooleanQuery)notLikeQuery;
        assertThat(notLikeBQuery.toString(), is("-(+d:a +d:b +d:c)"));



        Query ltQuery2 = convert(whereClause(AnyLtOperator.NAME, ref, stringArrayLiteral));
        assertThat(ltQuery2, instanceOf(BooleanQuery.class));
        BooleanQuery ltBQuery = (BooleanQuery)ltQuery2;
        assertThat(ltBQuery.toString(), is("(d:{* TO a} d:{* TO b} d:{* TO c})~1"));
    }

    private Query convert(WhereClause clause) {
        return builder.convert(clause, searchContext, indexCache).query;
    }
}
<code block>


package io.crate.analyze;

import com.google.common.base.MoreObjects;
import io.crate.metadata.ColumnIdent;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.common.Base64;
import org.elasticsearch.common.Nullable;
import org.elasticsearch.common.Strings;
import org.elasticsearch.common.bytes.BytesReference;
import org.elasticsearch.common.io.stream.BytesStreamOutput;
import org.elasticsearch.common.io.stream.StreamOutput;
import org.elasticsearch.common.lucene.BytesRefs;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

public class Id {

    private final List<BytesRef> values;
    private String routingValue;


    private String singleStringValue = null;

    public Id(List<ColumnIdent> primaryKeys, List<BytesRef> primaryKeyValues,
              ColumnIdent clusteredBy) {
        this(primaryKeys, primaryKeyValues, clusteredBy, true);
    }

    public Id(List<ColumnIdent> primaryKeys, List<BytesRef> primaryKeyValues,
              ColumnIdent clusteredBy, boolean create) {
        values = new ArrayList<>(primaryKeys.size());
        if ((primaryKeys.isEmpty() || (primaryKeys.size() == 1 && primaryKeys.get(0).name().equals("_id"))) && create) {
            singleStringValue = Strings.base64UUID();
        } else {
            singleStringValue = null;
            if (primaryKeys.size() != primaryKeyValues.size()) {

                if (create) {
                    throw new UnsupportedOperationException("Missing required primary key values");
                }
                return;
            }
            for (int i=0; i<primaryKeys.size(); i++)  {
                BytesRef primaryKeyValue = primaryKeyValues.get(i);
                if (primaryKeyValue == null) {

                    return;
                }
                if (primaryKeys.get(i).equals(clusteredBy)) {

                    values.add(0, primaryKeyValue);
                    routingValue = primaryKeyValue.utf8ToString();
                } else {
                    values.add(primaryKeyValue);
                }
            }
        }

    }

    public String routingValue(){
        return MoreObjects.firstNonNull(routingValue, stringValue());
    }

    public boolean isValid() {
        return values.size() > 0 || singleStringValue != null;
    }

    private void encodeValues(StreamOutput out) throws IOException {
        out.writeVInt(values.size());
        for (BytesRef value : values) {
            out.writeBytesRef(value);
        }
    }

    private BytesReference bytes() {
        assert values.size() > 0;
        BytesStreamOutput out = new BytesStreamOutput(estimateSize(values));
        try {
            encodeValues(out);
            out.close();
        } catch (IOException e) {

        }
        return out.bytes();
    }


    private int estimateSize(List<BytesRef> values) {
        int expectedEncodedSize = 0;
        for (BytesRef value : values) {

            expectedEncodedSize += 5 + (value != null ? value.length : 0);
        }
        return expectedEncodedSize;
    }

    @Nullable
    public String stringValue() {
        if (singleStringValue != null) {
            return singleStringValue;
        }
        if (values.size() == 0) {
            return null;
        } else if (values.size() == 1) {
            return BytesRefs.toString(values.get(0));
        }
        return Base64.encodeBytes(bytes().toBytes());
    }

    @Nullable
    public String toString() {
        return stringValue();
    }

    public List<BytesRef> values() {
        if (singleStringValue != null && values.size() == 0) {

            values.add(new BytesRef(singleStringValue));
        }
        return values;
    }
}

<code block>


package io.crate.analyze;

import com.google.common.base.Preconditions;
import io.crate.Constants;
import io.crate.exceptions.InvalidColumnNameException;
import io.crate.metadata.ColumnIdent;
import io.crate.metadata.ReferenceIdent;
import io.crate.metadata.ReferenceInfo;
import io.crate.planner.symbol.Reference;
import io.crate.sql.tree.DefaultTraversalVisitor;
import io.crate.sql.tree.Insert;
import io.crate.sql.tree.Node;

import java.util.ArrayList;

public abstract class AbstractInsertAnalyzer extends DefaultTraversalVisitor<AbstractInsertAnalyzedStatement, Analysis> {

    protected final AnalysisMetaData analysisMetaData;

    protected AbstractInsertAnalyzer(AnalysisMetaData analysisMetaData) {
        this.analysisMetaData = analysisMetaData;
    }

    protected void handleInsertColumns(Insert node, int maxInsertValues, AbstractInsertAnalyzedStatement context) {

        int numColumns;

        if (node.columns().size() == 0) { 
            numColumns = context.tableInfo().columns().size();
            if (maxInsertValues > numColumns) {
                throw new IllegalArgumentException("too many values");
            }
            context.columns(new ArrayList<Reference>(numColumns));

            int i = 0;
            for (ReferenceInfo columnInfo : context.tableInfo().columns()) {
                if (i >= maxInsertValues) {
                    break;
                }
                addColumn(columnInfo.ident().columnIdent().name(), context, i);
                i++;
            }

        } else {
            numColumns = node.columns().size();
            context.columns(new ArrayList<Reference>(numColumns));
            if (maxInsertValues > node.columns().size()) {
                throw new IllegalArgumentException("too few values");
            }
            for (int i = 0; i < node.columns().size(); i++) {
                addColumn(node.columns().get(i), context, i);
            }
        }

        if (context.primaryKeyColumnIndices().size() == 0 &&
                !(context.tableInfo().primaryKey().isEmpty() || context.tableInfo().hasAutoGeneratedPrimaryKey())) {
            throw new IllegalArgumentException("Primary key is required but is missing from the insert statement");
        }
        ColumnIdent clusteredBy = context.tableInfo().clusteredBy();
        if (clusteredBy != null && !clusteredBy.name().equalsIgnoreCase("_id") && context.routingColumnIndex() < 0) {
            throw new IllegalArgumentException("Clustered by value is required but is missing from the insert statement");
        }
    }


    protected Reference addColumn(String column, AbstractInsertAnalyzedStatement context, int i) {
        assert context.tableInfo() != null;
        return addColumn(new ReferenceIdent(context.tableInfo().ident(), column), context, i);
    }


    protected Reference addColumn(ReferenceIdent ident, AbstractInsertAnalyzedStatement context, int i) {
        final ColumnIdent columnIdent = ident.columnIdent();
        Preconditions.checkArgument(!columnIdent.name().startsWith("_"), "Inserting system columns is not allowed");
        if (Constants.INVALID_COLUMN_NAME_PREDICATE.apply(columnIdent.name())) {
            throw new InvalidColumnNameException(columnIdent.name());
        }


        for (ColumnIdent pkIdent : context.tableInfo().primaryKey()) {
            if (pkIdent.getRoot().equals(columnIdent)) {
                context.addPrimaryKeyColumnIdx(i);
            }
        }


        for (ColumnIdent partitionIdent : context.tableInfo().partitionedBy()) {
            if (partitionIdent.getRoot().equals(columnIdent)) {
                context.addPartitionedByIndex(i);
            }
        }


        ColumnIdent routing = context.tableInfo().clusteredBy();
        if (routing != null && (routing.equals(columnIdent) || routing.isChildOf(columnIdent))) {
            context.routingColumnIndex(i);
        }


        Reference columnReference = context.allocateUniqueReference(ident);
        context.columns().add(columnReference);
        return columnReference;
    }

    public AnalyzedStatement analyze(Node node, Analysis analysis) {
        analysis.expectsAffectedRows(true);
        return super.process(node, analysis);
    }
}

<code block>


package io.crate.metadata.doc;

import com.google.common.base.MoreObjects;
import com.google.common.collect.*;
import io.crate.Constants;
import io.crate.analyze.TableParameterInfo;
import io.crate.core.NumberOfReplicas;
import io.crate.exceptions.TableAliasSchemaException;
import io.crate.metadata.*;
import io.crate.metadata.table.ColumnPolicy;
import io.crate.planner.RowGranularity;
import io.crate.types.ArrayType;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.action.admin.indices.alias.Alias;
import org.elasticsearch.action.admin.indices.template.put.PutIndexTemplateRequest;
import org.elasticsearch.action.admin.indices.template.put.TransportPutIndexTemplateAction;
import org.elasticsearch.cluster.metadata.IndexMetaData;
import org.elasticsearch.cluster.metadata.MappingMetaData;
import org.elasticsearch.common.Booleans;
import org.elasticsearch.common.collect.Tuple;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.common.xcontent.XContentHelper;

import java.io.IOException;
import java.util.*;

public class DocIndexMetaData {

    private static final String ID = "_id";
    public static final ColumnIdent ID_IDENT = new ColumnIdent(ID);
    private final IndexMetaData metaData;

    private final MappingMetaData defaultMappingMetaData;
    private final Map<String, Object> defaultMappingMap;

    private final Map<ColumnIdent, IndexReferenceInfo.Builder> indicesBuilder = new HashMap<>();

    private final ImmutableSortedSet.Builder<ReferenceInfo> columnsBuilder = ImmutableSortedSet.orderedBy(new Comparator<ReferenceInfo>() {
        @Override
        public int compare(ReferenceInfo o1, ReferenceInfo o2) {
            return o1.ident().columnIdent().fqn().compareTo(o2.ident().columnIdent().fqn());
        }
    });


    private final ImmutableMap.Builder<ColumnIdent, ReferenceInfo> referencesBuilder = ImmutableSortedMap.naturalOrder();
    private final ImmutableList.Builder<ReferenceInfo> partitionedByColumnsBuilder = ImmutableList.builder();

    private final TableIdent ident;
    private final int numberOfShards;
    private final BytesRef numberOfReplicas;
    private final ImmutableMap<String, Object> tableParameters;
    private Map<String, Object> metaMap;
    private Map<String, Object> metaColumnsMap;
    private Map<String, Object> indicesMap;
    private List<List<String>> partitionedByList;
    private ImmutableList<ReferenceInfo> columns;
    private ImmutableMap<ColumnIdent, IndexReferenceInfo> indices;
    private ImmutableList<ReferenceInfo> partitionedByColumns;
    private ImmutableMap<ColumnIdent, ReferenceInfo> references;
    private ImmutableList<ColumnIdent> primaryKey;
    private ColumnIdent routingCol;
    private ImmutableList<ColumnIdent> partitionedBy;
    private final boolean isAlias;
    private final Set<String> aliases;
    private boolean hasAutoGeneratedPrimaryKey = false;

    private ColumnPolicy columnPolicy = ColumnPolicy.DYNAMIC;

    private final static ImmutableMap<String, DataType> dataTypeMap = ImmutableMap.<String, DataType>builder()
            .put("date", DataTypes.TIMESTAMP)
            .put("string", DataTypes.STRING)
            .put("boolean", DataTypes.BOOLEAN)
            .put("byte", DataTypes.BYTE)
            .put("short", DataTypes.SHORT)
            .put("integer", DataTypes.INTEGER)
            .put("long", DataTypes.LONG)
            .put("float", DataTypes.FLOAT)
            .put("double", DataTypes.DOUBLE)
            .put("ip", DataTypes.IP)
            .put("geo_point", DataTypes.GEO_POINT)
            .put("object", DataTypes.OBJECT)
            .put("nested", DataTypes.OBJECT).build();

    public DocIndexMetaData(IndexMetaData metaData, TableIdent ident) throws IOException {
        this.ident = ident;
        this.metaData = metaData;
        this.isAlias = !metaData.getIndex().equals(ident.esName());
        this.numberOfShards = metaData.numberOfShards();
        final Settings settings = metaData.getSettings();
        this.numberOfReplicas = NumberOfReplicas.fromSettings(settings);
        this.aliases = ImmutableSet.copyOf(metaData.aliases().keys().toArray(String.class));
        this.defaultMappingMetaData = this.metaData.mappingOrDefault(Constants.DEFAULT_MAPPING_TYPE);
        if (defaultMappingMetaData == null) {
            this.defaultMappingMap = new HashMap<>();
        } else {
            this.defaultMappingMap = this.defaultMappingMetaData.sourceAsMap();
        }
        this.tableParameters = TableParameterInfo.tableParametersFromIndexMetaData(metaData);

        prepareCrateMeta();
    }

    @SuppressWarnings("unchecked")
    private static <T> T getNested(Map map, String key) {
        return (T) map.get(key);
    }

    private void prepareCrateMeta() {
        metaMap = getNested(defaultMappingMap, "_meta");
        if (metaMap != null) {
            indicesMap = getNested(metaMap, "indices");
            if (indicesMap == null) {
                indicesMap = ImmutableMap.of();
            }
            metaColumnsMap = getNested(metaMap, "columns");
            if (metaColumnsMap == null) {
                metaColumnsMap = ImmutableMap.of();
            }

            partitionedByList = getNested(metaMap, "partitioned_by");
            if (partitionedByList == null) {
                partitionedByList = ImmutableList.of();
            }
        } else {
            metaMap = new HashMap<>();
            indicesMap = new HashMap<>();
            metaColumnsMap = new HashMap<>();
            partitionedByList = ImmutableList.of();
        }
    }

    private void addPartitioned(ColumnIdent column, DataType type) {
        add(column, type, ColumnPolicy.DYNAMIC, ReferenceInfo.IndexType.NOT_ANALYZED, true);
    }

    private void add(ColumnIdent column, DataType type, ReferenceInfo.IndexType indexType) {
        add(column, type, ColumnPolicy.DYNAMIC, indexType, false);
    }

    private void add(ColumnIdent column, DataType type, ColumnPolicy columnPolicy,
                     ReferenceInfo.IndexType indexType, boolean partitioned) {
        ReferenceInfo info = newInfo(column, type, columnPolicy, indexType);

        if (partitioned || !(partitionedBy != null && partitionedBy.contains(column))) {
            if (info.ident().isColumn()) {
                columnsBuilder.add(info);
            }
            referencesBuilder.put(info.ident().columnIdent(), info);
        }
        if (partitioned) {
            partitionedByColumnsBuilder.add(info);
        }
    }

    private ReferenceInfo newInfo(ColumnIdent column,
                                  DataType type,
                                  ColumnPolicy columnPolicy,
                                  ReferenceInfo.IndexType indexType) {
        RowGranularity granularity = RowGranularity.DOC;
        if (partitionedBy.contains(column)) {
            granularity = RowGranularity.PARTITION;
        }
        return new ReferenceInfo(new ReferenceIdent(ident, column), granularity, type,
                columnPolicy, indexType);
    }


    public static DataType getColumnDataType(Map<String, Object> columnProperties) {
        DataType type;
        String typeName = (String) columnProperties.get("type");

        if (typeName == null) {
            if (columnProperties.containsKey("properties")) {
                type = DataTypes.OBJECT;
            } else {
                return DataTypes.NOT_SUPPORTED;
            }
        } else if (typeName.equalsIgnoreCase("array")) {

            Map<String, Object> innerProperties = getNested(columnProperties, "inner");
            DataType innerType = getColumnDataType(innerProperties);
            type = new ArrayType(innerType);
        } else {
            typeName = typeName.toLowerCase(Locale.ENGLISH);
            type = MoreObjects.firstNonNull(dataTypeMap.get(typeName), DataTypes.NOT_SUPPORTED);
        }
        return type;
    }

    private ReferenceInfo.IndexType getColumnIndexType(Map<String, Object> columnProperties) {
        String indexType = (String) columnProperties.get("index");
        String analyzerName = (String) columnProperties.get("analyzer");
        if (indexType != null) {
            if (indexType.equals(ReferenceInfo.IndexType.NOT_ANALYZED.toString())) {
                return ReferenceInfo.IndexType.NOT_ANALYZED;
            } else if (indexType.equals(ReferenceInfo.IndexType.NO.toString())) {
                return ReferenceInfo.IndexType.NO;
            } else if (indexType.equals(ReferenceInfo.IndexType.ANALYZED.toString())
                    && analyzerName != null && !analyzerName.equals("keyword")) {
                return ReferenceInfo.IndexType.ANALYZED;
            }
        } 
        else if (analyzerName != null && !analyzerName.equals("keyword")) {
            return ReferenceInfo.IndexType.ANALYZED;
        }
        return ReferenceInfo.IndexType.NOT_ANALYZED;
    }

    private ColumnIdent childIdent(ColumnIdent ident, String name) {
        if (ident == null) {
            return new ColumnIdent(name);
        }
        if (ident.isColumn()) {
            return new ColumnIdent(ident.name(), name);
        } else {
            ImmutableList.Builder<String> builder = ImmutableList.builder();
            for (String s : ident.path()) {
                builder.add(s);
            }
            builder.add(name);
            return new ColumnIdent(ident.name(), builder.build());
        }
    }


    @SuppressWarnings("unchecked")
    private void internalExtractColumnDefinitions(ColumnIdent columnIdent,
                                                  Map<String, Object> propertiesMap) {
        if (propertiesMap == null) {
            return;
        }

        for (Map.Entry<String, Object> columnEntry : propertiesMap.entrySet()) {
            Map<String, Object> columnProperties = (Map) columnEntry.getValue();
            DataType columnDataType = getColumnDataType(columnProperties);
            ColumnIdent newIdent = childIdent(columnIdent, columnEntry.getKey());

            columnProperties = furtherColumnProperties(columnProperties);
            ReferenceInfo.IndexType columnIndexType = getColumnIndexType(columnProperties);
            if (columnDataType == DataTypes.OBJECT
                    || (columnDataType.id() == ArrayType.ID
                    && ((ArrayType) columnDataType).innerType() == DataTypes.OBJECT)) {
                ColumnPolicy columnPolicy =
                        ColumnPolicy.of(columnProperties.get("dynamic"));
                add(newIdent, columnDataType, columnPolicy, ReferenceInfo.IndexType.NO, false);

                if (columnProperties.get("properties") != null) {

                    internalExtractColumnDefinitions(newIdent, (Map<String, Object>) columnProperties.get("properties"));
                }
            } else if (columnDataType != DataTypes.NOT_SUPPORTED) {
                List<String> copyToColumns = getNested(columnProperties, "copy_to");


                if (copyToColumns != null) {
                    for (String copyToColumn : copyToColumns) {
                        ColumnIdent targetIdent = ColumnIdent.fromPath(copyToColumn);
                        IndexReferenceInfo.Builder builder = getOrCreateIndexBuilder(targetIdent);
                        builder.addColumn(newInfo(newIdent, columnDataType, ColumnPolicy.DYNAMIC, columnIndexType));
                    }
                }

                if (indicesMap.containsKey(newIdent.fqn())) {
                    IndexReferenceInfo.Builder builder = getOrCreateIndexBuilder(newIdent);
                    builder.indexType(columnIndexType)
                            .ident(new ReferenceIdent(ident, newIdent));
                } else {
                    add(newIdent, columnDataType, columnIndexType);
                }
            }
        }
    }


    private Map<String, Object> furtherColumnProperties(Map<String, Object> columnProperties) {
        if (columnProperties.get("inner") != null) {
            return (Map<String, Object>) columnProperties.get("inner");
        } else {
            return columnProperties;
        }
    }

    private IndexReferenceInfo.Builder getOrCreateIndexBuilder(ColumnIdent ident) {
        IndexReferenceInfo.Builder builder = indicesBuilder.get(ident);
        if (builder == null) {
            builder = new IndexReferenceInfo.Builder();
            indicesBuilder.put(ident, builder);
        }
        return builder;
    }

    private ImmutableList<ColumnIdent> getPrimaryKey() {
        Map<String, Object> metaMap = getNested(defaultMappingMap, "_meta");
        if (metaMap != null) {
            ImmutableList.Builder<ColumnIdent> builder = ImmutableList.builder();
            Object pKeys = metaMap.get("primary_keys");
            if (pKeys != null) {
                if (pKeys instanceof String) {
                    builder.add(ColumnIdent.fromPath((String) pKeys));
                    return builder.build();
                } else if (pKeys instanceof Collection) {
                    Collection keys = (Collection) pKeys;
                    if (!keys.isEmpty()) {
                        for (Object pkey : keys) {
                            builder.add(ColumnIdent.fromPath(pkey.toString()));
                        }
                        return builder.build();
                    }
                }
            }
        }
        if (getCustomRoutingCol() == null && partitionedByList.isEmpty()) {
            hasAutoGeneratedPrimaryKey = true;
            return ImmutableList.of(ID_IDENT);
        }
        return ImmutableList.of();
    }

    private ImmutableList<ColumnIdent> getPartitionedBy() {
        ImmutableList.Builder<ColumnIdent> builder = ImmutableList.builder();
        for (List<String> partitionedByInfo : partitionedByList) {
            builder.add(ColumnIdent.fromPath(partitionedByInfo.get(0)));
        }
        return builder.build();
    }

    private ColumnPolicy getColumnPolicy() {
        Object dynamic = getNested(defaultMappingMap, "dynamic");
        if (ColumnPolicy.STRICT.value().equals(String.valueOf(dynamic).toLowerCase(Locale.ENGLISH))) {
            return ColumnPolicy.STRICT;
        } else if (Booleans.isExplicitFalse(String.valueOf(dynamic))) {
            return ColumnPolicy.IGNORED;
        } else {
            return ColumnPolicy.DYNAMIC;
        }
    }

    private void createColumnDefinitions() {
        Map<String, Object> propertiesMap = getNested(defaultMappingMap, "properties");
        internalExtractColumnDefinitions(null, propertiesMap);
        extractPartitionedByColumns();
    }

    private ImmutableMap<ColumnIdent, IndexReferenceInfo> createIndexDefinitions() {
        ImmutableMap.Builder<ColumnIdent, IndexReferenceInfo> builder = ImmutableMap.builder();
        for (Map.Entry<ColumnIdent, IndexReferenceInfo.Builder> entry : indicesBuilder.entrySet()) {
            builder.put(entry.getKey(), entry.getValue().build());
        }
        indices = builder.build();
        return indices;
    }

    private void extractPartitionedByColumns() {
        for (Tuple<ColumnIdent, DataType> partitioned : PartitionedByMappingExtractor.extractPartitionedByColumns(partitionedByList)) {
            addPartitioned(partitioned.v1(), partitioned.v2());
        }
    }

    private ColumnIdent getCustomRoutingCol(){
        if (defaultMappingMetaData != null) {
            Map<String, Object> metaMap = getNested(defaultMappingMap, "_meta");
            if (metaMap != null) {
                String routingPath = (String) metaMap.get("routing");
                if (routingPath != null && !routingPath.equals(ID)) {
                    return ColumnIdent.fromPath(routingPath);
                }
            }
        }
        return null;
    }

    private ColumnIdent getRoutingCol() {
        ColumnIdent col = getCustomRoutingCol();
        if (col != null){
            return col;
        }
        if (primaryKey.size() == 1) {
            return primaryKey.get(0);
        }
        return ID_IDENT;
    }

    public DocIndexMetaData build() {
        partitionedBy = getPartitionedBy();
        columnPolicy = getColumnPolicy();
        createColumnDefinitions();
        indices = createIndexDefinitions();
        columns = ImmutableList.copyOf(columnsBuilder.build());
        partitionedByColumns = partitionedByColumnsBuilder.build();

        for (Tuple<ColumnIdent, ReferenceInfo> sysColumn : DocSysColumns.forTable(ident)) {
            referencesBuilder.put(sysColumn.v1(), sysColumn.v2());
        }
        references = referencesBuilder.build();
        primaryKey = getPrimaryKey();
        routingCol = getRoutingCol();
        return this;
    }

    public ImmutableMap<ColumnIdent, ReferenceInfo> references() {
        return references;
    }

    public ImmutableList<ReferenceInfo> columns() {
        return columns;
    }

    public ImmutableMap<ColumnIdent, IndexReferenceInfo> indices() {
        return indices;
    }

    public ImmutableList<ReferenceInfo> partitionedByColumns() {
        return partitionedByColumns;
    }

    public ImmutableList<ColumnIdent> primaryKey() {
        return primaryKey;
    }

    public ColumnIdent routingCol() {
        return routingCol;
    }


    public boolean schemaEquals(DocIndexMetaData other) {
        if (this == other) return true;
        if (other == null) return false;



        if (columns != null ? !columns.equals(other.columns) : other.columns != null) return false;
        if (primaryKey != null ? !primaryKey.equals(other.primaryKey) : other.primaryKey != null) return false;
        if (indices != null ? !indices.equals(other.indices) : other.indices != null) return false;
        if (references != null ? !references.equals(other.references) : other.references != null) return false;
        if (routingCol != null ? !routingCol.equals(other.routingCol) : other.routingCol != null) return false;

        return true;
    }

    protected DocIndexMetaData merge(DocIndexMetaData other,
                                     TransportPutIndexTemplateAction transportPutIndexTemplateAction,
                                     boolean thisIsCreatedFromTemplate) throws IOException {
        if (schemaEquals(other)) {
            return this;
        } else if (thisIsCreatedFromTemplate) {
            if (this.references.size() < other.references.size()) {



                updateTemplate(other, transportPutIndexTemplateAction, this.metaData.settings());

                return new DocIndexMetaData(IndexMetaData.builder(other.metaData).settings(this.metaData.settings()).build(), other.ident).build();
            } else if (references().size() == other.references().size() &&
                    !references().keySet().equals(other.references().keySet())) {
                XContentHelper.update(defaultMappingMap, other.defaultMappingMap, false);

                updateTemplate(this, transportPutIndexTemplateAction, this.metaData.settings());
                return this;
            }

            return this;
        } else {
            throw new TableAliasSchemaException(other.ident.name());
        }
    }

    private void updateTemplate(DocIndexMetaData md,
                                TransportPutIndexTemplateAction transportPutIndexTemplateAction,
                                Settings updateSettings) {
        String templateName = PartitionName.templateName(ident.schema(), ident.name());
        PutIndexTemplateRequest request = new PutIndexTemplateRequest(templateName)
                .mapping(Constants.DEFAULT_MAPPING_TYPE, md.defaultMappingMap)
                .create(false)
                .settings(updateSettings)
                .template(templateName + "*");
        for (String alias : md.aliases()) {
            request = request.alias(new Alias(alias));
        }
        transportPutIndexTemplateAction.execute(request);
    }


    public String concreteIndexName() {
        return metaData.index();
    }

    public boolean isAlias() {
        return isAlias;
    }

    public Set<String> aliases() {
        return aliases;
    }

    public boolean hasAutoGeneratedPrimaryKey() {
        return hasAutoGeneratedPrimaryKey;
    }

    public int numberOfShards() {
        return numberOfShards;
    }

    public BytesRef numberOfReplicas() {
        return numberOfReplicas;
    }

    public ImmutableList<ColumnIdent> partitionedBy() {
        return partitionedBy;
    }

    public ColumnPolicy columnPolicy() {
        return columnPolicy;
    }

    public ImmutableMap<String, Object> tableParameters() {
        return tableParameters;
    }
}

<code block>
package io.crate.metadata.doc;

import com.google.common.collect.ImmutableMap;
import io.crate.metadata.ColumnIdent;
import io.crate.metadata.ReferenceIdent;
import io.crate.metadata.ReferenceInfo;
import io.crate.metadata.TableIdent;
import io.crate.planner.RowGranularity;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.elasticsearch.common.collect.Tuple;

import java.util.ArrayList;
import java.util.List;
import java.util.Map;

public class DocSysColumns {


    public static final ColumnIdent ID = new ColumnIdent("_id");
    public static final ColumnIdent VERSION = new ColumnIdent("_version");
    public static final ColumnIdent SCORE = new ColumnIdent("_score");
    public static final ColumnIdent UID = new ColumnIdent("_uid");
    public static final ColumnIdent DOC = new ColumnIdent("_doc");
    public static final ColumnIdent RAW = new ColumnIdent("_raw");
    public static final ColumnIdent DOCID = new ColumnIdent("_docid");

    public static final ImmutableMap<ColumnIdent, DataType> COLUMN_IDENTS = ImmutableMap.<ColumnIdent, DataType>builder()
            .put(ID, DataTypes.STRING)
            .put(VERSION, DataTypes.LONG)
            .put(SCORE, DataTypes.FLOAT)
            .put(UID, DataTypes.STRING)
            .put(DOC, DataTypes.OBJECT)
            .put(RAW, DataTypes.STRING)
            .put(DOCID, DataTypes.LONG)
            .build();

    private static ReferenceInfo newInfo(TableIdent table, ColumnIdent column, DataType dataType) {
        return new ReferenceInfo(new ReferenceIdent(table, column), RowGranularity.DOC, dataType);
    }

    public static List<Tuple<ColumnIdent, ReferenceInfo>> forTable(TableIdent tableIdent) {
        List<Tuple<ColumnIdent, ReferenceInfo>> columns = new ArrayList<>(COLUMN_IDENTS.size());
        for (Map.Entry<ColumnIdent, DataType> entry : COLUMN_IDENTS.entrySet()) {
            columns.add(new Tuple<>(entry.getKey(), newInfo(tableIdent, entry.getKey(), entry.getValue())));
        }
        return columns;
    }

    public static ReferenceInfo forTable(TableIdent table, ColumnIdent column){
        return newInfo(table, column, COLUMN_IDENTS.get(column));
    }

}

<code block>


package io.crate.lucene;

import com.google.common.base.Preconditions;
import com.google.common.base.Predicates;
import com.google.common.base.Throwables;
import com.google.common.collect.ImmutableMap;
import com.google.common.collect.Iterables;
import com.spatial4j.core.context.jts.JtsSpatialContext;
import com.spatial4j.core.shape.Rectangle;
import com.spatial4j.core.shape.Shape;
import com.vividsolutions.jts.geom.Coordinate;
import com.vividsolutions.jts.geom.Geometry;
import io.crate.Constants;
import io.crate.analyze.WhereClause;
import io.crate.exceptions.UnsupportedFeatureException;
import io.crate.lucene.match.MatchQueryBuilder;
import io.crate.lucene.match.MultiMatchQueryBuilder;
import io.crate.metadata.DocReferenceConverter;
import io.crate.metadata.Functions;
import io.crate.metadata.doc.DocSysColumns;
import io.crate.operation.Input;
import io.crate.operation.collect.CollectInputSymbolVisitor;
import io.crate.operation.collect.LuceneDocCollector;
import io.crate.operation.operator.*;
import io.crate.operation.operator.any.*;
import io.crate.operation.predicate.IsNullPredicate;
import io.crate.operation.predicate.MatchPredicate;
import io.crate.operation.predicate.NotPredicate;
import io.crate.operation.reference.doc.lucene.CollectorContext;
import io.crate.operation.reference.doc.lucene.LuceneCollectorExpression;
import io.crate.operation.reference.doc.lucene.LuceneDocLevelReferenceResolver;
import io.crate.operation.scalar.geo.DistanceFunction;
import io.crate.operation.scalar.geo.WithinFunction;
import io.crate.planner.symbol.*;
import io.crate.types.CollectionType;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.apache.lucene.index.AtomicReader;
import org.apache.lucene.index.AtomicReaderContext;
import org.apache.lucene.index.Term;
import org.apache.lucene.queries.BooleanFilter;
import org.apache.lucene.queries.TermsFilter;
import org.apache.lucene.sandbox.queries.regex.JavaUtilRegexCapabilities;
import org.apache.lucene.sandbox.queries.regex.RegexQuery;
import org.apache.lucene.search.*;
import org.apache.lucene.util.Bits;
import org.apache.lucene.util.BytesRef;
import org.apache.lucene.util.automaton.RegExp;
import org.elasticsearch.ExceptionsHelper;
import org.elasticsearch.common.Nullable;
import org.elasticsearch.common.collect.Tuple;
import org.elasticsearch.common.geo.GeoDistance;
import org.elasticsearch.common.geo.GeoPoint;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.inject.Singleton;
import org.elasticsearch.common.lucene.BytesRefs;
import org.elasticsearch.common.lucene.docset.MatchDocIdSet;
import org.elasticsearch.common.lucene.search.Queries;
import org.elasticsearch.index.cache.IndexCache;
import org.elasticsearch.index.fielddata.IndexGeoPointFieldData;
import org.elasticsearch.index.mapper.FieldMapper;
import org.elasticsearch.index.mapper.MapperService;
import org.elasticsearch.index.mapper.Uid;
import org.elasticsearch.index.mapper.geo.GeoPointFieldMapper;
import org.elasticsearch.index.query.RegexpFlag;
import org.elasticsearch.index.search.geo.GeoDistanceRangeFilter;
import org.elasticsearch.index.search.geo.GeoPolygonFilter;
import org.elasticsearch.index.search.geo.InMemoryGeoBoundingBoxFilter;
import org.elasticsearch.search.internal.SearchContext;

import java.io.IOException;
import java.util.*;

import static com.google.common.base.Preconditions.checkArgument;
import static io.crate.operation.scalar.regex.RegexMatcher.isPcrePattern;

@Singleton
public class LuceneQueryBuilder {

    private final static Visitor VISITOR = new Visitor();
    private final CollectInputSymbolVisitor<LuceneCollectorExpression<?>> inputSymbolVisitor;

    @Inject
    public LuceneQueryBuilder(Functions functions) {
        inputSymbolVisitor = new CollectInputSymbolVisitor<>(functions, new LuceneDocLevelReferenceResolver(null));
    }

    public Context convert(WhereClause whereClause, SearchContext searchContext, IndexCache indexCache) throws UnsupportedFeatureException {
        Context ctx = new Context(inputSymbolVisitor, searchContext, indexCache);
        if (whereClause.noMatch()) {
            ctx.query = Queries.newMatchNoDocsQuery();
        } else if (!whereClause.hasQuery()) {
            ctx.query = Queries.newMatchAllQuery();
        } else {
            ctx.query = VISITOR.process(whereClause.query(), ctx);
        }
        return ctx;
    }

    public static class Context {
        Query query;

        final Map<String, Object> filteredFieldValues = new HashMap<>();

        final SearchContext searchContext;
        final CollectInputSymbolVisitor<LuceneCollectorExpression<?>> inputSymbolVisitor;
        final IndexCache indexCache;

        Context(CollectInputSymbolVisitor<LuceneCollectorExpression<?>> inputSymbolVisitor,
                SearchContext searchContext,
                IndexCache indexCache) {
            this.inputSymbolVisitor = inputSymbolVisitor;
            this.searchContext = searchContext;
            this.indexCache = indexCache;
        }

        public Query query() {
            return this.query;
        }

        @Nullable
        public Float minScore() {
            Object score = filteredFieldValues.get("_score");
            if (score == null) {
                return null;
            }
            return ((Number) score).floatValue();
        }

        @Nullable
        public String unsupportedMessage(String field){
            return UNSUPPORTED_FIELDS.get(field);
        }


        final static Set<String> FILTERED_FIELDS = new HashSet<String>(){{ add("_score"); }};


        final static Map<String, String> UNSUPPORTED_FIELDS = ImmutableMap.<String, String>builder()
                .put("_version", "\"_version\" column is not valid in the WHERE clause")
                .build();
    }

    public static String convertWildcardToRegex(String wildcardString) {



        wildcardString = wildcardString.replaceAll("(?<!\\\\)\\*", "\\\\*");
        wildcardString = wildcardString.replaceAll("(?<!\\\\)%", ".*");
        wildcardString = wildcardString.replaceAll("\\\\%", "%");

        wildcardString = wildcardString.replaceAll("(?<!\\\\)\\?", "\\\\?");
        wildcardString = wildcardString.replaceAll("(?<!\\\\)_", ".");
        return wildcardString.replaceAll("\\\\_", "_");
    }

    public static String convertWildcard(String wildcardString) {



        wildcardString = wildcardString.replaceAll("(?<!\\\\)\\*", "\\\\*");
        wildcardString = wildcardString.replaceAll("(?<!\\\\)%", "*");
        wildcardString = wildcardString.replaceAll("\\\\%", "%");

        wildcardString = wildcardString.replaceAll("(?<!\\\\)\\?", "\\\\?");
        wildcardString = wildcardString.replaceAll("(?<!\\\\)_", "?");
        return wildcardString.replaceAll("\\\\_", "_");
    }

    public static String negateWildcard(String wildCard) {
        return String.format(Locale.ENGLISH, "~(%s)", wildCard);
    }

    static class Visitor extends SymbolVisitor<Context, Query> {

        interface FunctionToQuery {

            @Nullable
            Query apply (Function input, Context context) throws IOException;
        }

        static abstract class CmpQuery implements FunctionToQuery {

            @Nullable
            protected Tuple<Reference, Literal> prepare(Function input) {
                assert input != null;
                assert input.arguments().size() == 2;

                Symbol left = input.arguments().get(0);
                Symbol right = input.arguments().get(1);

                if (!(left instanceof Reference) || !(right.symbolType().isValueSymbol())) {
                    return null;
                }
                assert right.symbolType() == SymbolType.LITERAL;
                return new Tuple<>((Reference)left, (Literal)right);
            }
        }

        static abstract class AbstractAnyQuery implements FunctionToQuery {

            @Override
            public Query apply(Function function, Context context) throws IOException {
                Symbol left = function.arguments().get(0);
                Symbol collectionSymbol = function.arguments().get(1);
                Preconditions.checkArgument(DataTypes.isCollectionType(collectionSymbol.valueType()),
                        "invalid argument for ANY expression");
                if (left.symbolType().isValueSymbol()) {

                    assert collectionSymbol.symbolType().isReference() : "no reference found in ANY expression";
                    return applyArrayReference((Reference)collectionSymbol, (Literal)left, context);
                } else if (collectionSymbol.symbolType().isValueSymbol()) {
                    assert left.symbolType().isReference() : "no reference found in ANY expression";
                    return applyArrayLiteral((Reference)left, (Literal)collectionSymbol, context);
                } else {

                    return null;
                }
            }


            public static Iterable<?> toIterable(Object value) {
                return Iterables.transform(AnyOperator.collectionValueToIterable(value), new com.google.common.base.Function<Object, Object>() {
                    @javax.annotation.Nullable
                    @Override
                    public Object apply(@javax.annotation.Nullable Object input) {
                        if (input != null && input instanceof String) {
                            input = new BytesRef((String)input);
                        }
                        return input;
                    }
                });
            }

            protected abstract Query applyArrayReference(Reference arrayReference, Literal literal, Context context) throws IOException;
            protected abstract Query applyArrayLiteral(Reference reference, Literal arrayLiteral, Context context) throws IOException;
        }

        static class AnyEqQuery extends AbstractAnyQuery {

            private TermsFilter termsFilter(String columnName, Literal arrayLiteral) {
                TermsFilter termsFilter;
                Object values = arrayLiteral.value();
                if (values instanceof Collection) {
                    termsFilter = new TermsFilter(
                            columnName,
                            getBytesRefs((Collection)values,
                                    TermBuilder.forType(arrayLiteral.valueType())));
                } else  {
                    termsFilter = new TermsFilter(
                            columnName,
                            getBytesRefs((Object[])values,
                                    TermBuilder.forType(arrayLiteral.valueType())));
                }
                return termsFilter;
            }

            @Override
            protected Query applyArrayReference(Reference arrayReference, Literal literal, Context context) throws IOException {
                QueryBuilderHelper builder = QueryBuilderHelper.forType(((CollectionType)arrayReference.valueType()).innerType());
                return builder.eq(arrayReference.ident().columnIdent().fqn(), literal.value());
            }

            @Override
            protected Query applyArrayLiteral(Reference reference, Literal arrayLiteral, Context context) throws IOException {
                String columnName = reference.ident().columnIdent().fqn();
                return new FilteredQuery(Queries.newMatchAllQuery(), termsFilter(columnName, arrayLiteral));
            }
        }

        static class AnyNeqQuery extends AbstractAnyQuery {

            @Override
            protected Query applyArrayReference(Reference arrayReference, Literal literal, Context context) throws IOException {

                String columnName = arrayReference.info().ident().columnIdent().fqn();
                Object value = literal.value();

                QueryBuilderHelper builder = QueryBuilderHelper.forType(arrayReference.valueType());
                BooleanQuery query = new BooleanQuery();
                query.setMinimumNumberShouldMatch(1);
                query.add(
                        builder.rangeQuery(columnName, value, null, false, false),
                        BooleanClause.Occur.SHOULD
                );
                query.add(
                        builder.rangeQuery(columnName, null, value, false, false),
                        BooleanClause.Occur.SHOULD
                );
                return query;
            }

            @Override
            protected Query applyArrayLiteral(Reference reference, Literal arrayLiteral, Context context) throws IOException {

                String columnName = reference.info().ident().columnIdent().fqn();
                QueryBuilderHelper builder = QueryBuilderHelper.forType(reference.valueType());
                BooleanFilter filter = new BooleanFilter();

                BooleanFilter notFilter = new BooleanFilter();
                for (Object value : toIterable(arrayLiteral.value())) {
                    notFilter.add(builder.eqFilter(columnName, value), BooleanClause.Occur.MUST);
                }
                filter.add(notFilter, BooleanClause.Occur.MUST_NOT);

                return new FilteredQuery(Queries.newMatchAllQuery(), filter);
            }
        }

        static class AnyNotLikeQuery extends AbstractAnyQuery {

            @Override
            protected Query applyArrayReference(Reference arrayReference, Literal literal, Context context) throws IOException {
                String notLike = negateWildcard(
                        convertWildcardToRegex(BytesRefs.toString(literal.value())));
                return new RegexpQuery(new Term(
                        arrayReference.info().ident().columnIdent().fqn(),
                        notLike),
                        RegexpFlag.COMPLEMENT.value()
                );
            }

            @Override
            protected Query applyArrayLiteral(Reference reference, Literal arrayLiteral, Context context) throws IOException {

                BooleanQuery query = new BooleanQuery();
                BooleanQuery notQuery = new BooleanQuery();

                String columnName = reference.ident().columnIdent().fqn();
                QueryBuilderHelper builder = QueryBuilderHelper.forType(reference.valueType());
                for (Object value : toIterable(arrayLiteral.value())) {
                    notQuery.add(builder.like(columnName, value), BooleanClause.Occur.MUST);
                }
                query.add(notQuery, BooleanClause.Occur.MUST_NOT);
                return query;
            }
        }

        static class AnyLikeQuery extends AbstractAnyQuery {

            private final LikeQuery likeQuery = new LikeQuery();

            @Override
            protected Query applyArrayReference(Reference arrayReference, Literal literal, Context context) throws IOException {
                return likeQuery.toQuery(arrayReference, literal.value());
            }

            @Override
            protected Query applyArrayLiteral(Reference reference, Literal arrayLiteral, Context context) throws IOException {

                BooleanQuery booleanQuery = new BooleanQuery();
                booleanQuery.setMinimumNumberShouldMatch(1);
                for (Object value : toIterable(arrayLiteral.value())) {
                    booleanQuery.add(likeQuery.toQuery(reference, value), BooleanClause.Occur.SHOULD);
                }
                return booleanQuery;
            }
        }

        static class LikeQuery extends CmpQuery {

            @Override
            public Query apply(Function input, Context context) {
                Tuple<Reference, Literal> tuple = prepare(input);
                if (tuple == null) {
                    return null;
                }
                return toQuery(tuple.v1(), tuple.v2().value());
            }

            public Query toQuery(Reference reference, Object value) {
                String columnName = reference.info().ident().columnIdent().fqn();
                QueryBuilderHelper builder = QueryBuilderHelper.forType(reference.valueType());
                return builder.like(columnName, value);
            }
        }

        static class InQuery extends CmpQuery {

            @Override
            public Query apply(Function input, Context context) throws IOException {
                Tuple<Reference, Literal> tuple = prepare(input);
                if (tuple == null) {
                    return null;
                }
                String field = tuple.v1().info().ident().columnIdent().fqn();
                Literal literal = tuple.v2();
                CollectionType dataType = ((CollectionType) literal.valueType());

                Set values = (Set) literal.value();
                DataType innerType = dataType.innerType();
                BytesRef[] terms = getBytesRefs(values, TermBuilder.forType(innerType));
                TermsFilter termsFilter = new TermsFilter(field, terms);
                return new FilteredQuery(Queries.newMatchAllQuery(), termsFilter);
            }
        }

        class NotQuery implements FunctionToQuery {

            @Override
            public Query apply(Function input, Context context) {
                assert input != null;
                assert input.arguments().size() == 1;
                BooleanQuery query = new BooleanQuery();

                query.add(process(input.arguments().get(0), context), BooleanClause.Occur.MUST_NOT);
                query.add(Queries.newMatchAllQuery(), BooleanClause.Occur.MUST);

                return query;
            }
        }

        static class IsNullQuery implements FunctionToQuery {

            @Override
            public Query apply(Function input, Context context) {
                assert input != null;
                assert input.arguments().size() == 1;
                Symbol arg = input.arguments().get(0);
                if (arg.symbolType() != SymbolType.REFERENCE) {
                    return null;
                }
                Reference reference = (Reference)arg;

                String columnName = reference.info().ident().columnIdent().fqn();
                QueryBuilderHelper builderHelper = QueryBuilderHelper.forType(reference.valueType());
                BooleanFilter boolFilter = new BooleanFilter();
                boolFilter.add(builderHelper.rangeFilter(columnName, null, null, true, true),
                        BooleanClause.Occur.MUST_NOT);
                return new FilteredQuery(Queries.newMatchAllQuery(), boolFilter);
            }
        }

        static class EqQuery extends CmpQuery {

            @Override
            public Query apply(Function input, Context context) {
                Tuple<Reference, Literal> tuple = super.prepare(input);
                if (tuple == null) {
                    return null;
                }
                Reference reference = tuple.v1();
                Literal literal = tuple.v2();
                String columnName = reference.info().ident().columnIdent().fqn();
                if (DataTypes.isCollectionType(reference.valueType()) && DataTypes.isCollectionType(literal.valueType())) {


                    BooleanFilter boolTermsFilter = new BooleanFilter();
                    DataType type = literal.valueType();
                    while (DataTypes.isCollectionType(type)) {
                        type = ((CollectionType) type).innerType();
                    }
                    QueryBuilderHelper builder = QueryBuilderHelper.forType(type);
                    Object value = literal.value();
                    buildTermsQuery(boolTermsFilter, value, columnName, builder);

                    if (boolTermsFilter.clauses().isEmpty()) {

                        return genericFunctionQuery(input, context.inputSymbolVisitor, context.searchContext);
                    }




                    BooleanFilter filterClauses = new BooleanFilter();
                    filterClauses.add(boolTermsFilter, BooleanClause.Occur.MUST);
                    filterClauses.add(
                            genericFunctionFilter(input, context.inputSymbolVisitor, context.searchContext),
                            BooleanClause.Occur.MUST);
                    return new FilteredQuery(Queries.newMatchAllQuery(), filterClauses);
                }
                QueryBuilderHelper builder = QueryBuilderHelper.forType(tuple.v1().valueType());
                return builder.eq(columnName, tuple.v2().value());
            }

            private void buildTermsQuery(BooleanFilter booleanFilter,
                                            Object value,
                                            String columnName,
                                            QueryBuilderHelper builder) {
                if (value == null) {
                    return;
                }
                if (value.getClass().isArray()) {
                    Object[] array = (Object[]) value;
                    for (Object o : array) {
                        buildTermsQuery(booleanFilter, o, columnName, builder);
                    }
                } else {
                    booleanFilter.add(builder.eqFilter(columnName, value), BooleanClause.Occur.MUST);
                }
            }
        }

        class AndQuery implements FunctionToQuery {
            @Override
            public Query apply(Function input, Context context) {
                assert input != null;
                BooleanQuery query = new BooleanQuery();
                for (Symbol symbol : input.arguments()) {
                    query.add(process(symbol, context), BooleanClause.Occur.MUST);
                }
                return query;
            }
        }

        class OrQuery implements FunctionToQuery {
            @Override
            public Query apply(Function input, Context context) {
                assert input != null;
                BooleanQuery query = new BooleanQuery();
                query.setMinimumNumberShouldMatch(1);
                for (Symbol symbol : input.arguments()) {
                    query.add(process(symbol, context), BooleanClause.Occur.SHOULD);
                }
                return query;
            }
        }

        static class AnyRangeQuery extends AbstractAnyQuery {

            private final RangeQuery rangeQuery;
            private final RangeQuery inverseRangeQuery;

            AnyRangeQuery(String comparison, String inverseComparison) {
                rangeQuery = new RangeQuery(comparison);
                inverseRangeQuery = new RangeQuery(inverseComparison);
            }

            @Override
            protected Query applyArrayReference(Reference arrayReference, Literal literal, Context context) throws IOException {

                return rangeQuery.toQuery(
                        arrayReference,
                        ((CollectionType)arrayReference.valueType()).innerType(),
                        literal.value()
                );
            }

            @Override
            protected Query applyArrayLiteral(Reference reference, Literal arrayLiteral, Context context) throws IOException {

                BooleanQuery booleanQuery = new BooleanQuery();
                booleanQuery.setMinimumNumberShouldMatch(1);
                for (Object value : toIterable(arrayLiteral.value())) {
                    booleanQuery.add(inverseRangeQuery.toQuery(reference, reference.valueType(), value), BooleanClause.Occur.SHOULD);
                }
                return booleanQuery;
            }
        }

        static class RangeQuery extends CmpQuery {

            private final boolean includeLower;
            private final boolean includeUpper;
            private final com.google.common.base.Function<Object, Tuple<?, ?>> boundsFunction;

            private static final com.google.common.base.Function<Object, Tuple<?, ?>> LOWER_BOUND = new com.google.common.base.Function<Object, Tuple<?,?>>() {
                @javax.annotation.Nullable
                @Override
                public Tuple<?, ?> apply(@Nullable Object input) {
                    return new Tuple<>(input, null);
                }
            };

            private static final com.google.common.base.Function<Object, Tuple<?, ?>> UPPER_BOUND = new com.google.common.base.Function<Object, Tuple<?,?>>() {
                @Override
                public Tuple<?, ?> apply(Object input) {
                    return new Tuple<>(null, input);
                }
            };

            public RangeQuery(String comparison) {
                switch (comparison) {
                    case "lt":
                        boundsFunction = UPPER_BOUND;
                        includeLower = false;
                        includeUpper = false;
                        break;
                    case "gt":
                        boundsFunction = LOWER_BOUND;
                        includeLower = false;
                        includeUpper = false;
                        break;
                    case "lte":
                        boundsFunction = UPPER_BOUND;
                        includeLower = false;
                        includeUpper = true;
                        break;
                    case "gte":
                        boundsFunction = LOWER_BOUND;
                        includeLower = true;
                        includeUpper = false;
                        break;
                    default:
                        throw new IllegalArgumentException("invalid comparison");
                }
            }

            @Override
            public Query apply(Function input, Context context) throws IOException {
                Tuple<Reference, Literal> tuple = super.prepare(input);
                if (tuple == null) {
                    return null;
                }
                return toQuery(tuple.v1(), tuple.v1().valueType(), tuple.v2().value());
            }

            public Query toQuery(Reference reference, DataType type, Object value) {
                String columnName = reference.info().ident().columnIdent().fqn();
                QueryBuilderHelper builder = QueryBuilderHelper.forType(type);
                Tuple<?, ?> bounds = boundsFunction.apply(value);
                assert bounds != null;
                return builder.rangeQuery(columnName, bounds.v1(), bounds.v2(), includeLower, includeUpper);
            }
        }

        static class ToMatchQuery implements FunctionToQuery {

            @Override
            public Query apply(Function input, Context context) throws IOException {
                List<Symbol> arguments = input.arguments();
                assert arguments.size() == 4 : "invalid number of arguments";
                assert Symbol.isLiteral(arguments.get(0), DataTypes.OBJECT);
                assert Symbol.isLiteral(arguments.get(1), DataTypes.STRING);
                assert Symbol.isLiteral(arguments.get(2), DataTypes.STRING);
                assert Symbol.isLiteral(arguments.get(3), DataTypes.OBJECT);

                @SuppressWarnings("unchecked")
                Map<String, Object> fields = (Map) ((Literal) arguments.get(0)).value();
                BytesRef queryString = (BytesRef) ((Literal) arguments.get(1)).value();
                BytesRef matchType = (BytesRef) ((Literal) arguments.get(2)).value();
                Map options = (Map) ((Literal) arguments.get(3)).value();

                checkArgument(queryString != null, "cannot use NULL as query term in match predicate");

                MatchQueryBuilder queryBuilder;
                if (fields.size() == 1) {
                    queryBuilder = new MatchQueryBuilder(context.searchContext, context.indexCache, matchType, options);
                } else {
                    queryBuilder = new MultiMatchQueryBuilder(context.searchContext, context.indexCache, matchType, options);
                }
                return queryBuilder.query(fields, queryString);
            }
        }

        static class RegexpMatchQuery extends CmpQuery {

            @Override
            public Query apply(Function input, Context context) throws IOException {
                Tuple<Reference, Literal> prepare = prepare(input);
                if (prepare == null) { return null; }
                String fieldName = prepare.v1().info().ident().columnIdent().fqn();
                Object value = prepare.v2().value();


                if (value instanceof String) {
                    if (isPcrePattern(value)) {
                        return new RegexQuery(new Term(fieldName, (String) value));
                    } else {
                        return new RegexpQuery(new Term(fieldName, (String) value), RegExp.ALL);
                    }
                }

                if (value instanceof BytesRef) {
                    if (isPcrePattern(value)) {
                        return new RegexQuery(new Term(fieldName, (BytesRef) value));
                    } else {
                        return new RegexpQuery(new Term(fieldName, (BytesRef) value), RegExp.ALL);
                    }
                }

                throw new IllegalArgumentException("Can only use ~ with patterns of type string");
            }
        }

        static class RegexMatchQueryCaseInsensitive extends CmpQuery {

            @Override
            public Query apply(Function input, Context context) throws IOException {
                Tuple<Reference, Literal> prepare = prepare(input);
                if (prepare == null) { return null; }
                String fieldName = prepare.v1().info().ident().columnIdent().fqn();
                Object value = prepare.v2().value();


                if (value instanceof String) {
                    RegexQuery query = new RegexQuery(new Term(fieldName, (String) value));
                    query.setRegexImplementation(new JavaUtilRegexCapabilities(
                            JavaUtilRegexCapabilities.FLAG_CASE_INSENSITIVE |
                            JavaUtilRegexCapabilities.FLAG_UNICODE_CASE));
                    return query;
                }

                if (value instanceof BytesRef) {
                    RegexQuery query = new RegexQuery(new Term(fieldName, (BytesRef) value));
                    query.setRegexImplementation(new JavaUtilRegexCapabilities(
                            JavaUtilRegexCapabilities.FLAG_CASE_INSENSITIVE |
                            JavaUtilRegexCapabilities.FLAG_UNICODE_CASE));
                    return query;
                }

                throw new IllegalArgumentException("Can only use ~* with patterns of type string");
            }
        }


         interface InnerFunctionToQuery {


            @Nullable
            Query apply(Function parent, Function inner, Context context) throws IOException;
        }


        static class WithinQuery implements FunctionToQuery, InnerFunctionToQuery {

            @Override
            public Query apply(Function parent, Function inner, Context context) throws IOException {
                FunctionLiteralPair outerPair = new FunctionLiteralPair(parent);
                if (!outerPair.isValid()) {
                    return null;
                }
                Query query = getQuery(inner, context);
                if (query == null) return null;
                Boolean negate = !(Boolean) outerPair.input().value();
                if (negate) {
                    BooleanQuery booleanQuery = new BooleanQuery();
                    booleanQuery.add(query, BooleanClause.Occur.MUST_NOT);
                    return booleanQuery;
                } else {
                    return query;
                }
            }

            private Query getQuery(Function inner, Context context) {
                RefLiteralPair innerPair = new RefLiteralPair(inner);
                if (!innerPair.isValid()) {
                    return null;
                }
                GeoPointFieldMapper mapper = getGeoPointFieldMapper(
                        innerPair.reference().info().ident().columnIdent().fqn(),
                        context.searchContext
                );
                Shape shape = (Shape) innerPair.input().value();
                Geometry geometry = JtsSpatialContext.GEO.getGeometryFrom(shape);
                IndexGeoPointFieldData fieldData = context.searchContext.fieldData().getForField(mapper);
                Filter filter;
                if (geometry.isRectangle()) {
                    Rectangle boundingBox = shape.getBoundingBox();
                    filter = new InMemoryGeoBoundingBoxFilter(
                            new GeoPoint(boundingBox.getMaxY(), boundingBox.getMinX()),
                            new GeoPoint(boundingBox.getMinY(), boundingBox.getMaxX()),
                            fieldData
                    );
                } else {
                    Coordinate[] coordinates = geometry.getCoordinates();
                    GeoPoint[] points = new GeoPoint[coordinates.length];
                    for (int i = 0; i < coordinates.length; i++) {
                        Coordinate coordinate = coordinates[i];
                        points[i] = new GeoPoint(coordinate.y, coordinate.x);
                    }
                    filter = new GeoPolygonFilter(fieldData, points);
                }
                return new FilteredQuery(Queries.newMatchAllQuery(), context.indexCache.filter().cache(filter));
            }

            @Override
            public Query apply(Function input, Context context) throws IOException {
                return getQuery(input, context);
            }
        }

        class DistanceQuery implements InnerFunctionToQuery {

            final GeoDistance geoDistance = GeoDistance.DEFAULT;
            final String optimizeBox = "memory";


            @Override
            public Query apply(Function parent, Function inner, Context context) {
                assert inner.info().ident().name().equals(DistanceFunction.NAME);

                RefLiteralPair distanceRefLiteral = new RefLiteralPair(inner);
                if (!distanceRefLiteral.isValid()) {

                    return null;
                }
                FunctionLiteralPair functionLiteralPair = new FunctionLiteralPair(parent);
                if (!functionLiteralPair.isValid()) {

                    return null;
                }
                Double distance = DataTypes.DOUBLE.value(functionLiteralPair.input().value());

                String fieldName = distanceRefLiteral.reference().info().ident().columnIdent().fqn();
                FieldMapper mapper = getGeoPointFieldMapper(fieldName, context.searchContext);
                GeoPointFieldMapper geoMapper = ((GeoPointFieldMapper) mapper);
                IndexGeoPointFieldData fieldData = context.searchContext.fieldData().getForField(mapper);

                Input geoPointInput = distanceRefLiteral.input();
                Double[] pointValue = (Double[]) geoPointInput.value();
                double lat = pointValue[1];
                double lon = pointValue[0];

                String parentName = functionLiteralPair.functionName();

                Double from = null;
                Double to = null;
                boolean includeLower = false;
                boolean includeUpper = false;

                switch (parentName) {
                    case EqOperator.NAME:
                        includeLower = true;
                        includeUpper = true;
                        from = distance;
                        to = distance;
                        break;
                    case LteOperator.NAME:
                        includeUpper = true;
                        to = distance;
                        break;
                    case LtOperator.NAME:
                        to = distance;
                        break;
                    case GteOperator.NAME:
                        from = distance;
                        includeLower = true;
                        break;
                    case GtOperator.NAME:
                        from = distance;
                        break;
                    default:

                        return null;
                }
                GeoPoint geoPoint = new GeoPoint(lat, lon);
                Filter filter = new GeoDistanceRangeFilter(
                        geoPoint,
                        from,
                        to,
                        includeLower,
                        includeUpper,
                        geoDistance,
                        geoMapper,
                        fieldData,
                        optimizeBox
                );
                return new FilteredQuery(Queries.newMatchAllQuery(), context.indexCache.filter().cache(filter));
            }
        }

        private static GeoPointFieldMapper getGeoPointFieldMapper(String fieldName, SearchContext searchContext) {
            MapperService.SmartNameFieldMappers smartMappers = searchContext.smartFieldMappers(fieldName);
            if (smartMappers == null || !smartMappers.hasMapper()) {
                throw new IllegalArgumentException(String.format("column \"%s\" doesn't exist", fieldName));
            }
            FieldMapper mapper = smartMappers.mapper();
            if (!(mapper instanceof GeoPointFieldMapper)) {
                throw new IllegalArgumentException(String.format("column \"%s\" isn't of type geo_point", fieldName));
            }
            return (GeoPointFieldMapper) mapper;
        }

        private static final EqQuery eqQuery = new EqQuery();
        private static final RangeQuery ltQuery = new RangeQuery("lt");
        private static final RangeQuery lteQuery = new RangeQuery("lte");
        private static final RangeQuery gtQuery = new RangeQuery("gt");
        private static final RangeQuery gteQuery = new RangeQuery("gte");
        private static final WithinQuery withinQuery = new WithinQuery();
        private final ImmutableMap<String, FunctionToQuery> functions =
                ImmutableMap.<String, FunctionToQuery>builder()
                        .put(WithinFunction.NAME, withinQuery)
                        .put(AndOperator.NAME, new AndQuery())
                        .put(OrOperator.NAME, new OrQuery())
                        .put(EqOperator.NAME, eqQuery)
                        .put(LtOperator.NAME, ltQuery)
                        .put(LteOperator.NAME, lteQuery)
                        .put(GteOperator.NAME, gteQuery)
                        .put(GtOperator.NAME, gtQuery)
                        .put(LikeOperator.NAME, new LikeQuery())
                        .put(InOperator.NAME, new InQuery())
                        .put(NotPredicate.NAME, new NotQuery())
                        .put(IsNullPredicate.NAME, new IsNullQuery())
                        .put(MatchPredicate.NAME, new ToMatchQuery())
                        .put(AnyEqOperator.NAME, new AnyEqQuery())
                        .put(AnyNeqOperator.NAME, new AnyNeqQuery())
                        .put(AnyLtOperator.NAME, new AnyRangeQuery("gt", "lt"))
                        .put(AnyLteOperator.NAME, new AnyRangeQuery("gte", "lte"))
                        .put(AnyGteOperator.NAME, new AnyRangeQuery("lte", "gte"))
                        .put(AnyGtOperator.NAME, new AnyRangeQuery("lt", "gt"))
                        .put(AnyLikeOperator.NAME, new AnyLikeQuery())
                        .put(AnyNotLikeOperator.NAME, new AnyNotLikeQuery())
                        .put(RegexpMatchOperator.NAME, new RegexpMatchQuery())
                        .put(RegexpMatchCaseInsensitiveOperator.NAME, new RegexMatchQueryCaseInsensitive())
                        .build();

        private final ImmutableMap<String, InnerFunctionToQuery> innerFunctions =
                ImmutableMap.<String, InnerFunctionToQuery>builder()
                        .put(DistanceFunction.NAME, new DistanceQuery())
                        .put(WithinFunction.NAME, withinQuery)
                        .build();

        @Override
        public Query visitFunction(Function function, Context context) {
            assert function != null;
            if (fieldIgnored(function, context)) {
                return Queries.newMatchAllQuery();
            }
            function = rewriteAndValidateFields(function, context);

            FunctionToQuery toQuery = functions.get(function.info().ident().name());
            if (toQuery == null) {
                return genericFunctionQuery(function, context.inputSymbolVisitor, context.searchContext);
            }

            Query query;
            try {
                query = toQuery.apply(function, context);
            } catch (IOException e) {
                throw ExceptionsHelper.convertToRuntime(e);
            } catch (UnsupportedOperationException e) {
                return genericFunctionQuery(function, context.inputSymbolVisitor, context.searchContext);
            }
            if (query == null) {
                query = queryFromInnerFunction(function, context);
                if (query == null) {
                    return genericFunctionQuery(function, context.inputSymbolVisitor, context.searchContext);
                }
            }
            return query;
        }

        private Query queryFromInnerFunction(Function function, Context context) {
            for (Symbol symbol : function.arguments()) {
                if (symbol.symbolType() == SymbolType.FUNCTION) {
                    String functionName = ((Function) symbol).info().ident().name();
                    InnerFunctionToQuery functionToQuery = innerFunctions.get(functionName);
                    if (functionToQuery != null) {
                        try {
                            Query query = functionToQuery.apply(function, (Function)symbol, context);
                            if (query != null) {
                                return query;
                            }
                        } catch (IOException e) {
                            throw ExceptionsHelper.convertToRuntime(e);
                        }
                    }
                }
            }
            return null;
        }

        private boolean fieldIgnored(Function function, Context context) {
            if (function.arguments().size() != 2) {
                return false;
            }

            Symbol left = function.arguments().get(0);
            Symbol right = function.arguments().get(1);
            if (left.symbolType() == SymbolType.REFERENCE && right.symbolType().isValueSymbol()) {
                String columnName = ((Reference) left).info().ident().columnIdent().name();
                if (Context.FILTERED_FIELDS.contains(columnName)) {
                    context.filteredFieldValues.put(columnName, ((Input) right).value());
                    return true;
                }
                String unsupportedMessage = Context.UNSUPPORTED_FIELDS.get(columnName);
                if (unsupportedMessage != null) {
                    throw new UnsupportedFeatureException(unsupportedMessage);
                }
            }
            return false;
        }

        @Nullable
        private Function rewriteAndValidateFields(Function function, Context context) {
            if (function.arguments().size() == 2) {
                Symbol left = function.arguments().get(0);
                Symbol right = function.arguments().get(1);
                if (left.symbolType() == SymbolType.REFERENCE && right.symbolType().isValueSymbol()) {
                    Reference ref = (Reference) left;
                    if (ref.info().ident().columnIdent().equals(DocSysColumns.ID)) {
                        function.setArgument(0,
                                new Reference(DocSysColumns.forTable(ref.ident().tableIdent(), DocSysColumns.UID)));
                        function.setArgument(1, Literal.newLiteral(Uid.createUid(Constants.DEFAULT_MAPPING_TYPE,
                                ValueSymbolVisitor.STRING.process(right))));
                    } else {
                        String unsupportedMessage = context.unsupportedMessage(ref.info().ident().columnIdent().name());
                        if (unsupportedMessage != null) {
                            throw new UnsupportedFeatureException(unsupportedMessage);
                        }
                    }
                }
            }
            return function;
        }

        private static Filter genericFunctionFilter(Function function,
                                                    CollectInputSymbolVisitor<LuceneCollectorExpression<?>> inputSymbolVisitor,
                                                    SearchContext searchContext) {
            if (function.valueType() != DataTypes.BOOLEAN) {
                raiseUnsupported(function);
            }




            function = (Function)DocReferenceConverter.convertIf(function, Predicates.<Reference>alwaysTrue());

            final CollectInputSymbolVisitor.Context ctx = inputSymbolVisitor.extractImplementations(function);
            assert ctx.topLevelInputs().size() == 1;
            @SuppressWarnings("unchecked")
            final Input<Boolean> condition = (Input<Boolean>) ctx.topLevelInputs().get(0);
            @SuppressWarnings("unchecked")
            final List<LuceneCollectorExpression> expressions = ctx.docLevelExpressions();
            final CollectorContext collectorContext = new CollectorContext();
            collectorContext.searchContext(searchContext);
            collectorContext.visitor(new LuceneDocCollector.CollectorFieldsVisitor(expressions.size()));

            for (LuceneCollectorExpression expression : expressions) {
                expression.startCollect(collectorContext);
            }
            return new Filter() {
                @Override
                public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
                    for (LuceneCollectorExpression expression : expressions) {
                        expression.setNextReader(context.reader().getContext());
                    }
                    return BitsFilteredDocIdSet.wrap(
                            new FunctionDocSet(
                                    context.reader(),
                                    collectorContext.visitor(),
                                    condition,
                                    expressions,
                                    context.reader().maxDoc(),
                                    acceptDocs
                            ),
                            acceptDocs
                    );
                }
            };
        }

        private static Query genericFunctionQuery(Function function,
                                                  CollectInputSymbolVisitor<LuceneCollectorExpression<?>> inputSymbolVisitor,
                                                  SearchContext searchContext) {
            return new FilteredQuery(
                    Queries.newMatchAllQuery(),
                    genericFunctionFilter(function, inputSymbolVisitor, searchContext));
        }

        static class FunctionDocSet extends MatchDocIdSet {

            private final AtomicReader reader;
            private final LuceneDocCollector.CollectorFieldsVisitor fieldsVisitor;
            private final Input<Boolean> condition;
            private final List<LuceneCollectorExpression> expressions;
            private final boolean fieldsVisitorEnabled;

            protected FunctionDocSet(AtomicReader reader,
                                     @Nullable LuceneDocCollector.CollectorFieldsVisitor fieldsVisitor,
                                     Input<Boolean> condition,
                                     List<LuceneCollectorExpression> expressions,
                                     int maxDoc,
                                     @Nullable Bits acceptDocs) {
                super(maxDoc, acceptDocs);
                this.reader = reader;
                this.fieldsVisitor = fieldsVisitor;

                this.fieldsVisitorEnabled = fieldsVisitor == null ? false : fieldsVisitor.required();
                this.condition = condition;
                this.expressions = expressions;
            }

            @Override
            protected boolean matchDoc(int doc) {
                if (fieldsVisitorEnabled) {
                    fieldsVisitor.reset();
                    try {
                        reader.document(doc, fieldsVisitor);
                    } catch (IOException e) {
                        throw Throwables.propagate(e);
                    }
                }
                for (LuceneCollectorExpression expression : expressions) {
                    expression.setNextDocId(doc);
                }
                Boolean value = condition.value();
                if (value == null) {
                    return false;
                }
                return value;
            }
        }

        private static Query raiseUnsupported(Function function) {
            throw new UnsupportedOperationException(
                    SymbolFormatter.format("Cannot convert function %s into a query", function));
        }

        @Override
        public Query visitReference(Reference symbol, Context context) {

            if (symbol.valueType() == DataTypes.BOOLEAN) {
                return QueryBuilderHelper.forType(DataTypes.BOOLEAN).eq(symbol.info().ident().columnIdent().fqn(), true);
            }
            return super.visitReference(symbol, context);
        }

        @Override
        protected Query visitSymbol(Symbol symbol, Context context) {
            throw new UnsupportedOperationException(
                    SymbolFormatter.format("Can't build query from symbol %s", symbol));
        }
    }

    static class FunctionLiteralPair {

        private final String functionName;
        private final Function function;
        private final Input input;

        FunctionLiteralPair(Function outerFunction) {
            assert outerFunction.arguments().size() == 2 : "function requires 2 arguments";
            Symbol left = outerFunction.arguments().get(0);
            Symbol right = outerFunction.arguments().get(1);

            functionName = outerFunction.info().ident().name();

            if (left instanceof Function) {
                function = (Function) left;
            } else if (right instanceof Function) {
                function = (Function) right;
            } else {
                function = null;
            }

            if (left.symbolType().isValueSymbol()) {
                input = (Input) left;
            } else if (right.symbolType().isValueSymbol()) {
                input = (Input) right;
            } else {
                input = null;
            }
        }

        public boolean isValid() {
            return input != null && function != null;
        }

        public Input input() {
            return input;
        }

        public String functionName() {
            return functionName;
        }
    }

    static class RefLiteralPair {

        private final Reference reference;
        private final Input input;

        RefLiteralPair(Function function) {
            assert function.arguments().size() == 2 : "function requires 2 arguments";
            Symbol left = function.arguments().get(0);
            Symbol right = function.arguments().get(1);

            if (left instanceof Reference) {
                reference = (Reference) left;
            } else if (right instanceof Reference) {
                reference = (Reference) right;
            } else {
                reference = null;
            }

            if (left.symbolType().isValueSymbol()) {
                input = (Input) left;
            } else if (right.symbolType().isValueSymbol()) {
                input = (Input) right;
            } else {
                input = null;
            }
        }

        public boolean isValid() {
            return input != null && reference != null;
        }

        public Reference reference() {
            return reference;
        }

        public Input input() {
            return input;
        }
    }

    @SuppressWarnings("unchecked")
    private static BytesRef[] getBytesRefs(Object[] values, TermBuilder termBuilder) {
        BytesRef[] terms = new BytesRef[values.length];
        int i = 0;
        for (Object value : values) {
            terms[i] = termBuilder.term(value);
            i++;
        }
        return terms;
    }

    @SuppressWarnings("unchecked")
    private static BytesRef[] getBytesRefs(Collection values, TermBuilder termBuilder) {
        BytesRef[] terms = new BytesRef[values.size()];
        int i = 0;
        for (Object value : values) {
            terms[i] = termBuilder.term(value);
            i++;
        }
        return terms;
    }
}

<code block>


package io.crate.integrationtests;

import io.crate.testing.TestingHelpers;
import org.elasticsearch.common.collect.MapBuilder;
import org.hamcrest.Matchers;
import org.junit.Test;

import java.util.Arrays;

import static org.hamcrest.core.Is.is;

public class WherePKIntegrationTest extends SQLTransportIntegrationTest {

    @Test
    public void testWherePkColInWithLimit() throws Exception {
        execute("create table users (" +
                "   id int primary key," +
                "   name string" +
                ") clustered into 2 shards with (number_of_replicas = 0)");
        ensureYellow();
        execute("insert into users (id, name) values (?, ?)", new Object[][] {
                new Object[] { 1, "Arthur" },
                new Object[] { 2, "Trillian" },
                new Object[] { 3, "Marvin" },
                new Object[] { 4, "Slartibartfast" },
        });
        execute("refresh table users");

        execute("select name from users where id in (1, 3, 4) order by name desc limit 2");
        assertThat(response.rowCount(), is(2L));
        assertThat((String) response.rows()[0][0], is("Slartibartfast"));
        assertThat((String) response.rows()[1][0], is("Marvin"));
    }

    @Test
    public void testWherePKWithFunctionInOutputsAndOrderBy() throws Exception {
        execute("create table users (" +
                "   id int primary key," +
                "   name string" +
                ") clustered into 2 shards with (number_of_replicas = 0)");
        ensureYellow();
        execute("insert into users (id, name) values (?, ?)", new Object[][]{
                new Object[]{1, "Arthur"},
                new Object[]{2, "Trillian"},
                new Object[]{3, "Marvin"},
                new Object[]{4, "Slartibartfast"},
        });
        execute("refresh table users");
        execute("select substr(name, 1, 1) from users where id in (1, 2, 3) order by substr(name, 1, 1) desc");
        assertThat(response.rowCount(), is(3L));
        assertThat((String) response.rows()[0][0], is("T"));
        assertThat((String) response.rows()[1][0], is("M"));
        assertThat((String) response.rows()[2][0], is("A"));
    }

    @Test
    public void testWherePKWithOrderBySymbolThatIsMissingInSelectList() throws Exception {
        execute("create table users (" +
                "   id int primary key," +
                "   name string" +
                ") clustered into 2 shards with (number_of_replicas = 0)");
        ensureYellow();
        execute("insert into users (id, name) values (?, ?)", new Object[][]{
                new Object[]{1, "Arthur"},
                new Object[]{2, "Trillian"},
                new Object[]{3, "Marvin"},
                new Object[]{4, "Slartibartfast"},
        });
        execute("refresh table users");
        execute("select name from users where id in (1, 2, 3) order by id desc");
        assertThat(response.rowCount(), is(3L));
        assertThat((String) response.rows()[0][0], is("Marvin"));
        assertThat((String) response.rows()[1][0], is("Trillian"));
        assertThat((String) response.rows()[2][0], is("Arthur"));
    }

    @Test
    public void testWherePkColLimit0() throws Exception {
        execute("create table users (id int primary key, name string) " +
                "clustered into 1 shards with (number_of_replicas = 0)");
        ensureYellow();
        execute("insert into users (id, name) values (1, 'Arthur')");
        execute("refresh table users");
        execute("select name from users where id = 1 limit 0");
        assertThat(response.rowCount(), is(0L));
    }

    @Test
    public void testSelectNestedObjectWherePk() throws Exception {
        execute("create table items (id string primary key, details object as (tags array(string)) )" +
            "clustered into 3 shards with (number_of_replicas = '0-1')");
        ensureYellow();

        execute("insert into items (id, details) values (?, ?)", new Object[]{
            "123", MapBuilder.newMapBuilder().put("tags", Arrays.asList("small", "blue")).map()
        });
        execute("refresh table items");

        execute("select id, details['tags'] from items where id = '123'");
        assertThat(response.rowCount(), is(1L));
        assertThat((String) response.rows()[0][0], is("123"));

        String[] tags = Arrays.copyOf((Object[])response.rows()[0][1], 2, String[].class);
        assertThat(tags, Matchers.arrayContaining("small", "blue"));
    }

    @Test
    public void testSelectByIdWithCustomRoutingUsesSearch() throws Exception {
        execute("create table users (name string)" +
                "clustered by (name) with (number_of_replicas = '0')");

        execute("insert into users values ('hoschi'), ('galoschi'), ('x')");
        execute("refresh table users");

        execute("select _id from users");
        for (Object[] row : response.rows()) {
            execute("select name from users where _id=?", row);
            assertThat(response.rowCount(), is(1L));
        }
    }

    @Test
    public void testSelectByIdWithPartitionsUsesSearch() throws Exception {
        execute("create table users (name string)" +
                "  with (number_of_replicas = '0')");

        execute("insert into users values ('hoschi'), ('galoschi'), ('x')");
        execute("refresh table users");
        execute("select _id from users");

        for (Object[] row : response.rows()) {
            execute("select name from users where _id=?", row);
            assertThat(response.rowCount(), is(1L));
        }
    }

    @Test
    public void testEmptyClusteredByUnderId() throws Exception {

        execute("create table auto_id (" +
                "  name string," +
                "  location geo_point" +
                ") with (number_of_replicas=0)");
        ensureYellow();

        execute("insert into auto_id (name, location) values (',', [36.567, 52.998]), ('Dornbirn', [54.45, 4.567])");
        execute("refresh table auto_id");


        execute("select * from auto_id where _id=''");
        assertThat(response.cols(), is(Matchers.arrayContaining("location", "name")));
        assertThat(response.rowCount(), is(0L));
    }

    @Test
    public void testEmptyClusteredByExplicit() throws Exception {

        execute("create table explicit_routing (" +
                "  name string," +
                "  location geo_point" +
                ") clustered by (name) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into explicit_routing (name, location) values (',', [36.567, 52.998]), ('Dornbirn', [54.45, 4.567])");
        execute("refresh table explicit_routing");
        execute("select * from explicit_routing where name=''");
        assertThat(response.cols(), is(Matchers.arrayContaining("location", "name")));
        assertThat(response.rowCount(), is(0L));

        execute("select * from explicit_routing where name=','");
        assertThat(response.cols(), is(Matchers.arrayContaining("location", "name")));
        assertThat(response.rowCount(), is(1L));
        assertThat(TestingHelpers.printedTable(response.rows()), is("[36.567, 52.998]| ,\n"));
    }

    @Test
    public void testQueryOnEmptyClusteredByColumn() throws Exception {
        execute("create table expl_routing (id int primary key, name string primary key) " +
                "clustered by (name) with (number_of_replicas = 0)");
        ensureYellow();

        if (randomInt(1) == 0) {
            execute("insert into expl_routing (id, name) values (?, ?)", new Object[][]{
                    new Object[]{1, ""},
                    new Object[]{2, ""},
                    new Object[]{1, "1"}
            });
        } else {
            execute("insert into expl_routing (id, name) values (?, ?)", new Object[]{1, ""});
            execute("insert into expl_routing (id, name) values (?, ?)", new Object[]{2, ""});
            execute("insert into expl_routing (id, name) values (?, ?)", new Object[]{1, "1"});
        }
        execute("refresh table expl_routing");

        execute("select count(*) from expl_routing where name = ''");
        assertThat((Long)response.rows()[0][0], is(2L));

        execute("select * from expl_routing where name = '' order by id");
        assertThat(response.rowCount(), is(2L));
        assertThat((Integer) response.rows()[0][0], is(1));
        assertThat((Integer)response.rows()[1][0], is(2));

        execute("delete from expl_routing where name = ''");
        execute("select count(*) from expl_routing");
        assertThat((Long) response.rows()[0][0], is(1L));
    }

    @Test
    public void testDeleteByQueryCommaRouting() throws Exception {
        execute("create table explicit_routing (" +
                "  name string," +
                "  location geo_point" +
                ") clustered by (name) into 3 shards with (number_of_replicas=0)");
        ensureYellow();




        execute("insert into explicit_routing (name, location) values ('A', [36.567, 52.998]), ('W', [54.45, 4.567])");
        execute("refresh table explicit_routing");


        execute("delete from explicit_routing where name='A,W'");
        assertThat(response.rowCount(), is(-1L));
        execute("refresh table explicit_routing");

        execute("select * from explicit_routing");
        assertThat(response.rowCount(), is(2L));

    }
}


<code block>


package io.crate.analyze.where;

import com.google.common.collect.ImmutableList;
import io.crate.analyze.*;
import io.crate.analyze.relations.TableRelation;
import io.crate.core.collections.TreeMapBuilder;
import io.crate.metadata.*;
import io.crate.metadata.sys.MetaDataSysModule;
import io.crate.metadata.table.ColumnPolicy;
import io.crate.metadata.table.SchemaInfo;
import io.crate.metadata.table.TestingTableInfo;
import io.crate.operation.operator.OperatorModule;
import io.crate.operation.operator.any.AnyEqOperator;
import io.crate.operation.operator.any.AnyLikeOperator;
import io.crate.operation.predicate.PredicateModule;
import io.crate.operation.scalar.ScalarFunctionModule;
import io.crate.planner.RowGranularity;
import io.crate.sql.parser.SqlParser;
import io.crate.test.integration.CrateUnitTest;
import io.crate.testing.MockedClusterServiceModule;
import io.crate.types.ArrayType;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import io.crate.types.SetType;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.common.inject.Injector;
import org.elasticsearch.common.inject.ModulesBuilder;
import org.elasticsearch.threadpool.ThreadPool;
import org.hamcrest.Matchers;
import org.junit.After;
import org.junit.Before;
import org.junit.Rule;
import org.junit.Test;
import org.junit.rules.ExpectedException;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.Map;
import java.util.concurrent.TimeUnit;

import static io.crate.testing.TestingHelpers.*;
import static org.hamcrest.Matchers.*;
import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;

@SuppressWarnings("unchecked")
public class WhereClauseAnalyzerTest extends CrateUnitTest {

    @Rule
    public ExpectedException expectedException = ExpectedException.none();

    private Analyzer analyzer;
    private AnalysisMetaData ctxMetaData;
    private ThreadPool threadPool;

    @Before
    public void setUp() throws Exception {
        super.setUp();
        threadPool = newMockedThreadPool();
        Injector injector = new ModulesBuilder()
                .add(new MockedClusterServiceModule())
                .add(new PredicateModule())
                .add(new OperatorModule())
                .add(new ScalarFunctionModule())
                .add(new MetaDataSysModule())
                .add(new TestMetaDataModule()).createInjector();
        analyzer = injector.getInstance(Analyzer.class);
        ctxMetaData = injector.getInstance(AnalysisMetaData.class);
    }

    @After
    public void after() throws Exception {
        threadPool.shutdown();
        threadPool.awaitTermination(1, TimeUnit.SECONDS);
    }

    static final Routing twoNodeRouting = new Routing(TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder()
            .put("nodeOne", TreeMapBuilder.<String, List<Integer>>newMapBuilder().put("t1", Arrays.asList(1, 2)).map())
            .put("nodeTow", TreeMapBuilder.<String, List<Integer>>newMapBuilder().put("t1", Arrays.asList(3, 4)).map())
            .map());

    class TestMetaDataModule extends MetaDataModule {
        @Override
        protected void bindSchemas() {
            super.bindSchemas();
            bind(ThreadPool.class).toInstance(threadPool);
            SchemaInfo schemaInfo = mock(SchemaInfo.class);
            when(schemaInfo.name()).thenReturn(ReferenceInfos.DEFAULT_SCHEMA_NAME);
            when(schemaInfo.getTableInfo("users")).thenReturn(
                    TestingTableInfo.builder(new TableIdent("doc", "users"), RowGranularity.DOC, twoNodeRouting)
                            .add("id", DataTypes.STRING, null)
                            .add("name", DataTypes.STRING, null)
                            .add("tags", new ArrayType(DataTypes.STRING), null)
                            .addPrimaryKey("id")
                            .clusteredBy("id")
                            .build());
            when(schemaInfo.getTableInfo("parted")).thenReturn(
                    TestingTableInfo.builder(new TableIdent("doc", "parted"), RowGranularity.DOC, twoNodeRouting)
                            .add("id", DataTypes.INTEGER, null)
                            .add("name", DataTypes.STRING, null)
                            .add("date", DataTypes.TIMESTAMP, null, true)
                            .add("obj", DataTypes.OBJECT, null, ColumnPolicy.IGNORED)
                            .addPartitions(
                                    new PartitionName("parted", Arrays.asList(new BytesRef("1395874800000"))).stringValue(),
                                    new PartitionName("parted", Arrays.asList(new BytesRef("1395961200000"))).stringValue(),
                                    new PartitionName("parted", new ArrayList<BytesRef>() {{
                                        add(null);
                                    }}).stringValue())
                            .build());
            when(schemaInfo.getTableInfo("parted_pk")).thenReturn(
                    TestingTableInfo.builder(new TableIdent("doc", "parted"), RowGranularity.DOC, twoNodeRouting)
                            .addPrimaryKey("id").addPrimaryKey("date")
                            .add("id", DataTypes.INTEGER, null)
                            .add("name", DataTypes.STRING, null)
                            .add("date", DataTypes.TIMESTAMP, null, true)
                            .add("obj", DataTypes.OBJECT, null, ColumnPolicy.IGNORED)
                            .addPartitions(
                                    new PartitionName("parted_pk", Arrays.asList(new BytesRef("1395874800000"))).stringValue(),
                                    new PartitionName("parted_pk", Arrays.asList(new BytesRef("1395961200000"))).stringValue(),
                                    new PartitionName("parted_pk", new ArrayList<BytesRef>() {{
                                        add(null);
                                    }}).stringValue())
                            .build());
            when(schemaInfo.getTableInfo("bystring")).thenReturn(
                    TestingTableInfo.builder(new TableIdent("doc", "bystring"), RowGranularity.DOC, twoNodeRouting)
                            .add("name", DataTypes.STRING, null)
                            .add("score", DataTypes.DOUBLE, null)
                            .addPrimaryKey("name")
                            .clusteredBy("name")
                            .build());
            when(schemaInfo.getTableInfo("users_multi_pk")).thenReturn(
                    TestingTableInfo.builder(new TableIdent("doc", "users_multi_pk"), RowGranularity.DOC, twoNodeRouting)
                            .add("id", DataTypes.LONG, null)
                            .add("name", DataTypes.STRING, null)
                            .add("details", DataTypes.OBJECT, null)
                            .add("awesome", DataTypes.BOOLEAN, null)
                            .add("friends", new ArrayType(DataTypes.OBJECT), null, ColumnPolicy.DYNAMIC)
                            .addPrimaryKey("id")
                            .addPrimaryKey("name")
                            .clusteredBy("id")
                            .build());
            when(schemaInfo.getTableInfo("users_clustered_by_only")).thenReturn(
                    TestingTableInfo.builder(new TableIdent("doc", "users_clustered_by_only"), RowGranularity.DOC, twoNodeRouting)
                            .add("id", DataTypes.LONG, null)
                            .add("name", DataTypes.STRING, null)
                            .add("details", DataTypes.OBJECT, null)
                            .add("awesome", DataTypes.BOOLEAN, null)
                            .add("friends", new ArrayType(DataTypes.OBJECT), null, ColumnPolicy.DYNAMIC)
                            .clusteredBy("id")
                            .build());
            schemaBinder.addBinding(ReferenceInfos.DEFAULT_SCHEMA_NAME).toInstance(schemaInfo);
        }
    }

    private DeleteAnalyzedStatement analyzeDelete(String stmt, Object[][] bulkArgs) {
        return (DeleteAnalyzedStatement) analyzer.analyze(SqlParser.createStatement(stmt),
                new ParameterContext(new Object[0], bulkArgs, ReferenceInfos.DEFAULT_SCHEMA_NAME)).analyzedStatement();
    }

    private DeleteAnalyzedStatement analyzeDelete(String stmt) {
        return analyzeDelete(stmt, new Object[0][]);
    }

    private UpdateAnalyzedStatement analyzeUpdate(String stmt) {
        return (UpdateAnalyzedStatement) analyzer.analyze(SqlParser.createStatement(stmt),
                new ParameterContext(new Object[0], new Object[0][], ReferenceInfos.DEFAULT_SCHEMA_NAME)).analyzedStatement();
    }

    private WhereClause analyzeSelect(String stmt, Object... args) {
        SelectAnalyzedStatement statement = (SelectAnalyzedStatement) analyzer.analyze(SqlParser.createStatement(stmt),
                new ParameterContext(args, new Object[0][], ReferenceInfos.DEFAULT_SCHEMA_NAME)).analyzedStatement();
        return statement.relation().querySpec().where();
    }

    private WhereClause analyzeSelectWhere(String stmt) {
        return analyzeSelect(stmt);
    }

    @Test
    public void testWhereSinglePKColumnEq() throws Exception {
        DeleteAnalyzedStatement statement = analyzeDelete("delete from users where id = ?", new Object[][]{
                new Object[]{1},
                new Object[]{2},
                new Object[]{3},
        });
        TableRelation tableRelation = statement.analyzedRelation();
        WhereClauseAnalyzer whereClauseAnalyzer = new WhereClauseAnalyzer(ctxMetaData, tableRelation);
        assertThat(whereClauseAnalyzer.analyze(statement.whereClauses().get(0)).docKeys().get(), contains(isDocKey("1")));
        assertThat(whereClauseAnalyzer.analyze(statement.whereClauses().get(1)).docKeys().get(), contains(isDocKey("2")));
        assertThat(whereClauseAnalyzer.analyze(statement.whereClauses().get(2)).docKeys().get(), contains(isDocKey("3")));
    }

    @Test
    public void testSelectByIdWithCustomRouting() throws Exception {
        WhereClause whereClause = analyzeSelect("select name from users_clustered_by_only where _id=1");
        assertFalse(whereClause.docKeys().isPresent());
    }

    @Test
    public void testSelectByIdWithPartitions() throws Exception {
        WhereClause whereClause = analyzeSelect("select id from parted where _id=1");
        assertFalse(whereClause.docKeys().isPresent());
    }

    @Test
    public void testSelectWherePartitionedByColumn() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select id from parted where date = 1395874800000");
        assertThat(whereClause.hasQuery(), is(false));
        assertThat(whereClause.noMatch(), is(false));
        assertThat(whereClause.partitions(),
                Matchers.contains(new PartitionName("parted", Arrays.asList(new BytesRef("1395874800000"))).stringValue()));
    }

    @Test
    public void testSelectPartitionedByPK() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select id from parted_pk where id = 1 and date = 1395874800000");
        assertThat(whereClause.docKeys().get(), contains(isDocKey(1, 1395874800000L)));

        assertThat(whereClause.partitions(), empty());

    }

    @Test
    public void testWherePartitionedByColumn() throws Exception {
        DeleteAnalyzedStatement statement = analyzeDelete("delete from parted where date = 1395874800000");
        WhereClause whereClause = statement.whereClauses().get(0);

        assertThat(whereClause.hasQuery(), is(false));
        assertThat(whereClause.noMatch(), is(false));
        assertThat(whereClause.partitions(),
                Matchers.contains(new PartitionName("parted", Arrays.asList(new BytesRef("1395874800000"))).stringValue()));
    }

    @Test
    public void testUpdateWherePartitionedByColumn() throws Exception {
        UpdateAnalyzedStatement updateAnalyzedStatement = analyzeUpdate("update parted set id = 2 where date = 1395874800000");
        UpdateAnalyzedStatement.NestedAnalyzedStatement nestedAnalyzedStatement = updateAnalyzedStatement.nestedStatements().get(0);

        assertThat(nestedAnalyzedStatement.whereClause().hasQuery(), is(false));
        assertThat(nestedAnalyzedStatement.whereClause().noMatch(), is(false));

        assertEquals(ImmutableList.of(
                        new PartitionName("parted", Arrays.asList(new BytesRef("1395874800000"))).stringValue()),
                nestedAnalyzedStatement.whereClause().partitions()
        );
    }

    @Test
    public void testClusteredByValueContainsComma() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select * from bystring where name = 'a,b,c'");
        assertThat(whereClause.clusteredBy().get(), contains(isLiteral("a,b,c")));
        assertThat(whereClause.docKeys().get().size(), is(1));
        assertThat(whereClause.docKeys().get().getOnlyKey(), isDocKey("a,b,c"));
    }

    @Test
    public void testEmptyClusteredByValue() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select * from bystring where name = ''");
        assertThat(whereClause.clusteredBy().get(), contains(isLiteral("")));
        assertThat(whereClause.docKeys().get().getOnlyKey(), isDocKey(""));
    }

    @Test
    public void testClusteredBy() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select name from users where id=1");
        assertThat(whereClause.clusteredBy().get(), contains(isLiteral("1")));
        assertThat(whereClause.docKeys().get().getOnlyKey(), isDocKey("1"));

        whereClause = analyzeSelectWhere("select name from users where id=1 or id=2");

        assertThat(whereClause.docKeys().get().size(), is(2));
        assertThat(whereClause.docKeys().get(), containsInAnyOrder(isDocKey("1"), isDocKey("2")));

        assertThat(whereClause.clusteredBy().get(), containsInAnyOrder(isLiteral("1"), isLiteral("2")));
    }


    @Test
    public void testClusteredByOnly() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select name from users_clustered_by_only where id=1");
        assertFalse(whereClause.docKeys().isPresent());
        assertThat(whereClause.clusteredBy().get(),  contains(isLiteral(1L)));

        whereClause = analyzeSelectWhere("select name from users_clustered_by_only where id=1 or id=2");
        assertFalse(whereClause.docKeys().isPresent());
        assertThat(whereClause.clusteredBy().get(), containsInAnyOrder(isLiteral(1L), isLiteral(2L)));

        whereClause = analyzeSelectWhere("select name from users_clustered_by_only where id in (3,4,5)");
        assertFalse(whereClause.docKeys().isPresent());
        assertThat(whereClause.clusteredBy().get(), containsInAnyOrder(
                isLiteral(3L), isLiteral(4L), isLiteral(5L)));



        whereClause = analyzeSelectWhere("select name from users_clustered_by_only where id=1 and id=2");
        assertFalse(whereClause.docKeys().isPresent());
        assertFalse(whereClause.clusteredBy().isPresent());
    }

    @Test
    public void testCompositePrimaryKey() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select name from users_multi_pk where id=1");
        assertFalse(whereClause.docKeys().isPresent());
        assertThat(whereClause.clusteredBy().get(), contains(isLiteral(1L)));

        whereClause = analyzeSelectWhere("select name from users_multi_pk where id=1 and name='Douglas'");
        assertThat(whereClause.docKeys().get(), contains(isDocKey(1L, "Douglas")));
        assertThat(whereClause.clusteredBy().get(), contains(isLiteral(1L)));

        whereClause = analyzeSelectWhere("select name from users_multi_pk where id=1 or id=2 and name='Douglas'");
        assertFalse(whereClause.docKeys().isPresent());
        assertThat(whereClause.clusteredBy().get(), containsInAnyOrder(
                isLiteral(1L), isLiteral(2L)));

        whereClause = analyzeSelectWhere("select name from users_multi_pk where id=1 and name='Douglas' or name='Arthur'");
        assertFalse(whereClause.docKeys().isPresent());
        assertFalse(whereClause.clusteredBy().isPresent());
    }

    @Test
    public void testPrimaryKeyAndVersion() throws Exception {
        WhereClause whereClause = analyzeSelectWhere(
                "select name from users where id = 2 and \"_version\" = 1");
        assertThat(whereClause.docKeys().get().getOnlyKey(), isDocKey("2", 1L));
    }

    @Test
    public void testMultiplePrimaryKeys() throws Exception {
        WhereClause whereClause = analyzeSelectWhere(
                "select name from users where id = 2 or id = 1");
        assertThat(whereClause.docKeys().get(), containsInAnyOrder(isDocKey("1"), isDocKey("2")));
        assertThat(whereClause.clusteredBy().get(), containsInAnyOrder(isLiteral("1"), isLiteral("2")));
    }

    @Test
    public void testMultiplePrimaryKeysAndInvalidColumn() throws Exception {
        WhereClause whereClause = analyzeSelectWhere(
                "select name from users where id = 2 or id = 1 and name = 'foo'");
        assertFalse(whereClause.docKeys().isPresent());
    }

    @Test
    public void testNotEqualsDoesntMatchPrimaryKey() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select name from users where id != 1");
        assertFalse(whereClause.docKeys().isPresent());
        assertFalse(whereClause.clusteredBy().isPresent());
    }

    @Test
    public void testMultipleCompoundPrimaryKeys() throws Exception {
        WhereClause whereClause = analyzeSelectWhere(
                "select * from sys.shards where (schema_name='doc' and id = 1 and table_name = 'foo' and partition_ident='') " +
                        "or (schema_name='doc' and id = 2 and table_name = 'bla' and partition_ident='')");

        assertThat(whereClause.docKeys().get(), containsInAnyOrder(
                isDocKey("doc", "foo", 1, ""), isDocKey("doc", "bla", 2, "")
        ));
        assertFalse(whereClause.clusteredBy().isPresent());

        whereClause = analyzeSelectWhere(
                "select * from sys.shards where (schema_name='doc' and id = 1 and table_name = 'foo') " +
                        "or (schema_name='doc' and id = 2 and table_name = 'bla') or id = 1");
        assertFalse(whereClause.docKeys().isPresent());
        assertFalse(whereClause.clusteredBy().isPresent());
    }

    @Test
    public void test1ColPrimaryKey() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select name from sys.nodes where id='jalla'");

        assertThat(whereClause.docKeys().get(), contains(isDocKey("jalla")));

        whereClause = analyzeSelectWhere("select name from sys.nodes where 'jalla'=id");
        assertThat(whereClause.docKeys().get(), contains(isDocKey("jalla")));

        whereClause = analyzeSelectWhere("select name from sys.nodes where id='jalla' and id='jalla'");
        assertThat(whereClause.docKeys().get(), contains(isDocKey("jalla")));

        whereClause = analyzeSelectWhere("select name from sys.nodes where id='jalla' and (id='jalla' or 1=1)");
        assertThat(whereClause.docKeys().get(), contains(isDocKey("jalla")));



        whereClause = analyzeSelectWhere("select name from sys.nodes where id='jalla' and id='kelle'");
        assertFalse(whereClause.docKeys().isPresent());


        whereClause = analyzeSelectWhere("select name from sys.nodes where id='jalla' or name = 'something'");
        assertFalse(whereClause.docKeys().isPresent());
        assertFalse(whereClause.noMatch());

        whereClause = analyzeSelectWhere("select name from sys.nodes where name = 'something'");
        assertFalse(whereClause.docKeys().isPresent());
        assertFalse(whereClause.noMatch());

    }

    @Test
    public void test3ColPrimaryKey() throws Exception {
        WhereClause whereClause = analyzeSelectWhere(
                "select id from sys.shards where id=1 and table_name='jalla' and schema_name='doc' and partition_ident=''");

        assertThat(whereClause.docKeys().get(), contains(isDocKey("doc", "jalla", 1, "")));
        assertFalse(whereClause.noMatch());

        whereClause = analyzeSelectWhere(
                "select id from sys.shards where id=1 and table_name='jalla' and id=1 and schema_name='doc' and partition_ident=''");
        assertThat(whereClause.docKeys().get(), contains(isDocKey("doc", "jalla", 1, "")));
        assertFalse(whereClause.noMatch());


        whereClause = analyzeSelectWhere("select id from sys.shards where id=1");
        assertFalse(whereClause.docKeys().isPresent());
        assertFalse(whereClause.noMatch());


        whereClause = analyzeSelectWhere(
                "select id from sys.shards where id=1 and schema_name='doc' and table_name='jalla' and id=2 and partition_ident=''");
        assertFalse(whereClause.docKeys().isPresent());


    }

    @Test
    public void test1ColPrimaryKeySetLiteralDiffMatches() throws Exception {
        WhereClause whereClause = analyzeSelectWhere(
                "select name from sys.nodes where id in ('jalla', 'kelle') and id in ('jalla', 'something')");
        assertFalse(whereClause.noMatch());
        assertThat(whereClause.docKeys().get(), contains(isDocKey("jalla")));
    }

    @Test
    public void test1ColPrimaryKeySetLiteral() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select name from sys.nodes where id in ('jalla', 'kelle')");
        assertFalse(whereClause.noMatch());
        assertThat(whereClause.docKeys().get(), containsInAnyOrder(isDocKey("jalla"), isDocKey("kelle")));
    }

    @Test
    public void test1ColPrimaryKeyNotSetLiteral() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select name from sys.nodes where id not in ('jalla', 'kelle')");
        assertFalse(whereClause.noMatch());
        assertFalse(whereClause.docKeys().isPresent());
        assertFalse(whereClause.clusteredBy().isPresent());
    }

    @Test
    public void test3ColPrimaryKeySetLiteral() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select id from sys.shards where id=1 and schema_name='doc' and" +
                " table_name in ('jalla', 'kelle') and partition_ident=''");
        assertThat(whereClause.docKeys().get(), containsInAnyOrder(
                isDocKey("doc", "jalla", 1, ""), isDocKey("doc", "kelle", 1, "")));
    }

    @Test
    public void test3ColPrimaryKeyWithOr() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select id from sys.shards where id=1 and schema_name='doc' and " +
                "(table_name = 'jalla' or table_name='kelle') and partition_ident=''");
        assertEquals(2, whereClause.docKeys().get().size());
        assertThat(whereClause.docKeys().get(), containsInAnyOrder(
                isDocKey("doc", "jalla", 1, ""), isDocKey("doc", "kelle", 1, "")));
    }

    @Test
    public void testSelectFromPartitionedTable() throws Exception {
        String partition1 = new PartitionName("parted", Arrays.asList(new BytesRef("1395874800000"))).stringValue();
        String partition2 = new PartitionName("parted", Arrays.asList(new BytesRef("1395961200000"))).stringValue();
        String partition3 = new PartitionName("parted", new ArrayList<BytesRef>() {{
            add(null);
        }}).stringValue();

        WhereClause whereClause = analyzeSelectWhere("select id, name from parted where date = 1395874800000");
        assertEquals(ImmutableList.of(partition1), whereClause.partitions());
        assertFalse(whereClause.hasQuery());
        assertFalse(whereClause.noMatch());

        whereClause = analyzeSelectWhere("select id, name from parted where date = 1395874800000 " +
                "and substr(name, 0, 4) = 'this'");
        assertEquals(ImmutableList.of(partition1), whereClause.partitions());
        assertThat(whereClause.hasQuery(), is(true));
        assertThat(whereClause.noMatch(), is(false));

        whereClause = analyzeSelectWhere("select id, name from parted where date >= 1395874800000");
        assertThat(whereClause.partitions(), containsInAnyOrder(partition1, partition2));
        assertFalse(whereClause.hasQuery());
        assertFalse(whereClause.noMatch());

        whereClause = analyzeSelectWhere("select id, name from parted where date < 1395874800000");
        assertEquals(ImmutableList.of(), whereClause.partitions());
        assertTrue(whereClause.noMatch());

        whereClause = analyzeSelectWhere("select id, name from parted where date = 1395874800000 and date = 1395961200000");
        assertEquals(ImmutableList.of(), whereClause.partitions());
        assertTrue(whereClause.noMatch());

        whereClause = analyzeSelectWhere("select id, name from parted where date = 1395874800000 or date = 1395961200000");
        assertThat(whereClause.partitions(), containsInAnyOrder(partition1, partition2));
        assertFalse(whereClause.hasQuery());
        assertFalse(whereClause.noMatch());

        whereClause = analyzeSelectWhere("select id, name from parted where date < 1395874800000 or date > 1395874800000");
        assertEquals(ImmutableList.of(partition2), whereClause.partitions());
        assertFalse(whereClause.hasQuery());
        assertFalse(whereClause.noMatch());

        whereClause = analyzeSelectWhere("select id, name from parted where date in (1395874800000, 1395961200000)");
        assertThat(whereClause.partitions(), containsInAnyOrder(partition1, partition2));
        assertFalse(whereClause.hasQuery());
        assertFalse(whereClause.noMatch());

        whereClause = analyzeSelectWhere("select id, name from parted where date in (1395874800000, 1395961200000) and id = 1");
        assertThat(whereClause.partitions(), containsInAnyOrder(partition1, partition2));
        assertTrue(whereClause.hasQuery());
        assertFalse(whereClause.noMatch());


        whereClause = analyzeSelectWhere("select id, name from parted where not (date = 1395874800000 and obj['col'] = 'undefined')");
        assertThat(whereClause.partitions(), containsInAnyOrder(partition1, partition2, partition3));
        assertThat(whereClause.hasQuery(), is(false));
        assertThat(whereClause.noMatch(), is(false));

        whereClause = analyzeSelectWhere("select id, name from parted where date in (1395874800000) or date in (1395961200000)");
        assertThat(whereClause.partitions(), containsInAnyOrder(partition1, partition2));
        assertFalse(whereClause.hasQuery());
        assertFalse(whereClause.noMatch());

        whereClause = analyzeSelectWhere("select id, name from parted where date = 1395961200000 and id = 1");
        assertEquals(ImmutableList.of(partition2), whereClause.partitions());
        assertTrue(whereClause.hasQuery());
        assertFalse(whereClause.noMatch());

        whereClause = analyzeSelectWhere("select id, name from parted where (date =1395874800000 or date = 1395961200000) and id = 1");
        assertThat(whereClause.partitions(), containsInAnyOrder(partition1, partition2));
        assertTrue(whereClause.hasQuery());
        assertFalse(whereClause.noMatch());

        whereClause = analyzeSelectWhere("select id, name from parted where date = 1395874800000 and id is null");
        assertEquals(ImmutableList.of(partition1), whereClause.partitions());
        assertTrue(whereClause.hasQuery());
        assertFalse(whereClause.noMatch());

        whereClause = analyzeSelectWhere("select id, name from parted where date is null and id = 1");
        assertEquals(ImmutableList.of(partition3), whereClause.partitions());
        assertTrue(whereClause.hasQuery());
        assertFalse(whereClause.noMatch());

        whereClause = analyzeSelectWhere("select id, name from parted where 1395874700000 < date and date < 1395961200001");
        assertThat(whereClause.partitions(), containsInAnyOrder(partition1, partition2));
        assertFalse(whereClause.hasQuery());
        assertFalse(whereClause.noMatch());

        whereClause = analyzeSelectWhere("select id, name from parted where '2014-03-16T22:58:20' < date and date < '2014-03-27T23:00:01'");
        assertThat(whereClause.partitions(), containsInAnyOrder(partition1, partition2));
        assertFalse(whereClause.hasQuery());
        assertFalse(whereClause.noMatch());
    }

    @Test
    public void testSelectFromPartitionedTableUnsupported() throws Exception {


        try {
            analyzeSelectWhere("select id, name from parted where date = 1395961200000 or id = 1");
            fail("Expected UnsupportedOperationException");
        } catch (UnsupportedOperationException e) {
            assertThat(e.getMessage(),
                    is("logical conjunction of the conditions in the WHERE clause which involve " +
                            "partitioned columns led to a query that can't be executed."));
        }

        try {
            analyzeSelectWhere("select id, name from parted where id = 1 or date = 1395961200000");
            fail("Expected UnsupportedOperationException");
        } catch (UnsupportedOperationException e) {
            assertThat(e.getMessage(),
                    is("logical conjunction of the conditions in the WHERE clause which involve " +
                            "partitioned columns led to a query that can't be executed."));
        }
    }

    @Test
    public void testAnyInvalidArrayType() throws Exception {
        expectedException.expect(IllegalArgumentException.class);
        expectedException.expectMessage("array expression of invalid type array(string)");
        analyzeSelectWhere("select * from users_multi_pk where awesome = any(['foo', 'bar', 'baz'])");
    }

    @Test
    public void testInConvertedToAnyIfOnlyLiterals() throws Exception {
        StringBuilder sb = new StringBuilder("select id from sys.shards where id in (");
        int i=0;
        for (; i < 1500; i++) {
            sb.append(i);
            sb.append(',');
        }
        sb.append(i++);
        sb.append(')');
        String s = sb.toString();

        WhereClause whereClause = analyzeSelectWhere(s);
        assertThat(whereClause.query(), isFunction(AnyEqOperator.NAME, ImmutableList.<DataType>of(DataTypes.INTEGER, new SetType(DataTypes.INTEGER))));
    }

    @Test
    public void testInNormalizedToAnyWithScalars() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select * from users where id in (null, 1+2, 3+4, abs(-99))");
        assertThat(whereClause.query(), isFunction(AnyEqOperator.NAME));
        assertThat(whereClause.docKeys().isPresent(), is(true));
        assertThat(whereClause.docKeys().get(), containsInAnyOrder(isNullDocKey(), isDocKey("3"), isDocKey("7"), isDocKey("99")));
    }

    @Test
    public void testAnyEqConvertableArrayTypeLiterals() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select * from users where name = any([1, 2, 3])");
        assertThat(whereClause.query(), isFunction(AnyEqOperator.NAME, ImmutableList.<DataType>of(DataTypes.STRING, new ArrayType(DataTypes.STRING))));
    }

    @Test
    public void testAnyLikeConvertableArrayTypeLiterals() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select * from users where name like any([1, 2, 3])");
        assertThat(whereClause.query(), isFunction(AnyLikeOperator.NAME, ImmutableList.<DataType>of(DataTypes.STRING, new ArrayType(DataTypes.STRING))));
    }

    @Test
    public void testAnyLikeArrayLiteral() throws Exception {
        WhereClause whereClause = analyzeSelectWhere("select * from users where name like any(['a', 'b', 'c'])");
        assertThat(whereClause.query(), isFunction(AnyLikeOperator.NAME, ImmutableList.<DataType>of(DataTypes.STRING, new ArrayType(DataTypes.STRING))));
    }
}

<code block>


package io.crate.metadata.table;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableMap;
import io.crate.analyze.AlterPartitionedTableParameterInfo;
import io.crate.analyze.TableParameterInfo;
import io.crate.analyze.WhereClause;
import io.crate.exceptions.ColumnUnknownException;
import io.crate.metadata.*;
import io.crate.metadata.doc.DocIndexMetaData;
import io.crate.metadata.doc.DocSysColumns;
import io.crate.planner.RowGranularity;
import io.crate.planner.symbol.DynamicReference;
import io.crate.types.DataType;
import org.mockito.Answers;

import javax.annotation.Nullable;
import java.util.Collection;
import java.util.Iterator;
import java.util.List;
import java.util.Map;

import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;

public class TestingTableInfo extends AbstractDynamicTableInfo {

    private final Routing routing;
    private final ColumnIdent clusteredBy;

    public static Builder builder(TableIdent ident, RowGranularity granularity, Routing routing) {
        return new Builder(ident, granularity, routing);
    }

    public static class Builder {

        private final ImmutableList.Builder<ReferenceInfo> columns = ImmutableList.builder();
        private final ImmutableMap.Builder<ColumnIdent, ReferenceInfo> references = ImmutableMap.builder();
        private final ImmutableList.Builder<ReferenceInfo> partitionedByColumns = ImmutableList.builder();
        private final ImmutableList.Builder<ColumnIdent> primaryKey = ImmutableList.builder();
        private final ImmutableList.Builder<ColumnIdent> partitionedBy = ImmutableList.builder();
        private final ImmutableList.Builder<PartitionName> partitions = ImmutableList.builder();
        private final ImmutableMap.Builder<ColumnIdent, IndexReferenceInfo> indexColumns = ImmutableMap.builder();
        private ColumnIdent clusteredBy;


        private final RowGranularity granularity;
        private final TableIdent ident;
        private final Routing routing;
        private boolean isAlias = false;
        private ColumnPolicy columnPolicy = ColumnPolicy.DYNAMIC;

        private SchemaInfo schemaInfo = mock(SchemaInfo.class, Answers.RETURNS_MOCKS.get());

        public Builder(TableIdent ident, RowGranularity granularity, Routing routing) {
            this.granularity = granularity;
            this.routing = routing;
            this.ident = ident;
        }

        private ReferenceInfo genInfo(ColumnIdent columnIdent, DataType type) {
            return new ReferenceInfo(
                    new ReferenceIdent(ident, columnIdent.name(), columnIdent.path()),
                    RowGranularity.DOC, type
            );
        }

        private void addDocSysColumns() {
            for (Map.Entry<ColumnIdent, DataType> entry : DocSysColumns.COLUMN_IDENTS.entrySet()) {
                references.put(
                        entry.getKey(),
                        genInfo(entry.getKey(), entry.getValue())
                );
            }
        }

        public Builder add(String column, DataType type, List<String> path) {
            return add(column, type, path, ColumnPolicy.DYNAMIC);
        }
        public Builder add(String column, DataType type, List<String> path, ColumnPolicy columnPolicy) {
            return add(column, type, path, columnPolicy, ReferenceInfo.IndexType.NOT_ANALYZED, false);
        }
        public Builder add(String column, DataType type, List<String> path, ReferenceInfo.IndexType indexType) {
            return add(column, type, path, ColumnPolicy.DYNAMIC, indexType, false);
        }
        public Builder add(String column, DataType type, List<String> path,
                           boolean partitionBy) {
            return add(column, type, path, ColumnPolicy.DYNAMIC,
                    ReferenceInfo.IndexType.NOT_ANALYZED, partitionBy);
        }

        public Builder add(String column, DataType type, List<String> path,
                           ColumnPolicy columnPolicy, ReferenceInfo.IndexType indexType,
                           boolean partitionBy) {
            RowGranularity rowGranularity = granularity;
            if (partitionBy) {
                rowGranularity = RowGranularity.PARTITION;
            }
            ReferenceInfo info = new ReferenceInfo(new ReferenceIdent(ident, column, path),
                    rowGranularity, type, columnPolicy, indexType);
            if (info.ident().isColumn()) {
                columns.add(info);
            }
            references.put(info.ident().columnIdent(), info);
            if (partitionBy) {
                partitionedByColumns.add(info);
                partitionedBy.add(info.ident().columnIdent());
            }
            return this;
        }

        public Builder addIndex(ColumnIdent columnIdent, ReferenceInfo.IndexType indexType) {
            IndexReferenceInfo.Builder builder = new IndexReferenceInfo.Builder()
                    .ident(new ReferenceIdent(ident, columnIdent))
                    .indexType(indexType);
            indexColumns.put(columnIdent, builder.build());
            return this;
        }

        public Builder addPrimaryKey(String column) {
            primaryKey.add(ColumnIdent.fromPath(column));
            return this;
        }

        public Builder clusteredBy(String clusteredBy) {
            this.clusteredBy = ColumnIdent.fromPath(clusteredBy);
            return this;
        }

        public Builder isAlias(boolean isAlias) {
            this.isAlias = isAlias;
            return this;
        }

        public Builder schemaInfo(SchemaInfo schemaInfo) {
            this.schemaInfo = schemaInfo;
            return this;
        }

        public Builder addPartitions(String... partitionNames) {
            for (String partitionName : partitionNames) {
                PartitionName partition = PartitionName.fromString(partitionName, ident.schema(), ident.name());
                partitions.add(partition);
            }
            return this;
        }

        public TableInfo build() {
            addDocSysColumns();
            return new TestingTableInfo(
                    columns.build(),
                    partitionedByColumns.build(),
                    indexColumns.build(),
                    references.build(),
                    ident,
                    granularity,
                    routing,
                    primaryKey.build(),
                    clusteredBy,
                    isAlias,
                    partitionedBy.build(),
                    partitions.build(),
                    columnPolicy,
                    schemaInfo == null ? mock(SchemaInfo.class, Answers.RETURNS_MOCKS.get()) : schemaInfo);
        }

    }


    private final List<ReferenceInfo> columns;
    private final List<ReferenceInfo> partitionedByColumns;
    private final Map<ColumnIdent, IndexReferenceInfo> indexColumns;
    private final Map<ColumnIdent, ReferenceInfo> references;
    private final TableIdent ident;
    private final RowGranularity granularity;
    private final List<ColumnIdent> primaryKey;
    private final boolean isAlias;
    private boolean hasAutoGeneratedPrimaryKey = false;
    private final List<ColumnIdent> partitionedBy;
    private final List<PartitionName> partitions;
    private final ColumnPolicy columnPolicy;
    private final TableParameterInfo tableParameterInfo;


    public TestingTableInfo(List<ReferenceInfo> columns,
                            List<ReferenceInfo> partitionedByColumns,
                            Map<ColumnIdent, IndexReferenceInfo> indexColumns,
                            Map<ColumnIdent, ReferenceInfo> references,
                            TableIdent ident, RowGranularity granularity,
                            Routing routing,
                            List<ColumnIdent> primaryKey,
                            ColumnIdent clusteredBy,
                            boolean isAlias,
                            List<ColumnIdent> partitionedBy,
                            List<PartitionName> partitions,
                            ColumnPolicy columnPolicy,
                            SchemaInfo schemaInfo
                            ) {
        super(schemaInfo);
        this.columns = columns;
        this.partitionedByColumns = partitionedByColumns;
        this.indexColumns = indexColumns;
        this.references = references;
        this.ident = ident;
        this.granularity = granularity;
        this.routing = routing;
        if (primaryKey == null || primaryKey.isEmpty()){
            if ((clusteredBy == null || clusteredBy.equals("_id")) && partitionedBy.isEmpty()){
                this.primaryKey = ImmutableList.of(DocIndexMetaData.ID_IDENT);
                this.hasAutoGeneratedPrimaryKey = true;
            } else {
                this.primaryKey = ImmutableList.of();
            }
        } else {
            this.primaryKey = primaryKey;
        }
        this.clusteredBy = clusteredBy;
        this.isAlias = isAlias;
        this.columnPolicy = columnPolicy;
        this.partitionedBy = partitionedBy;
        this.partitions = partitions;
        if (partitionedByColumns.isEmpty()) {
            tableParameterInfo = new TableParameterInfo();
        } else {
            tableParameterInfo = new AlterPartitionedTableParameterInfo();
        }
    }

    @Override
    public ReferenceInfo getReferenceInfo(ColumnIdent columnIdent) {
        return references.get(columnIdent);
    }

    @Override
    public Collection<ReferenceInfo> columns() {
        return columns;
    }


    @Override
    public List<ReferenceInfo> partitionedByColumns() {
        return partitionedByColumns;
    }

    @Override
    public IndexReferenceInfo indexColumn(ColumnIdent ident) {
        return indexColumns.get(ident);
    }

    @Override
    public boolean isPartitioned() {
        return !partitionedByColumns.isEmpty();
    }

    @Override
    public RowGranularity rowGranularity() {
        return granularity;
    }

    @Override
    public TableIdent ident() {
        return ident;
    }

    @Override
    public Routing getRouting(WhereClause whereClause, @Nullable String preference) {
        return routing;
    }

    @Override
    public List<ColumnIdent> primaryKey() {
        return primaryKey;
    }

    @Override
    public boolean hasAutoGeneratedPrimaryKey() {
        return hasAutoGeneratedPrimaryKey;
    }

    @Override
    public ColumnIdent clusteredBy() {
        return clusteredBy;
    }

    @Override
    public boolean isAlias() {
        return isAlias;
    }

    @Override
    public String[] concreteIndices() {
        return new String[]{ident.esName()};
    }

    @Override
    public DynamicReference getDynamic(ColumnIdent ident) {
        if (!ident.isColumn()) {
            ColumnIdent parentIdent = ident.getParent();
            ReferenceInfo parentInfo = getReferenceInfo(parentIdent);
            if (parentInfo != null && parentInfo.columnPolicy() == ColumnPolicy.STRICT) {
                throw new ColumnUnknownException(ident.sqlFqn());
            }
        }
        return new DynamicReference(new ReferenceIdent(ident(), ident), rowGranularity());
    }

    @Override
    public Iterator<ReferenceInfo> iterator() {
        return references.values().iterator();
    }

    @Override
    public List<ColumnIdent> partitionedBy() {
        return partitionedBy;
    }

    @Override
    public List<PartitionName> partitions() {
        return partitions;
    }

    @Override
    public ColumnPolicy columnPolicy() {
        return columnPolicy;
    }

    @Override
    public TableParameterInfo tableParameterInfo () {
        return tableParameterInfo;
    }

    @Override
    public SchemaInfo schemaInfo() {
        final SchemaInfo schemaInfo = super.schemaInfo();
        when(schemaInfo.name()).thenReturn(ident.schema());
        return schemaInfo;
    }
}

<code block>


package io.crate.lucene;

import com.google.common.collect.Sets;
import io.crate.analyze.WhereClause;
import io.crate.metadata.Functions;
import io.crate.operation.operator.*;
import io.crate.operation.operator.any.*;
import io.crate.planner.symbol.Literal;
import io.crate.planner.symbol.Reference;
import io.crate.test.integration.CrateUnitTest;
import io.crate.types.ArrayType;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import io.crate.types.SetType;
import org.apache.lucene.queries.BooleanFilter;
import org.apache.lucene.queries.TermsFilter;
import org.apache.lucene.sandbox.queries.regex.RegexQuery;
import org.apache.lucene.search.*;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.common.inject.ModulesBuilder;
import org.elasticsearch.common.lucene.search.MatchNoDocsQuery;
import org.elasticsearch.common.lucene.search.XConstantScoreQuery;
import org.elasticsearch.index.cache.IndexCache;
import org.elasticsearch.search.internal.SearchContext;
import org.junit.Before;
import org.junit.Test;
import org.mockito.Answers;

import java.util.Arrays;

import static io.crate.testing.TestingHelpers.*;
import static org.hamcrest.Matchers.instanceOf;
import static org.hamcrest.Matchers.is;
import static org.mockito.Mockito.mock;

public class LuceneQueryBuilderTest extends CrateUnitTest {

    private LuceneQueryBuilder builder;
    private SearchContext searchContext;
    private IndexCache indexCache;

    @Before
    public void prepare() throws Exception {
        Functions functions = new ModulesBuilder()
                .add(new OperatorModule()).createInjector().getInstance(Functions.class);
        builder = new LuceneQueryBuilder(functions);
        searchContext = mock(SearchContext.class, Answers.RETURNS_MOCKS.get());
        indexCache = mock(IndexCache.class, Answers.RETURNS_MOCKS.get());
    }

    @Test
    public void testNoMatchWhereClause() throws Exception {
        Query query = convert(WhereClause.NO_MATCH);
        assertThat(query, instanceOf(MatchNoDocsQuery.class));
    }

    @Test
    public void testWhereRefEqNullWithDifferentTypes() throws Exception {
        for (DataType type : DataTypes.PRIMITIVE_TYPES) {
            Reference foo = createReference("foo", type);
            Query query = convert(whereClause(EqOperator.NAME, foo, Literal.newLiteral(type, null)));





            assertThat(query, instanceOf(MatchNoDocsQuery.class));
        }
    }

    @Test
    public void testWhereRefEqRef() throws Exception {
        Reference foo = createReference("foo", DataTypes.STRING);
        Query query = convert(whereClause(EqOperator.NAME, foo, foo));
        assertThat(query, instanceOf(FilteredQuery.class));
    }

    @Test
    public void testLteQuery() throws Exception {
        Query query = convert(new WhereClause(createFunction(LteOperator.NAME,
                DataTypes.BOOLEAN,
                createReference("x", DataTypes.INTEGER),
                Literal.newLiteral(10))));
        assertThat(query, instanceOf(NumericRangeQuery.class));
        assertThat(query.toString(), is("x:{* TO 10]"));
    }

    @Test
    public void testEqOnTwoArraysBecomesGenericFunctionQuery() throws Exception {
        DataType longArray = new ArrayType(DataTypes.LONG);
        Query query = convert(new WhereClause(createFunction(EqOperator.NAME,
                DataTypes.BOOLEAN,
                createReference("x", longArray),
                Literal.newLiteral(longArray, new Object[] { 10L, null, 20L }))));
        assertThat(query, instanceOf(FilteredQuery.class));
        FilteredQuery filteredQuery = (FilteredQuery) query;

        assertThat(filteredQuery.getFilter(), instanceOf(BooleanFilter.class));
        assertThat(filteredQuery.getQuery(), instanceOf(XConstantScoreQuery.class));

        BooleanFilter filter = (BooleanFilter) filteredQuery.getFilter();
        assertThat(filter.clauses().get(0).getFilter(), instanceOf(BooleanFilter.class)); 
        assertThat(filter.clauses().get(1).getFilter(), instanceOf(Filter.class)); 
    }

    @Test
    public void testEqOnTwoArraysBecomesGenericFunctionQueryAllValuesNull() throws Exception {
        DataType longArray = new ArrayType(DataTypes.LONG);
        Query query = convert(new WhereClause(createFunction(EqOperator.NAME,
                DataTypes.BOOLEAN,
                createReference("x", longArray),
                Literal.newLiteral(longArray, new Object[] { null, null, null }))));
        assertThat(query, instanceOf(FilteredQuery.class));
    }

    @Test
    public void testEqOnArrayWithTooManyClauses() throws Exception {
        Object[] values = new Object[2000]; 
        Arrays.fill(values, 10L);
        DataType longArray = new ArrayType(DataTypes.LONG);
        Query query = convert(new WhereClause(createFunction(EqOperator.NAME,
                DataTypes.BOOLEAN,
                createReference("x", longArray),
                Literal.newLiteral(longArray, values))));
        assertThat(query, instanceOf(FilteredQuery.class));
    }

    @Test
    public void testGteQuery() throws Exception {
        Query query = convert(new WhereClause(createFunction(GteOperator.NAME,
                DataTypes.BOOLEAN,
                createReference("x", DataTypes.INTEGER),
                Literal.newLiteral(10))));
        assertThat(query, instanceOf(NumericRangeQuery.class));
        assertThat(query.toString(), is("x:[10 TO *}"));
    }

    @Test
    public void testWhereRefInSetLiteralIsConvertedToBooleanQuery() throws Exception {
        DataType dataType = new SetType(DataTypes.INTEGER);
        Reference foo = createReference("foo", DataTypes.INTEGER);
        WhereClause whereClause = new WhereClause(
                createFunction(InOperator.NAME, DataTypes.BOOLEAN,
                        foo,
                        Literal.newLiteral(dataType, Sets.newHashSet(1, 3))));
        Query query = convert(whereClause);
        assertThat(query, instanceOf(FilteredQuery.class));
        assertThat(((FilteredQuery)query).getFilter(), instanceOf(TermsFilter.class));
    }

    @Test
    public void testWhereStringRefInSetLiteralIsConvertedToBooleanQuery() throws Exception {
        DataType dataType = new SetType(DataTypes.STRING);
        Reference foo = createReference("foo", DataTypes.STRING);
        WhereClause whereClause = new WhereClause(
                createFunction(InOperator.NAME, DataTypes.BOOLEAN,
                        foo,
                        Literal.newLiteral(dataType, Sets.newHashSet(new BytesRef("foo"), new BytesRef("bar")))
                ));
        Query query = convert(whereClause);
        assertThat(query, instanceOf(FilteredQuery.class));
        assertThat(((FilteredQuery)query).getFilter(), instanceOf(TermsFilter.class));
    }


    @Test
    public void testRegexQueryFast() throws Exception {
        Reference value = createReference("foo", DataTypes.STRING);
        Literal pattern = Literal.newLiteral(new BytesRef("[a-z]"));
        Query query = convert(whereClause(RegexpMatchOperator.NAME, value, pattern));
        assertThat(query, instanceOf(RegexpQuery.class));
    }


    @Test
    public void testRegexQueryPcre() throws Exception {
        Reference value = createReference("foo", DataTypes.STRING);
        Literal pattern = Literal.newLiteral(new BytesRef("\\D"));
        Query query = convert(whereClause(RegexpMatchOperator.NAME, value, pattern));
        assertThat(query, instanceOf(RegexQuery.class));
    }

    @Test
    public void testIdQuery() throws Exception {
        Reference ref = createReference("_id", DataTypes.STRING);
        Query query = convert(whereClause(EqOperator.NAME, ref, Literal.newLiteral("i1")));
        assertThat(query, instanceOf(TermQuery.class));
        assertThat(query.toString(), is("_uid:default#i1"));
    }

    @Test
    public void testAnyEqArrayLiteral() throws Exception {
        Reference ref = createReference("d", DataTypes.DOUBLE);
        Literal doubleArrayLiteral = Literal.newLiteral(new Object[]{-1.5d, 0.0d, 1.5d}, new ArrayType(DataTypes.DOUBLE));
        Query query = convert(whereClause(AnyEqOperator.NAME, ref, doubleArrayLiteral));
        assertThat(query, instanceOf(FilteredQuery.class));
        assertThat(((FilteredQuery)query).getFilter(), instanceOf(TermsFilter.class));
    }

    @Test
    public void testAnyEqArrayReference() throws Exception {
        Reference ref = createReference("d_array", new ArrayType(DataTypes.DOUBLE));
        Literal doubleLiteral = Literal.newLiteral(1.5d);
        Query query = convert(whereClause(AnyEqOperator.NAME, doubleLiteral, ref));
        assertThat(query.toString(), is("d_array:[1.5 TO 1.5]"));
    }

    @Test
    public void testAnyGreaterAndSmaller() throws Exception {

        Reference arrayRef = createReference("d_array", new ArrayType(DataTypes.DOUBLE));
        Literal doubleLiteral = Literal.newLiteral(1.5d);

        Reference ref = createReference("d", DataTypes.DOUBLE);
        Literal arrayLiteral = Literal.newLiteral(new Object[]{1.2d, 3.5d}, new ArrayType(DataTypes.DOUBLE));


        Query ltQuery = convert(whereClause(AnyLtOperator.NAME, doubleLiteral, arrayRef));
        assertThat(ltQuery.toString(), is("d_array:{1.5 TO *}"));


        Query ltQuery2 = convert(whereClause(AnyLtOperator.NAME, ref, arrayLiteral));
        assertThat(ltQuery2.toString(), is("(d:{* TO 1.2} d:{* TO 3.5})~1"));


        Query lteQuery = convert(whereClause(AnyLteOperator.NAME, doubleLiteral, arrayRef));
        assertThat(lteQuery.toString(), is("d_array:[1.5 TO *}"));


        Query lteQuery2 = convert(whereClause(AnyLteOperator.NAME, ref, arrayLiteral));
        assertThat(lteQuery2.toString(), is("(d:{* TO 1.2] d:{* TO 3.5])~1"));


        Query gtQuery = convert(whereClause(AnyGtOperator.NAME, doubleLiteral, arrayRef));
        assertThat(gtQuery.toString(), is("d_array:{* TO 1.5}"));


        Query gtQuery2 = convert(whereClause(AnyGtOperator.NAME, ref, arrayLiteral));
        assertThat(gtQuery2.toString(), is("(d:{1.2 TO *} d:{3.5 TO *})~1"));


        Query gteQuery = convert(whereClause(AnyGteOperator.NAME, doubleLiteral, arrayRef));
        assertThat(gteQuery.toString(), is("d_array:{* TO 1.5]"));


        Query gteQuery2 = convert(whereClause(AnyGteOperator.NAME, ref, arrayLiteral));
        assertThat(gteQuery2.toString(), is("(d:[1.2 TO *} d:[3.5 TO *})~1"));
    }

    @Test
    public void testAnyOnArrayLiteral() throws Exception {
        Reference ref = createReference("d", DataTypes.STRING);
        Literal stringArrayLiteral = Literal.newLiteral(new Object[]{new BytesRef("a"), new BytesRef("b"), new BytesRef("c")}, new ArrayType(DataTypes.STRING));


        Query neqQuery = convert(whereClause(AnyNeqOperator.NAME, ref, stringArrayLiteral));
        assertThat(neqQuery, instanceOf(FilteredQuery.class));
        assertThat(((FilteredQuery)neqQuery).getFilter(), instanceOf(BooleanFilter.class));
        BooleanFilter filter = (BooleanFilter)((FilteredQuery) neqQuery).getFilter();
        assertThat(filter.toString(), is("BooleanFilter(-BooleanFilter(+d:a +d:b +d:c))"));


        Query likeQuery = convert(whereClause(AnyLikeOperator.NAME, ref, stringArrayLiteral));
        assertThat(likeQuery, instanceOf(BooleanQuery.class));
        BooleanQuery likeBQuery = (BooleanQuery)likeQuery;
        assertThat(likeBQuery.clauses().size(), is(3));
        for (int i = 0; i < 2; i++) {
            assertThat(likeBQuery.clauses().get(i).getQuery(), instanceOf(WildcardQuery.class));
        }


        Query notLikeQuery = convert(whereClause(AnyNotLikeOperator.NAME, ref, stringArrayLiteral));
        assertThat(notLikeQuery, instanceOf(BooleanQuery.class));
        BooleanQuery notLikeBQuery = (BooleanQuery)notLikeQuery;
        assertThat(notLikeBQuery.toString(), is("-(+d:a +d:b +d:c)"));



        Query ltQuery2 = convert(whereClause(AnyLtOperator.NAME, ref, stringArrayLiteral));
        assertThat(ltQuery2, instanceOf(BooleanQuery.class));
        BooleanQuery ltBQuery = (BooleanQuery)ltQuery2;
        assertThat(ltBQuery.toString(), is("(d:{* TO a} d:{* TO b} d:{* TO c})~1"));
    }

    private Query convert(WhereClause clause) {
        return builder.convert(clause, searchContext, indexCache).query;
    }
}
<code block>


package io.crate.operation.projectors;

import com.google.common.collect.Lists;
import io.crate.Streamer;
import io.crate.executor.transport.distributed.DistributingDownstream;
import io.crate.executor.transport.distributed.SingleBucketBuilder;
import io.crate.executor.transport.distributed.TransportDistributedResultAction;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.node.ExecutionNodes;
import io.crate.planner.node.StreamerVisitor;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.inject.Singleton;

import java.util.ArrayList;
import java.util.Collections;
import java.util.UUID;

@Singleton
public class InternalResultProviderFactory implements ResultProviderFactory {

    private final ClusterService clusterService;
    private final TransportDistributedResultAction transportDistributedResultAction;
    private final StreamerVisitor streamerVisitor;

    @Inject
    public InternalResultProviderFactory(ClusterService clusterService,
                                         TransportDistributedResultAction transportDistributedResultAction,
                                         StreamerVisitor streamerVisitor) {
        this.clusterService = clusterService;
        this.transportDistributedResultAction = transportDistributedResultAction;
        this.streamerVisitor = streamerVisitor;
    }

    public ResultProvider createDownstream(ExecutionNode node, UUID jobId) {
        Streamer<?>[] streamers = getStreamers(node);

        if (ExecutionNodes.hasDirectResponseDownstream(node.downstreamNodes())) {
            return new SingleBucketBuilder(streamers);
        } else {
            assert node.downstreamNodes().size() > 0 : "must have at least one downstream";


            ArrayList<String> server = Lists.newArrayList(node.executionNodes());
            Collections.sort(server);
            int bucketIdx = server.indexOf(clusterService.localNode().id());

            return new DistributingDownstream(
                    jobId,
                    node.downstreamExecutionNodeId(),
                    bucketIdx,
                    node.downstreamNodes(),
                    transportDistributedResultAction,
                    streamers
            );
        }
    }

    protected Streamer<?>[] getStreamers(ExecutionNode node) {
        return streamerVisitor.processExecutionNode(node).outputStreamers();
    }
}

<code block>


package io.crate.operation.collect;

import com.google.common.base.Function;
import com.google.common.base.Throwables;
import com.google.common.collect.ImmutableMap;
import com.google.common.collect.Lists;
import com.google.common.util.concurrent.*;
import io.crate.analyze.EvaluatingNormalizer;
import io.crate.exceptions.TableUnknownException;
import io.crate.exceptions.UnhandledServerException;
import io.crate.executor.transport.TransportActionProvider;
import io.crate.metadata.Functions;
import io.crate.metadata.ReferenceResolver;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.*;
import io.crate.operation.collect.files.FileCollectInputSymbolVisitor;
import io.crate.operation.collect.files.FileInputFactory;
import io.crate.operation.collect.files.FileReadingCollector;
import io.crate.operation.projectors.FlatProjectorChain;
import io.crate.operation.projectors.ProjectionToProjectorVisitor;
import io.crate.operation.projectors.ResultProvider;
import io.crate.operation.projectors.ResultProviderFactory;
import io.crate.operation.reference.file.FileLineReferenceResolver;
import io.crate.operation.reference.sys.node.NodeSysExpression;
import io.crate.operation.reference.sys.node.NodeSysReferenceResolver;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.FileUriCollectNode;
import io.crate.planner.symbol.ValueSymbolVisitor;
import org.elasticsearch.action.bulk.BulkRetryCoordinatorPool;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.inject.Injector;
import org.elasticsearch.common.inject.Singleton;
import org.elasticsearch.common.logging.ESLogger;
import org.elasticsearch.common.logging.Loggers;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.index.IndexService;
import org.elasticsearch.index.IndexShardMissingException;
import org.elasticsearch.indices.IndexMissingException;
import org.elasticsearch.indices.IndicesService;
import org.elasticsearch.threadpool.ThreadPool;

import javax.annotation.Nullable;
import java.util.*;
import java.util.concurrent.Callable;
import java.util.concurrent.CancellationException;
import java.util.concurrent.RejectedExecutionException;
import java.util.concurrent.ThreadPoolExecutor;


@Singleton
public class MapSideDataCollectOperation implements CollectOperation, RowUpstream {

    private static final ESLogger LOGGER = Loggers.getLogger(MapSideDataCollectOperation.class);

    private static class VoidFunction<Arg> implements Function<Arg, Void> {
        @Nullable
        @Override
        public Void apply(@Nullable Arg input) {
            return null;
        }
    }

    private final IndicesService indicesService;
    protected final EvaluatingNormalizer nodeNormalizer;
    protected final ClusterService clusterService;
    private final FileCollectInputSymbolVisitor fileInputSymbolVisitor;
    private final CollectServiceResolver collectServiceResolver;
    private final ProjectionToProjectorVisitor projectorVisitor;
    private final ThreadPoolExecutor executor;
    private final ListeningExecutorService listeningExecutorService;
    private final int poolSize;
    private final ResultProviderFactory resultProviderFactory;

    private final InformationSchemaCollectService informationSchemaCollectService;
    private final UnassignedShardsCollectService unassignedShardsCollectService;

    private final OneRowCollectService clusterCollectService;
    private final CollectService nodeCollectService;

    private final Functions functions;
    private final NodeSysExpression nodeSysExpression;
    private ThreadPool threadPool;
    private final BulkRetryCoordinatorPool bulkRetryCoordinatorPool;
    private final TransportActionProvider transportActionProvider;
    private final Settings settings;

    @Inject
    public MapSideDataCollectOperation(ClusterService clusterService,
                                       Settings settings,
                                       TransportActionProvider transportActionProvider,
                                       BulkRetryCoordinatorPool bulkRetryCoordinatorPool,
                                       Functions functions,
                                       ReferenceResolver referenceResolver,
                                       NodeSysExpression nodeSysExpression,
                                       IndicesService indicesService,
                                       ThreadPool threadPool,
                                       CollectServiceResolver collectServiceResolver,
                                       ResultProviderFactory resultProviderFactory,
                                       InformationSchemaCollectService informationSchemaCollectService,
                                       UnassignedShardsCollectService unassignedShardsCollectService) {
        this.resultProviderFactory = resultProviderFactory;
        this.informationSchemaCollectService = informationSchemaCollectService;
        this.unassignedShardsCollectService = unassignedShardsCollectService;
        this.executor = (ThreadPoolExecutor)threadPool.executor(ThreadPool.Names.SEARCH);
        this.poolSize = executor.getCorePoolSize();
        this.listeningExecutorService = MoreExecutors.listeningDecorator(executor);

        this.clusterService = clusterService;
        this.indicesService = indicesService;
        this.nodeNormalizer = new EvaluatingNormalizer(functions, RowGranularity.NODE, referenceResolver);

        this.collectServiceResolver = collectServiceResolver;

        this.settings = settings;
        this.functions = functions;
        this.nodeSysExpression = nodeSysExpression;

        this.clusterCollectService = new OneRowCollectService(new ImplementationSymbolVisitor(
                referenceResolver,
                functions,
                RowGranularity.CLUSTER
        ));
        this.nodeCollectService = new CollectService() {
            @Override
            public CrateCollector getCollector(CollectNode node, RowDownstream downstream) {
                return getNodeLevelCollector(node, downstream);
            }
        };
        this.fileInputSymbolVisitor =
                new FileCollectInputSymbolVisitor(functions, FileLineReferenceResolver.INSTANCE);

        this.threadPool = threadPool;
        this.bulkRetryCoordinatorPool = bulkRetryCoordinatorPool;
        this.transportActionProvider = transportActionProvider;

        ImplementationSymbolVisitor nodeImplementationSymbolVisitor = new ImplementationSymbolVisitor(
                referenceResolver,
                functions,
                RowGranularity.NODE
        );
        this.projectorVisitor = new ProjectionToProjectorVisitor(
                clusterService,
                threadPool,
                settings,
                transportActionProvider,
                bulkRetryCoordinatorPool,
                nodeImplementationSymbolVisitor
        );
    }


    public ResultProvider createDownstream(CollectNode collectNode) {
        return resultProviderFactory.createDownstream(collectNode, collectNode.jobId());
    }


    @Override
    public ListenableFuture<List<Void>> collect(CollectNode collectNode,
                                                RowDownstream downstream,
                                                final JobCollectContext jobCollectContext) {
        assert collectNode.isRouted(); 
        assert collectNode.jobId() != null : "no jobId present for collect operation";
        String localNodeId = clusterService.state().nodes().localNodeId();
        Set<String> routingNodes = collectNode.routing().nodes();
        if (routingNodes.contains(localNodeId) || localNodeId.equals(collectNode.handlerSideCollect())) {
            if (collectNode.routing().containsShards(localNodeId)) {

                return handleShardCollect(collectNode, downstream, jobCollectContext);
            } else {
                ListenableFuture<List<Void>> results;

                if (collectNode instanceof FileUriCollectNode) {
                    results = handleWithService(nodeCollectService, collectNode, downstream, jobCollectContext);
                } else if (collectNode.isPartitioned() && collectNode.maxRowGranularity() == RowGranularity.DOC) {


                    downstream.registerUpstream(this).finish();
                    results = IMMEDIATE_LIST;
                } else {
                    CollectService collectService = getCollectService(collectNode, localNodeId);
                    results = handleWithService(collectService, collectNode, downstream, jobCollectContext);
                }


                Futures.addCallback(results, new FutureCallback<List<Void>>() {
                    @Override
                    public void onSuccess(@Nullable List<Void> result) {
                        jobCollectContext.close();
                    }

                    @Override
                    public void onFailure(Throwable t) {
                        jobCollectContext.close();
                    }
                });

                return results;
            }
        }
        throw new UnhandledServerException("unsupported routing");
    }

    private CollectService getCollectService(CollectNode collectNode, String localNodeId) {
        switch (collectNode.maxRowGranularity()) {
            case CLUSTER:

                return clusterCollectService;
            case NODE:

                return nodeCollectService;
            case SHARD:

                return unassignedShardsCollectService;
            case DOC:
                if (localNodeId.equals(collectNode.handlerSideCollect())) {

                    return informationSchemaCollectService;
                } else {

                    return nodeCollectService;
                }
            default:
                throw new UnsupportedOperationException("Unsupported rowGranularity " + collectNode.maxRowGranularity());
        }
    }

    private ListenableFuture<List<Void>> handleWithService(final CollectService collectService,
                                                           final CollectNode node,
                                                           final RowDownstream rowDownstream,
                                                           final JobCollectContext jobCollectContext) {
        return listeningExecutorService.submit(new Callable<List<Void>>() {
            @Override
            public List<Void> call() throws Exception {
                try {
                    EvaluatingNormalizer nodeNormalizer = MapSideDataCollectOperation.this.nodeNormalizer;
                    if (node.maxRowGranularity().finerThan(RowGranularity.CLUSTER)) {
                        nodeNormalizer = new EvaluatingNormalizer(functions,
                                RowGranularity.NODE,
                                new NodeSysReferenceResolver(nodeSysExpression));
                    }
                    CollectNode localCollectNode = node.normalize(nodeNormalizer);
                    RowDownstream localRowDownStream = rowDownstream;
                    if (localCollectNode.whereClause().noMatch()) {
                        localRowDownStream.registerUpstream(MapSideDataCollectOperation.this).finish();
                    } else {
                        if (!localCollectNode.projections().isEmpty()) {
                            FlatProjectorChain projectorChain = FlatProjectorChain.withAttachedDownstream(
                                    projectorVisitor,
                                    jobCollectContext.ramAccountingContext(),
                                    localCollectNode.projections(),
                                    localRowDownStream,
                                    node.jobId()
                            );
                            projectorChain.startProjections(jobCollectContext);
                            localRowDownStream = projectorChain.firstProjector();
                        }
                        CrateCollector collector = collectService.getCollector(localCollectNode, localRowDownStream); 
                        collector.doCollect(jobCollectContext);
                    }
                } catch (Throwable t) {
                    LOGGER.error("error during collect", t);
                    rowDownstream.registerUpstream(MapSideDataCollectOperation.this).fail(t);
                    Throwables.propagate(t);
                }
                return ONE_LIST;
            }
        });
    }

    private CrateCollector getNodeLevelCollector(CollectNode collectNode,
                                                 RowDownstream downstream) {
        if (collectNode instanceof FileUriCollectNode) {
            FileCollectInputSymbolVisitor.Context context = fileInputSymbolVisitor.extractImplementations(collectNode);
            FileUriCollectNode fileUriCollectNode = (FileUriCollectNode) collectNode;

            String[] readers = fileUriCollectNode.executionNodes().toArray(
                    new String[fileUriCollectNode.executionNodes().size()]);
            Arrays.sort(readers);
            return new FileReadingCollector(
                    ValueSymbolVisitor.STRING.process(fileUriCollectNode.targetUri()),
                    context.topLevelInputs(),
                    context.expressions(),
                    downstream,
                    fileUriCollectNode.fileFormat(),
                    fileUriCollectNode.compression(),
                    ImmutableMap.<String, FileInputFactory>of(),
                    fileUriCollectNode.sharedStorage(),
                    readers.length,
                    Arrays.binarySearch(readers, clusterService.state().nodes().localNodeId())
            );
        } else {
            CollectService service = collectServiceResolver.getService(collectNode.routing());
            if (service != null) {
                return service.getCollector(collectNode, downstream);
            }
            ImplementationSymbolVisitor nodeImplementationSymbolVisitor = new ImplementationSymbolVisitor(
                    new NodeSysReferenceResolver(nodeSysExpression),
                    functions,
                    RowGranularity.NODE
            );
            ImplementationSymbolVisitor.Context ctx = nodeImplementationSymbolVisitor.extractImplementations(collectNode);
            assert ctx.maxGranularity().ordinal() <= RowGranularity.NODE.ordinal() : "wrong RowGranularity";
            return new SimpleOneRowCollector(
                    ctx.topLevelInputs(), ctx.collectExpressions(), downstream);
        }
    }

    private int numShards(CollectNode collectNode, String localNodeId) {
        int numShards = collectNode.routing().numShards(localNodeId);
        if (localNodeId.equals(collectNode.handlerSideCollect()) && collectNode.routing().nodes().contains(TableInfo.NULL_NODE_ID)) {


            numShards += 1;
        }
        return numShards;
    }


    protected ListenableFuture<List<Void>> handleShardCollect(CollectNode collectNode,
                                                              RowDownstream downstream,
                                                              JobCollectContext jobCollectContext) {
        String localNodeId = clusterService.state().nodes().localNodeId();

        final int numShards = numShards(collectNode, localNodeId);

        NodeSysReferenceResolver referenceResolver = new NodeSysReferenceResolver(nodeSysExpression);
        EvaluatingNormalizer nodeNormalizer = new EvaluatingNormalizer(functions,
                RowGranularity.NODE,
                referenceResolver);
        CollectNode normalizedCollectNode = collectNode.normalize(nodeNormalizer);

        if (normalizedCollectNode.whereClause().noMatch()) {
            downstream.registerUpstream(this).finish();
            return IMMEDIATE_LIST;
        }

        assert normalizedCollectNode.jobId() != null : "jobId must be set on CollectNode";

        ImplementationSymbolVisitor implementationSymbolVisitor = new ImplementationSymbolVisitor(
                referenceResolver,
                functions,
                RowGranularity.NODE
        );
        ProjectionToProjectorVisitor projectorVisitor = new ProjectionToProjectorVisitor(
                clusterService,
                threadPool,
                settings,
                transportActionProvider,
                bulkRetryCoordinatorPool,
                implementationSymbolVisitor
        );

        ShardProjectorChain projectorChain = new ShardProjectorChain(
                collectNode.jobId(),
                numShards,
                normalizedCollectNode.projections(),
                downstream,
                projectorVisitor,
                jobCollectContext.ramAccountingContext()
        );
        TableUnknownException lastException = null;
        int jobSearchContextId = normalizedCollectNode.routing().jobSearchContextIdBase();

        final List<CrateCollector> shardCollectors = new ArrayList<>(numShards);
        for (Map.Entry<String, Map<String, List<Integer>>> nodeEntry : normalizedCollectNode.routing().locations().entrySet()) {
            if (nodeEntry.getKey().equals(localNodeId)) {
                Map<String, List<Integer>> shardIdMap = nodeEntry.getValue();
                for (Map.Entry<String, List<Integer>> entry : shardIdMap.entrySet()) {
                    String indexName = entry.getKey();
                    IndexService indexService;
                    try {
                        indexService = indicesService.indexServiceSafe(indexName);
                    } catch (IndexMissingException e) {
                        lastException = new TableUnknownException(entry.getKey(), e);
                        continue;
                    }

                    for (Integer shardId : entry.getValue()) {
                        Injector shardInjector;
                        try {
                            shardInjector = indexService.shardInjectorSafe(shardId);
                            ShardCollectService shardCollectService = shardInjector.getInstance(ShardCollectService.class);
                            CrateCollector collector = shardCollectService.getCollector(
                                    normalizedCollectNode,
                                    projectorChain,
                                    jobCollectContext,
                                    jobSearchContextId
                            );
                            shardCollectors.add(collector);
                        } catch (IndexShardMissingException e) {
                            throw new UnhandledServerException(
                                    String.format(Locale.ENGLISH, "unknown shard id %d on index '%s'",
                                            shardId, entry.getKey()), e);
                        } catch (CancellationException e) {
                            throw e;
                        } catch (Exception e) {
                            LOGGER.error("Error while getting collector", e);
                            throw new UnhandledServerException(e);
                        }
                        jobSearchContextId++;
                    }
                }
            } else if (TableInfo.NULL_NODE_ID.equals(nodeEntry.getKey()) && localNodeId.equals(collectNode.handlerSideCollect())) {

                LOGGER.trace("collecting unassigned shards on node {}", localNodeId);
                EvaluatingNormalizer clusterNormalizer = new EvaluatingNormalizer(functions,
                        RowGranularity.CLUSTER,
                        referenceResolver);
                CollectNode clusterNormalizedCollectNode = collectNode.normalize(clusterNormalizer);

                RowDownstream projectorChainDownstream = projectorChain.newShardDownstreamProjector(projectorVisitor);
                CrateCollector collector = unassignedShardsCollectService.getCollector(
                        clusterNormalizedCollectNode,
                        projectorChainDownstream
                );
                shardCollectors.add(collector);
            } else if (jobSearchContextId > -1) {

                for (List<Integer> shardIdMap : nodeEntry.getValue().values()) {
                    jobSearchContextId += shardIdMap.size();
                }
            }
        }
        assert shardCollectors.size() == numShards : "invalid number of shardcollectors";

        if (lastException != null
                && jobSearchContextId == collectNode.routing().jobSearchContextIdBase()) {

            throw lastException;
        }


        projectorChain.startProjections(jobCollectContext);
        try {
            LOGGER.trace("starting {} shardCollectors...", numShards);
            return runCollectThreaded(collectNode, shardCollectors, jobCollectContext);
        } catch (RejectedExecutionException e) {



            downstream.registerUpstream(this).fail(e);
            return Futures.immediateFailedFuture(e);
        }

    }

    private ListenableFuture<List<Void>> runCollectThreaded(CollectNode collectNode,
                                                            final List<CrateCollector> shardCollectors,
                                                            final JobCollectContext jobCollectContext) throws RejectedExecutionException {
        if (collectNode.maxRowGranularity() == RowGranularity.SHARD) {


            return listeningExecutorService.submit(new Callable<List<Void>>() {
                @Override
                public List<Void> call() throws Exception {
                    for (CrateCollector collector : shardCollectors) {
                        collector.doCollect(jobCollectContext);
                    }
                    return ONE_LIST;
                }
            });
        } else {
            return ThreadPools.runWithAvailableThreads(
                    executor,
                    poolSize,
                    collectors2Callables(shardCollectors, jobCollectContext),
                    new VoidFunction<List<Void>>());
        }
    }

    private Collection<Callable<Void>> collectors2Callables(List<CrateCollector> collectors,
                                                            final JobCollectContext jobCollectContext) {
        return Lists.transform(collectors, new Function<CrateCollector, Callable<Void>>() {

            @Override
            public Callable<Void> apply(final CrateCollector collector) {
                return new Callable<Void>() {
                    @Override
                    public Void call() throws Exception {
                        collector.doCollect(jobCollectContext);
                        return null;
                    }
                };
            }
        });
    }

    private static class OneRowCollectService implements CollectService {

        private final ImplementationSymbolVisitor clusterImplementationSymbolVisitor;

        private OneRowCollectService(ImplementationSymbolVisitor clusterImplementationSymbolVisitor) {
            this.clusterImplementationSymbolVisitor = clusterImplementationSymbolVisitor;
        }

        @Override
        public CrateCollector getCollector(CollectNode node, RowDownstream downstream) {

            ImplementationSymbolVisitor.Context ctx = clusterImplementationSymbolVisitor.extractImplementations(node);
            List<Input<?>> inputs = ctx.topLevelInputs();
            Set<CollectExpression<?>> collectExpressions = ctx.collectExpressions();
            return new SimpleOneRowCollector(inputs, collectExpressions, downstream);
        }
    }
}

<code block>


package io.crate.operation.collect;

import com.carrotsearch.hppc.IntObjectOpenHashMap;
import com.carrotsearch.hppc.cursors.IntObjectCursor;
import io.crate.breaker.RamAccountingContext;
import io.crate.jobs.ContextCallback;
import io.crate.jobs.ExecutionState;
import io.crate.jobs.ExecutionSubContext;
import io.crate.operation.RowDownstream;
import io.crate.operation.RowDownstreamHandle;
import io.crate.operation.RowUpstream;
import io.crate.planner.node.dql.CollectNode;
import org.elasticsearch.common.logging.ESLogger;
import org.elasticsearch.common.logging.Loggers;
import org.elasticsearch.index.engine.Engine;
import org.elasticsearch.index.shard.IndexShard;
import org.elasticsearch.index.shard.ShardId;

import javax.annotation.Nullable;
import java.util.ArrayList;
import java.util.Iterator;
import java.util.Locale;
import java.util.UUID;
import java.util.concurrent.CancellationException;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ConcurrentMap;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicInteger;

public class JobCollectContext implements ExecutionSubContext, RowUpstream, ExecutionState {

    private final UUID id;
    private final CollectNode collectNode;
    private final CollectOperation collectOperation;
    private final RamAccountingContext ramAccountingContext;
    private final RowDownstream downstream;

    private final IntObjectOpenHashMap<JobQueryShardContext> queryContexts = new IntObjectOpenHashMap<>();
    private final IntObjectOpenHashMap<JobFetchShardContext> fetchContexts = new IntObjectOpenHashMap<>();
    private final ConcurrentMap<ShardId, EngineSearcherDelegate> shardsSearcherMap = new ConcurrentHashMap<>();
    private final AtomicInteger activeQueryContexts = new AtomicInteger(0);
    private final AtomicInteger activeFetchContexts = new AtomicInteger(0);
    private final Object subContextLock = new Object();

    private final AtomicBoolean closed = new AtomicBoolean(false);
    private final ArrayList<ContextCallback> contextCallbacks = new ArrayList<>(1);

    private volatile boolean isKilled = false;
    private long usedBytesOfQueryPhase = 0L;

    private static final ESLogger LOGGER = Loggers.getLogger(JobCollectContext.class);

    public JobCollectContext(UUID jobId,
                             CollectNode collectNode,
                             CollectOperation collectOperation,
                             RamAccountingContext ramAccountingContext,
                             RowDownstream downstream) {
        id = jobId;
        this.collectNode = collectNode;
        this.collectOperation = collectOperation;
        this.ramAccountingContext = ramAccountingContext;
        this.downstream = downstream;
    }

    @Override
    public void addCallback(ContextCallback contextCallback) {
        assert !closed.get() : "may not add a callback on a closed context";
        contextCallbacks.add(contextCallback);
    }

    public void addContext(int jobSearchContextId, JobQueryShardContext shardQueryContext) {
        interruptIfKilled();
        if (closed.get()) {
            throw new IllegalStateException("context already closed");
        }
        synchronized (subContextLock) {
            if (queryContexts.put(jobSearchContextId, shardQueryContext) != null) {
                throw new IllegalArgumentException(String.format(Locale.ENGLISH,
                        "ExecutionSubContext for %d already added", jobSearchContextId));
            }
        }
        int numActive = activeQueryContexts.incrementAndGet();
        LOGGER.trace("adding query subContext {}, now there are {} query subContexts", jobSearchContextId, numActive);

        shardQueryContext.addCallback(new RemoveQueryContextCallback(jobSearchContextId));
        EngineSearcherDelegate searcherDelegate;
        try {
            searcherDelegate = acquireSearcher(shardQueryContext.indexShard());
            shardQueryContext.searcher(searcherDelegate);
        } catch (Exception e) {

            shardQueryContext.close();
            throw e;
        }

        if (collectNode.keepContextForFetcher()) {
            JobFetchShardContext shardFetchContext = new JobFetchShardContext(
                    searcherDelegate,
                    shardQueryContext.searchContext());
            synchronized (subContextLock) {
                fetchContexts.put(jobSearchContextId, shardFetchContext);
            }
            shardFetchContext.addCallback(new RemoveFetchContextCallback(jobSearchContextId));

            int numActiveFetch = activeFetchContexts.incrementAndGet();
            LOGGER.trace("adding fetch subContext {}, now there are {} fetch subContexts", jobSearchContextId, numActiveFetch);
        }
    }


    @Nullable
    public JobFetchShardContext getFetchContext(int jobSearchContextId) {
        synchronized (subContextLock) {
            return fetchContexts.get(jobSearchContextId);
        }
    }

    @Override
    public void close() {
        if (closed.compareAndSet(false, true)) { 
            synchronized (subContextLock) {
                if (queryContexts.size() != 0 || fetchContexts.size() != 0) {
                    LOGGER.trace("closing query subContexts {}", id);
                    Iterator<IntObjectCursor<JobQueryShardContext>> queryIterator = queryContexts.iterator();
                    while (queryIterator.hasNext()) {
                        queryIterator.next().value.close();
                    }
                    LOGGER.trace("closing fetch subContexts {}", id);
                    Iterator<IntObjectCursor<JobFetchShardContext>> fetchIterator = fetchContexts.iterator();
                    while (fetchIterator.hasNext()) {
                        fetchIterator.next().value.close();
                    }
                } else {
                    callContextCallback();
                }
            }
            ramAccountingContext.close();
        } else {
            LOGGER.trace("close called on an already closed JobCollectContext: {}", id);
        }
    }

    @Override
    public void kill() {
        isKilled = true;
        if (closed.compareAndSet(false, true)) { 
            synchronized (subContextLock) {
                if (queryContexts.size() != 0 || fetchContexts.size() != 0) {
                    LOGGER.trace("killing query subContexts {}", id);
                    Iterator<IntObjectCursor<JobQueryShardContext>> queryIterator = queryContexts.iterator();
                    while (queryIterator.hasNext()) {
                        IntObjectCursor<JobQueryShardContext> cursor = queryIterator.next();
                        cursor.value.kill();
                    }
                    LOGGER.trace("killing fetch subContexts {}", id);
                    Iterator<IntObjectCursor<JobFetchShardContext>> fetchIterator = fetchContexts.iterator();
                    while (fetchIterator.hasNext()) {
                        fetchIterator.next().value.kill();
                    }
                } else {
                    callContextCallback();
                }
            }
            ramAccountingContext.close();
        } else {
            LOGGER.trace("killed called on an already closed JobCollectContext: {}", id);
        }
    }

    @Override
    public String name() {
        return collectNode.name();
    }

    @Override
        public void start() {
        startQueryPhase();
    }

    protected void startQueryPhase() {
        try {
            collectOperation.collect(collectNode, downstream, this);
        } catch (Throwable t) {
            RowDownstreamHandle rowDownstreamHandle = downstream.registerUpstream(this);
            rowDownstreamHandle.fail(t);
            close();
        }
    }

    @Override
    public boolean isKilled() {
        return isKilled;
    }

    public void interruptIfKilled() {
        if (isKilled) {
            throw new CancellationException();
        }
    }

    public RamAccountingContext ramAccountingContext() {
        return ramAccountingContext;
    }

    private void callContextCallback() {
        if (contextCallbacks.isEmpty()) {
            return;
        }
        if (activeQueryContexts.get() == 0 && activeFetchContexts.get() == 0) {
            for (ContextCallback contextCallback : contextCallbacks) {
                contextCallback.onClose(null, usedBytesOfQueryPhase);
            }
        }
    }


    protected EngineSearcherDelegate acquireSearcher(IndexShard indexShard) {
        EngineSearcherDelegate engineSearcherDelegate;
        for (;;) {
            engineSearcherDelegate = shardsSearcherMap.get(indexShard.shardId());
            if (engineSearcherDelegate == null) {
                engineSearcherDelegate = new EngineSearcherDelegate(acquireNewSearcher(indexShard));
                if (shardsSearcherMap.putIfAbsent(indexShard.shardId(), engineSearcherDelegate) == null) {
                    return engineSearcherDelegate;
                }
            } else {
                return engineSearcherDelegate;
            }
        }
    }


    protected Engine.Searcher acquireNewSearcher(IndexShard indexShard) {
        return EngineSearcher.getSearcherWithRetry(indexShard, "search", null);
    }


    class RemoveQueryContextCallback implements ContextCallback {

        private final int jobSearchContextId;

        public RemoveQueryContextCallback(int jobSearchContextId) {
            this.jobSearchContextId = jobSearchContextId;
        }

        @Override
        public void onClose(@Nullable Throwable error, long bytesUsed) {
            if (LOGGER.isTraceEnabled()) {
                LOGGER.trace("[{}] Closing query subContext {}",
                        System.identityHashCode(queryContexts), jobSearchContextId);
            }

            JobQueryShardContext remove;
            synchronized (subContextLock) {
                remove = queryContexts.remove(jobSearchContextId);
            }
            int remaining;
            if (remove == null) {
                LOGGER.trace("Closed query context {} which was already closed.", jobSearchContextId);
                remaining = activeQueryContexts.get();
            } else {
                remaining = activeQueryContexts.decrementAndGet();

                if (collectNode.keepContextForFetcher()
                        && (remove.collector() == null || !remove.collector().producedRows() || remove.collector().failed())) {


                    JobFetchShardContext fetchShardContext;
                    synchronized (subContextLock) {
                        fetchShardContext = fetchContexts.get(jobSearchContextId);
                    }
                    if (fetchShardContext != null) {
                        fetchShardContext.close();
                    }
                }
            }


            if (remaining == 0) {
                usedBytesOfQueryPhase = ramAccountingContext.totalBytes();
                ramAccountingContext.close();
                callContextCallback();
            }
        }
    }

    class RemoveFetchContextCallback implements ContextCallback {

        private final int jobSearchContextId;

        public RemoveFetchContextCallback(int jobSearchContextId) {
            this.jobSearchContextId = jobSearchContextId;
        }

        @Override
        public void onClose(@Nullable Throwable error, long bytesUsed) {
            if (LOGGER.isTraceEnabled()) {
                LOGGER.trace("[{}] Closing fetch subContext {}",
                        System.identityHashCode(fetchContexts), jobSearchContextId);
            }

            JobFetchShardContext remove;
            synchronized (subContextLock) {
                remove = fetchContexts.remove(jobSearchContextId);
            }
            int remaining;
            if (remove == null) {
                LOGGER.trace("Closed fetch context {} which was already closed.", jobSearchContextId);
                remaining = activeFetchContexts.get();
            } else {
                remaining = activeFetchContexts.decrementAndGet();
            }
            if (remaining == 0) {
                callContextCallback();
            }
        }
    }

}

<code block>


package io.crate.jobs;

import com.google.common.util.concurrent.SettableFuture;
import io.crate.Streamer;
import io.crate.breaker.RamAccountingContext;
import io.crate.core.collections.Bucket;
import io.crate.core.collections.BucketPage;
import io.crate.operation.PageConsumeListener;
import io.crate.operation.PageDownstream;
import io.crate.operation.PageResultListener;
import org.elasticsearch.common.logging.ESLogger;
import org.elasticsearch.common.logging.Loggers;

import java.util.ArrayList;
import java.util.BitSet;
import java.util.concurrent.CancellationException;
import java.util.concurrent.atomic.AtomicBoolean;

public class PageDownstreamContext implements ExecutionSubContext, ExecutionState {

    private static final ESLogger LOGGER = Loggers.getLogger(PageDownstreamContext.class);

    private final Object lock = new Object();
    private String name;
    private final PageDownstream pageDownstream;
    private final Streamer<?>[] streamer;
    private final RamAccountingContext ramAccountingContext;
    private final int numBuckets;
    private final ArrayList<SettableFuture<Bucket>> bucketFutures;
    private final BitSet allFuturesSet;
    private final BitSet exhausted;
    private final ArrayList<PageResultListener> listeners = new ArrayList<>();
    private final ArrayList<ContextCallback> callbacks = new ArrayList<>(1);
    private final AtomicBoolean closed = new AtomicBoolean(false);
    private volatile boolean isKilled = false;


    public PageDownstreamContext(String name,
                                 PageDownstream pageDownstream,
                                 Streamer<?>[] streamer,
                                 RamAccountingContext ramAccountingContext,
                                 int numBuckets) {
        this.name = name;
        this.pageDownstream = pageDownstream;
        this.streamer = streamer;
        this.ramAccountingContext = ramAccountingContext;
        this.numBuckets = numBuckets;
        bucketFutures = new ArrayList<>(numBuckets);
        allFuturesSet = new BitSet(numBuckets);
        exhausted = new BitSet(numBuckets);
        initBucketFutures();
    }

    private void initBucketFutures() {
        bucketFutures.clear();
        for (int i = 0; i < numBuckets; i++) {
            bucketFutures.add(SettableFuture.<Bucket>create());
        }
    }

    private boolean pageEmpty() {
        return allFuturesSet.cardinality() == 0;
    }

    private boolean allExhausted() {
        return exhausted.cardinality() == numBuckets;
    }

    private boolean isExhausted(int bucketIdx) {
        return exhausted.get(bucketIdx);
    }

    public void setBucket(int bucketIdx, Bucket rows, boolean isLast, PageResultListener pageResultListener) {
        synchronized (listeners) {
            listeners.add(pageResultListener);
        }
        synchronized (lock) {
            LOGGER.trace("setBucket: {}", bucketIdx);
            if (allFuturesSet.get(bucketIdx)) {
                pageDownstream.fail(new IllegalStateException("May not set the same bucket of a page more than once"));
                return;
            }

            if (pageEmpty()) {
                LOGGER.trace("calling nextPage");
                pageDownstream.nextPage(new BucketPage(bucketFutures), new ResultListenerBridgingConsumeListener());
            }
            setExhaustedUpstreams();

            if (isLast) {
                exhausted.set(bucketIdx);
            }
            bucketFutures.get(bucketIdx).set(rows);
            allFuturesSet.set(bucketIdx);

            clearPageIfFull();
        }
    }

    public synchronized void failure(int bucketIdx, Throwable throwable) {


        synchronized (lock) {
            LOGGER.trace("failure: bucket: {} {}", bucketIdx, throwable);
            if (allFuturesSet.get(bucketIdx)) {
                pageDownstream.fail(new IllegalStateException("May not set the same bucket %d of a page more than once"));
                return;
            }
            if (pageEmpty()) {
                LOGGER.trace("calling nextPage");
                pageDownstream.nextPage(new BucketPage(bucketFutures), new ResultListenerBridgingConsumeListener());
            }
            setExhaustedUpstreams();

            LOGGER.trace("failure: {}", bucketIdx);
            exhausted.set(bucketIdx);
            bucketFutures.get(bucketIdx).setException(throwable);
            allFuturesSet.set(bucketIdx);
            clearPageIfFull();
        }
    }

    private void clearPageIfFull() {
        if (allFuturesSet.cardinality() == numBuckets) {
            allFuturesSet.clear();
            initBucketFutures();
        }
    }


    private void setExhaustedUpstreams() {
        for (int i = 0; i < exhausted.size(); i++) {
            if (exhausted.get(i)) {
                bucketFutures.get(i).set(Bucket.EMPTY);
                allFuturesSet.set(i);
            }
        }
    }

    public Streamer<?>[] streamer() {
        return streamer;
    }

    public void finish() {
        LOGGER.trace("calling finish on pageDownstream {}", pageDownstream);
        if (!closed.getAndSet(true)) {
            for (ContextCallback contextCallback : callbacks) {
                contextCallback.onClose(null, -1L);
            }
            pageDownstream.finish();
            ramAccountingContext.close();
        } else {
            LOGGER.warn("called finish on an already closed PageDownstreamContext");
        }
    }

    public void addCallback(ContextCallback contextCallback) {
        assert !closed.get() : "may not add a callback on a closed context";
        callbacks.add(contextCallback);
    }

    @Override
    public void start() {

    }

    @Override
    public void close() {
        finish();
    }

    @Override
    public void kill() {
        isKilled = true;
        if (!closed.getAndSet(true)) {
            CancellationException cancellationException = new CancellationException();
            for (ContextCallback contextCallback : callbacks) {
                contextCallback.onClose(cancellationException, -1L);
            }
            pageDownstream.fail(cancellationException);
            ramAccountingContext.close();
        } else {
            LOGGER.warn("called kill on an already closed PageDownstreamContext");
        }
    }

    @Override
    public String name() {
        return name;
    }

    @Override
    public boolean isKilled() {
        return isKilled;
    }

    private class ResultListenerBridgingConsumeListener implements PageConsumeListener {

        @Override
        public void needMore() {
            boolean allExhausted = allExhausted();
            LOGGER.trace("allExhausted: {}", allExhausted);
            synchronized (listeners) {
                LOGGER.trace("calling needMore on all listeners({})", listeners.size());
                for (PageResultListener listener : listeners) {
                    if (allExhausted) {
                        listener.needMore(false);
                    } else {
                        listener.needMore(!isExhausted(listener.buckedIdx()));
                    }
                }
                listeners.clear();
            }
            if (allExhausted) {
                PageDownstreamContext.this.finish();
            }
        }

        @Override
        public void finish() {
            synchronized (listeners) {
                LOGGER.trace("calling finish() on all listeners({})", listeners.size());
                for (PageResultListener listener : listeners) {
                    listener.needMore(false);
                }
                listeners.clear();
                PageDownstreamContext.this.finish();
            }
        }
    }
}

<code block>


package io.crate.action.job;

import com.google.common.base.Optional;
import com.google.common.util.concurrent.ListenableFuture;
import io.crate.Streamer;
import io.crate.breaker.CrateCircuitBreakerService;
import io.crate.breaker.RamAccountingContext;
import io.crate.core.collections.Bucket;
import io.crate.executor.transport.distributed.SingleBucketBuilder;
import io.crate.jobs.CountContext;
import io.crate.jobs.JobExecutionContext;
import io.crate.jobs.PageDownstreamContext;
import io.crate.operation.PageDownstream;
import io.crate.operation.PageDownstreamFactory;
import io.crate.operation.collect.JobCollectContext;
import io.crate.operation.collect.MapSideDataCollectOperation;
import io.crate.operation.count.CountOperation;
import io.crate.operation.projectors.FlatProjectorChain;
import io.crate.operation.projectors.ResultProvider;
import io.crate.operation.projectors.ResultProviderFactory;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.node.ExecutionNodeVisitor;
import io.crate.planner.node.ExecutionNodes;
import io.crate.planner.node.StreamerVisitor;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.CountNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.types.DataTypes;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.common.breaker.CircuitBreaker;
import org.elasticsearch.common.collect.Tuple;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.inject.Singleton;
import org.elasticsearch.common.logging.ESLogger;
import org.elasticsearch.common.logging.Loggers;
import org.elasticsearch.threadpool.ThreadPool;

import javax.annotation.Nullable;
import java.util.List;
import java.util.Map;
import java.util.UUID;

@Singleton
public class ContextPreparer {

    private static final ESLogger LOGGER = Loggers.getLogger(ContextPreparer.class);

    private final MapSideDataCollectOperation collectOperation;
    private ClusterService clusterService;
    private CountOperation countOperation;
    private final CircuitBreaker circuitBreaker;
    private final ThreadPool threadPool;
    private final PageDownstreamFactory pageDownstreamFactory;
    private final ResultProviderFactory resultProviderFactory;
    private final StreamerVisitor streamerVisitor;
    private final InnerPreparer innerPreparer;

    @Inject
    public ContextPreparer(MapSideDataCollectOperation collectOperation,
                           ClusterService clusterService,
                           CrateCircuitBreakerService breakerService,
                           ThreadPool threadPool,
                           CountOperation countOperation,
                           PageDownstreamFactory pageDownstreamFactory,
                           ResultProviderFactory resultProviderFactory,
                           StreamerVisitor streamerVisitor) {
        this.collectOperation = collectOperation;
        this.clusterService = clusterService;
        this.countOperation = countOperation;
        circuitBreaker = breakerService.getBreaker(CrateCircuitBreakerService.QUERY_BREAKER);
        this.threadPool = threadPool;
        this.pageDownstreamFactory = pageDownstreamFactory;
        this.resultProviderFactory = resultProviderFactory;
        this.streamerVisitor = streamerVisitor;
        innerPreparer = new InnerPreparer();
    }

    @Nullable
    public ListenableFuture<Bucket> prepare(UUID jobId,
                                            ExecutionNode executionNode,
                                            JobExecutionContext.Builder contextBuilder) {
        PreparerContext preparerContext = new PreparerContext(jobId, contextBuilder);
        innerPreparer.process(executionNode, preparerContext);
        return preparerContext.directResultFuture;
    }

    private static class PreparerContext {

        private final UUID jobId;
        private final JobExecutionContext.Builder contextBuilder;
        private ListenableFuture<Bucket> directResultFuture;

        private PreparerContext(UUID jobId,
                                JobExecutionContext.Builder contextBuilder) {
            this.contextBuilder = contextBuilder;
            this.jobId = jobId;
        }
    }

    private class InnerPreparer extends ExecutionNodeVisitor<PreparerContext, Void> {

        @Override
        public Void visitCountNode(CountNode countNode, PreparerContext context) {
            Map<String, Map<String, List<Integer>>> locations = countNode.routing().locations();
            if (locations == null) {
                throw new IllegalArgumentException("locations are empty. Can't start count operation");
            }
            String localNodeId = clusterService.localNode().id();
            Map<String, List<Integer>> indexShardMap = locations.get(localNodeId);
            if (indexShardMap == null) {
                throw new IllegalArgumentException("The routing of the countNode doesn't contain the current nodeId");
            }

            final SingleBucketBuilder singleBucketBuilder = new SingleBucketBuilder(new Streamer[]{DataTypes.LONG});
            CountContext countContext = new CountContext(
                    countOperation,
                    singleBucketBuilder,
                    indexShardMap,
                    countNode.whereClause()
            );
            context.directResultFuture = singleBucketBuilder.result();
            context.contextBuilder.addSubContext(countNode.executionNodeId(), countContext);
            return null;
        }

        @Override
        public Void visitMergeNode(final MergeNode node, final PreparerContext context) {
            RamAccountingContext ramAccountingContext = RamAccountingContext.forExecutionNode(circuitBreaker, node);
            ResultProvider downstream = resultProviderFactory.createDownstream(node, node.jobId());
            Tuple<PageDownstream, FlatProjectorChain> pageDownstreamProjectorChain =
                    pageDownstreamFactory.createMergeNodePageDownstream(
                            node,
                            downstream,
                            ramAccountingContext,
                            Optional.of(threadPool.executor(ThreadPool.Names.SEARCH)));
            StreamerVisitor.Context streamerContext = streamerVisitor.processPlanNode(node);
            PageDownstreamContext pageDownstreamContext = new PageDownstreamContext(
                    node.name(),
                    pageDownstreamProjectorChain.v1(),
                    streamerContext.inputStreamers(),
                    ramAccountingContext,
                    node.numUpstreams());

            context.contextBuilder.addSubContext(node.executionNodeId(), pageDownstreamContext);

            FlatProjectorChain flatProjectorChain = pageDownstreamProjectorChain.v2();
            if (flatProjectorChain != null) {
                flatProjectorChain.startProjections(pageDownstreamContext);
            }
            return null;
        }

        @Override
        public Void visitCollectNode(final CollectNode node, final PreparerContext context) {
            RamAccountingContext ramAccountingContext = RamAccountingContext.forExecutionNode(circuitBreaker, node);
            ResultProvider downstream = collectOperation.createDownstream(node);

            if (ExecutionNodes.hasDirectResponseDownstream(node.downstreamNodes())) {
                context.directResultFuture = downstream.result();
            }
            final JobCollectContext jobCollectContext = new JobCollectContext(
                    context.jobId,
                    node,
                    collectOperation,
                    ramAccountingContext,
                    downstream
            );
            context.contextBuilder.addSubContext(node.executionNodeId(), jobCollectContext);
            return null;
        }
    }
}

<code block>


package io.crate.executor;

import java.util.ArrayList;
import java.util.Collection;
import java.util.List;
import java.util.UUID;

public class Job {

    private final UUID id;
    private List<Task> tasks = new ArrayList<>();

    public Job() {
        this(UUID.randomUUID());
    }

    public Job(UUID id) {
        this.id = id;
    }

    public UUID id() {
        return id;
    }

    public void addTasks(Collection<Task> tasks) {
        this.tasks.addAll(tasks);
    }

    public List<Task> tasks() {
        return tasks;
    }
}

<code block>


package io.crate.executor.transport;

import com.google.common.base.Optional;
import com.google.common.util.concurrent.FutureCallback;
import com.google.common.util.concurrent.Futures;
import com.google.common.util.concurrent.ListenableFuture;
import com.google.common.util.concurrent.SettableFuture;
import io.crate.Streamer;
import io.crate.action.job.ContextPreparer;
import io.crate.action.job.JobRequest;
import io.crate.action.job.JobResponse;
import io.crate.action.job.TransportJobAction;
import io.crate.breaker.RamAccountingContext;
import io.crate.core.collections.Bucket;
import io.crate.executor.JobTask;
import io.crate.executor.TaskResult;
import io.crate.jobs.JobContextService;
import io.crate.jobs.JobExecutionContext;
import io.crate.jobs.PageDownstreamContext;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.*;
import io.crate.operation.projectors.FlatProjectorChain;
import io.crate.planner.node.*;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.MergeNode;
import org.elasticsearch.action.ActionListener;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.common.breaker.CircuitBreaker;
import org.elasticsearch.common.collect.Tuple;
import org.elasticsearch.common.logging.ESLogger;
import org.elasticsearch.common.logging.Loggers;
import org.elasticsearch.threadpool.ThreadPool;

import javax.annotation.Nonnull;
import javax.annotation.Nullable;
import java.util.*;


public class ExecutionNodesTask extends JobTask {

    private static final ESLogger LOGGER = Loggers.getLogger(ExecutionNodesTask.class);

    private final TransportJobAction transportJobAction;
    private final List<List<ExecutionNode>> groupedExecutionNodes;
    private final List<SettableFuture<TaskResult>> results;
    private final boolean hasDirectResponse;
    private final ClusterService clusterService;
    private ContextPreparer contextPreparer;
    private final JobContextService jobContextService;
    private final PageDownstreamFactory pageDownstreamFactory;
    private final ThreadPool threadPool;
    private TransportCloseContextNodeAction transportCloseContextNodeAction;
    private final StreamerVisitor streamerVisitor;
    private final CircuitBreaker circuitBreaker;
    private List<MergeNode> mergeNodes;
    private boolean rowCountResult = false;


    protected ExecutionNodesTask(UUID jobId,
                                 ClusterService clusterService,
                                 ContextPreparer contextPreparer,
                                 JobContextService jobContextService,
                                 PageDownstreamFactory pageDownstreamFactory,
                                 ThreadPool threadPool,
                                 TransportJobAction transportJobAction,
                                 TransportCloseContextNodeAction transportCloseContextNodeAction,
                                 StreamerVisitor streamerVisitor,
                                 CircuitBreaker circuitBreaker,
                                 @Nullable List<MergeNode> mergeNodes,
                                 List<List<ExecutionNode>> groupedExecutionNodes) {
        super(jobId);
        this.clusterService = clusterService;
        this.contextPreparer = contextPreparer;
        this.jobContextService = jobContextService;
        this.pageDownstreamFactory = pageDownstreamFactory;
        this.threadPool = threadPool;
        this.transportCloseContextNodeAction = transportCloseContextNodeAction;
        this.streamerVisitor = streamerVisitor;
        this.circuitBreaker = circuitBreaker;
        this.mergeNodes = mergeNodes;
        this.transportJobAction = transportJobAction;
        this.groupedExecutionNodes = groupedExecutionNodes;
        hasDirectResponse = hasDirectResponse(groupedExecutionNodes);

        List<SettableFuture<TaskResult>> results = new ArrayList<>(groupedExecutionNodes.size());
        for (int i = 0; i < groupedExecutionNodes.size(); i++) {
            results.add(SettableFuture.<TaskResult>create());
        }
        this.results = results;
    }

    public void mergeNodes(List<MergeNode> mergeNodes) {
        assert this.mergeNodes == null : "can only overwrite mergeNodes if it was null";
        this.mergeNodes = mergeNodes;
    }

    public void rowCountResult(boolean rowCountResult) {
        this.rowCountResult = rowCountResult;
    }

    @Override
    public void start() {
        assert mergeNodes != null : "mergeNodes must not be null";

        Map<String, Collection<ExecutionNode>> nodesByServer = ExecutionNodeGrouper.groupByServer(clusterService.state().nodes().localNodeId(), groupedExecutionNodes);
        RowDownstream rowDownstream;
        if (rowCountResult) {
            rowDownstream = new RowCountResultRowDownstream(results);
        } else {
            rowDownstream = new QueryResultRowDownstream(results);
        }
        Streamer<?>[] streamers = streamerVisitor.processExecutionNode(mergeNodes.get(0)).inputStreamers();
        List<PageDownstreamContext> pageDownstreamContexts = new ArrayList<>(groupedExecutionNodes.size());

        for (int i = 0; i < groupedExecutionNodes.size(); i++) {
            RamAccountingContext ramAccountingContext = RamAccountingContext.forExecutionNode(
                    circuitBreaker, mergeNodes.get(i));

            PageDownstreamContext pageDownstreamContext = createPageDownstreamContext(ramAccountingContext, streamers,
                    mergeNodes.get(i), groupedExecutionNodes.get(i), rowDownstream);
            if (nodesByServer.size() == 0) {
                pageDownstreamContext.finish();
                continue;
            }
            if (!hasDirectResponse) {
                createLocalContextAndStartOperation(pageDownstreamContext, nodesByServer, mergeNodes.get(i).executionNodeId());
            }
            pageDownstreamContexts.add(pageDownstreamContext);
        }
        if (nodesByServer.size() == 0) {
            return;
        }
        addCloseContextCallback(transportCloseContextNodeAction, groupedExecutionNodes, nodesByServer.keySet());
        sendJobRequests(streamers, pageDownstreamContexts, nodesByServer);
    }

    private PageDownstreamContext createPageDownstreamContext(
            RamAccountingContext ramAccountingContext,
            Streamer<?>[] streamers,
            MergeNode mergeNode,
            List<ExecutionNode> executionNodes,
            RowDownstream rowDownstream) {
        Tuple<PageDownstream, FlatProjectorChain> pageDownstreamProjectorChain = pageDownstreamFactory.createMergeNodePageDownstream(
                mergeNode,
                rowDownstream,
                ramAccountingContext,
                Optional.of(threadPool.executor(ThreadPool.Names.SEARCH))
        );
        PageDownstreamContext pageDownstreamContext = new PageDownstreamContext(
                mergeNode.name(),
                pageDownstreamProjectorChain.v1(),
                streamers,
                ramAccountingContext,
                executionNodes.get(executionNodes.size() - 1).executionNodes().size()
        );
        FlatProjectorChain flatProjectorChain = pageDownstreamProjectorChain.v2();
        if (flatProjectorChain != null) {
            flatProjectorChain.startProjections(pageDownstreamContext);
        }
        return pageDownstreamContext;
    }

    private void sendJobRequests(Streamer<?>[] streamers,
                                 List<PageDownstreamContext> pageDownstreamContexts,
                                 Map<String, Collection<ExecutionNode>> nodesByServer) {
        int idx = 0;
        for (Map.Entry<String, Collection<ExecutionNode>> entry : nodesByServer.entrySet()) {
            String serverNodeId = entry.getKey();
            if (TableInfo.NULL_NODE_ID.equals(serverNodeId)) {
                continue; 
            }
            Collection<ExecutionNode> executionNodes = entry.getValue();

            JobRequest request = new JobRequest(jobId(), executionNodes);
            if (hasDirectResponse) {
                transportJobAction.execute(serverNodeId, request,
                        new DirectResponseListener(idx, streamers, pageDownstreamContexts));
            } else {
                transportJobAction.execute(serverNodeId, request,
                        new FailureOnlyResponseListener(results));
            }
            idx++;
        }
    }


    private void createLocalContextAndStartOperation(PageDownstreamContext finalLocalMerge,
                                                     Map<String, Collection<ExecutionNode>> nodesByServer,
                                                     int localMergeExecutionNodeId) {
        String localNodeId = clusterService.localNode().id();
        Collection<ExecutionNode> localExecutionNodes = nodesByServer.remove(localNodeId);

        JobExecutionContext.Builder builder = jobContextService.newBuilder(jobId());
        builder.addSubContext(localMergeExecutionNodeId, finalLocalMerge);

        if (localExecutionNodes == null || localExecutionNodes.isEmpty()) {

            jobContextService.createContext(builder);
        } else {
            for (ExecutionNode executionNode : localExecutionNodes) {
                contextPreparer.prepare(jobId(), executionNode, builder);
            }
            JobExecutionContext context = jobContextService.createContext(builder);
            context.start();
        }
    }

    private void addCloseContextCallback(TransportCloseContextNodeAction transportCloseContextNodeAction,
                                         final List<List<ExecutionNode>> groupedExecutionNodes,
                                         final Set<String> server) {
        if (server.isEmpty()) {
            return;
        }
        final ContextCloser contextCloser = new ContextCloser(transportCloseContextNodeAction);
        Futures.addCallback(Futures.allAsList(results), new FutureCallback<List<TaskResult>>() {
            @Override
            public void onSuccess(@Nullable List<TaskResult> result) {

            }

            @Override
            public void onFailure(@Nonnull Throwable t) {

                for (List<ExecutionNode> executionNodeGroup : groupedExecutionNodes) {
                    for (ExecutionNode executionNode : executionNodeGroup) {
                        contextCloser.process(executionNode, server);
                    }
                }
            }
        });
    }

    @Override
    public List<? extends ListenableFuture<TaskResult>> result() {
        return results;
    }

    @Override
    public void upstreamResult(List<? extends ListenableFuture<TaskResult>> result) {
        throw new UnsupportedOperationException("ExecutionNodesTask doesn't support upstreamResult");
    }

    static boolean hasDirectResponse(List<List<ExecutionNode>> groupedExecutionNodes) {
        for (List<ExecutionNode> executionNodeGroup : groupedExecutionNodes) {
            for (ExecutionNode executionNode : executionNodeGroup) {
                if (ExecutionNodes.hasDirectResponseDownstream(executionNode.downstreamNodes())) {
                    return true;
                }
            }
        }
        return false;
    }

    private static class DirectResponseListener implements ActionListener<JobResponse> {

        private final int bucketIdx;
        private final Streamer<?>[] streamer;
        private final List<PageDownstreamContext> pageDownstreamContexts;

        public DirectResponseListener(int bucketIdx, Streamer<?>[] streamer, List<PageDownstreamContext> pageDownstreamContexts) {
            this.bucketIdx = bucketIdx;
            this.streamer = streamer;
            this.pageDownstreamContexts = pageDownstreamContexts;
        }

        @Override
        public void onResponse(JobResponse jobResponse) {
            jobResponse.streamers(streamer);
            for (int i = 0; i < pageDownstreamContexts.size(); i++) {
                PageDownstreamContext pageDownstreamContext = pageDownstreamContexts.get(i);
                Bucket bucket = jobResponse.directResponse().get(i);
                if (bucket == null) {
                    pageDownstreamContext.failure(bucketIdx, new IllegalStateException("expected directResponse but didn't get one"));
                }
                pageDownstreamContext.setBucket(bucketIdx, bucket, true, new PageResultListener() {
                    @Override
                    public void needMore(boolean needMore) {

                    }

                    @Override
                    public int buckedIdx() {
                        return bucketIdx;
                    }
                });
            }
        }

        @Override
        public void onFailure(Throwable e) {
            for (PageDownstreamContext pageDownstreamContext : pageDownstreamContexts) {
                pageDownstreamContext.failure(bucketIdx, e);
            }
        }
    }

    private static class FailureOnlyResponseListener implements ActionListener<JobResponse> {

        private final List<SettableFuture<TaskResult>> results;

        public FailureOnlyResponseListener(List<SettableFuture<TaskResult>> results) {
            this.results = results;
        }

        @Override
        public void onResponse(JobResponse jobResponse) {
            if (jobResponse.directResponse().size() > 0) {
                for (SettableFuture<TaskResult> result : results) {
                    result.setException(new IllegalStateException("Got a directResponse but didn't expect one"));
                }
            }
        }

        @Override
        public void onFailure(Throwable e) {

            LOGGER.warn(e.getMessage(), e);
        }
    }

    private static class ContextCloser extends ExecutionNodeVisitor<Set<String>, Void> {

        private final TransportCloseContextNodeAction transportCloseContextNodeAction;

        public ContextCloser(TransportCloseContextNodeAction transportCloseContextNodeAction) {
            this.transportCloseContextNodeAction = transportCloseContextNodeAction;
        }

        @Override
        public Void visitCollectNode(final CollectNode node, Set<String> nodeIds) {
            if (!node.keepContextForFetcher()) {
                return null;
            }

            LOGGER.trace("closing job context {} on {} nodes", node.jobId(), nodeIds.size());
            for (final String nodeId : nodeIds) {
                transportCloseContextNodeAction.execute(
                        nodeId,
                        new NodeCloseContextRequest(node.jobId(), node.executionNodeId()),
                        new ActionListener<NodeCloseContextResponse>() {

                    @Override
                    public void onResponse(NodeCloseContextResponse nodeCloseContextResponse) {
                    }

                    @Override
                    public void onFailure(Throwable e) {
                        LOGGER.warn("Closing job context {} failed on node {} with: {}", node.jobId(), nodeId, e.getMessage());
                    }
                });
            }
            return null;
        }
    }
}

<code block>


package io.crate.executor.transport;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.Iterables;
import com.google.common.util.concurrent.ListenableFuture;
import io.crate.action.job.ContextPreparer;
import io.crate.action.sql.DDLStatementDispatcher;
import io.crate.breaker.CrateCircuitBreakerService;
import io.crate.executor.*;
import io.crate.executor.task.DDLTask;
import io.crate.executor.task.NoopTask;
import io.crate.executor.transport.task.CreateTableTask;
import io.crate.executor.transport.task.DropTableTask;
import io.crate.executor.transport.task.KillTask;
import io.crate.executor.transport.task.SymbolBasedUpsertByIdTask;
import io.crate.executor.transport.task.elasticsearch.*;
import io.crate.jobs.JobContextService;
import io.crate.metadata.Functions;
import io.crate.metadata.ReferenceResolver;
import io.crate.operation.ImplementationSymbolVisitor;
import io.crate.operation.PageDownstreamFactory;
import io.crate.operation.projectors.ProjectionToProjectorVisitor;
import io.crate.planner.*;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.node.PlanNode;
import io.crate.planner.node.PlanNodeVisitor;
import io.crate.planner.node.StreamerVisitor;
import io.crate.planner.node.ddl.*;
import io.crate.planner.node.dml.*;
import io.crate.planner.node.dql.*;
import io.crate.planner.node.management.KillPlan;
import org.elasticsearch.action.bulk.BulkRetryCoordinatorPool;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.common.Nullable;
import org.elasticsearch.common.breaker.CircuitBreaker;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.inject.Provider;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.threadpool.ThreadPool;

import java.util.ArrayList;
import java.util.Collection;
import java.util.List;
import java.util.UUID;

public class TransportExecutor implements Executor, TaskExecutor {

    private final Functions functions;
    private final TaskCollectingVisitor planVisitor;
    private Provider<DDLStatementDispatcher> ddlAnalysisDispatcherProvider;
    private final NodeVisitor nodeVisitor;
    private final ThreadPool threadPool;

    private final ClusterService clusterService;
    private final JobContextService jobContextService;
    private final ContextPreparer contextPreparer;
    private final TransportActionProvider transportActionProvider;
    private final BulkRetryCoordinatorPool bulkRetryCoordinatorPool;

    private final ProjectionToProjectorVisitor globalProjectionToProjectionVisitor;


    private final CircuitBreaker circuitBreaker;

    private final PageDownstreamFactory pageDownstreamFactory;

    private final StreamerVisitor streamerVisitor;

    @Inject
    public TransportExecutor(Settings settings,
                             JobContextService jobContextService,
                             ContextPreparer contextPreparer,
                             TransportActionProvider transportActionProvider,
                             ThreadPool threadPool,
                             Functions functions,
                             ReferenceResolver referenceResolver,
                             PageDownstreamFactory pageDownstreamFactory,
                             Provider<DDLStatementDispatcher> ddlAnalysisDispatcherProvider,
                             ClusterService clusterService,
                             CrateCircuitBreakerService breakerService,
                             BulkRetryCoordinatorPool bulkRetryCoordinatorPool,
                             StreamerVisitor streamerVisitor) {
        this.jobContextService = jobContextService;
        this.contextPreparer = contextPreparer;
        this.transportActionProvider = transportActionProvider;
        this.pageDownstreamFactory = pageDownstreamFactory;
        this.threadPool = threadPool;
        this.functions = functions;
        this.ddlAnalysisDispatcherProvider = ddlAnalysisDispatcherProvider;
        this.clusterService = clusterService;
        this.bulkRetryCoordinatorPool = bulkRetryCoordinatorPool;
        this.streamerVisitor = streamerVisitor;
        this.nodeVisitor = new NodeVisitor();
        this.planVisitor = new TaskCollectingVisitor();
        this.circuitBreaker = breakerService.getBreaker(CrateCircuitBreakerService.QUERY_BREAKER);
        ImplementationSymbolVisitor globalImplementationSymbolVisitor = new ImplementationSymbolVisitor(
                referenceResolver, functions, RowGranularity.CLUSTER);
        this.globalProjectionToProjectionVisitor = new ProjectionToProjectorVisitor(
                clusterService,
                threadPool,
                settings,
                transportActionProvider,
                bulkRetryCoordinatorPool,
                globalImplementationSymbolVisitor);
    }

    @Override
    public Job newJob(Plan plan) {
        final Job job = new Job();
        List<Task> tasks = planVisitor.process(plan, job);
        job.addTasks(tasks);
        return job;
    }

    @Override
    public List<? extends ListenableFuture<TaskResult>> execute(Job job) {
        assert job.tasks().size() > 0;
        return execute(job.tasks());

    }

    @Override
    public List<Task> newTasks(PlanNode planNode, UUID jobId) {
        return planNode.accept(nodeVisitor, jobId);
    }

    @Override
    public List<? extends ListenableFuture<TaskResult>> execute(Collection<Task> tasks) {
        Task lastTask = null;
        assert tasks.size() > 0 : "need at least one task to execute";
        for (Task task : tasks) {

            if (lastTask != null) {
                task.upstreamResult(lastTask.result());
            }
            task.start();
            lastTask = task;
        }
        assert lastTask != null;
        return lastTask.result();
    }

    class TaskCollectingVisitor extends PlanVisitor<Job, List<Task>> {

        @Override
        public List<Task> visitIterablePlan(IterablePlan plan, Job job) {
            List<Task> tasks = new ArrayList<>();
            for (PlanNode planNode : plan) {
                tasks.addAll(planNode.accept(nodeVisitor, job.id()));
            }
            return tasks;
        }

        @Override
        public List<Task> visitNoopPlan(NoopPlan plan, Job job) {
            return ImmutableList.<Task>of(NoopTask.INSTANCE);
        }

        @Override
        public List<Task> visitGlobalAggregate(GlobalAggregate plan, Job job) {
            return ImmutableList.of(createExecutableNodesTask(job, plan.collectNode(), plan.mergeNode()));
        }

        @Override
        public List<Task> visitCollectAndMerge(CollectAndMerge plan, Job job) {
            return ImmutableList.of(createExecutableNodesTask(job, plan.collectNode(), plan.localMergeNode()));
        }

        @Override
        public List<Task> visitQueryAndFetch(QueryAndFetch plan, Job job) {
            return ImmutableList.of(createExecutableNodesTask(job, plan.collectNode(), plan.localMergeNode()));
        }

        @Override
        public List<Task> visitCountPlan(CountPlan countPlan, Job job) {
            return ImmutableList.of(createExecutableNodesTask(job, countPlan.countNode(), countPlan.mergeNode()));
        }

        private Task createExecutableNodesTask(Job job, ExecutionNode executionNode, @Nullable MergeNode localMergeNode) {
            return createExecutableNodesTask(job,
                    ImmutableList.<List<ExecutionNode>>of(ImmutableList.of(executionNode)),
                    localMergeNode == null ? null : ImmutableList.of(localMergeNode));
        }

        private ExecutionNodesTask createExecutableNodesTask(Job job, List<List<ExecutionNode>> groupedExecutionNodes, @Nullable List<MergeNode> localMergeNodes) {
            for (List<ExecutionNode> executionNodeGroup : groupedExecutionNodes) {
                for (ExecutionNode executionNode : executionNodeGroup) {
                    executionNode.jobId(job.id());
                }
            }
            if (localMergeNodes != null) {
                for (MergeNode localMergeNode : localMergeNodes) {
                    localMergeNode.jobId(job.id());
                }
            }
            return new ExecutionNodesTask(
                    job.id(),
                    clusterService,
                    contextPreparer,
                    jobContextService,
                    pageDownstreamFactory,
                    threadPool,
                    transportActionProvider.transportJobInitAction(),
                    transportActionProvider.transportCloseContextNodeAction(),
                    streamerVisitor,
                    circuitBreaker,
                    localMergeNodes,
                    groupedExecutionNodes
            );
        }

        @Override
        public List<Task> visitNonDistributedGroupBy(NonDistributedGroupBy plan, Job job) {
            return ImmutableList.of(createExecutableNodesTask(job, plan.collectNode(), plan.localMergeNode()));
        }

        @Override
        public List<Task> visitUpsert(Upsert plan, Job job) {
            if (plan.nodes().size() == 1 && plan.nodes().get(0) instanceof IterablePlan) {
                return process(plan.nodes().get(0), job);
            }

            List<List<ExecutionNode>> groupedExecutionNodes = new ArrayList<>(plan.nodes().size());
            List<MergeNode> mergeNodes = new ArrayList<>(plan.nodes().size());
            for (Plan subPlan : plan.nodes()) {
                assert subPlan instanceof CollectAndMerge;
                groupedExecutionNodes.add(ImmutableList.<ExecutionNode>of(((CollectAndMerge) subPlan).collectNode()));
                mergeNodes.add(((CollectAndMerge) subPlan).localMergeNode());
            }
            ExecutionNodesTask task = createExecutableNodesTask(job, groupedExecutionNodes, mergeNodes);
            task.rowCountResult(true);
            return ImmutableList.<Task>of(task);
        }

        @Override
        public List<Task> visitDistributedGroupBy(DistributedGroupBy plan, Job job) {
            plan.collectNode().jobId(job.id());
            plan.reducerMergeNode().jobId(job.id());
            MergeNode localMergeNode = plan.localMergeNode();
            List<MergeNode> mergeNodes = null;
            if (localMergeNode != null) {
                localMergeNode.jobId(job.id());
                mergeNodes = ImmutableList.of(localMergeNode);
            }
            return ImmutableList.<Task>of(
                    createExecutableNodesTask(job,
                            ImmutableList.<List<ExecutionNode>>of(
                                    ImmutableList.<ExecutionNode>of(
                                            plan.collectNode(),
                                            plan.reducerMergeNode())),
                            mergeNodes));
        }

        @Override
        public List<Task> visitInsertByQuery(InsertFromSubQuery node, Job job) {
            List<Task> tasks = process(node.innerPlan(), job);
            if(node.handlerMergeNode().isPresent()) {

                Task previousTask = Iterables.getLast(tasks);
                if (previousTask instanceof ExecutionNodesTask) {
                    ((ExecutionNodesTask) previousTask).mergeNodes(ImmutableList.of(node.handlerMergeNode().get()));
                } else {
                    ArrayList<Task> tasks2 = new ArrayList<>(tasks);
                    tasks2.addAll(nodeVisitor.visitMergeNode(node.handlerMergeNode().get(), job.id()));
                    return tasks2;
                }
            }
            return tasks;
        }

        @Override
        public List<Task> visitQueryThenFetch(QueryThenFetch plan, Job job) {
            return ImmutableList.of(createExecutableNodesTask(job, plan.collectNode(), plan.mergeNode()));
        }

        @Override
        public List<Task> visitKillPlan(KillPlan killPlan, Job job) {
            return ImmutableList.<Task>of(new KillTask(
                    clusterService,
                    transportActionProvider.transportKillAllNodeAction(),
                    job.id()));
        }
    }

    class NodeVisitor extends PlanNodeVisitor<UUID, ImmutableList<Task>> {

        private ImmutableList<Task> singleTask(Task task) {
            return ImmutableList.of(task);
        }

        @Override
        public ImmutableList<Task> visitGenericDDLNode(GenericDDLNode node, UUID jobId) {
            return singleTask(new DDLTask(jobId, ddlAnalysisDispatcherProvider.get(), node));
        }

        @Override
        public ImmutableList<Task> visitESGetNode(ESGetNode node, UUID jobId) {
            return singleTask(new ESGetTask(
                    jobId,
                    functions,
                    globalProjectionToProjectionVisitor,
                    transportActionProvider.transportMultiGetAction(),
                    transportActionProvider.transportGetAction(),
                    node,
                    jobContextService));
        }

        @Override
        public ImmutableList<Task> visitESDeleteByQueryNode(ESDeleteByQueryNode node, UUID jobId) {
            return singleTask(new ESDeleteByQueryTask(
                    jobId,
                    node,
                    transportActionProvider.transportDeleteByQueryAction(),
                    jobContextService));
        }

        @Override
        public ImmutableList<Task> visitESDeleteNode(ESDeleteNode node, UUID jobId) {
            return singleTask(new ESDeleteTask(
                    jobId,
                    node,
                    transportActionProvider.transportDeleteAction(),
                    jobContextService));
        }

        @Override
        public ImmutableList<Task> visitCreateTableNode(CreateTableNode node, UUID jobId) {
            return singleTask(new CreateTableTask(
                            jobId,
                            clusterService,
                            transportActionProvider.transportCreateIndexAction(),
                            transportActionProvider.transportDeleteIndexAction(),
                            transportActionProvider.transportPutIndexTemplateAction(),
                            node)
            );
        }

        @Override
        public ImmutableList<Task> visitESCreateTemplateNode(ESCreateTemplateNode node, UUID jobId) {
            return singleTask(new ESCreateTemplateTask(jobId,
                    node,
                    transportActionProvider.transportPutIndexTemplateAction()));
        }

        @Override
        public ImmutableList<Task> visitSymbolBasedUpsertByIdNode(SymbolBasedUpsertByIdNode node, UUID jobId) {
            return singleTask(new SymbolBasedUpsertByIdTask(jobId,
                    clusterService,
                    clusterService.state().metaData().settings(),
                    transportActionProvider.symbolBasedTransportShardUpsertActionDelegate(),
                    transportActionProvider.transportCreateIndexAction(),
                    transportActionProvider.transportBulkCreateIndicesAction(),
                    bulkRetryCoordinatorPool,
                    node,
                    jobContextService));
        }

        @Override
        public ImmutableList<Task> visitDropTableNode(DropTableNode node, UUID jobId) {
            return singleTask(new DropTableTask(jobId,
                    transportActionProvider.transportDeleteIndexTemplateAction(),
                    transportActionProvider.transportDeleteIndexAction(),
                    node));
        }

        @Override
        public ImmutableList<Task> visitESDeletePartitionNode(ESDeletePartitionNode node, UUID jobId) {
            return singleTask(new ESDeletePartitionTask(jobId,
                    transportActionProvider.transportDeleteIndexAction(),
                    node));
        }

        @Override
        public ImmutableList<Task> visitESClusterUpdateSettingsNode(ESClusterUpdateSettingsNode node, UUID jobId) {
            return singleTask(new ESClusterUpdateSettingsTask(
                    jobId,
                    transportActionProvider.transportClusterUpdateSettingsAction(),
                    node));
        }

        @Override
        protected ImmutableList<Task> visitPlanNode(PlanNode node, UUID jobId) {
            throw new UnsupportedOperationException(
                    String.format("Can't generate job/task for planNode %s", node));
        }
    }
}

<code block>


package io.crate.executor.transport.distributed;

import io.crate.Streamer;
import io.crate.core.collections.Bucket;
import io.crate.exceptions.UnknownUpstreamFailure;
import io.crate.executor.transport.StreamBucket;
import org.elasticsearch.common.io.ThrowableObjectInputStream;
import org.elasticsearch.common.io.ThrowableObjectOutputStream;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;
import org.elasticsearch.transport.TransportRequest;

import javax.annotation.Nullable;
import java.io.IOException;
import java.util.UUID;

public class DistributedResultRequest extends TransportRequest {

    private int executionNodeId;
    private int bucketIdx;

    private Streamer<?>[] streamers;
    private Bucket rows;
    private UUID jobId;
    private boolean isLast = true;

    private Throwable throwable = null;

    public DistributedResultRequest() {
    }

    public DistributedResultRequest(UUID jobId, int executionNodeId, int bucketIdx, Streamer<?>[] streamers) {
        this.jobId = jobId;
        this.executionNodeId = executionNodeId;
        this.bucketIdx = bucketIdx;
        this.streamers = streamers;
    }

    public UUID jobId() {
        return jobId;
    }

    public int executionNodeId() {
        return executionNodeId;
    }

    public int bucketIdx() {
        return bucketIdx;
    }

    public void streamers(Streamer<?>[] streamers) {
        if (rows instanceof StreamBucket) {
            assert streamers != null;
            ((StreamBucket) rows).streamers(streamers);
        }
        this.streamers = streamers;
    }

    public boolean rowsCanBeRead(){
        if (rows instanceof StreamBucket){
            return streamers != null;
        }
        return true;
    }

    public Bucket rows() {
        return rows;
    }

    public void rows(Bucket rows) {
        this.rows = rows;
    }

    public boolean isLast() {
        return isLast;
    }

    public void isLast(boolean isLast) {
        this.isLast = isLast;
    }

    public void throwable(Throwable throwable) {
        this.throwable = throwable;
    }

    @Nullable
    public Throwable throwable() {
        return throwable;
    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        super.readFrom(in);
        jobId = new UUID(in.readLong(), in.readLong());
        executionNodeId = in.readVInt();
        bucketIdx = in.readVInt();
        isLast = in.readBoolean();

        boolean failure = in.readBoolean();
        if (failure) {
            ThrowableObjectInputStream tis = new ThrowableObjectInputStream(in);
            try {
                throwable = (Throwable) tis.readObject();
            } catch (ClassNotFoundException e) {
                throwable = new UnknownUpstreamFailure();
            }
        } else {
            StreamBucket bucket = new StreamBucket(streamers);
            bucket.readFrom(in);
            rows = bucket;
        }
    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        super.writeTo(out);
        out.writeLong(jobId.getMostSignificantBits());
        out.writeLong(jobId.getLeastSignificantBits());
        out.writeVInt(executionNodeId);
        out.writeVInt(bucketIdx);
        out.writeBoolean(isLast);

        boolean failure = throwable != null;
        out.writeBoolean(failure);
        if (failure) {
            ThrowableObjectOutputStream too = new ThrowableObjectOutputStream(out);
            too.writeObject(throwable);
        } else {

            StreamBucket.writeBucket(out, streamers, rows);
        }
    }
}

<code block>


package io.crate.executor.transport.distributed;

import io.crate.Constants;
import io.crate.Streamer;
import io.crate.core.collections.Bucket;
import io.crate.core.collections.Row;
import org.elasticsearch.action.ActionListener;
import org.elasticsearch.common.logging.ESLogger;
import org.elasticsearch.common.logging.Loggers;

import java.io.IOException;
import java.util.Collection;
import java.util.Deque;
import java.util.UUID;
import java.util.concurrent.CancellationException;
import java.util.concurrent.ConcurrentLinkedDeque;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicInteger;

public class DistributingDownstream extends ResultProviderBase {

    private static final ESLogger LOGGER = Loggers.getLogger(DistributingDownstream.class);

    private final UUID jobId;
    private final TransportDistributedResultAction transportDistributedResultAction;
    private final MultiBucketBuilder bucketBuilder;
    private Downstream[] downstreams;
    private final AtomicInteger finishedDownstreams = new AtomicInteger(0);

    public DistributingDownstream(UUID jobId,
                                  int targetExecutionNodeId,
                                  int bucketIdx,
                                  Collection<String> downstreamNodeIds,
                                  TransportDistributedResultAction transportDistributedResultAction,
                                  Streamer<?>[] streamers) {
        this.jobId = jobId;
        this.transportDistributedResultAction = transportDistributedResultAction;

        downstreams = new Downstream[downstreamNodeIds.size()];
        bucketBuilder = new MultiBucketBuilder(streamers, downstreams.length);

        int idx = 0;
        for (String downstreamNodeId : downstreamNodeIds) {
            downstreams[idx] = new Downstream(downstreamNodeId, jobId, targetExecutionNodeId, bucketIdx, streamers);
            idx++;
        }
    }

    @Override
    public boolean setNextRow(Row row) {
        if (allDownstreamsFinished()) {
            return false;
        }
        try {
            int downstreamIdx = bucketBuilder.getBucket(row);

            if (downstreams[downstreamIdx].wantMore.get()) {
                bucketBuilder.setNextRow(downstreamIdx, row);
                sendRequestIfNeeded(downstreamIdx);
            }
        } catch (IOException e) {
            fail(e);
            return false;
        }
        return true;
    }

    protected void sendRequestIfNeeded(int downstreamIdx) {
        int size = bucketBuilder.size(downstreamIdx);
        if (size >= Constants.PAGE_SIZE || remainingUpstreams.get() <= 0) {
            Downstream downstream = downstreams[downstreamIdx];
            downstream.bucketQueue.add(bucketBuilder.build(downstreamIdx));
            sendRequest(downstream);
        }
    }

    protected void onAllUpstreamsFinished() {
        for (int i = 0; i < downstreams.length; i++) {
            sendRequestIfNeeded(i);
        }
    }

    private void forwardFailures(Throwable throwable) {
        for (Downstream downstream : downstreams) {
            downstream.request.throwable(throwable);
            sendRequest(downstream.request, downstream);
        }
    }

    private boolean allDownstreamsFinished() {
        return finishedDownstreams.get() == downstreams.length;
    }

    private void sendRequest(Downstream downstream) {
        if (downstream.requestPending.compareAndSet(false, true)) {
            DistributedResultRequest request = downstream.request;
            Deque<Bucket> queue = downstream.bucketQueue;
            int size = queue.size();
            if (size > 0) {
                request.rows(queue.poll());
            } else {
                request.rows(Bucket.EMPTY);
            }
            request.isLast(!(size > 1 || remainingUpstreams.get() > 0));
            sendRequest(request, downstream);
        }
    }

    private void sendRequest(final DistributedResultRequest request, final Downstream downstream) {
        if (LOGGER.isTraceEnabled()) {
            LOGGER.trace("[{}] sending distributing collect request to {}, isLast? {} ...",
                    jobId.toString(),
                    downstream.node, request.isLast());
        }
        try {
            transportDistributedResultAction.pushResult(
                    downstream.node,
                    request,
                    new DistributedResultResponseActionListener(downstream)
            );
        } catch (IllegalArgumentException e) {
            LOGGER.error(e.getMessage(), e);
            downstream.wantMore.set(false);
        }
    }

    @Override
    public Bucket doFinish() {
        onAllUpstreamsFinished();
        return null;
    }

    @Override
    public Throwable doFail(Throwable t) {
        if (t instanceof CancellationException) {

            LOGGER.debug("{} killed", getClass().getSimpleName());
        } else {
            forwardFailures(t);
        }
        return t;
    }

    static class Downstream {

        final AtomicBoolean wantMore = new AtomicBoolean(true);
        final AtomicBoolean requestPending = new AtomicBoolean(false);
        final Deque<Bucket> bucketQueue = new ConcurrentLinkedDeque<>();
        final DistributedResultRequest request;
        final String node;

        public Downstream(String node,
                          UUID jobId,
                          int targetExecutionNodeId,
                          int bucketIdx,
                          Streamer<?>[] streamers) {
            this.node = node;
            this.request = new DistributedResultRequest(jobId, targetExecutionNodeId, bucketIdx, streamers);
        }
    }

    private class DistributedResultResponseActionListener implements ActionListener<DistributedResultResponse> {
        private final Downstream downstream;

        public DistributedResultResponseActionListener(Downstream downstream) {
            this.downstream = downstream;
        }

        @Override
        public void onResponse(DistributedResultResponse response) {
            if (LOGGER.isTraceEnabled()) {
                LOGGER.trace("[{}] successfully sent distributing collect request to {}, needMore? {}",
                        jobId,
                        downstream.node,
                        response.needMore());
            }

            downstream.wantMore.set(response.needMore());
            if (!response.needMore()) {
                finishedDownstreams.incrementAndGet();

                downstream.bucketQueue.clear();
            } else {

                downstream.requestPending.set(false);
                sendRequest(downstream);
            }
        }

        @Override
        public void onFailure(Throwable exp) {
            LOGGER.error("[{}] Exception sending distributing collect request to {}", exp, jobId, downstream.node);
            downstream.wantMore.set(false);
            downstream.bucketQueue.clear();
            finishedDownstreams.incrementAndGet();
        }
    }
}

<code block>


package io.crate.executor.transport.distributed;

import io.crate.executor.transport.DefaultTransportResponseHandler;
import io.crate.executor.transport.NodeAction;
import io.crate.executor.transport.NodeActionRequestHandler;
import io.crate.executor.transport.Transports;
import io.crate.jobs.JobContextService;
import io.crate.jobs.JobExecutionContext;
import io.crate.jobs.PageDownstreamContext;
import io.crate.operation.PageResultListener;
import io.crate.planner.node.ExecutionNode;
import org.elasticsearch.action.ActionListener;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.logging.ESLogger;
import org.elasticsearch.common.logging.Loggers;
import org.elasticsearch.threadpool.ThreadPool;
import org.elasticsearch.transport.TransportService;

import java.util.Locale;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;


public class TransportDistributedResultAction implements NodeAction<DistributedResultRequest, DistributedResultResponse> {

    private static final ESLogger LOGGER = Loggers.getLogger(TransportDistributedResultAction.class);

    public final static String DISTRIBUTED_RESULT_ACTION = "crate/sql/node/merge/add_rows";
    private final static String EXECUTOR_NAME = ThreadPool.Names.SEARCH;

    private final Transports transports;
    private final JobContextService jobContextService;
    private final ScheduledExecutorService scheduler;

    @Inject
    public TransportDistributedResultAction(Transports transports,
                                            JobContextService jobContextService,
                                            ThreadPool threadPool,
                                            TransportService transportService) {
        this.transports = transports;
        this.jobContextService = jobContextService;
        scheduler = threadPool.scheduler();
        transportService.registerHandler(DISTRIBUTED_RESULT_ACTION, new NodeActionRequestHandler<DistributedResultRequest, DistributedResultResponse>(this) {
            @Override
            public DistributedResultRequest newInstance() {
                return new DistributedResultRequest();
            }
        });
    }

    public void pushResult(String node, DistributedResultRequest request, ActionListener<DistributedResultResponse> listener) {
        transports.executeLocalOrWithTransport(this, node, request, listener,
                new DefaultTransportResponseHandler<DistributedResultResponse>(listener, EXECUTOR_NAME) {
                    @Override
                    public DistributedResultResponse newInstance() {
                        return new DistributedResultResponse();
                    }
                });
    }

    @Override
    public String actionName() {
        return DISTRIBUTED_RESULT_ACTION;
    }

    @Override
    public String executorName() {
        return EXECUTOR_NAME;
    }

    @Override
    public void nodeOperation(DistributedResultRequest request,
                              ActionListener<DistributedResultResponse> listener) {
        nodeOperation(request, listener, 0);
    }

    private void nodeOperation(final DistributedResultRequest request,
                               final ActionListener<DistributedResultResponse> listener,
                               final int retry) {
        if (request.executionNodeId() == ExecutionNode.NO_EXECUTION_NODE) {
            listener.onFailure(new IllegalStateException("request must contain a valid executionNodeId"));
            return;
        }
        JobExecutionContext context = jobContextService.getContextOrNull(request.jobId());
        if (context == null) {
            retryOrFailureResponse(request, listener, retry);
            return;
        }

        PageDownstreamContext pageDownstreamContext = context.getSubContextOrNull(request.executionNodeId());
        if (pageDownstreamContext == null) {

            listener.onFailure(new IllegalStateException(String.format(Locale.ENGLISH,
                    "Couldn't find pageDownstreamContext for %d", request.executionNodeId())));
            return;
        }

        Throwable throwable = request.throwable();
        if (throwable == null) {
            request.streamers(pageDownstreamContext.streamer());
            pageDownstreamContext.setBucket(
                    request.bucketIdx(),
                    request.rows(),
                    request.isLast(),
                    new SendResponsePageResultListener(listener, request));
        } else {
            pageDownstreamContext.failure(request.bucketIdx(), throwable);
            listener.onResponse(new DistributedResultResponse(false));
        }
    }

    private void retryOrFailureResponse(DistributedResultRequest request,
                                        ActionListener<DistributedResultResponse> listener,
                                        int retry) {
        if (retry > 20) {
            listener.onFailure(new IllegalStateException(
                    String.format("Couldn't find JobExecutionContext for %s", request.jobId())));
        } else {
            scheduler.schedule(new NodeOperationRunnable(request, listener, retry), (retry + 1) * 2, TimeUnit.MILLISECONDS);
        }
    }

    private static class SendResponsePageResultListener implements PageResultListener {
        private final ActionListener<DistributedResultResponse> listener;
        private final DistributedResultRequest request;

        public SendResponsePageResultListener(ActionListener<DistributedResultResponse> listener, DistributedResultRequest request) {
            this.listener = listener;
            this.request = request;
        }

        @Override
        public void needMore(boolean needMore) {
            LOGGER.trace("sending needMore response, need more? {}", needMore);
            listener.onResponse(new DistributedResultResponse(needMore));
        }

        @Override
        public int buckedIdx() {
            return request.bucketIdx();
        }
    }

    private class NodeOperationRunnable implements Runnable {
        private final DistributedResultRequest request;
        private final ActionListener<DistributedResultResponse> listener;
        private final int retry;

        public NodeOperationRunnable(DistributedResultRequest request, ActionListener<DistributedResultResponse> listener, int retry) {
            this.request = request;
            this.listener = listener;
            this.retry = retry;
        }

        @Override
        public void run() {
            nodeOperation(request, listener, retry + 1);
        }
    }
}

<code block>


package io.crate.planner;

import io.crate.planner.node.dml.InsertFromSubQuery;
import io.crate.planner.node.dql.*;
import io.crate.planner.node.dml.Upsert;
import io.crate.planner.node.dql.join.NestedLoop;
import io.crate.planner.node.management.KillPlan;
import org.elasticsearch.common.Nullable;

public class PlanVisitor<C, R> {

    public R process(Plan plan, @Nullable C context) {
        return plan.accept(this, context);
    }

    protected R visitPlan(Plan plan, C context) {
        return null;
    }

    public R visitIterablePlan(IterablePlan plan, C context) {
        return visitPlan(plan, context);
    }

    public R visitNoopPlan(NoopPlan plan, C context) {
        return visitPlan(plan, context);
    }

    public R visitGlobalAggregate(GlobalAggregate plan, C context) {
        return visitPlan(plan, context);
    }

    public R visitQueryAndFetch(QueryAndFetch node, C context){
        return visitPlan(node, context);
    }

    public R visitQueryThenFetch(QueryThenFetch node, C context){
        return visitPlan(node, context);
    }

    public R visitNonDistributedGroupBy(NonDistributedGroupBy node, C context) {
        return visitPlan(node, context);
    }

    public R visitUpsert(Upsert node, C context) {
        return visitPlan(node, context);
    }

    public R visitDistributedGroupBy(DistributedGroupBy node, C context) {
        return visitPlan(node, context);
    }

    public R visitInsertByQuery(InsertFromSubQuery node, C context) {
        return visitPlan(node, context);
    }

    public R visitCollectAndMerge(CollectAndMerge plan, C context) {
        return visitPlan(plan, context);
    }

    public R visitCountPlan(CountPlan countPlan, C context) {
        return visitPlan(countPlan, context);
    }

    public R visitKillPlan(KillPlan killPlan, C context) {
        return visitPlan(killPlan, context);
    }

    public R visitNestedLoop(NestedLoop nestedLoop, C context) {
        return visitPlan(nestedLoop, context);
    }
}

<code block>


package io.crate.planner;

import com.google.common.base.MoreObjects;
import com.google.common.base.Predicates;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableSet;
import com.google.common.collect.Iterables;
import io.crate.analyze.OrderBy;
import io.crate.analyze.WhereClause;
import io.crate.metadata.PartitionName;
import io.crate.metadata.Routing;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.consumer.OrderByPositionVisitor;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.DQLPlanNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.InputColumn;
import io.crate.planner.symbol.Symbol;
import io.crate.planner.symbol.Symbols;

import javax.annotation.Nullable;
import java.util.List;
import java.util.Map;
import java.util.TreeMap;

public class PlanNodeBuilder {

    public static CollectNode distributingCollect(TableInfo tableInfo,
                                                  Planner.Context plannerContext,
                                                  WhereClause whereClause,
                                                  List<Symbol> toCollect,
                                                  List<String> downstreamNodes,
                                                  ImmutableList<Projection> projections) {
        Routing routing = tableInfo.getRouting(whereClause, null);
        plannerContext.allocateJobSearchContextIds(routing);
        CollectNode node = new CollectNode(
                plannerContext.nextExecutionNodeId(),
                "distributing collect",
                routing);
        node.whereClause(whereClause);
        node.maxRowGranularity(tableInfo.rowGranularity());
        node.downstreamNodes(downstreamNodes);
        node.toCollect(toCollect);
        node.projections(projections);

        node.isPartitioned(tableInfo.isPartitioned());
        setOutputTypes(node);
        return node;
    }

    public static MergeNode distributedMerge(CollectNode collectNode,
                                             Planner.Context plannerContext,
                                             List<Projection> projections) {
        MergeNode node = new MergeNode(
                plannerContext.nextExecutionNodeId(),
                "distributed merge",
                collectNode.executionNodes().size());
        node.projections(projections);

        assert collectNode.hasDistributingDownstreams();
        node.executionNodes(ImmutableSet.copyOf(collectNode.downstreamNodes()));
        connectTypes(collectNode, node);
        return node;
    }

    public static MergeNode localMerge(List<Projection> projections,
                                       DQLPlanNode previousNode,
                                       Planner.Context plannerContext) {
        MergeNode node = new MergeNode(
                plannerContext.nextExecutionNodeId(),
                "localMerge",
                previousNode.executionNodes().size());
        node.projections(projections);
        connectTypes(previousNode, node);
        return node;
    }


    public static MergeNode sortedLocalMerge(List<Projection> projections,
                                             OrderBy orderBy,
                                             List<Symbol> sourceSymbols,
                                             @Nullable List<Symbol> orderBySymbols,
                                             DQLPlanNode previousNode,
                                             Planner.Context plannerContext) {
        int[] orderByIndices = OrderByPositionVisitor.orderByPositions(
                MoreObjects.firstNonNull(orderBySymbols, orderBy.orderBySymbols()),
                sourceSymbols
        );
        MergeNode node = MergeNode.sortedMergeNode(
                plannerContext.nextExecutionNodeId(),
                "sortedLocalMerge",
                previousNode.executionNodes().size(),
                orderByIndices,
                orderBy.reverseFlags(),
                orderBy.nullsFirst()
        );
        node.projections(projections);
        connectTypes(previousNode, node);
        return node;
    }


    public static void setOutputTypes(CollectNode node) {
        if (node.projections().isEmpty()) {
            node.outputTypes(Symbols.extractTypes(node.toCollect()));
        } else {
            node.outputTypes(Planner.extractDataTypes(node.projections(), Symbols.extractTypes(node.toCollect())));
        }
    }


    public static void connectTypes(DQLPlanNode previousNode, DQLPlanNode nextNode) {
        nextNode.inputTypes(previousNode.outputTypes());
        nextNode.outputTypes(Planner.extractDataTypes(nextNode.projections(), nextNode.inputTypes()));
    }

    public static CollectNode collect(TableInfo tableInfo,
                                      Planner.Context plannerContext,
                                      WhereClause whereClause,
                                      List<Symbol> toCollect,
                                      ImmutableList<Projection> projections,
                                      @Nullable String partitionIdent,
                                      @Nullable String routingPreference,
                                      @Nullable OrderBy orderBy,
                                      @Nullable Integer limit) {
        assert !Iterables.any(toCollect, Predicates.instanceOf(InputColumn.class)) : "cannot collect inputcolumns";
        Routing routing = tableInfo.getRouting(whereClause, routingPreference);
        if (partitionIdent != null && routing.hasLocations()) {
            routing = filterRouting(routing, PartitionName.fromPartitionIdent(
                    tableInfo.ident().schema(), tableInfo.ident().name(), partitionIdent).stringValue());
        }
        plannerContext.allocateJobSearchContextIds(routing);
        CollectNode node = new CollectNode(
                plannerContext.nextExecutionNodeId(),
                "collect",
                routing,
                toCollect,
                projections);
        node.whereClause(whereClause);
        node.maxRowGranularity(tableInfo.rowGranularity());
        node.isPartitioned(tableInfo.isPartitioned());
        setOutputTypes(node);
        node.orderBy(orderBy);
        node.limit(limit);
        return node;
    }

    private static Routing filterRouting(Routing routing, String includeTableName) {
        assert routing.hasLocations();
        assert includeTableName != null;
        Map<String, Map<String, List<Integer>>> newLocations = new TreeMap<>();

        for (Map.Entry<String, Map<String, List<Integer>>> entry : routing.locations().entrySet()) {
            Map<String, List<Integer>> tableMap = new TreeMap<>();
            for (Map.Entry<String, List<Integer>> tableEntry : entry.getValue().entrySet()) {
                if (includeTableName.equals(tableEntry.getKey())) {
                    tableMap.put(tableEntry.getKey(), tableEntry.getValue());
                }
            }
            if (tableMap.size()>0){
                newLocations.put(entry.getKey(), tableMap);
            }

        }
        if (newLocations.size()>0) {
            return new Routing(newLocations);
        } else {
            return new Routing();
        }

    }

    public static CollectNode collect(TableInfo tableInfo,
                                      Planner.Context plannerContext,
                                      WhereClause whereClause,
                                      List<Symbol> toCollect,
                                      ImmutableList<Projection> projections) {
        return collect(tableInfo, plannerContext, whereClause, toCollect, projections, null, null, null, null);
    }

    public static CollectNode collect(TableInfo tableInfo,
                                      Planner.Context plannerContext,
                                      WhereClause whereClause,
                                      List<Symbol> toCollect,
                                      ImmutableList<Projection> projections,
                                      @Nullable String partitionIdent,
                                      @Nullable String routingPreference) {
        return collect(tableInfo, plannerContext, whereClause, toCollect, projections, partitionIdent, routingPreference, null, null);
    }

    public static CollectNode collect(TableInfo tableInfo,
                                      Planner.Context plannerContext,
                                      WhereClause whereClause,
                                      List<Symbol> toCollect,
                                      ImmutableList<Projection> projections,
                                      @Nullable String partitionIdent) {
        return collect(tableInfo, plannerContext, whereClause, toCollect, projections, partitionIdent, null);
    }

    public static CollectNode collect(TableInfo tableInfo,
                                      Planner.Context plannerContext,
                                      WhereClause whereClause,
                                      List<Symbol> toCollect,
                                      ImmutableList<Projection> projections,
                                      @Nullable OrderBy orderBy,
                                      @Nullable Integer limit) {
        return collect(tableInfo, plannerContext, whereClause, toCollect, projections, null, null, orderBy, limit);
    }
}

<code block>


package io.crate.planner.node;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.Lists;
import io.crate.Streamer;
import io.crate.exceptions.ResourceUnknownException;
import io.crate.metadata.Functions;
import io.crate.operation.aggregation.AggregationFunction;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.AggregationProjection;
import io.crate.planner.projection.GroupProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.ProjectionType;
import io.crate.planner.symbol.Aggregation;
import io.crate.planner.symbol.InputColumn;
import io.crate.planner.symbol.Symbol;
import io.crate.planner.symbol.SymbolType;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import io.crate.types.UndefinedType;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.inject.Singleton;

import java.util.ArrayList;
import java.util.Collections;
import java.util.List;


@Singleton
public class StreamerVisitor {

    public static class Context {
        private List<Streamer<?>> inputStreamers = new ArrayList<>();
        private List<Streamer<?>> outputStreamers = new ArrayList<>();

        public Context() {
        }

        public Streamer<?>[] inputStreamers() {
            return inputStreamers.toArray(new Streamer<?>[inputStreamers.size()]);
        }

        public Streamer<?>[] outputStreamers() {
            return outputStreamers.toArray(new Streamer<?>[outputStreamers.size()]);
        }
    }

    private final Functions functions;
    private final PlanNodeStreamerVisitor planNodeStreamerVisitor;
    private final ExecutionNodeStreamerVisitor executionNodeStreamerVisitor;

    @Inject
    public StreamerVisitor(Functions functions) {
        this.functions = functions;
        this.planNodeStreamerVisitor = new PlanNodeStreamerVisitor();
        this.executionNodeStreamerVisitor = new ExecutionNodeStreamerVisitor();
    }

    public Context processPlanNode(PlanNode node) {
        Context context = new Context();
        planNodeStreamerVisitor.process(node, context);
        return context;
    }

    public Context processExecutionNode(ExecutionNode executionNode) {
        Context context = new Context();
        executionNodeStreamerVisitor.process(executionNode, context);
        return context;
    }

    private class PlanNodeStreamerVisitor extends PlanNodeVisitor<Context, Void> {

        @Override
        public Void visitCollectNode(CollectNode node, Context context) {
            extractFromCollectNode(node, context);
            return null;
        }

        @Override
        public Void visitMergeNode(MergeNode node, Context context) {
            extractFromMergeNode(node, context);
            return null;
        }
    }

    private class ExecutionNodeStreamerVisitor extends ExecutionNodeVisitor<Context, Void> {

        @Override
        public Void visitMergeNode(MergeNode node, Context context) {
            extractFromMergeNode(node, context);
            return null;
        }

        @Override
        public Void visitCollectNode(CollectNode collectNode, Context context) {
            extractFromCollectNode(collectNode, context);
            return null;
        }

        @Override
        protected Void visitExecutionNode(ExecutionNode node, Context context) {
            throw new UnsupportedOperationException(String.format("Got unsupported ExecutionNode %s", node.getClass().getName()));
        }
    }


    private Streamer<?> resolveStreamer(Aggregation aggregation, Aggregation.Step step) {
        Streamer<?> streamer;
        AggregationFunction<?, ?> aggFunction = (AggregationFunction<?, ?>)functions.get(aggregation.functionIdent());
        if (aggFunction == null) {
            throw new ResourceUnknownException("unknown aggregation function");
        }
        switch (step) {
            case ITER:
                assert aggFunction.info().ident().argumentTypes().size() == 1;
                streamer = aggFunction.info().ident().argumentTypes().get(0).streamer();
                break;
            case PARTIAL:
                streamer = aggFunction.partialType().streamer();
                break;
            case FINAL:
                streamer = aggFunction.info().returnType().streamer();
                break;
            default:
                throw new UnsupportedOperationException("step not supported");
        }
        return streamer;
    }

    private void extractFromCollectNode(CollectNode node, Context context) {

        List<Aggregation> aggregations = ImmutableList.of();
        List<Projection> projections = Lists.reverse(node.projections());
        for(Projection projection : projections){
            if (projection.projectionType() == ProjectionType.AGGREGATION) {
                aggregations = ((AggregationProjection)projection).aggregations();
                break;
            } else if (projection.projectionType() == ProjectionType.GROUP) {
                aggregations = ((GroupProjection)projection).values();
                break;
            }
        }


        int aggIdx = 0;
        Aggregation aggregation;
        for (DataType outputType : node.outputTypes()) {
            if (outputType == null || outputType == UndefinedType.INSTANCE) {

                try {
                    aggregation = aggregations.get(aggIdx);
                    if (aggregation != null) {
                        context.outputStreamers.add(resolveStreamer(aggregation, aggregation.toStep()));
                    }
                } catch (IndexOutOfBoundsException e) {

                    context.outputStreamers.add(UndefinedType.INSTANCE.streamer());
                }
                aggIdx++;
            } else {

                context.outputStreamers.add(outputType.streamer());
            }
        }
    }

    private void extractFromMergeNode(MergeNode node, Context context) {
        if (node.projections().isEmpty()) {
            for (DataType dataType : node.inputTypes()) {
                if (dataType != null) {
                    context.inputStreamers.add(dataType.streamer());
                } else {
                    throw new IllegalStateException("Can't resolve Streamer from null dataType");
                }
            }
            return;
        }

        Projection firstProjection = node.projections().get(0);
        setInputStreamers(node.inputTypes(), firstProjection, context);
        setOutputStreamers(node.outputTypes(), node.inputTypes(), node.projections(), context);
    }

    private void setOutputStreamers(List<DataType> outputTypes,
                                    List<DataType> inputTypes,
                                    List<Projection> projections, Context context) {
        final Streamer<?>[] streamers = new Streamer[outputTypes.size()];

        int idx = 0;
        for (DataType outputType : outputTypes) {
            if (outputType == UndefinedType.INSTANCE) {
                resolveStreamer(streamers, projections, idx, projections.size() - 1, inputTypes);
                if (streamers[idx] == null) {
                    streamers[idx] = outputType.streamer();
                }
            } else {
                streamers[idx] = outputType.streamer();
            }
            idx++;
        }

        for (Streamer<?> streamer : streamers) {
            if (streamer == null) {
                throw new IllegalStateException("Could not resolve all output streamers");
            }
        }
        Collections.addAll(context.outputStreamers, streamers);
    }


    private void resolveStreamer(Streamer<?>[] streamers,
                                 List<Projection> projections,
                                 int columnIdx,
                                 int projectionIdx,
                                 List<DataType> inputTypes) {
        final Projection projection = projections.get(projectionIdx);
        final Symbol symbol = projection.outputs().get(columnIdx);

        if (!symbol.valueType().equals(DataTypes.UNDEFINED)) {
            streamers[columnIdx] = symbol.valueType().streamer();
        } else if (symbol.symbolType() == SymbolType.AGGREGATION) {
            Aggregation aggregation = (Aggregation)symbol;
            streamers[columnIdx] = resolveStreamer(aggregation, aggregation.toStep());
        } else if (symbol.symbolType() == SymbolType.INPUT_COLUMN) {
            columnIdx = ((InputColumn)symbol).index();
            if (projectionIdx > 0) {
                projectionIdx--;
                resolveStreamer(streamers, projections, columnIdx, projectionIdx, inputTypes);
            } else {
                streamers[columnIdx] = inputTypes.get(((InputColumn)symbol).index()).streamer();
            }
        }
    }

    private void setInputStreamers(List<DataType> inputTypes, Projection projection, Context context) {
        List<Aggregation> aggregations;
        switch (projection.projectionType()) {
            case TOPN:
            case FETCH:
                aggregations = ImmutableList.of();
                break;
            case GROUP:
                aggregations = ((GroupProjection)projection).values();
                break;
            case AGGREGATION:
                aggregations = ((AggregationProjection)projection).aggregations();
                break;
            default:
                throw new UnsupportedOperationException("projectionType not supported");
        }

        int idx = 0;
        for (DataType inputType : inputTypes) {
            if (inputType != null && inputType != UndefinedType.INSTANCE) {
                context.inputStreamers.add(inputType.streamer());
            } else {
                Aggregation aggregation = aggregations.get(idx);
                context.inputStreamers.add(resolveStreamer(aggregation, aggregation.fromStep()));
                idx++;
            }
        }
    }
}

<code block>


package io.crate.planner.node;

import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.CountNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.dql.join.NestedLoopNode;

public class ExecutionNodeVisitor<C, R> {

    public R process(ExecutionNode node, C context) {
        return node.accept(this, context);
    }

    protected R visitExecutionNode(ExecutionNode node, C context) {
        return null;
    }

    public R visitCollectNode(CollectNode node, C context) {
        return visitExecutionNode(node, context);
    }

    public R visitMergeNode(MergeNode node, C context) {
        return visitExecutionNode(node, context);
    }

    public R visitCountNode(CountNode countNode, C context) {
        return visitExecutionNode(countNode, context);
    }

    public R visitNestedLoopNode(NestedLoopNode nestedLoopNode, C context) {
        return visitExecutionNode(nestedLoopNode, context);
    }
}

<code block>


package io.crate.planner.node;

import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.CountNode;
import io.crate.planner.node.dql.FileUriCollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.dql.join.NestedLoopNode;
import org.elasticsearch.common.io.stream.Streamable;

import java.util.List;
import java.util.Set;
import java.util.UUID;

public interface ExecutionNode extends Streamable {

    String DIRECT_RETURN_DOWNSTREAM_NODE = "_response";

    int NO_EXECUTION_NODE = Integer.MAX_VALUE;

    interface ExecutionNodeFactory<T extends ExecutionNode> {
        T create();
    }

    enum Type {
        COLLECT(CollectNode.FACTORY),
        COUNT(CountNode.FACTORY),
        FILE_URI_COLLECT(FileUriCollectNode.FACTORY),
        MERGE(MergeNode.FACTORY),
        NESTED_LOOP(NestedLoopNode.FACTORY);

        private final ExecutionNodeFactory factory;

        Type(ExecutionNodeFactory factory) {
            this.factory = factory;
        }

        public ExecutionNodeFactory factory() {
            return factory;
        }
    }

    Type type();

    String name();

    int executionNodeId();

    Set<String> executionNodes();

    List<String> downstreamNodes();

    int downstreamExecutionNodeId();

    UUID jobId();

    void jobId(UUID jobId);


    <C, R> R accept(ExecutionNodeVisitor<C, R> visitor, C context);
}

<code block>


package io.crate.planner.node.dql;

import io.crate.planner.PlanAndPlannedAnalyzedRelation;
import io.crate.planner.PlanVisitor;
import io.crate.planner.projection.Projection;

public class CountPlan extends PlanAndPlannedAnalyzedRelation{

    private final CountNode countNode;
    private final MergeNode mergeNode;

    public CountPlan(CountNode countNode, MergeNode mergeNode) {
        this.countNode = countNode;
        this.mergeNode = mergeNode;
    }

    public CountNode countNode() {
        return countNode;
    }

    public MergeNode mergeNode() {
        return mergeNode;
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitCountPlan(this, context);
    }

    @Override
    public void addProjection(Projection projection) {
        mergeNode.addProjection(projection);
    }

    @Override
    public boolean resultIsDistributed() {
        return false;
    }

    @Override
    public DQLPlanNode resultNode() {
        return mergeNode;
    }
}

<code block>


package io.crate.planner.node.dql;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.Sets;
import io.crate.analyze.WhereClause;
import io.crate.metadata.Routing;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.node.ExecutionNodeVisitor;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;

import java.io.IOException;
import java.util.List;
import java.util.Set;
import java.util.UUID;

public class CountNode implements ExecutionNode {

    public static final ExecutionNodeFactory<CountNode> FACTORY = new ExecutionNodeFactory<CountNode>() {
        @Override
        public CountNode create() {
            return new CountNode();
        }
    };
    private UUID jobId;
    private int executionNodeId;
    private Routing routing;
    private WhereClause whereClause;

    CountNode() {}

    public CountNode(int executionNodeId, Routing routing, WhereClause whereClause) {
        this.executionNodeId = executionNodeId;
        this.routing = routing;
        this.whereClause = whereClause;
    }

    @Override
    public Type type() {
        return Type.COUNT;
    }

    @Override
    public String name() {
        return "count";
    }

    @Override
    public UUID jobId() {
        return jobId;
    }

    @Override
    public void jobId(UUID jobId) {
        this.jobId = jobId;
    }

    public Routing routing() {
        return routing;
    }

    public WhereClause whereClause() {
        return whereClause;
    }

    @Override
    public int executionNodeId() {
        return executionNodeId;
    }

    @Override
    public Set<String> executionNodes() {
        if (routing.isNullRouting()) {
            return routing.nodes();
        } else {
            return Sets.filter(routing.nodes(), TableInfo.IS_NOT_NULL_NODE_ID);
        }
    }

    @Override
    public List<String> downstreamNodes() {
        return ImmutableList.of(ExecutionNode.DIRECT_RETURN_DOWNSTREAM_NODE);
    }

    @Override
    public int downstreamExecutionNodeId() {
        return ExecutionNode.NO_EXECUTION_NODE;
    }

    @Override
    public <C, R> R accept(ExecutionNodeVisitor<C, R> visitor, C context) {
        return visitor.visitCountNode(this, context);
    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        jobId = new UUID(in.readLong(), in.readLong());
        executionNodeId = in.readVInt();
        routing = new Routing();
        routing.readFrom(in);
        whereClause = new WhereClause(in);
    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        assert jobId != null : "jobId must not be null";
        out.writeLong(jobId.getMostSignificantBits());
        out.writeLong(jobId.getLeastSignificantBits());
        out.writeVInt(executionNodeId);
        routing.writeTo(out);
        whereClause.writeTo(out);
    }
}

<code block>


package io.crate.planner.node.dql;

import com.google.common.base.MoreObjects;
import com.google.common.base.Optional;
import com.google.common.collect.ImmutableList;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.projection.Projection;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;
import org.elasticsearch.common.io.stream.Streamable;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import java.util.UUID;

public abstract class AbstractDQLPlanNode implements DQLPlanNode, Streamable, ExecutionNode {

    private UUID jobId;
    private int executionNodeId;
    private String name;
    protected List<Projection> projections = ImmutableList.of();
    protected List<DataType> outputTypes = ImmutableList.of();
    private List<DataType> inputTypes;

    public AbstractDQLPlanNode() {

    }

    protected AbstractDQLPlanNode(int executionNodeId, String name) {
        this.executionNodeId = executionNodeId;
        this.name = name;
    }

    @Override
    public String name() {
        return name;
    }

    @Override
    public UUID jobId() {
        return jobId;
    }

    @Override
    public void jobId(UUID jobId) {
        this.jobId = jobId;
    }

    @Override
    public int executionNodeId() {
        return executionNodeId;
    }

    @Override
    public boolean hasProjections() {
        return projections != null && projections.size() > 0;
    }

    @Override
    public List<Projection> projections() {
        return projections;
    }

    public void projections(List<Projection> projections) {
        this.projections = projections;
    }

    @Override
    public void addProjection(Projection projection) {
        List<Projection> projections = new ArrayList<>(this.projections);
        projections.add(projection);
        this.projections = ImmutableList.copyOf(projections);
    }

    public Optional<Projection> finalProjection() {
        if (projections.size() == 0) {
            return Optional.absent();
        } else {
            return Optional.of(projections.get(projections.size()-1));
        }
    }


    public void outputTypes(List<DataType> outputTypes) {
        this.outputTypes = outputTypes;
    }

    public List<DataType> outputTypes() {
        return outputTypes;
    }

    @Override
    public void inputTypes(List<DataType> dataTypes) {
        this.inputTypes = dataTypes;
    }

    @Override
    public List<DataType> inputTypes() {
        return inputTypes;
    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        name = in.readString();
        jobId = new UUID(in.readLong(), in.readLong());
        executionNodeId = in.readVInt();

        int numCols = in.readVInt();
        if (numCols > 0) {
            outputTypes = new ArrayList<>(numCols);
            for (int i = 0; i < numCols; i++) {
                outputTypes.add(DataTypes.fromStream(in));
            }
        }

        int numProjections = in.readVInt();
        if (numProjections > 0) {
            projections = new ArrayList<>(numProjections);
            for (int i = 0; i < numProjections; i++) {
                projections.add(Projection.fromStream(in));
            }
        }

    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        out.writeString(name);
        assert jobId != null : "jobId must not be null";
        out.writeLong(jobId.getMostSignificantBits());
        out.writeLong(jobId.getLeastSignificantBits());
        out.writeVInt(executionNodeId);

        int numCols = outputTypes.size();
        out.writeVInt(numCols);
        for (int i = 0; i < numCols; i++) {
            DataTypes.toStream(outputTypes.get(i), out);
        }

        if (hasProjections()) {
            out.writeVInt(projections.size());
            for (Projection p : projections) {
                Projection.toStream(p, out);
            }
        } else {
            out.writeVInt(0);
        }
    }

    @Override
    public boolean equals(Object o) {
        if (this == o) return true;
        if (o == null || getClass() != o.getClass()) return false;

        AbstractDQLPlanNode node = (AbstractDQLPlanNode) o;

        return !(name != null ? !name.equals(node.name) : node.name != null);

    }

    @Override
    public int hashCode() {
        return name != null ? name.hashCode() : 0;
    }

    @Override
    public String toString() {
        return MoreObjects.toStringHelper(this)
                .add("name", name)
                .add("projections", projections)
                .add("outputTypes", outputTypes)
                .toString();
    }
}

<code block>


package io.crate.planner.node.dql;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableSet;
import com.google.common.collect.Sets;
import io.crate.analyze.EvaluatingNormalizer;
import io.crate.analyze.OrderBy;
import io.crate.analyze.WhereClause;
import io.crate.metadata.Routing;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.node.ExecutionNodeVisitor;
import io.crate.planner.node.PlanNodeVisitor;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.Symbol;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;

import javax.annotation.Nullable;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import java.util.Set;


public class CollectNode extends AbstractDQLPlanNode {

    public static final ExecutionNodeFactory<CollectNode> FACTORY = new ExecutionNodeFactory<CollectNode>() {
        @Override
        public CollectNode create() {
            return new CollectNode();
        }
    };
    private Routing routing;
    private List<Symbol> toCollect;
    private WhereClause whereClause = WhereClause.MATCH_ALL;
    private RowGranularity maxRowGranularity = RowGranularity.CLUSTER;

    @Nullable
    private List<String> downstreamNodes;

    private int downstreamExecutionNodeId = ExecutionNode.NO_EXECUTION_NODE;

    private boolean isPartitioned = false;
    private boolean keepContextForFetcher = false;
    private @Nullable String handlerSideCollect = null;

    private @Nullable Integer limit = null;
    private @Nullable OrderBy orderBy = null;

    protected CollectNode() {
        super();
    }

    public CollectNode(int executionNodeId, String name) {
        super(executionNodeId, name);
    }

    public CollectNode(int executionNodeId, String name, Routing routing) {
        this(executionNodeId, name, routing, ImmutableList.<Symbol>of(), ImmutableList.<Projection>of());
    }

    public CollectNode(int executionNodeId, String name, Routing routing, List<Symbol> toCollect, List<Projection> projections) {
        super(executionNodeId, name);
        this.routing = routing;
        this.toCollect = toCollect;
        this.projections = projections;
        this.downstreamNodes = ImmutableList.of(ExecutionNode.DIRECT_RETURN_DOWNSTREAM_NODE);
    }

    @Override
    public Type type() {
        return Type.COLLECT;
    }


    @Override
    public Set<String> executionNodes() {
        if (routing != null) {
            if (routing.isNullRouting()) {
                return routing.nodes();
            } else {
                return Sets.filter(routing.nodes(), TableInfo.IS_NOT_NULL_NODE_ID);
            }
        } else {
            return ImmutableSet.of();
        }
    }

    public @Nullable Integer limit() {
        return limit;
    }

    public void limit(Integer limit) {
        this.limit = limit;
    }

    public @Nullable OrderBy orderBy() {
        return orderBy;
    }

    public void orderBy(@Nullable OrderBy orderBy) {
        this.orderBy = orderBy;
    }

    @Nullable
    public List<String> downstreamNodes() {
        return downstreamNodes;
    }


    public boolean hasDistributingDownstreams() {
        if (downstreamNodes != null && downstreamNodes.size() > 0) {
            if (downstreamNodes.size() == 1
                    && downstreamNodes.get(0).equals(ExecutionNode.DIRECT_RETURN_DOWNSTREAM_NODE)) {
                return false;
            }
            return true;
        }
        return false;
    }

    public void downstreamNodes(List<String> downStreamNodes) {
        this.downstreamNodes = downStreamNodes;
    }

    public void downstreamExecutionNodeId(int executionNodeId) {
        this.downstreamExecutionNodeId = executionNodeId;
    }

    public int downstreamExecutionNodeId() {
        return downstreamExecutionNodeId;
    }

    public WhereClause whereClause() {
        return whereClause;
    }

    public void whereClause(WhereClause whereClause) {
        assert whereClause != null;
        this.whereClause = whereClause;
    }

    public Routing routing() {
        return routing;
    }

    public List<Symbol> toCollect() {
        return toCollect;
    }

    public void toCollect(List<Symbol> toCollect) {
        assert toCollect != null;
        this.toCollect = toCollect;
    }

    public boolean isRouted() {
        return routing != null && routing.hasLocations();
    }


    public boolean isPartitioned() {
        return isPartitioned;
    }

    public void isPartitioned(boolean isPartitioned) {
        this.isPartitioned = isPartitioned;
    }

    public RowGranularity maxRowGranularity() {
        return maxRowGranularity;
    }

    public void maxRowGranularity(RowGranularity newRowGranularity) {
        if (maxRowGranularity.compareTo(newRowGranularity) < 0) {
            maxRowGranularity = newRowGranularity;
        }
    }

    @Override
    public <C, R> R accept(PlanNodeVisitor<C, R> visitor, C context) {
        return visitor.visitCollectNode(this, context);
    }

    @Override
    public <C, R> R accept(ExecutionNodeVisitor<C, R> visitor, C context) {
        return visitor.visitCollectNode(this, context);
    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        super.readFrom(in);
        downstreamExecutionNodeId = in.readVInt();

        int numCols = in.readVInt();
        if (numCols > 0) {
            toCollect = new ArrayList<>(numCols);
            for (int i = 0; i < numCols; i++) {
                toCollect.add(Symbol.fromStream(in));
            }
        } else {
            toCollect = ImmutableList.of();
        }

        maxRowGranularity = RowGranularity.fromStream(in);

        if (in.readBoolean()) {
            routing = new Routing();
            routing.readFrom(in);
        }

        whereClause = new WhereClause(in);

        int numDownStreams = in.readVInt();
        downstreamNodes = new ArrayList<>(numDownStreams);
        for (int i = 0; i < numDownStreams; i++) {
            downstreamNodes.add(in.readString());
        }
        keepContextForFetcher = in.readBoolean();

        if( in.readBoolean()) {
            limit = in.readVInt();
        }

        if (in.readBoolean()) {
            orderBy = OrderBy.fromStream(in);
        }
        isPartitioned = in.readBoolean();
        handlerSideCollect = in.readOptionalString();
    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        super.writeTo(out);
        out.writeVInt(downstreamExecutionNodeId);

        int numCols = toCollect.size();
        out.writeVInt(numCols);
        for (int i = 0; i < numCols; i++) {
            Symbol.toStream(toCollect.get(i), out);
        }

        RowGranularity.toStream(maxRowGranularity, out);

        if (routing != null) {
            out.writeBoolean(true);
            routing.writeTo(out);
        } else {
            out.writeBoolean(false);
        }
        whereClause.writeTo(out);

        if (downstreamNodes != null) {
            out.writeVInt(downstreamNodes.size());
            for (String downstreamNode : downstreamNodes) {
                out.writeString(downstreamNode);
            }
        } else {
            out.writeVInt(0);
        }
        out.writeBoolean(keepContextForFetcher);
        if (limit != null ) {
            out.writeBoolean(true);
            out.writeVInt(limit);
        } else {
            out.writeBoolean(false);
        }
        if (orderBy != null) {
            out.writeBoolean(true);
            OrderBy.toStream(orderBy, out);
        } else {
            out.writeBoolean(false);
        }
        out.writeBoolean(isPartitioned);
        out.writeOptionalString(handlerSideCollect);
    }


    public CollectNode normalize(EvaluatingNormalizer normalizer) {
        assert whereClause() != null;
        CollectNode result = this;
        List<Symbol> newToCollect = normalizer.normalize(toCollect());
        boolean changed = newToCollect != toCollect();
        WhereClause newWhereClause = whereClause().normalize(normalizer);
        if (newWhereClause != whereClause()) {
            changed = changed || newWhereClause != whereClause();
        }
        if (changed) {
            result = new CollectNode(executionNodeId(), name(), routing, newToCollect, projections);
            result.downstreamNodes = downstreamNodes;
            result.maxRowGranularity = maxRowGranularity;
            result.jobId(jobId());
            result.keepContextForFetcher = keepContextForFetcher;
            result.handlerSideCollect = handlerSideCollect;
            result.isPartitioned(isPartitioned);
            result.whereClause(newWhereClause);
        }
        return result;
    }

    public void keepContextForFetcher(boolean keepContextForFetcher) {
        this.keepContextForFetcher = keepContextForFetcher;
    }

    public boolean keepContextForFetcher() {
        return keepContextForFetcher;
    }

    public void handlerSideCollect(String handlerSideCollect) {
        this.handlerSideCollect = handlerSideCollect;
    }

    @Nullable
    public String handlerSideCollect() {
        return handlerSideCollect;
    }
}
<code block>


package io.crate.planner.node.dql;

import com.google.common.base.MoreObjects;
import com.google.common.base.Preconditions;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableSet;
import io.crate.planner.node.ExecutionNodeVisitor;
import io.crate.planner.node.PlanNodeVisitor;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;

import javax.annotation.Nullable;
import java.io.IOException;
import java.util.*;


public class MergeNode extends AbstractDQLPlanNode {

    public static final ExecutionNodeFactory<MergeNode> FACTORY = new ExecutionNodeFactory<MergeNode>() {
        @Override
        public MergeNode create() {
            return new MergeNode();
        }
    };

    private List<DataType> inputTypes;
    private int numUpstreams;
    private Set<String> executionNodes;


    private boolean sortedInputOutput = false;
    private int[] orderByIndices;
    private boolean[] reverseFlags;
    private Boolean[] nullsFirst;
    private int downstreamExecutionNodeId = NO_EXECUTION_NODE;
    private List<String> downstreamNodes = ImmutableList.of();

    public MergeNode() {
        numUpstreams = 0;
    }

    public MergeNode(int executionNodeId, String name, int numUpstreams) {
        super(executionNodeId, name);
        this.numUpstreams = numUpstreams;
    }

    public static MergeNode sortedMergeNode(int executionNodeId,
                                            String name,
                                            int numUpstreams,
                                            int[] orderByIndices,
                                            boolean[] reverseFlags,
                                            Boolean[] nullsFirst) {
        Preconditions.checkArgument(
                orderByIndices.length == reverseFlags.length && reverseFlags.length == nullsFirst.length,
                "ordering parameters must be of the same length");
        MergeNode mergeNode = new MergeNode(executionNodeId, name, numUpstreams);
        mergeNode.sortedInputOutput = true;
        mergeNode.orderByIndices = orderByIndices;
        mergeNode.reverseFlags = reverseFlags;
        mergeNode.nullsFirst = nullsFirst;
        return mergeNode;
    }

    @Override
    public Type type() {
        return Type.MERGE;
    }

    @Override
    public Set<String> executionNodes() {
        if (executionNodes == null) {
            return ImmutableSet.of();
        } else {
            return executionNodes;
        }
    }

    @Override
    public List<String> downstreamNodes() {
        return downstreamNodes;
    }

    public void downstreamNodes(Set<String> nodes) {
        downstreamNodes = ImmutableList.copyOf(nodes);
    }

    @Override
    public int downstreamExecutionNodeId() {
        return downstreamExecutionNodeId;
    }

    public void executionNodes(Set<String> executionNodes) {
        this.executionNodes = executionNodes;
    }

    public int numUpstreams() {
        return numUpstreams;
    }

    @Override
    public List<DataType> inputTypes() {
        return inputTypes;
    }

    @Override
    public void inputTypes(List<DataType> inputTypes) {
        this.inputTypes = inputTypes;
    }

    public boolean sortedInputOutput() {
        return sortedInputOutput;
    }

    @Nullable
    public int[] orderByIndices() {
        return orderByIndices;
    }

    @Nullable
    public boolean[] reverseFlags() {
        return reverseFlags;
    }

    @Nullable
    public Boolean[] nullsFirst() {
        return nullsFirst;
    }

    @Override
    public <C, R> R accept(PlanNodeVisitor<C, R> visitor, C context) {
        return visitor.visitMergeNode(this, context);
    }

    @Override
    public <C, R> R accept(ExecutionNodeVisitor<C, R> visitor, C context) {
        return visitor.visitMergeNode(this, context);
    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        super.readFrom(in);
        downstreamExecutionNodeId = in.readVInt();

        int numDownstreamNodes = in.readVInt();
        downstreamNodes = new ArrayList<>(numDownstreamNodes);
        for (int i = 0; i < numDownstreamNodes; i++) {
            downstreamNodes.add(in.readString());
        }

        numUpstreams = in.readVInt();

        int numCols = in.readVInt();
        if (numCols > 0) {
            inputTypes = new ArrayList<>(numCols);
            for (int i = 0; i < numCols; i++) {
                inputTypes.add(DataTypes.fromStream(in));
            }
        }
        int numExecutionNodes = in.readVInt();

        if (numExecutionNodes > 0) {
            executionNodes = new HashSet<>(numExecutionNodes);
            for (int i = 0; i < numExecutionNodes; i++) {
                executionNodes.add(in.readString());
            }
        }

        sortedInputOutput = in.readBoolean();
        if (sortedInputOutput) {
            int orderByIndicesLength = in.readVInt();
            orderByIndices = new int[orderByIndicesLength];
            reverseFlags = new boolean[orderByIndicesLength];
            nullsFirst = new Boolean[orderByIndicesLength];
            for (int i = 0; i < orderByIndicesLength; i++) {
                orderByIndices[i] = in.readVInt();
                reverseFlags[i] = in.readBoolean();
                nullsFirst[i] = in.readOptionalBoolean();
            }
        }
    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        super.writeTo(out);
        out.writeVInt(downstreamExecutionNodeId);

        out.writeVInt(downstreamNodes.size());
        for (String downstreamNode : downstreamNodes) {
            out.writeString(downstreamNode);
        }

        out.writeVInt(numUpstreams);

        int numCols = inputTypes.size();
        out.writeVInt(numCols);
        for (DataType inputType : inputTypes) {
            DataTypes.toStream(inputType, out);
        }

        if (executionNodes == null) {
            out.writeVInt(0);
        } else {
            out.writeVInt(executionNodes.size());
            for (String node : executionNodes) {
                out.writeString(node);
            }
        }

        out.writeBoolean(sortedInputOutput);
        if (sortedInputOutput) {
            out.writeVInt(orderByIndices.length);
            for (int i = 0; i < orderByIndices.length; i++) {
                out.writeVInt(orderByIndices[i]);
                out.writeBoolean(reverseFlags[i]);
                out.writeOptionalBoolean(nullsFirst[i]);
            }
        }
    }

    @Override
    public String toString() {
        MoreObjects.ToStringHelper helper = MoreObjects.toStringHelper(this)
                .add("executionNodeId", executionNodeId())
                .add("name", name())
                .add("projections", projections)
                .add("outputTypes", outputTypes)
                .add("jobId", jobId())
                .add("numUpstreams", numUpstreams)
                .add("executionNodes", executionNodes)
                .add("inputTypes", inputTypes)
                .add("sortedInputOutput", sortedInputOutput);
        if (sortedInputOutput) {
            helper.add("orderByIndices", Arrays.toString(orderByIndices))
                  .add("reverseFlags", Arrays.toString(reverseFlags))
                  .add("nullsFirst", Arrays.toString(nullsFirst));
        }
        return helper.toString();
    }

    public void downstreamExecutionNodeId(int downstreamExecutionNodeId) {
        this.downstreamExecutionNodeId = downstreamExecutionNodeId;
    }
}

<code block>
package io.crate.planner.node.dql;


import io.crate.planner.PlanAndPlannedAnalyzedRelation;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.PlanVisitor;
import io.crate.planner.projection.Projection;

import javax.annotation.Nullable;

public class QueryAndFetch extends PlanAndPlannedAnalyzedRelation {

    private final CollectNode collectNode;
    private MergeNode localMergeNode;

    public QueryAndFetch(CollectNode collectNode, @Nullable MergeNode localMergeNode){
        this.collectNode = collectNode;
        this.localMergeNode = localMergeNode;
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitQueryAndFetch(this, context);
    }

    public CollectNode collectNode() {
        return collectNode;
    }

    public MergeNode localMergeNode(){
        return localMergeNode;
    }

    @Override
    public void addProjection(Projection projection) {
        DQLPlanNode node = resultNode();
        node.addProjection(projection);
        if (node instanceof CollectNode) {
            PlanNodeBuilder.setOutputTypes((CollectNode)node);
        } else if (node instanceof MergeNode) {
            PlanNodeBuilder.connectTypes(collectNode, node);
        }
    }

    @Override
    public boolean resultIsDistributed() {
        return localMergeNode == null;
    }

    @Override
    public DQLPlanNode resultNode() {
        return localMergeNode == null ? collectNode : localMergeNode;
    }
}

<code block>


package io.crate.planner.node.dql.join;

import com.google.common.base.MoreObjects;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableSet;
import io.crate.planner.node.ExecutionNodeVisitor;
import io.crate.planner.node.PlanNodeVisitor;
import io.crate.planner.node.dql.AbstractDQLPlanNode;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;

import java.io.IOException;
import java.util.*;

public class NestedLoopNode extends AbstractDQLPlanNode {

    public static final ExecutionNodeFactory<NestedLoopNode> FACTORY = new ExecutionNodeFactory<NestedLoopNode>() {
        @Override
        public NestedLoopNode create() {
            return new NestedLoopNode();
        }
    };

    private int downstreamExecutionNodeId = NO_EXECUTION_NODE;
    private Set<String> executionNodes;
    private List<String> downstreamNodes = ImmutableList.of();

    private int leftExecutionNodeId;
    private int rightExecutionNodeId;

    private List<DataType> leftInputTypes;
    private List<DataType> rightInputTypes;

    public NestedLoopNode() {}

    public NestedLoopNode(int leftExecutionNodeId, int rightExecutionNodeId, String name) {
        super(0, name); 
        this.leftExecutionNodeId = leftExecutionNodeId;
        this.rightExecutionNodeId = rightExecutionNodeId;
    }

    @Override
    public Type type() {
        return Type.NESTED_LOOP;
    }

    @Override
    public Set<String> executionNodes() {
        if (executionNodes == null) {
            return ImmutableSet.of();
        } else {
            return executionNodes;
        }
    }

    public void executionNodes(Set<String> executionNodes) {
        this.executionNodes = executionNodes;
    }

    @Override
    public List<String> downstreamNodes() {
        return downstreamNodes;
    }

    public void downstreamNodes(Set<String> nodes) {
        downstreamNodes = ImmutableList.copyOf(nodes);
    }

    @Override
    public int downstreamExecutionNodeId() {
        return downstreamExecutionNodeId;
    }

    public void downstreamExecutionNodeId(int downstreamExecutionNodeId) {
        this.downstreamExecutionNodeId = downstreamExecutionNodeId;
    }

    public List<DataType> leftInputTypes() {
        return leftInputTypes;
    }

    public void leftInputTypes(List<DataType> leftInputTypes) {
        this.leftInputTypes = leftInputTypes;
    }

    public List<DataType> rightInputTypes() {
        return rightInputTypes;
    }

    public void rightInputTypes(List<DataType> rightInputTypes) {
        this.rightInputTypes = rightInputTypes;
    }

    public int leftExecutionNodeId() {
        return leftExecutionNodeId;
    }

    public int rightExecutionNodeId() {
        return rightExecutionNodeId;
    }

    @Override
    public List<DataType> inputTypes() {
        throw new UnsupportedOperationException("inputsTypes not supported. " +
                "Use leftInputTypes() or rightInputTypes()");
    }

    @Override
    public void inputTypes(List<DataType> inputTypes) {
        throw new UnsupportedOperationException("inputsTypes not supported. " +
                "Use leftInputTypes() or rightInputTypes()");
    }

    @Override
    public int executionNodeId() {
        throw  new UnsupportedOperationException("executionNodeId() not supported. " +
                "Use leftExecutionNOdeId() or rightExecutionNodeId()");
    }

    @Override
    public <C, R> R accept(ExecutionNodeVisitor<C, R> visitor, C context) {
        return visitor.visitNestedLoopNode(this, context);
    }


    @Override
    public <C, R> R accept(PlanNodeVisitor<C, R> visitor, C context) {
        return visitor.visitNestedLoopNode(this, context);
    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        super.readFrom(in);
        leftExecutionNodeId = in.readVInt();
        rightExecutionNodeId = in.readVInt();
        downstreamExecutionNodeId = in.readVInt();

        int numDownstreamNodes = in.readVInt();
        downstreamNodes = new ArrayList<>(numDownstreamNodes);
        for (int i = 0; i < numDownstreamNodes; i++) {
            downstreamNodes.add(in.readString());
        }

        int leftNumCols = in.readVInt();
        if (leftNumCols > 0) {
            leftInputTypes = new ArrayList<>(leftNumCols);
            for (int i = 0; i < leftNumCols; i++) {
                leftInputTypes.add(DataTypes.fromStream(in));
            }
        }

        int rightNumCols = in.readVInt();
        if (rightNumCols > 0) {
            rightInputTypes = new ArrayList<>(rightNumCols);
            for (int i = 0; i < rightNumCols; i++) {
                rightInputTypes.add(DataTypes.fromStream(in));
            }
        }
        int numExecutionNodes = in.readVInt();
        if (numExecutionNodes > 0) {
            executionNodes = new HashSet<>(numExecutionNodes);
            for (int i = 0; i < numExecutionNodes; i++) {
                executionNodes.add(in.readString());
            }
        }
    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        super.writeTo(out);
        out.writeVInt(leftExecutionNodeId);
        out.writeVInt(rightExecutionNodeId);
        out.writeVInt(downstreamExecutionNodeId);
        out.writeVInt(downstreamNodes.size());
        for (String downstreamNode : downstreamNodes) {
            out.writeString(downstreamNode);
        }

        int leftNumCols = leftInputTypes().size();
        out.writeVInt(leftNumCols);
        for (DataType inputType : leftInputTypes) {
            DataTypes.toStream(inputType, out);
        }

        int rightNumCols = rightInputTypes().size();
        out.writeVInt(rightNumCols);
        for (DataType inputType : rightInputTypes) {
            DataTypes.toStream(inputType, out);
        }

        if (executionNodes == null) {
            out.writeVInt(0);
        } else {
            out.writeVInt(executionNodes.size());
            for (String node : executionNodes) {
                out.writeString(node);
            }
        }
    }

    @Override
    public String toString() {
        MoreObjects.ToStringHelper helper = MoreObjects.toStringHelper(this)
                .add("executionNodeId", executionNodeId())
                .add("name", name())
                .add("outputTypes", outputTypes)
                .add("jobId", jobId())
                .add("executionNodes", executionNodes)
                .add("leftInputTypes", leftInputTypes)
                .add("rightInputTypes", rightInputTypes);
        return helper.toString();
    }
}

<code block>

package io.crate.planner.node.dql.join;

import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.planner.PlanAndPlannedAnalyzedRelation;
import io.crate.planner.PlanVisitor;
import io.crate.planner.node.dql.DQLPlanNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.Projection;
import org.elasticsearch.common.Nullable;


public class NestedLoop extends PlanAndPlannedAnalyzedRelation {


    private final PlannedAnalyzedRelation left;
    private final PlannedAnalyzedRelation right;
    private final NestedLoopNode nestedLoopNode;
    @Nullable
    private MergeNode leftMergeNode;
    @Nullable
    private MergeNode rightMergeNode;
    @Nullable
    private MergeNode localMergeNode;

    private boolean leftOuterLoop = true;


    public NestedLoop(PlannedAnalyzedRelation left,
                      PlannedAnalyzedRelation right,
                      NestedLoopNode nestedLoopNode,
                      boolean leftOuterLoop) {
        this.leftOuterLoop = leftOuterLoop;
        this.left = left;
        this.right = right;
        this.nestedLoopNode = nestedLoopNode;
    }

    public PlannedAnalyzedRelation left() {
        return left;
    }

    public PlannedAnalyzedRelation right() {
        return right;
    }

    public PlannedAnalyzedRelation inner() {
        return leftOuterLoop() ? right : left;
    }

    public PlannedAnalyzedRelation outer() {
        return leftOuterLoop() ? left : right;
    }

    public boolean leftOuterLoop() {
        return leftOuterLoop;
    }

    public void leftMergeNode(MergeNode leftMergeNode) {
        this.leftMergeNode = leftMergeNode;
    }

    @Nullable
    public MergeNode leftMergeNode() {
        return leftMergeNode;
    }

    public void rightMergeNode(MergeNode rightMergeNode) {
        this.rightMergeNode = rightMergeNode;
    }

    @Nullable
    public MergeNode rightMergeNode() {
        return rightMergeNode;
    }

    public void localMergeNode(MergeNode localMergeNode) {
        this.localMergeNode = localMergeNode;
    }

    public NestedLoopNode nestedLoopNode() {
        return nestedLoopNode;
    }

    @Nullable
    public MergeNode localMergeNode() {
        return localMergeNode;
    }

    @Override
    public void addProjection(Projection projection) {
    }

    @Override
    public boolean resultIsDistributed() {
        return false;
    }

    @Override
    public DQLPlanNode resultNode() {
        return localMergeNode == null ? outer().resultNode() : localMergeNode;
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitNestedLoop(this, context);
    }
}

<code block>


package io.crate.integrationtests;

import io.crate.Constants;
import io.crate.action.sql.SQLResponse;
import io.crate.testing.SQLTransportExecutor;
import org.elasticsearch.common.settings.ImmutableSettings;

import java.util.Arrays;
import java.util.HashMap;
import java.util.Map;

import static com.google.common.collect.Maps.newHashMap;
import static org.hamcrest.MatcherAssert.assertThat;
import static org.hamcrest.Matchers.is;

public class Setup {

    private final SQLTransportExecutor transportExecutor;

    public Setup(SQLTransportExecutor transportExecutor) {
        this.transportExecutor = transportExecutor;
    }

    public void setUpLocations() throws Exception {
        transportExecutor.exec("create table locations (" +
            " id string primary key," +
            " name string," +
            " date timestamp," +
            " kind string," +
            " position integer," +
            " description string," +
            " race object," +
            " index name_description_ft using fulltext(name, description) with (analyzer='english')" +
            ") clustered by(id) into 2 shards with(number_of_replicas=0)");

        String insertStmt = "insert into locations " +
                "(id, name, date, kind, position, description, race) " +
                "values (?, ?, ?, ?, ?, ?, ?)";
        Object[][] rows = new Object[][]{
                new Object[]{"1", "North West Ripple", "1979-10-12",
                        "Galaxy", 1, "Relative to life on NowWhat, living on an affluent " +
                        "world in the North West ripple of the Galaxy is said to be easier " +
                        "by a factor of about seventeen million.", null
                },
                new Object[]{
                        "2", "Outer Eastern Rim", "1979-10-12", "Galaxy", 2, "The Outer Eastern Rim " +
                        "of the Galaxy where the Guide has supplanted the Encyclopedia Galactica " +
                        "among its more relaxed civilisations.", null
                },
                new Object[]{
                        "3","Galactic Sector QQ7 Active J Gamma", "2013-05-01",  "Galaxy",  4,
                        "Galactic Sector QQ7 Active J Gamma contains the Sun Zarss, " +
                        "the planet Preliumtarn of the famed Sevorbeupstry and " +
                        "Quentulus Quazgar Mountains.", null
                },
                new Object[]{
                        "4", "Aldebaran", "2013-07-16",  "Star System",  1,
                        "Max Quordlepleen claims that the only thing left after the end " +
                        "of the Universe will be the sweets trolley and a fine selection " +
                        "of Aldebaran liqueurs.", null
                },
                new Object[]{
                        "5",  "Algol", "2013-07-16",  "Star System",  2,
                        "Algol is the home of the Algolian Suntiger, " +
                        "the tooth of which is one of the ingredients of the " +
                        "Pan Galactic Gargle Blaster.", null
                },
                new Object[]{
                        "6",  "Alpha Centauri", "1979-10-12",  "Star System",  3,
                        "4.1 light-years northwest of earth", null
                },
                new Object[]{
                        "7",  "Altair", "2013-07-16",  "Star System",  4,
                        "The Altairian dollar is one of three freely convertible currencies in the galaxy, " +
                        "though by the time of the novels it had apparently recently collapsed.",
                        null
                },
                new Object[]{
                        "8",  "Allosimanius Syneca", "2013-07-16",  "Planet",  1,
                        "Allosimanius Syneca is a planet noted for ice, snow, " +
                        "mind-hurtling beauty and stunning cold.", null
                },
                new Object[]{
                        "9",  "Argabuthon", "2013-07-16",  "Planet",  2,
                        "It is also the home of Prak, a man placed into solitary confinement " +
                        "after an overdose of truth drug caused him to tell the Truth in its absolute " +
                        "and final form, causing anyone to hear it to go insane.", null,
                },
                new Object[]{
                        "10",  "Arkintoofle Minor", "1979-10-12",  "Planet",  3,
                        "Motivated by the fact that the only thing in the Universe that " +
                        "travels faster than light is bad news, the Hingefreel people native " +
                        "to Arkintoofle Minor constructed a starship driven by bad news.", null
                },
                new Object[]{
                        "11",  "Bartledan", "2013-07-16",  "Planet",  4,
                        "An Earthlike planet on which Arthur Dent lived for a short time, " +
                                "Bartledan is inhabited by Bartledanians, a race that appears human but only physically.",
                        new HashMap<String, Object>(){{
                            put("name", "Bartledannians");
                            put("description", "Similar to humans, but do not breathe");
                            put("interests", "netball");
                        }}
                },
                new Object[]{
                        "12",  "", "2013-07-16",  "Planet",  5,  "This Planet doesn't really exist", null
                },
                new Object[]{
                        "13",  "End of the Galaxy", "2013-07-16",  "Galaxy",  6,  "The end of the Galaxy.%", null
                }
        };
        transportExecutor.exec(insertStmt, rows);
    }

    public void groupBySetup() throws Exception {
        groupBySetup("integer");
    }

    public void groupBySetup(String numericType) throws Exception {
        transportExecutor.exec(String.format("create table characters (" +
            " race string," +
            " gender string," +
            " age %s," +
            " birthdate timestamp," +
            " name string," +
            " details object as (job string)," +
            " details_ignored object(ignored)" +
            ")", numericType));
        transportExecutor.ensureYellowOrGreen();

        Map<String, String> details = newHashMap();
        details.put("job", "Sandwitch Maker");
        transportExecutor.exec("insert into characters (race, gender, age, birthdate, name, details) values (?, ?, ?, ?, ?, ?)",
            new Object[]{"Human", "male", 34, "1975-10-01", "Arthur Dent", details});

        details = newHashMap();
        details.put("job", "Mathematician");
        transportExecutor.exec("insert into characters (race, gender, age, birthdate, name, details) values (?, ?, ?, ?, ?, ?)",
            new Object[]{"Human", "female", 32, "1978-10-11", "Trillian", details});
        transportExecutor.exec("insert into characters (race, gender, age, birthdate, name, details) values (?, ?, ?, ?, ?, ?)",
            new Object[]{"Human", "female", 43, "1970-01-01", "Anjie", null});
        transportExecutor.exec("insert into characters (race, gender, age, name) values (?, ?, ?, ?)",
            new Object[]{"Human", "male", 112, "Ford Perfect"});

        transportExecutor.exec("insert into characters (race, gender, name) values ('Android', 'male', 'Marving')");
        transportExecutor.exec("insert into characters (race, gender, name) values ('Vogon', 'male', 'Jeltz')");
        transportExecutor.exec("insert into characters (race, gender, name) values ('Vogon', 'male', 'Kwaltz')");
        transportExecutor.refresh("characters");
    }

    public void setUpEmployees() {
        transportExecutor.exec("create table employees (" +
            " name string, " +
            " department string," +
            " hired timestamp, " +
            " age short," +
            " income double, " +
            " good boolean" +
            ") with (number_of_replicas=0)");
        transportExecutor.ensureGreen();
        transportExecutor.exec("insert into employees (name, department, hired, age, income, good) values (?, ?, ?, ?, ?, ?)",
            new Object[]{"dilbert", "engineering", "1985-01-01", 47, 4000.0, true});
        transportExecutor.exec("insert into employees (name, department, hired, age, income, good) values (?, ?, ?, ?, ?, ?)",
            new Object[]{"wally", "engineering", "2000-01-01", 54, 6000.0, true});
        transportExecutor.exec("insert into employees (name, department, hired, age, income, good) values (?, ?, ?, ?, ?, ?)",
            new Object[]{"pointy haired boss", "management", "2010-10-10", 45, Double.MAX_VALUE, false});

        transportExecutor.exec("insert into employees (name, department, hired, age, income, good) values (?, ?, ?, ?, ?, ?)",
            new Object[]{"catbert", "HR", "1990-01-01", 12, 999999999.99, false});
        transportExecutor.exec("insert into employees (name, department, income) values (?, ?, ?)",
            new Object[]{"ratbert", "HR", 0.50});
        transportExecutor.exec("insert into employees (name, department, age) values (?, ?, ?)",
            new Object[]{"asok", "internship", 28});
        transportExecutor.refresh("employees");
    }

    public void setUpObjectTable() {
        transportExecutor.exec("create table ot (" +
                "  title string," +
                "  author object(dynamic) as (" +
                "    name object(strict) as (" +
                "      first_name string," +
                "      last_name string" +
                "    )," +
                "    age integer" +
                "  )," +
                "  details object(ignored) as (" +
                "    num_pages integer" +
                "  )" +
                ") with (number_of_replicas = 0)");
        transportExecutor.exec("insert into ot (title, author, details) values (?, ?, ?)",
                new Object[]{
                        "The Hitchhiker's Guide to the Galaxy",
                        new HashMap<String, Object>() {{
                            put("name", new HashMap<String, Object>() {{
                                put("first_name", "Douglas");
                                put("last_name", "Adams");
                            }});
                            put("age", 49);
                        }},
                        new HashMap<String, Object>() {{
                            put("num_pages", 224);
                        }}
                }
        );
        transportExecutor.refresh("ot");
    }

    public void setUpObjectMappingWithUnknownTypes() throws Exception {
        transportExecutor.prepareCreate("ut")
                .setSettings(ImmutableSettings.builder().put("number_of_replicas", 0).put("number_of_shards", 2).build())
                .addMapping(Constants.DEFAULT_MAPPING_TYPE, new HashMap<String, Object>(){{
                    put("properties", new HashMap<String, Object>(){{
                        put("name", new HashMap<String, Object>(){{
                            put("type", "string");
                            put("store", "false");
                            put("index", "not_analyzed");
                        }});
                        put("location", new HashMap<String, Object>(){{
                            put("type", "geo_shape");
                        }});
                        put("o", new HashMap<String, Object>(){{
                            put("type", "object");
                        }});
                        put("population", new HashMap<String, Object>(){{
                            put("type", "long");
                            put("store", "false");
                            put("index", "not_analyzed");
                        }});
                    }});
                }}).execute().actionGet();
        transportExecutor.client().prepareIndex("ut", Constants.DEFAULT_MAPPING_TYPE, "id1")
                .setSource("{\"name\":\"Berlin\",\"location\":{\"type\": \"point\", \"coordinates\": [52.5081, 13.4416]}, \"population\":3500000}")
                .execute().actionGet();
        transportExecutor.client().prepareIndex("ut", Constants.DEFAULT_MAPPING_TYPE, "id2")
                .setSource("{\"name\":\"Dornbirn\",\"location\":{\"type\": \"point\", \"coordinates\": [47.3904,9.7562]}, \"population\":46080}")
                .execute().actionGet();
        transportExecutor.refresh("ut");
    }

    public void setUpArrayTables() {
        transportExecutor.exec("create table any_table (" +
                "  id int primary key," +
                "  temps array(double)," +
                "  names array(string)," +
                "  tags array(string)" +
                ") with (number_of_replicas=0)");
        transportExecutor.ensureGreen();
        SQLResponse response = transportExecutor.exec("insert into any_table (id, temps, names, tags) values (?,?,?,?), (?,?,?,?), (?,?,?,?), (?,?,?,?)",
                        1, Arrays.asList(0L, 0L, 0L), Arrays.asList("Dornbirn", "Berlin", "St. Margrethen"), Arrays.asList("cool"),
                        2, Arrays.asList(0, 1, -1), Arrays.asList("Dornbirn", "Dornbirn", "Dornbirn"), Arrays.asList("cool", null),
                        3, Arrays.asList(42, -42), Arrays.asList("Hangelsberg", "Berlin"), Arrays.asList("kuhl", "cool"),
                        4, null, null, Arrays.asList("kuhl", null)
                );
        assertThat(response.rowCount(), is(4L));
        transportExecutor.refresh("any_table");
    }

    public void partitionTableSetup() {
        transportExecutor.exec("create table parted (" +
                "id int primary key," +
                "date timestamp primary key," +
                "o object(ignored)" +
                ") partitioned by (date) with (number_of_replicas=0)");
        transportExecutor.ensureGreen();
        transportExecutor.exec("insert into parted (id, date) values (1, '2014-01-01')");
        transportExecutor.exec("insert into parted (id, date) values (2, '2014-01-01')");
        transportExecutor.exec("insert into parted (id, date) values (3, '2014-02-01')");
        transportExecutor.exec("insert into parted (id, date) values (4, '2014-02-01')");
        transportExecutor.exec("insert into parted (id, date) values (5, '2014-02-01')");
        transportExecutor.refresh("parted");
    }

    public void createTestTableWithPrimaryKey() {
        transportExecutor.exec("create table test (" +
                "  pk_col string primary key, " +
                "  message string" +
                ") with (number_of_replicas=0)");
        transportExecutor.ensureGreen();
    }

    public void setUpCharacters() {
        transportExecutor.exec("create table characters (id int primary key, name string, female boolean, details object)");
        transportExecutor.ensureGreen();
        transportExecutor.exec("insert into characters (id, name, female) values (?, ?, ?)",
                new Object[][]{
                        new Object[] { 1, "Arthur", false},
                        new Object[] { 2, "Ford", false},
                        new Object[] { 3, "Trillian", true},
                        new Object[] { 4, "Arthur", true}
                }
        );
        transportExecutor.refresh("characters");
    }

    public void setUpPartitionedTableWithName() {
        transportExecutor.exec("create table parted (id int, name string, date timestamp) partitioned by (date)");
        transportExecutor.ensureGreen();
        transportExecutor.exec("insert into parted (id, name, date) values (?, ?, ?), (?, ?, ?), (?, ?, ?)",
                new Object[]{
                        1, "Trillian", null,
                        2, null, 0L,
                        3, "Ford", 1396388720242L
                });
        transportExecutor.ensureGreen();
        transportExecutor.refresh("parted");
    }
}

<code block>


package io.crate.executor.transport;

import com.google.common.collect.ImmutableList;
import io.crate.analyze.QuerySpec;
import io.crate.analyze.WhereClause;
import io.crate.analyze.where.DocKeys;
import io.crate.integrationtests.SQLTransportIntegrationTest;
import io.crate.integrationtests.Setup;
import io.crate.metadata.ColumnIdent;
import io.crate.metadata.ReferenceIdent;
import io.crate.metadata.ReferenceInfo;
import io.crate.metadata.TableIdent;
import io.crate.metadata.doc.DocSchemaInfo;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.dql.ESGetNode;
import io.crate.planner.symbol.Literal;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.testing.TestingHelpers;
import io.crate.types.DataTypes;
import org.junit.After;
import org.junit.Before;

import java.util.ArrayList;
import java.util.List;

public class BaseTransportExecutorTest extends SQLTransportIntegrationTest {

    Setup setup = new Setup(sqlExecutor);

    static {
        ClassLoader.getSystemClassLoader().setDefaultAssertionStatus(true);
    }

    TransportExecutor executor;
    DocSchemaInfo docSchemaInfo;

    TableIdent charactersIdent = new TableIdent(null, "characters");
    TableIdent booksIdent = new TableIdent(null, "books");

    Reference idRef = new Reference(new ReferenceInfo(
            new ReferenceIdent(charactersIdent, "id"), RowGranularity.DOC, DataTypes.INTEGER));
    Reference nameRef = new Reference(new ReferenceInfo(
            new ReferenceIdent(charactersIdent, "name"), RowGranularity.DOC, DataTypes.STRING));
    Reference femaleRef = TestingHelpers.createReference(charactersIdent.name(), new ColumnIdent("female"), DataTypes.BOOLEAN);

    TableIdent partedTable = new TableIdent("doc", "parted");
    Reference partedIdRef = new Reference(new ReferenceInfo(
            new ReferenceIdent(partedTable, "id"), RowGranularity.DOC, DataTypes.INTEGER));
    Reference partedNameRef = new Reference(new ReferenceInfo(
            new ReferenceIdent(partedTable, "name"), RowGranularity.DOC, DataTypes.STRING));
    Reference partedDateRef = new Reference(new ReferenceInfo(
            new ReferenceIdent(partedTable, "date"), RowGranularity.PARTITION, DataTypes.TIMESTAMP));

    public static ESGetNode newGetNode(TableInfo tableInfo, List<Symbol> outputs, List<String> singleStringKeys, int executionNodeId) {
        QuerySpec querySpec = new QuerySpec();
        querySpec.outputs(outputs);
        List<List<Symbol>> keys = new ArrayList<>(singleStringKeys.size());
        for (String v : singleStringKeys) {
            keys.add(ImmutableList.<Symbol>of(Literal.newLiteral(v)));
        }
        WhereClause whereClause = new WhereClause(null, new DocKeys(keys, false, -1, null), null);
        querySpec.where(whereClause);
        return new ESGetNode(executionNodeId, tableInfo, querySpec);
    }

    @Before
    public void transportSetUp() {
        executor = internalCluster().getInstance(TransportExecutor.class);
        docSchemaInfo = internalCluster().getInstance(DocSchemaInfo.class);
    }

    @After
    public void transportTearDown() {
        executor = null;
        docSchemaInfo = null;
    }
}

<code block>


package io.crate.executor.transport;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.Lists;
import com.google.common.util.concurrent.ListenableFuture;
import io.crate.Constants;
import io.crate.analyze.OrderBy;
import io.crate.analyze.WhereClause;
import io.crate.core.collections.Bucket;
import io.crate.executor.Job;
import io.crate.executor.RowCountResult;
import io.crate.executor.Task;
import io.crate.executor.TaskResult;
import io.crate.executor.transport.task.KillTask;
import io.crate.executor.transport.task.SymbolBasedUpsertByIdTask;
import io.crate.executor.transport.task.elasticsearch.ESDeleteByQueryTask;
import io.crate.executor.transport.task.elasticsearch.ESGetTask;
import io.crate.metadata.*;
import io.crate.metadata.doc.DocSysColumns;
import io.crate.metadata.doc.DocTableInfo;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.aggregation.impl.CountAggregation;
import io.crate.operation.operator.EqOperator;
import io.crate.operation.projectors.TopN;
import io.crate.operation.scalar.DateTruncFunction;
import io.crate.planner.*;
import io.crate.planner.node.dml.ESDeleteByQueryNode;
import io.crate.planner.node.dml.SymbolBasedUpsertByIdNode;
import io.crate.planner.node.dml.Upsert;
import io.crate.planner.node.dql.*;
import io.crate.planner.node.management.KillPlan;
import io.crate.planner.projection.*;
import io.crate.planner.symbol.*;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.cluster.routing.operation.plain.Preference;
import org.elasticsearch.common.collect.MapBuilder;
import org.elasticsearch.search.SearchHits;
import org.junit.Before;
import org.junit.Rule;
import org.junit.Test;
import org.junit.rules.ExpectedException;

import java.util.*;

import static io.crate.testing.TestingHelpers.isRow;
import static java.util.Arrays.asList;
import static org.hamcrest.Matchers.*;
import static org.hamcrest.core.Is.is;

public class TransportExecutorTest extends BaseTransportExecutorTest {

    @Rule
    public ExpectedException expectedException = ExpectedException.none();

    @Before
    public void setup() {
    }

    protected ESGetNode newGetNode(String tableName, List<Symbol> outputs, String singleStringKey, int executionNodeId) {
        return newGetNode(tableName, outputs, Collections.singletonList(singleStringKey), executionNodeId);
    }

    protected ESGetNode newGetNode(String tableName, List<Symbol> outputs, List<String> singleStringKeys, int executionNodeId) {
        return newGetNode(docSchemaInfo.getTableInfo(tableName), outputs, singleStringKeys, executionNodeId);
    }

    @Test
    public void testESGetTask() throws Exception {
        setup.setUpCharacters();


        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef);
        Planner.Context ctx = new Planner.Context(clusterService());
        ESGetNode node = newGetNode("characters", outputs, "2", ctx.nextExecutionNodeId());
        Plan plan = new IterablePlan(node);
        Job job = executor.newJob(plan);


        assertThat(job.tasks().size(), is(1));
        Task task = job.tasks().get(0);
        assertThat(task, instanceOf(ESGetTask.class));


        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, contains(isRow(2, "Ford")));
    }

    @Test
    public void testESGetTaskWithDynamicReference() throws Exception {
        setup.setUpCharacters();

        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, new DynamicReference(
                new ReferenceIdent(new TableIdent(null, "characters"), "foo"), RowGranularity.DOC));
        Planner.Context ctx = new Planner.Context(clusterService());
        ESGetNode node = newGetNode("characters", outputs, "2", ctx.nextExecutionNodeId());
        Plan plan = new IterablePlan(node);
        Job job = executor.newJob(plan);
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, contains(isRow(2, null)));
    }

    @Test
    public void testESMultiGet() throws Exception {
        setup.setUpCharacters();
        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef);
        Planner.Context ctx = new Planner.Context(clusterService());
        ESGetNode node = newGetNode("characters", outputs, asList("1", "2"), ctx.nextExecutionNodeId());
        Plan plan = new IterablePlan(node);
        Job job = executor.newJob(plan);
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();

        assertThat(objects.size(), is(2));
    }

    @Test
    public void testQTFTask() throws Exception {

        setup.setUpCharacters();
        DocTableInfo characters = docSchemaInfo.getTableInfo("characters");
        ReferenceInfo docIdRefInfo = characters.getReferenceInfo(new ColumnIdent(DocSysColumns.DOCID.name()));
        List<Symbol> collectSymbols = Lists.<Symbol>newArrayList(new Reference(docIdRefInfo));
        List<Symbol> outputSymbols = Lists.<Symbol>newArrayList(idRef, nameRef);

        Planner.Context ctx = new Planner.Context(clusterService());

        CollectNode collectNode = PlanNodeBuilder.collect(
                characters,
                ctx,
                WhereClause.MATCH_ALL,
                collectSymbols,
                ImmutableList.<Projection>of(),
                null,
                Constants.DEFAULT_SELECT_LIMIT
        );
        collectNode.keepContextForFetcher(true);

        FetchProjection fetchProjection = getFetchProjection((DocTableInfo) characters, (List<Symbol>) collectSymbols, (List<Symbol>) outputSymbols, (CollectNode) collectNode, ctx);

        MergeNode localMergeNode = PlanNodeBuilder.localMerge(
                ImmutableList.<Projection>of(fetchProjection),
                collectNode,
                ctx);

        Plan plan = new QueryThenFetch(collectNode, localMergeNode);

        Job job = executor.newJob(plan);
        assertThat(job.tasks().size(), is(1));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, containsInAnyOrder(
                isRow(1, "Arthur"),
                isRow(4, "Arthur"),
                isRow(2, "Ford"),
                isRow(3, "Trillian")
        ));
    }

    @Test
    public void testQTFTaskWithFilter() throws Exception {

        setup.setUpCharacters();
        DocTableInfo characters = docSchemaInfo.getTableInfo("characters");
        ReferenceInfo docIdRefInfo = characters.getReferenceInfo(new ColumnIdent(DocSysColumns.DOCID.name()));
        List<Symbol> collectSymbols = Lists.<Symbol>newArrayList(new Reference(docIdRefInfo));
        List<Symbol> outputSymbols = Lists.<Symbol>newArrayList(idRef, nameRef);

        Function whereClause = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.STRING, DataTypes.STRING)),
                DataTypes.BOOLEAN),
                Arrays.<Symbol>asList(nameRef, Literal.newLiteral("Ford")));

        Planner.Context ctx = new Planner.Context(clusterService());

        CollectNode collectNode = PlanNodeBuilder.collect(
                characters,
                ctx,
                new WhereClause(whereClause),
                collectSymbols,
                ImmutableList.<Projection>of(),
                null,
                Constants.DEFAULT_SELECT_LIMIT
        );
        collectNode.keepContextForFetcher(true);

        FetchProjection fetchProjection = getFetchProjection(characters, collectSymbols, outputSymbols, collectNode, ctx);

        MergeNode localMergeNode = PlanNodeBuilder.localMerge(
                ImmutableList.<Projection>of(fetchProjection),
                collectNode,
                ctx);

        Plan plan = new QueryThenFetch(collectNode, localMergeNode);

        Job job = executor.newJob(plan);
        assertThat(job.tasks().size(), is(1));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, contains(isRow(2, "Ford")));
    }

    private FetchProjection getFetchProjection(DocTableInfo characters, List<Symbol> collectSymbols, List<Symbol> outputSymbols, CollectNode collectNode, Planner.Context ctx) {
        Map<Integer, List<String>> executionNodes = new HashMap<>();
        executionNodes.put(collectNode.executionNodeId(), new ArrayList<>(collectNode.executionNodes()));
        return new FetchProjection(
                ctx.jobSearchContextIdToExecutionNodeId(),
                new InputColumn(0, DataTypes.STRING), collectSymbols, outputSymbols,
                characters.partitionedByColumns(),
                executionNodes,
                5,
                false,
                ctx.jobSearchContextIdToNode(),
                ctx.jobSearchContextIdToShard()
        );
    }

    @Test
    public void testQTFTaskOrdered() throws Exception {

        setup.setUpCharacters();
        DocTableInfo characters = docSchemaInfo.getTableInfo("characters");

        OrderBy orderBy = new OrderBy(Arrays.<Symbol>asList(nameRef, femaleRef),
                new boolean[]{false, false},
                new Boolean[]{false, false});

        ReferenceInfo docIdRefInfo = characters.getReferenceInfo(new ColumnIdent(DocSysColumns.DOCID.name()));

        List<Symbol> collectSymbols = Lists.<Symbol>newArrayList(new Reference(docIdRefInfo), nameRef, femaleRef);
        List<Symbol> outputSymbols = Lists.<Symbol>newArrayList(idRef, nameRef);

        MergeProjection mergeProjection = new MergeProjection(
                collectSymbols,
                orderBy.orderBySymbols(),
                orderBy.reverseFlags(),
                orderBy.nullsFirst()
        );
        Planner.Context ctx = new Planner.Context(clusterService());

        CollectNode collectNode = PlanNodeBuilder.collect(
                characters,
                ctx,
                WhereClause.MATCH_ALL,
                collectSymbols,
                ImmutableList.<Projection>of(),
                orderBy,
                Constants.DEFAULT_SELECT_LIMIT
        );
        collectNode.projections(ImmutableList.<Projection>of(mergeProjection));
        collectNode.keepContextForFetcher(true);

        FetchProjection fetchProjection = getFetchProjection(characters, collectSymbols, outputSymbols, collectNode, ctx);

        MergeNode localMergeNode = PlanNodeBuilder.sortedLocalMerge(
                ImmutableList.<Projection>of(fetchProjection),
                orderBy,
                collectSymbols,
                null,
                collectNode,
                ctx);

        Plan plan = new QueryThenFetch(collectNode, localMergeNode);

        Job job = executor.newJob(plan);
        assertThat(job.tasks().size(), is(1));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, contains(
                isRow(1, "Arthur"),
                isRow(4, "Arthur"),
                isRow(2, "Ford"),
                isRow(3, "Trillian")
        ));
    }

    @Test
    public void testQTFTaskWithFunction() throws Exception {

        execute("create table searchf (id int primary key, date timestamp) with (number_of_replicas=0)");
        ensureGreen();
        execute("insert into searchf (id, date) values (1, '1980-01-01'), (2, '1980-01-02')");
        refresh();

        Reference id_ref = new Reference(new ReferenceInfo(
                new ReferenceIdent(
                        new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "searchf"),
                        "id"),
                RowGranularity.DOC,
                DataTypes.INTEGER
        ));
        Reference date_ref = new Reference(new ReferenceInfo(
                new ReferenceIdent(
                        new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "searchf"),
                        "date"),
                RowGranularity.DOC,
                DataTypes.TIMESTAMP
        ));
        Function function = new Function(new FunctionInfo(
                new FunctionIdent(DateTruncFunction.NAME, Arrays.<DataType>asList(DataTypes.STRING, DataTypes.TIMESTAMP)),
                DataTypes.TIMESTAMP
        ), Arrays.asList(Literal.newLiteral("month"), date_ref));
        Function whereClause = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.INTEGER, DataTypes.INTEGER)),
                DataTypes.BOOLEAN),
                Arrays.asList(id_ref, Literal.newLiteral(2))
        );

        DocTableInfo searchf = docSchemaInfo.getTableInfo("searchf");
        ReferenceInfo docIdRefInfo = searchf.getReferenceInfo(new ColumnIdent(DocSysColumns.DOCID.name()));

        Planner.Context ctx = new Planner.Context(clusterService());
        List<Symbol> collectSymbols = ImmutableList.<Symbol>of(new Reference(docIdRefInfo));
        CollectNode collectNode = PlanNodeBuilder.collect(
                searchf,
                ctx,
                new WhereClause(whereClause),
                collectSymbols,
                ImmutableList.<Projection>of(),
                null,
                Constants.DEFAULT_SELECT_LIMIT
        );
        collectNode.keepContextForFetcher(true);

        TopNProjection topN = new TopNProjection(2, TopN.NO_OFFSET);
        topN.outputs(Collections.<Symbol>singletonList(new InputColumn(0)));

        FetchProjection fetchProjection = getFetchProjection(searchf, collectSymbols, Arrays.asList(id_ref, function), collectNode, ctx);

        MergeNode mergeNode = PlanNodeBuilder.localMerge(
                ImmutableList.of(topN, fetchProjection),
                collectNode,
                ctx);
        Plan plan = new QueryThenFetch(collectNode, mergeNode);

        Job job = executor.newJob(plan);
        assertThat(job.tasks().size(), is(1));

        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, contains(isRow(2, 315532800000L)));
    }

    @Test
    public void testQTFTaskPartitioned() throws Exception {
        setup.setUpPartitionedTableWithName();
        DocTableInfo parted = docSchemaInfo.getTableInfo("parted");
        Planner.Context ctx = new Planner.Context(clusterService());

        ReferenceInfo docIdRefInfo = parted.getReferenceInfo(new ColumnIdent(DocSysColumns.DOCID.name()));
        List<Symbol> collectSymbols = Lists.<Symbol>newArrayList(new Reference(docIdRefInfo));
        List<Symbol> outputSymbols =  Arrays.<Symbol>asList(partedIdRef, partedNameRef, partedDateRef);

        CollectNode collectNode = PlanNodeBuilder.collect(
                parted,
                ctx,
                WhereClause.MATCH_ALL,
                collectSymbols,
                ImmutableList.<Projection>of(),
                null,
                Constants.DEFAULT_SELECT_LIMIT
        );
        collectNode.keepContextForFetcher(true);

        FetchProjection fetchProjection = getFetchProjection(parted, collectSymbols, outputSymbols, collectNode, ctx);

        MergeNode localMergeNode = PlanNodeBuilder.localMerge(
                ImmutableList.<Projection>of(fetchProjection),
                collectNode,
                ctx);

        Plan plan = new QueryThenFetch(collectNode, localMergeNode);
        Job job = executor.newJob(plan);

        assertThat(job.tasks().size(), is(1));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, containsInAnyOrder(
                isRow(3, "Ford", 1396388720242L),
                isRow(1, "Trillian", null),
                isRow(2, null, 0L)
        ));
    }

    @Test
    public void testESDeleteByQueryTask() throws Exception {
        setup.setUpCharacters();

        Function whereClause = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.STRING, DataTypes.STRING)),
                DataTypes.BOOLEAN),
                Arrays.<Symbol>asList(idRef, Literal.newLiteral(2)));

        ESDeleteByQueryNode node = new ESDeleteByQueryNode(
                1,
                ImmutableList.of(new String[]{"characters"}),
                ImmutableList.of(new WhereClause(whereClause)));
        Plan plan = new IterablePlan(node);
        Job job = executor.newJob(plan);
        ESDeleteByQueryTask task = (ESDeleteByQueryTask) job.tasks().get(0);

        task.start();
        TaskResult taskResult = task.result().get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(-1L)));


        execute("select * from characters where id = 2");
        assertThat(response.rowCount(), is(0L));
    }

    @Test
    public void testInsertWithSymbolBasedUpsertByIdTask() throws Exception {
        execute("create table characters (id int primary key, name string)");
        ensureGreen();


        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(),
                false,
                false,
                null,
                new Reference[]{idRef, nameRef});
        updateNode.add("characters", "99", "99", null, null, new Object[]{99, new BytesRef("Marvin")});

        Plan plan = new IterablePlan(updateNode);
        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));

        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(1L)));


        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef);
        ESGetNode getNode = newGetNode("characters", outputs, "99", ctx.nextExecutionNodeId());
        plan = new IterablePlan(getNode);
        job = executor.newJob(plan);
        result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();

        assertThat(objects, contains(isRow(99, "Marvin")));
    }

    @Test
    public void testInsertIntoPartitionedTableWithSymbolBasedUpsertByIdTask() throws Exception {
        execute("create table parted (" +
                "  id int, " +
                "  name string, " +
                "  date timestamp" +
                ") partitioned by (date)");
        ensureGreen();


        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(),
                true,
                false,
                null,
                new Reference[]{idRef, nameRef});

        PartitionName partitionName = new PartitionName("parted", Arrays.asList(new BytesRef("13959981214861")));
        updateNode.add(partitionName.stringValue(), "123", "123", null, null, new Object[]{0L, new BytesRef("Trillian")});

        Plan plan = new IterablePlan(updateNode);
        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));

        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket indexResult = taskResult.rows();
        assertThat(indexResult, contains(isRow(1L)));

        refresh();

        assertTrue(
                client().admin().indices().prepareExists(partitionName.stringValue())
                        .execute().actionGet().isExists()
        );
        assertTrue(
                client().admin().indices().prepareAliasesExist("parted")
                        .execute().actionGet().exists()
        );
        SearchHits hits = client().prepareSearch(partitionName.stringValue())
                .setTypes(Constants.DEFAULT_MAPPING_TYPE)
                .addFields("id", "name")
                .setQuery(new MapBuilder<String, Object>()
                                .put("match_all", new HashMap<String, Object>())
                                .map()
                ).execute().actionGet().getHits();
        assertThat(hits.getTotalHits(), is(1L));
        assertThat((Integer) hits.getHits()[0].field("id").getValues().get(0), is(0));
        assertThat((String) hits.getHits()[0].field("name").getValues().get(0), is("Trillian"));
    }

    @Test
    public void testInsertMultiValuesWithSymbolBasedUpsertByIdTask() throws Exception {
        execute("create table characters (id int primary key, name string)");
        ensureGreen();


        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(),
                false,
                false,
                null,
                new Reference[]{idRef, nameRef});

        updateNode.add("characters", "99", "99", null, null, new Object[]{99, new BytesRef("Marvin")});
        updateNode.add("characters", "42", "42", null, null, new Object[]{42, new BytesRef("Deep Thought")});

        Plan plan = new IterablePlan(updateNode);
        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));

        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(2L)));


        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef);
        ESGetNode getNode = newGetNode("characters", outputs, Arrays.asList("99", "42"), ctx.nextExecutionNodeId());
        plan = new IterablePlan(getNode);
        job = executor.newJob(plan);
        result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();

        assertThat(objects, contains(
                isRow(99, "Marvin"),
                isRow(42, "Deep Thought")
        ));
    }

    @Test
    public void testUpdateWithSymbolBasedUpsertByIdTask() throws Exception {
        setup.setUpCharacters();


        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(), false, false, new String[]{nameRef.ident().columnIdent().fqn()}, null);
        updateNode.add("characters", "1", "1", new Symbol[]{Literal.newLiteral("Vogon lyric fan")}, null);
        Plan plan = new IterablePlan(updateNode);

        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(1L)));


        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef);
        ESGetNode getNode = newGetNode("characters", outputs, "1", ctx.nextExecutionNodeId());
        plan = new IterablePlan(getNode);
        job = executor.newJob(plan);
        result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();

        assertThat(objects, contains(isRow(1, "Vogon lyric fan")));
    }

    @Test
    public void testInsertOnDuplicateWithSymbolBasedUpsertByIdTask() throws Exception {
        setup.setUpCharacters();

        Object[] missingAssignments = new Object[]{5, new BytesRef("Zaphod Beeblebrox"), false};
        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(),
                false,
                false,
                new String[]{nameRef.ident().columnIdent().fqn()},
                new Reference[]{idRef, nameRef, femaleRef});

        updateNode.add("characters", "5", "5", new Symbol[]{Literal.newLiteral("Zaphod Beeblebrox")}, null, missingAssignments);
        Plan plan = new IterablePlan(updateNode);
        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(1L)));


        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef, femaleRef);
        ESGetNode getNode = newGetNode("characters", outputs, "5", ctx.nextExecutionNodeId());
        plan = new IterablePlan(getNode);
        job = executor.newJob(plan);
        result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();
        assertThat(objects, contains(isRow(5, "Zaphod Beeblebrox", false)));

    }

    @Test
    public void testUpdateOnDuplicateWithSymbolBasedUpsertByIdTask() throws Exception {
        setup.setUpCharacters();

        Object[] missingAssignments = new Object[]{1, new BytesRef("Zaphod Beeblebrox"), true};
        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(),
                false,
                false,
                new String[]{femaleRef.ident().columnIdent().fqn()},
                new Reference[]{idRef, nameRef, femaleRef});
        updateNode.add("characters", "1", "1", new Symbol[]{Literal.newLiteral(true)}, null, missingAssignments);
        Plan plan = new IterablePlan(updateNode);
        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(1L)));


        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef, femaleRef);
        ESGetNode getNode = newGetNode("characters", outputs, "1", ctx.nextExecutionNodeId());
        plan = new IterablePlan(getNode);
        job = executor.newJob(plan);
        result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();

        assertThat(objects, contains(isRow(1, "Arthur", true)));
    }

    @Test
    public void testBulkUpdateByQueryTask() throws Exception {
        setup.setUpCharacters();


        List<Plan> childNodes = new ArrayList<>();
        Planner.Context plannerContext = new Planner.Context(clusterService());

        TableInfo tableInfo = docSchemaInfo.getTableInfo("characters");
        Reference uidReference = new Reference(
                new ReferenceInfo(
                        new ReferenceIdent(tableInfo.ident(), "_uid"),
                        RowGranularity.DOC, DataTypes.STRING));


        Function whereClause1 = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.BOOLEAN, DataTypes.BOOLEAN)),
                DataTypes.BOOLEAN),
                Arrays.<Symbol>asList(femaleRef, Literal.newLiteral(true)));

        UpdateProjection updateProjection = new UpdateProjection(
                new InputColumn(0, DataTypes.STRING),
                new String[]{"name"},
                new Symbol[]{Literal.newLiteral("Zaphod Beeblebrox")},
                null);

        CollectNode collectNode1 = PlanNodeBuilder.collect(
                tableInfo,
                plannerContext,
                new WhereClause(whereClause1),
                ImmutableList.<Symbol>of(uidReference),
                ImmutableList.<Projection>of(updateProjection),
                null,
                Preference.PRIMARY.type()
        );
        MergeNode mergeNode1 = PlanNodeBuilder.localMerge(
                ImmutableList.<Projection>of(CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION), collectNode1,
                plannerContext);
        childNodes.add(new CollectAndMerge(collectNode1, mergeNode1));


        Function whereClause2 = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.BOOLEAN, DataTypes.BOOLEAN)),
                DataTypes.BOOLEAN),
                Arrays.<Symbol>asList(femaleRef, Literal.newLiteral(true)));

        CollectNode collectNode2 = PlanNodeBuilder.collect(
                tableInfo,
                plannerContext,
                new WhereClause(whereClause2),
                ImmutableList.<Symbol>of(uidReference),
                ImmutableList.<Projection>of(updateProjection),
                null,
                Preference.PRIMARY.type()
        );
        MergeNode mergeNode2 = PlanNodeBuilder.localMerge(
                ImmutableList.<Projection>of(CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION), collectNode2,
                plannerContext);
        childNodes.add(new CollectAndMerge(collectNode2, mergeNode2));

        Upsert plan = new Upsert(childNodes);
        Job job = executor.newJob(plan);

        assertThat(job.tasks().size(), is(1));
        assertThat(job.tasks().get(0), instanceOf(ExecutionNodesTask.class));
        List<? extends ListenableFuture<TaskResult>> results = executor.execute(job);
        assertThat(results.size(), is(2));

        for (int i = 0; i < results.size(); i++) {
            TaskResult result = results.get(i).get();
            assertThat(result, instanceOf(RowCountResult.class));

            assertThat(((RowCountResult)result).rowCount(), is(2L));
        }
    }

    @Test
    public void testKillTask() throws Exception {
        Job job = executor.newJob(KillPlan.INSTANCE);
        assertThat(job.tasks(), hasSize(1));
        assertThat(job.tasks().get(0), instanceOf(KillTask.class));

        List<? extends ListenableFuture<TaskResult>> results = executor.execute(job);
        assertThat(results, hasSize(1));
        results.get(0).get();
    }
}

<code block>


package io.crate.executor.transport.distributed;

import io.crate.Constants;
import io.crate.Streamer;
import io.crate.core.collections.Row1;
import io.crate.test.integration.CrateUnitTest;
import io.crate.testing.TestingHelpers;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.action.ActionListener;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;
import org.mockito.ArgumentCaptor;
import org.mockito.Captor;
import org.mockito.MockitoAnnotations;

import java.util.Arrays;
import java.util.List;
import java.util.UUID;
import java.util.concurrent.CancellationException;

import static org.hamcrest.Matchers.is;
import static org.mockito.Matchers.any;
import static org.mockito.Matchers.eq;
import static org.mockito.Mockito.*;

public class DistributingDownstreamTest extends CrateUnitTest {

    private TransportDistributedResultAction distributedResultAction;
    private DistributingDownstream downstream;

    @Captor
    public ArgumentCaptor<ActionListener<DistributedResultResponse>> listenerArgumentCaptor;
    private int originalPageSize;

    @Before
    public void before() throws Exception {
        originalPageSize = Constants.PAGE_SIZE;
        MockitoAnnotations.initMocks(this);

        List<String> downstreamNodes = Arrays.asList("n1", "n2");
        distributedResultAction = mock(TransportDistributedResultAction.class);
        Streamer<?>[] streamers = {DataTypes.STRING.streamer()};
        downstream = new DistributingDownstream(
                UUID.randomUUID(),
                1,
                0,
                downstreamNodes,
                distributedResultAction,
                streamers
        );
        downstream.registerUpstream(null);
    }

    @After
    public void after() throws Exception {
        Constants.PAGE_SIZE = originalPageSize;
    }

    @Test
    public void testBucketing() throws Exception {
        ArgumentCaptor<DistributedResultRequest> r1Captor = ArgumentCaptor.forClass(DistributedResultRequest.class);
        doNothing().when(distributedResultAction).pushResult(eq("n1"), r1Captor.capture(), any(ActionListener.class));

        ArgumentCaptor<DistributedResultRequest> r2Captor = ArgumentCaptor.forClass(DistributedResultRequest.class);
        doNothing().when(distributedResultAction).pushResult(eq("n2"), r2Captor.capture(), any(ActionListener.class));


        downstream.setNextRow(new Row1(new BytesRef("Trillian")));
        downstream.setNextRow(new Row1(new BytesRef("Marvin")));
        downstream.setNextRow(new Row1(new BytesRef("Arthur")));
        downstream.setNextRow(new Row1(new BytesRef("Slartibartfast")));

        downstream.finish();

        assertRows(r2Captor, "Trillian\nMarvin\n");
        assertRows(r1Captor, "Arthur\nSlartibartfast\n");
    }

    @Test
    public void testOperationIsStoppedOnFailureResponse() throws Exception {
        Constants.PAGE_SIZE = 2;

        ArgumentCaptor<DistributedResultRequest> captor = ArgumentCaptor.forClass(DistributedResultRequest.class);
        doNothing().when(distributedResultAction).pushResult(any(String.class), captor.capture(), listenerArgumentCaptor.capture());

        int iterations = 0;
        int expected = -1;
        while (true) {
            if (!downstream.setNextRow(new Row1(new BytesRef("Trillian")))) {
                break;
            }
            List<ActionListener<DistributedResultResponse>> allValues = listenerArgumentCaptor.getAllValues();
            if (allValues.size() == 1) {
                ActionListener<DistributedResultResponse> distributedResultResponseActionListener = allValues.get(0);
                distributedResultResponseActionListener.onFailure(new IllegalStateException("epic fail"));
                expected = iterations + 1;
            }
            iterations++;
        }
        assertThat(iterations, is(expected));
    }

    @Test
    public void testRequestsAreSentWithoutRows() throws Exception {
        ArgumentCaptor<DistributedResultRequest> captor = ArgumentCaptor.forClass(DistributedResultRequest.class);
        doNothing().when(distributedResultAction).pushResult(any(String.class), captor.capture(), any(ActionListener.class));

        downstream.finish();
        assertThat(captor.getAllValues().size(), is(2));
        for (DistributedResultRequest distributedResultRequest : captor.getAllValues()) {
            assertThat(distributedResultRequest.rows().size(), is(0));
        }

    }

    @Test
    public void testNoRequestsSendWhenCancelled() throws Exception {
        downstream.setNextRow(new Row1(new BytesRef("LateNightSprintFinishingAwesomeness")));
        downstream.fail(new CancellationException());

        verify(distributedResultAction, never()).pushResult(any(String.class), any(DistributedResultRequest.class), any(ActionListener.class));

    }

    private void assertRows(ArgumentCaptor<DistributedResultRequest> r2Captor, String expectedRows) {
        List<DistributedResultRequest> allRequestsForNodeN1 = r2Captor.getAllValues();
        assertThat(allRequestsForNodeN1.size(), is(1));
        DistributedResultRequest n1Request = allRequestsForNodeN1.get(0);
        assertThat(n1Request.isLast(), is(true));
        assertThat(n1Request.rowsCanBeRead(), is(true));

        assertThat(TestingHelpers.printedTable(n1Request.rows()), is(expectedRows));
    }
}
<code block>


package io.crate.executor.transport.merge;

import io.crate.Streamer;
import io.crate.core.collections.ArrayBucket;
import io.crate.executor.transport.distributed.DistributedResultRequest;
import io.crate.test.integration.CrateUnitTest;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.common.io.stream.BytesStreamInput;
import org.elasticsearch.common.io.stream.BytesStreamOutput;
import org.junit.Test;

import java.util.UUID;

import static io.crate.testing.TestingHelpers.isNullRow;
import static io.crate.testing.TestingHelpers.isRow;
import static org.hamcrest.Matchers.contains;

public class DistributedResultRequestTest extends CrateUnitTest {

    @Test
    public void testStreaming() throws Exception {
        Streamer<?>[] streamers = new Streamer[]{DataTypes.STRING.streamer()};

        Object[][] rows = new Object[][]{
                {new BytesRef("ab")},{null},{new BytesRef("cd")}
        };
        UUID uuid = UUID.randomUUID();

        DistributedResultRequest r1 = new DistributedResultRequest(uuid, 1, 1, streamers);
        r1.rows(new ArrayBucket(rows));

        BytesStreamOutput out = new BytesStreamOutput();
        r1.writeTo(out);
        BytesStreamInput in = new BytesStreamInput(out.bytes());
        DistributedResultRequest r2 = new DistributedResultRequest();
        r2.readFrom(in);
        r2.streamers(streamers);
        assertTrue(r2.rowsCanBeRead());

        assertEquals(r1.rows().size(), r2.rows().size());

        assertThat(r2.rows(), contains(isRow("ab"), isNullRow(), isRow("cd")));
    }
}

<code block>


package io.crate.planner.node.dql;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.Sets;
import io.crate.planner.node.dql.join.NestedLoopNode;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.TopNProjection;
import io.crate.test.integration.CrateUnitTest;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.elasticsearch.common.io.stream.BytesStreamInput;
import org.elasticsearch.common.io.stream.BytesStreamOutput;
import org.hamcrest.core.Is;
import org.junit.Test;

import java.util.Arrays;
import java.util.UUID;

import static org.hamcrest.CoreMatchers.is;

public class NestedLoopNodeTest extends CrateUnitTest {

    @Test
    public void testSerialization() throws Exception {

        NestedLoopNode node = new NestedLoopNode(3, 2, "nestedLoop");
        node.jobId(UUID.randomUUID());
        node.executionNodes(Sets.newHashSet("node1", "node2"));
        node.leftInputTypes(Arrays.<DataType>asList(DataTypes.UNDEFINED, DataTypes.STRING));
        node.rightInputTypes(Arrays.<DataType>asList(DataTypes.BOOLEAN, DataTypes.INTEGER, DataTypes.DOUBLE));
        node.downstreamNodes(Sets.newHashSet("node3", "node4"));
        node.downstreamExecutionNodeId(5);
        TopNProjection topNProjection = new TopNProjection(10, 0);
        node.projections(ImmutableList.<Projection>of(topNProjection));

        BytesStreamOutput output = new BytesStreamOutput();
        node.writeTo(output);

        BytesStreamInput input = new BytesStreamInput(output.bytes());
        NestedLoopNode node2 = new NestedLoopNode();
        node2.readFrom(input);

        assertThat(node.downstreamExecutionNodeId(), is(node2.downstreamExecutionNodeId()));
        assertThat(node.downstreamNodes(), is(node2.downstreamNodes()));
        assertThat(node.executionNodes(), Is.is(node2.executionNodes()));
        assertThat(node.jobId(), Is.is(node2.jobId()));
        assertThat(node.leftInputTypes(), is(node2.leftInputTypes()));
        assertThat(node.rightInputTypes(), is(node2.rightInputTypes()));
        assertThat(node.leftExecutionNodeId(), is(node2.leftExecutionNodeId()));
        assertThat(node.rightExecutionNodeId(), is(node2.rightExecutionNodeId()));
        assertThat(node.name(), is(node2.name()));
        assertThat(node.outputTypes(), is(node2.outputTypes()));
    }
}

<code block>


package io.crate.operation.join;

import io.crate.core.collections.Row;
import io.crate.core.collections.RowN;
import io.crate.operation.RowDownstream;
import io.crate.operation.RowDownstreamHandle;
import io.crate.operation.RowUpstream;
import org.elasticsearch.common.logging.ESLogger;
import org.elasticsearch.common.logging.Loggers;

import java.util.ArrayList;
import java.util.concurrent.ArrayBlockingQueue;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicInteger;

public class NestedLoopOperation implements RowUpstream, RowDownstream {

    private final ArrayList<Row> innerRows = new ArrayList<>();
    private final CombinedRow combinedRow = new CombinedRow();
    private final RowDownstreamHandle leftDownstreamHandle;
    private final RowDownstreamHandle rightDownstreamHandle;
    private final AtomicBoolean leftFinished = new AtomicBoolean(false);
    private final AtomicBoolean rightFinished = new AtomicBoolean(false);
    private final ArrayBlockingQueue<Row> innerRowsQ = new ArrayBlockingQueue<>(1);
    private final Object finishedLock = new Object();
    private final AtomicInteger numUpstreams = new AtomicInteger(0);

    private final static ESLogger LOGGER = Loggers.getLogger(NestedLoopOperation.class);
    private final static Row SENTINEL = new Row() {
        @Override
        public int size() {
            return 0;
        }

        @Override
        public Object get(int index) {
            return null;
        }

        @Override
        public Object[] materialize() {
            return new Object[0];
        }
    };

    private RowDownstreamHandle downstream;


    public NestedLoopOperation() {
        LOGGER.setLevel("trace");
        leftDownstreamHandle = new RowDownstreamHandle() {
            @Override
            public boolean setNextRow(Row row) {
                LOGGER.trace("left downstream received a row {}", row);
                if (rightFinished.get()) {
                    return loopInnerRowAndEmit(row);
                } else {
                    Row innerRow;
                    while (true) {
                        if (rightFinished.get()) {
                            while ((innerRow = innerRowsQ.poll()) != null) {
                                if (innerRow == SENTINEL) {
                                    break;
                                }
                                boolean shouldContinue = emitAndSaveInnerRow(innerRow, row);
                                if (!shouldContinue) {
                                    return false;
                                }
                            }
                            return true;
                        }

                        try {
                            innerRow = innerRowsQ.take();
                            if (innerRow == SENTINEL) {
                                continue;
                            }
                            boolean shouldContinue = emitAndSaveInnerRow(innerRow, row);
                            if (!shouldContinue) {
                                return false;
                            }
                        } catch (InterruptedException e) {
                            fail(e);
                            return false;
                        }
                    }
                }
            }

            private boolean loopInnerRowAndEmit(Row row) {
                for (Row innerRow : innerRows) {
                    combinedRow.outerRow = row;
                    combinedRow.innerRow = innerRow;
                    boolean shouldContinue = downstream.setNextRow(combinedRow);
                    if (!shouldContinue) {
                        return false;
                    }
                }
                return true;
            }

            @Override
            public void finish() {
                LOGGER.trace("left downstream finished");
                synchronized (finishedLock) {
                    leftFinished.set(true);
                    if (rightFinished.get()) {
                        downstream.finish();
                    }
                }
            }

            @Override
            public void fail(Throwable throwable) {
                downstream.fail(throwable);
            }
        };

        rightDownstreamHandle = new RowDownstreamHandle() {
            @Override
            public boolean setNextRow(Row row) {
                LOGGER.trace("right downstream received a row {}", row);
                try {
                    Row materializedRow = new RowN(row.materialize());
                    while (true) {
                        boolean added = innerRowsQ.offer(materializedRow, 100, TimeUnit.MICROSECONDS);
                        if (added) {
                            return true;
                        } else if (leftFinished.get()) {
                            return true;
                        }
                    }
                } catch (InterruptedException e) {
                    fail(e);
                    return false;
                }
            }

            @Override
            public void finish() {
                LOGGER.trace("right downstream finished");
                synchronized (finishedLock) {
                    rightFinished.set(true);
                    innerRowsQ.offer(SENTINEL); 
                    if (leftFinished.get()) {
                        downstream.finish();
                    }
                }
            }

            @Override
            public void fail(Throwable throwable) {
                downstream.fail(throwable);
            }
        };
    }


    @Override
    public RowDownstreamHandle registerUpstream(RowUpstream upstream) {
        if (numUpstreams.incrementAndGet() == 1) {
            return leftDownstreamHandle;
        } else {
            assert numUpstreams.get() <= 2: "Only 2 upstreams supported";
            return rightDownstreamHandle;
        }
    }

    public void downstream(RowDownstream downstream) {
        this.downstream = downstream.registerUpstream(this);
    }

    private boolean emitAndSaveInnerRow(Row innerRow, Row outerRow) {
        if (innerRow instanceof RowN) {
            innerRows.add(innerRow);
        } else {
            innerRows.add(new RowN(innerRow.materialize()));
        }
        combinedRow.outerRow = outerRow;
        combinedRow.innerRow = innerRow;
        return downstream.setNextRow(combinedRow);
    }


    static class CombinedRow implements Row {

        Row outerRow;
        Row innerRow;

        @Override
        public int size() {
            return outerRow.size() + innerRow.size();
        }

        @Override
        public Object get(int index) {
            if (index < outerRow.size()) {
                return outerRow.get(index);
            }
            return innerRow.get(index - outerRow.size());
        }

        @Override
        public Object[] materialize() {
            Object[] left = outerRow.materialize();
            Object[] right = innerRow.materialize();

            Object[] newRow = new Object[left.length + right.length];
            System.arraycopy(left, 0, newRow, 0, left.length);
            System.arraycopy(right, 0, newRow, left.length, right.length);
            return newRow;
        }
    }
}

<code block>


package io.crate.operation.projectors;

import com.google.common.collect.Lists;
import io.crate.Streamer;
import io.crate.executor.transport.distributed.DistributingDownstream;
import io.crate.executor.transport.distributed.SingleBucketBuilder;
import io.crate.executor.transport.distributed.TransportDistributedResultAction;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.node.ExecutionNodes;
import io.crate.planner.node.StreamerVisitor;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.inject.Singleton;

import java.util.ArrayList;
import java.util.Collections;
import java.util.UUID;

@Singleton
public class InternalResultProviderFactory implements ResultProviderFactory {

    private final ClusterService clusterService;
    private final TransportDistributedResultAction transportDistributedResultAction;
    private final StreamerVisitor streamerVisitor;

    @Inject
    public InternalResultProviderFactory(ClusterService clusterService,
                                         TransportDistributedResultAction transportDistributedResultAction,
                                         StreamerVisitor streamerVisitor) {
        this.clusterService = clusterService;
        this.transportDistributedResultAction = transportDistributedResultAction;
        this.streamerVisitor = streamerVisitor;
    }

    public ResultProvider createDownstream(ExecutionNode node, UUID jobId) {
        Streamer<?>[] streamers = getStreamers(node);

        if (ExecutionNodes.hasDirectResponseDownstream(node.downstreamNodes())) {
            return new SingleBucketBuilder(streamers);
        } else {
            assert node.downstreamNodes().size() > 0 : "must have at least one downstream";


            ArrayList<String> server = Lists.newArrayList(node.executionNodes());
            Collections.sort(server);
            int bucketIdx = server.indexOf(clusterService.localNode().id());

            return new DistributingDownstream(
                    jobId,
                    node.downstreamExecutionNodeId(),
                    node.downstreamInputId(),
                    bucketIdx,
                    node.downstreamNodes(),
                    transportDistributedResultAction,
                    streamers
            );
        }
    }

    protected Streamer<?>[] getStreamers(ExecutionNode node) {
        return streamerVisitor.processExecutionNode(node).outputStreamers();
    }
}

<code block>


package io.crate.operation.collect;

import com.google.common.base.Function;
import com.google.common.base.Throwables;
import com.google.common.collect.ImmutableMap;
import com.google.common.collect.Lists;
import com.google.common.util.concurrent.*;
import io.crate.analyze.EvaluatingNormalizer;
import io.crate.exceptions.TableUnknownException;
import io.crate.exceptions.UnhandledServerException;
import io.crate.executor.transport.TransportActionProvider;
import io.crate.metadata.Functions;
import io.crate.metadata.ReferenceResolver;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.*;
import io.crate.operation.collect.files.FileCollectInputSymbolVisitor;
import io.crate.operation.collect.files.FileInputFactory;
import io.crate.operation.collect.files.FileReadingCollector;
import io.crate.operation.projectors.FlatProjectorChain;
import io.crate.operation.projectors.ProjectionToProjectorVisitor;
import io.crate.operation.projectors.ResultProviderFactory;
import io.crate.operation.reference.file.FileLineReferenceResolver;
import io.crate.operation.reference.sys.node.NodeSysExpression;
import io.crate.operation.reference.sys.node.NodeSysReferenceResolver;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.FileUriCollectNode;
import io.crate.planner.symbol.ValueSymbolVisitor;
import org.elasticsearch.action.bulk.BulkRetryCoordinatorPool;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.inject.Injector;
import org.elasticsearch.common.inject.Singleton;
import org.elasticsearch.common.logging.ESLogger;
import org.elasticsearch.common.logging.Loggers;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.index.IndexService;
import org.elasticsearch.index.IndexShardMissingException;
import org.elasticsearch.indices.IndexMissingException;
import org.elasticsearch.indices.IndicesService;
import org.elasticsearch.threadpool.ThreadPool;

import javax.annotation.Nullable;
import java.util.*;
import java.util.concurrent.Callable;
import java.util.concurrent.CancellationException;
import java.util.concurrent.RejectedExecutionException;
import java.util.concurrent.ThreadPoolExecutor;


@Singleton
public class MapSideDataCollectOperation implements CollectOperation, RowUpstream {

    private static final ESLogger LOGGER = Loggers.getLogger(MapSideDataCollectOperation.class);

    private static class VoidFunction<Arg> implements Function<Arg, Void> {
        @Nullable
        @Override
        public Void apply(@Nullable Arg input) {
            return null;
        }
    }

    private final IndicesService indicesService;
    protected final EvaluatingNormalizer nodeNormalizer;
    protected final ClusterService clusterService;
    private final FileCollectInputSymbolVisitor fileInputSymbolVisitor;
    private final CollectServiceResolver collectServiceResolver;
    private final ProjectionToProjectorVisitor projectorVisitor;
    private final ThreadPoolExecutor executor;
    private final ListeningExecutorService listeningExecutorService;
    private final int poolSize;
    private final ResultProviderFactory resultProviderFactory;

    private final InformationSchemaCollectService informationSchemaCollectService;
    private final UnassignedShardsCollectService unassignedShardsCollectService;

    private final OneRowCollectService clusterCollectService;
    private final CollectService nodeCollectService;

    private final Functions functions;
    private final NodeSysExpression nodeSysExpression;
    private ThreadPool threadPool;
    private final BulkRetryCoordinatorPool bulkRetryCoordinatorPool;
    private final TransportActionProvider transportActionProvider;
    private final Settings settings;

    @Inject
    public MapSideDataCollectOperation(ClusterService clusterService,
                                       Settings settings,
                                       TransportActionProvider transportActionProvider,
                                       BulkRetryCoordinatorPool bulkRetryCoordinatorPool,
                                       Functions functions,
                                       ReferenceResolver referenceResolver,
                                       NodeSysExpression nodeSysExpression,
                                       IndicesService indicesService,
                                       ThreadPool threadPool,
                                       CollectServiceResolver collectServiceResolver,
                                       ResultProviderFactory resultProviderFactory,
                                       InformationSchemaCollectService informationSchemaCollectService,
                                       UnassignedShardsCollectService unassignedShardsCollectService) {
        this.resultProviderFactory = resultProviderFactory;
        this.informationSchemaCollectService = informationSchemaCollectService;
        this.unassignedShardsCollectService = unassignedShardsCollectService;
        this.executor = (ThreadPoolExecutor)threadPool.executor(ThreadPool.Names.SEARCH);
        this.poolSize = executor.getCorePoolSize();
        this.listeningExecutorService = MoreExecutors.listeningDecorator(executor);

        this.clusterService = clusterService;
        this.indicesService = indicesService;
        this.nodeNormalizer = new EvaluatingNormalizer(functions, RowGranularity.NODE, referenceResolver);

        this.collectServiceResolver = collectServiceResolver;

        this.settings = settings;
        this.functions = functions;
        this.nodeSysExpression = nodeSysExpression;

        this.clusterCollectService = new OneRowCollectService(new ImplementationSymbolVisitor(
                referenceResolver,
                functions,
                RowGranularity.CLUSTER
        ));
        this.nodeCollectService = new CollectService() {
            @Override
            public CrateCollector getCollector(CollectNode node, RowDownstream downstream) {
                return getNodeLevelCollector(node, downstream);
            }
        };
        this.fileInputSymbolVisitor =
                new FileCollectInputSymbolVisitor(functions, FileLineReferenceResolver.INSTANCE);

        this.threadPool = threadPool;
        this.bulkRetryCoordinatorPool = bulkRetryCoordinatorPool;
        this.transportActionProvider = transportActionProvider;

        ImplementationSymbolVisitor nodeImplementationSymbolVisitor = new ImplementationSymbolVisitor(
                referenceResolver,
                functions,
                RowGranularity.NODE
        );
        this.projectorVisitor = new ProjectionToProjectorVisitor(
                clusterService,
                threadPool,
                settings,
                transportActionProvider,
                bulkRetryCoordinatorPool,
                nodeImplementationSymbolVisitor
        );
    }



    @Override
    public ListenableFuture<List<Void>> collect(CollectNode collectNode,
                                                RowDownstream downstream,
                                                final JobCollectContext jobCollectContext) {
        assert collectNode.isRouted(); 
        assert collectNode.jobId() != null : "no jobId present for collect operation";
        String localNodeId = clusterService.state().nodes().localNodeId();
        Set<String> routingNodes = collectNode.routing().nodes();
        if (routingNodes.contains(localNodeId) || localNodeId.equals(collectNode.handlerSideCollect())) {
            if (collectNode.routing().containsShards(localNodeId)) {

                return handleShardCollect(collectNode, downstream, jobCollectContext);
            } else {
                ListenableFuture<List<Void>> results;

                if (collectNode instanceof FileUriCollectNode) {
                    results = handleWithService(nodeCollectService, collectNode, downstream, jobCollectContext);
                } else if (collectNode.isPartitioned() && collectNode.maxRowGranularity() == RowGranularity.DOC) {


                    downstream.registerUpstream(this).finish();
                    results = IMMEDIATE_LIST;
                } else {
                    CollectService collectService = getCollectService(collectNode, localNodeId);
                    results = handleWithService(collectService, collectNode, downstream, jobCollectContext);
                }


                Futures.addCallback(results, new FutureCallback<List<Void>>() {
                    @Override
                    public void onSuccess(@Nullable List<Void> result) {
                        jobCollectContext.close();
                    }

                    @Override
                    public void onFailure(Throwable t) {
                        jobCollectContext.close();
                    }
                });

                return results;
            }
        }
        throw new UnhandledServerException("unsupported routing");
    }

    private CollectService getCollectService(CollectNode collectNode, String localNodeId) {
        switch (collectNode.maxRowGranularity()) {
            case CLUSTER:

                return clusterCollectService;
            case NODE:

                return nodeCollectService;
            case SHARD:

                return unassignedShardsCollectService;
            case DOC:
                if (localNodeId.equals(collectNode.handlerSideCollect())) {

                    return informationSchemaCollectService;
                } else {

                    return nodeCollectService;
                }
            default:
                throw new UnsupportedOperationException("Unsupported rowGranularity " + collectNode.maxRowGranularity());
        }
    }

    private ListenableFuture<List<Void>> handleWithService(final CollectService collectService,
                                                           final CollectNode node,
                                                           final RowDownstream rowDownstream,
                                                           final JobCollectContext jobCollectContext) {
        return listeningExecutorService.submit(new Callable<List<Void>>() {
            @Override
            public List<Void> call() throws Exception {
                try {
                    EvaluatingNormalizer nodeNormalizer = MapSideDataCollectOperation.this.nodeNormalizer;
                    if (node.maxRowGranularity().finerThan(RowGranularity.CLUSTER)) {
                        nodeNormalizer = new EvaluatingNormalizer(functions,
                                RowGranularity.NODE,
                                new NodeSysReferenceResolver(nodeSysExpression));
                    }
                    CollectNode localCollectNode = node.normalize(nodeNormalizer);
                    RowDownstream localRowDownStream = rowDownstream;
                    if (localCollectNode.whereClause().noMatch()) {
                        localRowDownStream.registerUpstream(MapSideDataCollectOperation.this).finish();
                    } else {
                        if (!localCollectNode.projections().isEmpty()) {
                            FlatProjectorChain projectorChain = FlatProjectorChain.withAttachedDownstream(
                                    projectorVisitor,
                                    jobCollectContext.ramAccountingContext(),
                                    localCollectNode.projections(),
                                    localRowDownStream,
                                    node.jobId()
                            );
                            projectorChain.startProjections(jobCollectContext);
                            localRowDownStream = projectorChain.firstProjector();
                        }
                        CrateCollector collector = collectService.getCollector(localCollectNode, localRowDownStream); 
                        collector.doCollect(jobCollectContext);
                    }
                } catch (Throwable t) {
                    LOGGER.error("error during collect", t);
                    rowDownstream.registerUpstream(MapSideDataCollectOperation.this).fail(t);
                    Throwables.propagate(t);
                }
                return ONE_LIST;
            }
        });
    }

    private CrateCollector getNodeLevelCollector(CollectNode collectNode,
                                                 RowDownstream downstream) {
        if (collectNode instanceof FileUriCollectNode) {
            FileCollectInputSymbolVisitor.Context context = fileInputSymbolVisitor.extractImplementations(collectNode);
            FileUriCollectNode fileUriCollectNode = (FileUriCollectNode) collectNode;

            String[] readers = fileUriCollectNode.executionNodes().toArray(
                    new String[fileUriCollectNode.executionNodes().size()]);
            Arrays.sort(readers);
            return new FileReadingCollector(
                    ValueSymbolVisitor.STRING.process(fileUriCollectNode.targetUri()),
                    context.topLevelInputs(),
                    context.expressions(),
                    downstream,
                    fileUriCollectNode.fileFormat(),
                    fileUriCollectNode.compression(),
                    ImmutableMap.<String, FileInputFactory>of(),
                    fileUriCollectNode.sharedStorage(),
                    readers.length,
                    Arrays.binarySearch(readers, clusterService.state().nodes().localNodeId())
            );
        } else {
            CollectService service = collectServiceResolver.getService(collectNode.routing());
            if (service != null) {
                return service.getCollector(collectNode, downstream);
            }
            ImplementationSymbolVisitor nodeImplementationSymbolVisitor = new ImplementationSymbolVisitor(
                    new NodeSysReferenceResolver(nodeSysExpression),
                    functions,
                    RowGranularity.NODE
            );
            ImplementationSymbolVisitor.Context ctx = nodeImplementationSymbolVisitor.extractImplementations(collectNode);
            assert ctx.maxGranularity().ordinal() <= RowGranularity.NODE.ordinal() : "wrong RowGranularity";
            return new SimpleOneRowCollector(
                    ctx.topLevelInputs(), ctx.collectExpressions(), downstream);
        }
    }

    private int numShards(CollectNode collectNode, String localNodeId) {
        int numShards = collectNode.routing().numShards(localNodeId);
        if (localNodeId.equals(collectNode.handlerSideCollect()) && collectNode.routing().nodes().contains(TableInfo.NULL_NODE_ID)) {


            numShards += 1;
        }
        return numShards;
    }


    protected ListenableFuture<List<Void>> handleShardCollect(CollectNode collectNode,
                                                              RowDownstream downstream,
                                                              JobCollectContext jobCollectContext) {
        String localNodeId = clusterService.state().nodes().localNodeId();

        final int numShards = numShards(collectNode, localNodeId);

        NodeSysReferenceResolver referenceResolver = new NodeSysReferenceResolver(nodeSysExpression);
        EvaluatingNormalizer nodeNormalizer = new EvaluatingNormalizer(functions,
                RowGranularity.NODE,
                referenceResolver);
        CollectNode normalizedCollectNode = collectNode.normalize(nodeNormalizer);

        if (normalizedCollectNode.whereClause().noMatch()) {
            downstream.registerUpstream(this).finish();
            return IMMEDIATE_LIST;
        }

        assert normalizedCollectNode.jobId() != null : "jobId must be set on CollectNode";

        ImplementationSymbolVisitor implementationSymbolVisitor = new ImplementationSymbolVisitor(
                referenceResolver,
                functions,
                RowGranularity.NODE
        );
        ProjectionToProjectorVisitor projectorVisitor = new ProjectionToProjectorVisitor(
                clusterService,
                threadPool,
                settings,
                transportActionProvider,
                bulkRetryCoordinatorPool,
                implementationSymbolVisitor
        );

        ShardProjectorChain projectorChain = new ShardProjectorChain(
                collectNode.jobId(),
                numShards,
                normalizedCollectNode.projections(),
                downstream,
                projectorVisitor,
                jobCollectContext.ramAccountingContext()
        );
        TableUnknownException lastException = null;
        int jobSearchContextId = normalizedCollectNode.routing().jobSearchContextIdBase();

        final List<CrateCollector> shardCollectors = new ArrayList<>(numShards);
        for (Map.Entry<String, Map<String, List<Integer>>> nodeEntry : normalizedCollectNode.routing().locations().entrySet()) {
            if (nodeEntry.getKey().equals(localNodeId)) {
                Map<String, List<Integer>> shardIdMap = nodeEntry.getValue();
                for (Map.Entry<String, List<Integer>> entry : shardIdMap.entrySet()) {
                    String indexName = entry.getKey();
                    IndexService indexService;
                    try {
                        indexService = indicesService.indexServiceSafe(indexName);
                    } catch (IndexMissingException e) {
                        lastException = new TableUnknownException(entry.getKey(), e);
                        continue;
                    }

                    for (Integer shardId : entry.getValue()) {
                        Injector shardInjector;
                        try {
                            shardInjector = indexService.shardInjectorSafe(shardId);
                            ShardCollectService shardCollectService = shardInjector.getInstance(ShardCollectService.class);
                            CrateCollector collector = shardCollectService.getCollector(
                                    normalizedCollectNode,
                                    projectorChain,
                                    jobCollectContext,
                                    jobSearchContextId
                            );
                            shardCollectors.add(collector);
                        } catch (IndexShardMissingException e) {
                            throw new UnhandledServerException(
                                    String.format(Locale.ENGLISH, "unknown shard id %d on index '%s'",
                                            shardId, entry.getKey()), e);
                        } catch (CancellationException e) {
                            throw e;
                        } catch (Exception e) {
                            LOGGER.error("Error while getting collector", e);
                            throw new UnhandledServerException(e);
                        }
                        jobSearchContextId++;
                    }
                }
            } else if (TableInfo.NULL_NODE_ID.equals(nodeEntry.getKey()) && localNodeId.equals(collectNode.handlerSideCollect())) {

                LOGGER.trace("collecting unassigned shards on node {}", localNodeId);
                EvaluatingNormalizer clusterNormalizer = new EvaluatingNormalizer(functions,
                        RowGranularity.CLUSTER,
                        referenceResolver);
                CollectNode clusterNormalizedCollectNode = collectNode.normalize(clusterNormalizer);

                RowDownstream projectorChainDownstream = projectorChain.newShardDownstreamProjector(projectorVisitor);
                CrateCollector collector = unassignedShardsCollectService.getCollector(
                        clusterNormalizedCollectNode,
                        projectorChainDownstream
                );
                shardCollectors.add(collector);
            } else if (jobSearchContextId > -1) {

                for (List<Integer> shardIdMap : nodeEntry.getValue().values()) {
                    jobSearchContextId += shardIdMap.size();
                }
            }
        }
        assert shardCollectors.size() == numShards : "invalid number of shardcollectors";

        if (lastException != null
                && jobSearchContextId == collectNode.routing().jobSearchContextIdBase()) {

            throw lastException;
        }


        projectorChain.startProjections(jobCollectContext);
        try {
            LOGGER.trace("starting {} shardCollectors...", numShards);
            return runCollectThreaded(collectNode, shardCollectors, jobCollectContext);
        } catch (RejectedExecutionException e) {



            downstream.registerUpstream(this).fail(e);
            return Futures.immediateFailedFuture(e);
        }

    }

    private ListenableFuture<List<Void>> runCollectThreaded(CollectNode collectNode,
                                                            final List<CrateCollector> shardCollectors,
                                                            final JobCollectContext jobCollectContext) throws RejectedExecutionException {
        if (collectNode.maxRowGranularity() == RowGranularity.SHARD) {


            return listeningExecutorService.submit(new Callable<List<Void>>() {
                @Override
                public List<Void> call() throws Exception {
                    for (CrateCollector collector : shardCollectors) {
                        collector.doCollect(jobCollectContext);
                    }
                    return ONE_LIST;
                }
            });
        } else {
            return ThreadPools.runWithAvailableThreads(
                    executor,
                    poolSize,
                    collectors2Callables(shardCollectors, jobCollectContext),
                    new VoidFunction<List<Void>>());
        }
    }

    private Collection<Callable<Void>> collectors2Callables(List<CrateCollector> collectors,
                                                            final JobCollectContext jobCollectContext) {
        return Lists.transform(collectors, new Function<CrateCollector, Callable<Void>>() {

            @Override
            public Callable<Void> apply(final CrateCollector collector) {
                return new Callable<Void>() {
                    @Override
                    public Void call() throws Exception {
                        collector.doCollect(jobCollectContext);
                        return null;
                    }
                };
            }
        });
    }

    private static class OneRowCollectService implements CollectService {

        private final ImplementationSymbolVisitor clusterImplementationSymbolVisitor;

        private OneRowCollectService(ImplementationSymbolVisitor clusterImplementationSymbolVisitor) {
            this.clusterImplementationSymbolVisitor = clusterImplementationSymbolVisitor;
        }

        @Override
        public CrateCollector getCollector(CollectNode node, RowDownstream downstream) {

            ImplementationSymbolVisitor.Context ctx = clusterImplementationSymbolVisitor.extractImplementations(node);
            List<Input<?>> inputs = ctx.topLevelInputs();
            Set<CollectExpression<?>> collectExpressions = ctx.collectExpressions();
            return new SimpleOneRowCollector(inputs, collectExpressions, downstream);
        }
    }
}

<code block>


package io.crate.operation.collect;

import com.carrotsearch.hppc.IntObjectOpenHashMap;
import com.carrotsearch.hppc.cursors.IntObjectCursor;
import io.crate.breaker.RamAccountingContext;
import io.crate.jobs.ContextCallback;
import io.crate.jobs.ExecutionState;
import io.crate.jobs.ExecutionSubContext;
import io.crate.operation.RowDownstream;
import io.crate.operation.RowDownstreamHandle;
import io.crate.operation.RowUpstream;
import io.crate.planner.node.dql.CollectNode;
import org.elasticsearch.common.logging.ESLogger;
import org.elasticsearch.common.logging.Loggers;
import org.elasticsearch.index.engine.Engine;
import org.elasticsearch.index.shard.IndexShard;
import org.elasticsearch.index.shard.ShardId;

import javax.annotation.Nullable;
import java.util.ArrayList;
import java.util.Iterator;
import java.util.Locale;
import java.util.UUID;
import java.util.concurrent.CancellationException;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ConcurrentMap;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicInteger;

public class JobCollectContext implements ExecutionSubContext, RowUpstream, ExecutionState {

    private final UUID id;
    private final CollectNode collectNode;
    private final CollectOperation collectOperation;
    private final RamAccountingContext ramAccountingContext;
    private final RowDownstream downstream;

    private final IntObjectOpenHashMap<JobQueryShardContext> queryContexts = new IntObjectOpenHashMap<>();
    private final IntObjectOpenHashMap<JobFetchShardContext> fetchContexts = new IntObjectOpenHashMap<>();
    private final ConcurrentMap<ShardId, EngineSearcherDelegate> shardsSearcherMap = new ConcurrentHashMap<>();
    private final AtomicInteger activeQueryContexts = new AtomicInteger(0);
    private final AtomicInteger activeFetchContexts = new AtomicInteger(0);
    private final Object subContextLock = new Object();

    private final AtomicBoolean closed = new AtomicBoolean(false);
    private final ArrayList<ContextCallback> contextCallbacks = new ArrayList<>(1);

    private volatile boolean isKilled = false;
    private long usedBytesOfQueryPhase = 0L;

    private static final ESLogger LOGGER = Loggers.getLogger(JobCollectContext.class);

    public JobCollectContext(UUID jobId,
                             CollectNode collectNode,
                             CollectOperation collectOperation,
                             RamAccountingContext ramAccountingContext,
                             RowDownstream downstream) {
        id = jobId;
        this.collectNode = collectNode;
        this.collectOperation = collectOperation;
        this.ramAccountingContext = ramAccountingContext;
        this.downstream = downstream;
    }

    @Override
    public void addCallback(ContextCallback contextCallback) {
        assert !closed.get() : "may not add a callback on a closed context";
        contextCallbacks.add(contextCallback);
    }

    public void addContext(int jobSearchContextId, JobQueryShardContext shardQueryContext) {
        interruptIfKilled();
        if (closed.get()) {
            throw new IllegalStateException("context already closed");
        }
        synchronized (subContextLock) {
            if (queryContexts.put(jobSearchContextId, shardQueryContext) != null) {
                throw new IllegalArgumentException(String.format(Locale.ENGLISH,
                        "ExecutionSubContext for %d already added", jobSearchContextId));
            }
        }
        int numActive = activeQueryContexts.incrementAndGet();
        LOGGER.trace("adding query subContext {}, now there are {} query subContexts", jobSearchContextId, numActive);

        shardQueryContext.addCallback(new RemoveQueryContextCallback(jobSearchContextId));
        EngineSearcherDelegate searcherDelegate;
        try {
            searcherDelegate = acquireSearcher(shardQueryContext.indexShard());
            shardQueryContext.searcher(searcherDelegate);
        } catch (Exception e) {

            shardQueryContext.close();
            throw e;
        }

        if (collectNode.keepContextForFetcher()) {
            JobFetchShardContext shardFetchContext = new JobFetchShardContext(
                    searcherDelegate,
                    shardQueryContext.searchContext());
            synchronized (subContextLock) {
                fetchContexts.put(jobSearchContextId, shardFetchContext);
            }
            shardFetchContext.addCallback(new RemoveFetchContextCallback(jobSearchContextId));

            int numActiveFetch = activeFetchContexts.incrementAndGet();
            LOGGER.trace("adding fetch subContext {}, now there are {} fetch subContexts", jobSearchContextId, numActiveFetch);
        }
    }


    @Nullable
    public JobFetchShardContext getFetchContext(int jobSearchContextId) {
        synchronized (subContextLock) {
            return fetchContexts.get(jobSearchContextId);
        }
    }

    @Override
    public void close() {
        if (closed.compareAndSet(false, true)) { 
            synchronized (subContextLock) {
                if (queryContexts.size() != 0 || fetchContexts.size() != 0) {
                    LOGGER.trace("closing query subContexts {}", id);
                    Iterator<IntObjectCursor<JobQueryShardContext>> queryIterator = queryContexts.iterator();
                    while (queryIterator.hasNext()) {
                        queryIterator.next().value.close();
                    }
                    LOGGER.trace("closing fetch subContexts {}", id);
                    Iterator<IntObjectCursor<JobFetchShardContext>> fetchIterator = fetchContexts.iterator();
                    while (fetchIterator.hasNext()) {
                        fetchIterator.next().value.close();
                    }
                } else {
                    callContextCallback();
                }
            }
            ramAccountingContext.close();
        } else {
            LOGGER.trace("close called on an already closed JobCollectContext: {}", id);
        }
    }

    @Override
    public void kill() {
        isKilled = true;
        if (closed.compareAndSet(false, true)) { 
            synchronized (subContextLock) {
                if (queryContexts.size() != 0 || fetchContexts.size() != 0) {
                    LOGGER.trace("killing query subContexts {}", id);
                    Iterator<IntObjectCursor<JobQueryShardContext>> queryIterator = queryContexts.iterator();
                    while (queryIterator.hasNext()) {
                        IntObjectCursor<JobQueryShardContext> cursor = queryIterator.next();
                        cursor.value.kill();
                    }
                    LOGGER.trace("killing fetch subContexts {}", id);
                    Iterator<IntObjectCursor<JobFetchShardContext>> fetchIterator = fetchContexts.iterator();
                    while (fetchIterator.hasNext()) {
                        fetchIterator.next().value.kill();
                    }
                } else {
                    callContextCallback();
                }
            }
            ramAccountingContext.close();
        } else {
            LOGGER.trace("killed called on an already closed JobCollectContext: {}", id);
        }
    }

    @Override
    public String name() {
        return collectNode.name();
    }

    @Override
    public void start() {
        startQueryPhase();
    }

    protected void startQueryPhase() {
        try {
            collectOperation.collect(collectNode, downstream, this);
        } catch (Throwable t) {
            RowDownstreamHandle rowDownstreamHandle = downstream.registerUpstream(this);
            rowDownstreamHandle.fail(t);
            close();
        }
    }

    @Override
    public boolean isKilled() {
        return isKilled;
    }

    public void interruptIfKilled() {
        if (isKilled) {
            throw new CancellationException();
        }
    }

    public RamAccountingContext ramAccountingContext() {
        return ramAccountingContext;
    }

    private void callContextCallback() {
        if (contextCallbacks.isEmpty()) {
            return;
        }
        if (activeQueryContexts.get() == 0 && activeFetchContexts.get() == 0) {
            for (ContextCallback contextCallback : contextCallbacks) {
                contextCallback.onClose(null, usedBytesOfQueryPhase);
            }
        }
    }


    protected EngineSearcherDelegate acquireSearcher(IndexShard indexShard) {
        EngineSearcherDelegate engineSearcherDelegate;
        for (;;) {
            engineSearcherDelegate = shardsSearcherMap.get(indexShard.shardId());
            if (engineSearcherDelegate == null) {
                engineSearcherDelegate = new EngineSearcherDelegate(acquireNewSearcher(indexShard));
                if (shardsSearcherMap.putIfAbsent(indexShard.shardId(), engineSearcherDelegate) == null) {
                    return engineSearcherDelegate;
                }
            } else {
                return engineSearcherDelegate;
            }
        }
    }


    protected Engine.Searcher acquireNewSearcher(IndexShard indexShard) {
        return EngineSearcher.getSearcherWithRetry(indexShard, "search", null);
    }


    class RemoveQueryContextCallback implements ContextCallback {

        private final int jobSearchContextId;

        public RemoveQueryContextCallback(int jobSearchContextId) {
            this.jobSearchContextId = jobSearchContextId;
        }

        @Override
        public void onClose(@Nullable Throwable error, long bytesUsed) {
            if (LOGGER.isTraceEnabled()) {
                LOGGER.trace("[{}] Closing query subContext {}",
                        System.identityHashCode(queryContexts), jobSearchContextId);
            }

            JobQueryShardContext remove;
            synchronized (subContextLock) {
                remove = queryContexts.remove(jobSearchContextId);
            }
            int remaining;
            if (remove == null) {
                LOGGER.trace("Closed query context {} which was already closed.", jobSearchContextId);
                remaining = activeQueryContexts.get();
            } else {
                remaining = activeQueryContexts.decrementAndGet();

                if (collectNode.keepContextForFetcher()
                        && (remove.collector() == null || !remove.collector().producedRows() || remove.collector().failed())) {


                    JobFetchShardContext fetchShardContext;
                    synchronized (subContextLock) {
                        fetchShardContext = fetchContexts.get(jobSearchContextId);
                    }
                    if (fetchShardContext != null) {
                        fetchShardContext.close();
                    }
                }
            }


            if (remaining == 0) {
                usedBytesOfQueryPhase = ramAccountingContext.totalBytes();
                ramAccountingContext.close();
                callContextCallback();
            }
        }
    }

    class RemoveFetchContextCallback implements ContextCallback {

        private final int jobSearchContextId;

        public RemoveFetchContextCallback(int jobSearchContextId) {
            this.jobSearchContextId = jobSearchContextId;
        }

        @Override
        public void onClose(@Nullable Throwable error, long bytesUsed) {
            if (LOGGER.isTraceEnabled()) {
                LOGGER.trace("[{}] Closing fetch subContext {}",
                        System.identityHashCode(fetchContexts), jobSearchContextId);
            }

            JobFetchShardContext remove;
            synchronized (subContextLock) {
                remove = fetchContexts.remove(jobSearchContextId);
            }
            int remaining;
            if (remove == null) {
                LOGGER.trace("Closed fetch context {} which was already closed.", jobSearchContextId);
                remaining = activeFetchContexts.get();
            } else {
                remaining = activeFetchContexts.decrementAndGet();
            }
            if (remaining == 0) {
                callContextCallback();
            }
        }
    }

}

<code block>


package io.crate.jobs;

import com.google.common.base.Optional;
import io.crate.breaker.RamAccountingContext;
import io.crate.operation.PageDownstream;
import io.crate.operation.PageDownstreamFactory;
import io.crate.operation.RowDownstream;
import io.crate.operation.join.NestedLoopOperation;
import io.crate.operation.projectors.FlatProjectorChain;
import io.crate.planner.node.StreamerVisitor;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.dql.join.NestedLoopNode;
import org.elasticsearch.common.collect.Tuple;
import org.elasticsearch.common.logging.ESLogger;
import org.elasticsearch.common.logging.Loggers;
import org.elasticsearch.threadpool.ThreadPool;

import javax.annotation.Nullable;
import java.util.ArrayList;
import java.util.concurrent.CancellationException;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicInteger;

public class NestedLoopContext implements DownstreamExecutionSubContext, ExecutionState {

    private static final ESLogger LOGGER = Loggers.getLogger(NestedLoopContext.class);

    private final AtomicBoolean closed = new AtomicBoolean(false);
    private final AtomicBoolean killed = new AtomicBoolean(false);
    private final AtomicInteger activeSubContexts = new AtomicInteger(0);
    private final ArrayList<ContextCallback> contextCallbacks = new ArrayList<>(1);
    private final PageDownstreamContext leftDownstreamContext;
    private final PageDownstreamContext rightDownstreamContext;

    private final NestedLoopNode nestedLoopNode;
    private final RamAccountingContext ramAccountingContext;
    private final PageDownstreamFactory pageDownstreamFactory;
    private final ThreadPool threadPool;
    private final StreamerVisitor streamerVisitor;

    private final NestedLoopOperation nestedLoopOperation;

    public NestedLoopContext(NestedLoopNode nestedLoopNode,
                             RowDownstream downstream,
                             RamAccountingContext ramAccountingContext,
                             PageDownstreamFactory pageDownstreamFactory,
                             ThreadPool threadPool,
                             StreamerVisitor streamerVisitor) {
        this.nestedLoopNode = nestedLoopNode;
        this.ramAccountingContext = ramAccountingContext;
        this.pageDownstreamFactory = pageDownstreamFactory;
        this.threadPool = threadPool;
        this.streamerVisitor = streamerVisitor;

        nestedLoopOperation = new NestedLoopOperation();
        nestedLoopOperation.downstream(downstream);


        if (nestedLoopNode.leftMergeNode() != null) {
            leftDownstreamContext = createPageDownstreamContext(nestedLoopNode.leftMergeNode());
            leftDownstreamContext.addCallback(new RemoveContextCallback(0));
            activeSubContexts.incrementAndGet();
        } else {
            leftDownstreamContext = null;
        }

        if (nestedLoopNode.rightMergeNode() != null) {
            rightDownstreamContext = createPageDownstreamContext(nestedLoopNode.rightMergeNode());
            rightDownstreamContext.addCallback(new RemoveContextCallback(1));
            activeSubContexts.incrementAndGet();
        } else {
            rightDownstreamContext = null;
        }

    }


    @Override
    public void addCallback(ContextCallback contextCallback) {
        contextCallbacks.add(contextCallback);
    }

    @Override
    public void start() {
        if (leftDownstreamContext != null) {
            leftDownstreamContext.start();
        }
        if (rightDownstreamContext != null) {
            rightDownstreamContext.start();
        }
    }

    @Override
    public void close() {
        if (!closed.getAndSet(true)) {
            if (activeSubContexts.get() == 0) {
                callContextCallback();
            } else {
                if (leftDownstreamContext != null) {
                    leftDownstreamContext.close();
                }
                if (rightDownstreamContext != null) {
                    rightDownstreamContext.close();
                }
            }
        }
    }

    @Override
    public void kill() {
        if (!killed.getAndSet(true)) {
            if (activeSubContexts.get() == 0) {
                callContextCallback();
            } else {
                if (leftDownstreamContext != null) {
                    leftDownstreamContext.kill();
                }
                if (rightDownstreamContext != null) {
                    rightDownstreamContext.kill();
                }
            }
        }
    }

    @Override
    public String name() {
        return nestedLoopNode.name();
    }

    @Override
    public boolean isKilled() {
        return killed.get();
    }

    @Override
    public PageDownstreamContext pageDownstreamContext(byte inputId) {
        assert inputId < 2 : "Only 0 and 1 inputId's supported";
        if (inputId == 0) {
            return leftDownstreamContext;
        }
        return rightDownstreamContext;
    }

    private void callContextCallback() {
        if (contextCallbacks.isEmpty()) {
            return;
        }
        if (activeSubContexts.get() == 0) {
            for (ContextCallback contextCallback : contextCallbacks) {
                contextCallback.onClose(null, ramAccountingContext.totalBytes());
            }
            ramAccountingContext.close();
        }
    }

    public void interruptIfKilled() {
        if (killed.get()) {
            throw new CancellationException();
        }
    }

    private PageDownstreamContext createPageDownstreamContext(MergeNode node) {
        Tuple<PageDownstream, FlatProjectorChain> pageDownstreamProjectorChain =
                pageDownstreamFactory.createMergeNodePageDownstream(
                        node,
                        nestedLoopOperation,
                        ramAccountingContext,
                        Optional.of(threadPool.executor(ThreadPool.Names.SEARCH)));
        StreamerVisitor.Context streamerContext = streamerVisitor.processPlanNode(node);
        PageDownstreamContext pageDownstreamContext = new PageDownstreamContext(
                node.name(),
                pageDownstreamProjectorChain.v1(),
                streamerContext.inputStreamers(),
                ramAccountingContext,
                node.numUpstreams());

        FlatProjectorChain flatProjectorChain = pageDownstreamProjectorChain.v2();
        if (flatProjectorChain != null) {
            flatProjectorChain.startProjections(pageDownstreamContext);
        }

        return pageDownstreamContext;
    }

    private class RemoveContextCallback implements ContextCallback {

        private final int inputId;

        public RemoveContextCallback(int inputId) {
            this.inputId = inputId;
        }

        @Override
        public void onClose(@Nullable Throwable error, long bytesUsed) {
            if (LOGGER.isTraceEnabled()) {
                LOGGER.trace("calling removal listener of subContext {}", inputId);
            }
            activeSubContexts.decrementAndGet();
            callContextCallback();
        }
    }

}

<code block>


package io.crate.jobs;

import com.google.common.util.concurrent.SettableFuture;
import io.crate.Streamer;
import io.crate.breaker.RamAccountingContext;
import io.crate.core.collections.Bucket;
import io.crate.core.collections.BucketPage;
import io.crate.operation.PageConsumeListener;
import io.crate.operation.PageDownstream;
import io.crate.operation.PageResultListener;
import org.elasticsearch.common.logging.ESLogger;
import org.elasticsearch.common.logging.Loggers;

import java.util.ArrayList;
import java.util.BitSet;
import java.util.concurrent.CancellationException;
import java.util.concurrent.atomic.AtomicBoolean;

public class PageDownstreamContext implements DownstreamExecutionSubContext, ExecutionState {

    private static final ESLogger LOGGER = Loggers.getLogger(PageDownstreamContext.class);

    private final Object lock = new Object();
    private String name;
    private final PageDownstream pageDownstream;
    private final Streamer<?>[] streamer;
    private final RamAccountingContext ramAccountingContext;
    private final int numBuckets;
    private final ArrayList<SettableFuture<Bucket>> bucketFutures;
    private final BitSet allFuturesSet;
    private final BitSet exhausted;
    private final ArrayList<PageResultListener> listeners = new ArrayList<>();
    private final ArrayList<ContextCallback> callbacks = new ArrayList<>(1);
    private final AtomicBoolean closed = new AtomicBoolean(false);
    private volatile boolean isKilled = false;


    public PageDownstreamContext(String name,
                                 PageDownstream pageDownstream,
                                 Streamer<?>[] streamer,
                                 RamAccountingContext ramAccountingContext,
                                 int numBuckets) {
        this.name = name;
        this.pageDownstream = pageDownstream;
        this.streamer = streamer;
        this.ramAccountingContext = ramAccountingContext;
        this.numBuckets = numBuckets;
        bucketFutures = new ArrayList<>(numBuckets);
        allFuturesSet = new BitSet(numBuckets);
        exhausted = new BitSet(numBuckets);
        initBucketFutures();
    }

    private void initBucketFutures() {
        bucketFutures.clear();
        for (int i = 0; i < numBuckets; i++) {
            bucketFutures.add(SettableFuture.<Bucket>create());
        }
    }

    private boolean pageEmpty() {
        return allFuturesSet.cardinality() == 0;
    }

    private boolean allExhausted() {
        return exhausted.cardinality() == numBuckets;
    }

    private boolean isExhausted(int bucketIdx) {
        return exhausted.get(bucketIdx);
    }

    public void setBucket(int bucketIdx, Bucket rows, boolean isLast, PageResultListener pageResultListener) {
        synchronized (listeners) {
            listeners.add(pageResultListener);
        }
        synchronized (lock) {
            LOGGER.trace("setBucket: {}", bucketIdx);
            if (allFuturesSet.get(bucketIdx)) {
                pageDownstream.fail(new IllegalStateException("May not set the same bucket of a page more than once"));
                return;
            }

            if (pageEmpty()) {
                LOGGER.trace("calling nextPage");
                pageDownstream.nextPage(new BucketPage(bucketFutures), new ResultListenerBridgingConsumeListener());
            }
            setExhaustedUpstreams();

            if (isLast) {
                exhausted.set(bucketIdx);
            }
            bucketFutures.get(bucketIdx).set(rows);
            allFuturesSet.set(bucketIdx);

            clearPageIfFull();
        }
    }

    public synchronized void failure(int bucketIdx, Throwable throwable) {


        synchronized (lock) {
            LOGGER.trace("failure: bucket: {} {}", bucketIdx, throwable);
            if (allFuturesSet.get(bucketIdx)) {
                pageDownstream.fail(new IllegalStateException("May not set the same bucket %d of a page more than once"));
                return;
            }
            if (pageEmpty()) {
                LOGGER.trace("calling nextPage");
                pageDownstream.nextPage(new BucketPage(bucketFutures), new ResultListenerBridgingConsumeListener());
            }
            setExhaustedUpstreams();

            LOGGER.trace("failure: {}", bucketIdx);
            exhausted.set(bucketIdx);
            bucketFutures.get(bucketIdx).setException(throwable);
            allFuturesSet.set(bucketIdx);
            clearPageIfFull();
        }
    }

    private void clearPageIfFull() {
        if (allFuturesSet.cardinality() == numBuckets) {
            allFuturesSet.clear();
            initBucketFutures();
        }
    }


    private void setExhaustedUpstreams() {
        for (int i = 0; i < exhausted.size(); i++) {
            if (exhausted.get(i)) {
                bucketFutures.get(i).set(Bucket.EMPTY);
                allFuturesSet.set(i);
            }
        }
    }

    public Streamer<?>[] streamer() {
        return streamer;
    }

    public void finish() {
        LOGGER.trace("calling finish on pageDownstream {}", pageDownstream);
        if (!closed.getAndSet(true)) {
            for (ContextCallback contextCallback : callbacks) {
                contextCallback.onClose(null, -1L);
            }
            pageDownstream.finish();
            ramAccountingContext.close();
        } else {
            LOGGER.warn("called finish on an already closed PageDownstreamContext");
        }
    }

    public void addCallback(ContextCallback contextCallback) {
        assert !closed.get() : "may not add a callback on a closed context";
        callbacks.add(contextCallback);
    }

    @Override
    public void start() {

    }

    @Override
    public void close() {
        finish();
    }

    @Override
    public void kill() {
        isKilled = true;
        if (!closed.getAndSet(true)) {
            CancellationException cancellationException = new CancellationException();
            for (ContextCallback contextCallback : callbacks) {
                contextCallback.onClose(cancellationException, -1L);
            }
            pageDownstream.fail(cancellationException);
            ramAccountingContext.close();
        } else {
            LOGGER.warn("called kill on an already closed PageDownstreamContext");
        }
    }

    @Override
    public String name() {
        return name;
    }

    @Override
    public PageDownstreamContext pageDownstreamContext(byte inputId) {
        assert inputId == 0 : "This downstream context only support 1 input";
        return this;
    }

    @Override
    public boolean isKilled() {
        return isKilled;
    }

    private class ResultListenerBridgingConsumeListener implements PageConsumeListener {

        @Override
        public void needMore() {
            boolean allExhausted = allExhausted();
            LOGGER.trace("allExhausted: {}", allExhausted);
            synchronized (listeners) {
                LOGGER.trace("calling needMore on all listeners({})", listeners.size());
                for (PageResultListener listener : listeners) {
                    if (allExhausted) {
                        listener.needMore(false);
                    } else {
                        listener.needMore(!isExhausted(listener.buckedIdx()));
                    }
                }
                listeners.clear();
            }
            if (allExhausted) {
                PageDownstreamContext.this.finish();
            }
        }

        @Override
        public void finish() {
            synchronized (listeners) {
                LOGGER.trace("calling finish() on all listeners({})", listeners.size());
                for (PageResultListener listener : listeners) {
                    listener.needMore(false);
                }
                listeners.clear();
                PageDownstreamContext.this.finish();
            }
        }
    }
}

<code block>


package io.crate.jobs;

import javax.annotation.Nullable;

public interface DownstreamExecutionSubContext extends ExecutionSubContext {

    @Nullable
    PageDownstreamContext pageDownstreamContext(byte inputId);
}

<code block>


package io.crate.action.job;

import com.google.common.base.Optional;
import com.google.common.util.concurrent.ListenableFuture;
import io.crate.Streamer;
import io.crate.breaker.CrateCircuitBreakerService;
import io.crate.breaker.RamAccountingContext;
import io.crate.core.collections.Bucket;
import io.crate.executor.transport.distributed.SingleBucketBuilder;
import io.crate.jobs.CountContext;
import io.crate.jobs.JobExecutionContext;
import io.crate.jobs.NestedLoopContext;
import io.crate.jobs.PageDownstreamContext;
import io.crate.operation.PageDownstream;
import io.crate.operation.PageDownstreamFactory;
import io.crate.operation.collect.JobCollectContext;
import io.crate.operation.collect.MapSideDataCollectOperation;
import io.crate.operation.count.CountOperation;
import io.crate.operation.projectors.FlatProjectorChain;
import io.crate.operation.projectors.ResultProvider;
import io.crate.operation.projectors.ResultProviderFactory;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.node.ExecutionNodeVisitor;
import io.crate.planner.node.ExecutionNodes;
import io.crate.planner.node.StreamerVisitor;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.CountNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.dql.join.NestedLoopNode;
import io.crate.types.DataTypes;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.common.breaker.CircuitBreaker;
import org.elasticsearch.common.collect.Tuple;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.inject.Singleton;
import org.elasticsearch.common.logging.ESLogger;
import org.elasticsearch.common.logging.Loggers;
import org.elasticsearch.threadpool.ThreadPool;

import javax.annotation.Nullable;
import java.util.List;
import java.util.Map;
import java.util.UUID;

@Singleton
public class ContextPreparer {

    private static final ESLogger LOGGER = Loggers.getLogger(ContextPreparer.class);

    private final MapSideDataCollectOperation collectOperation;
    private ClusterService clusterService;
    private CountOperation countOperation;
    private final CircuitBreaker circuitBreaker;
    private final ThreadPool threadPool;
    private final PageDownstreamFactory pageDownstreamFactory;
    private final ResultProviderFactory resultProviderFactory;
    private final StreamerVisitor streamerVisitor;
    private final InnerPreparer innerPreparer;

    @Inject
    public ContextPreparer(MapSideDataCollectOperation collectOperation,
                           ClusterService clusterService,
                           CrateCircuitBreakerService breakerService,
                           ThreadPool threadPool,
                           CountOperation countOperation,
                           PageDownstreamFactory pageDownstreamFactory,
                           ResultProviderFactory resultProviderFactory,
                           StreamerVisitor streamerVisitor) {
        this.collectOperation = collectOperation;
        this.clusterService = clusterService;
        this.countOperation = countOperation;
        circuitBreaker = breakerService.getBreaker(CrateCircuitBreakerService.QUERY_BREAKER);
        this.threadPool = threadPool;
        this.pageDownstreamFactory = pageDownstreamFactory;
        this.resultProviderFactory = resultProviderFactory;
        this.streamerVisitor = streamerVisitor;
        innerPreparer = new InnerPreparer();
    }

    @Nullable
    public ListenableFuture<Bucket> prepare(UUID jobId,
                                            ExecutionNode executionNode,
                                            JobExecutionContext.Builder contextBuilder) {
        PreparerContext preparerContext = new PreparerContext(jobId, contextBuilder);
        innerPreparer.process(executionNode, preparerContext);
        return preparerContext.directResultFuture;
    }

    private static class PreparerContext {

        private final UUID jobId;
        private final JobExecutionContext.Builder contextBuilder;
        private ListenableFuture<Bucket> directResultFuture;

        private PreparerContext(UUID jobId,
                                JobExecutionContext.Builder contextBuilder) {
            this.contextBuilder = contextBuilder;
            this.jobId = jobId;
        }
    }

    private class InnerPreparer extends ExecutionNodeVisitor<PreparerContext, Void> {

        @Override
        public Void visitCountNode(CountNode node, PreparerContext context) {
            Map<String, Map<String, List<Integer>>> locations = node.routing().locations();
            if (locations == null) {
                throw new IllegalArgumentException("locations are empty. Can't start count operation");
            }
            String localNodeId = clusterService.localNode().id();
            Map<String, List<Integer>> indexShardMap = locations.get(localNodeId);
            if (indexShardMap == null) {
                throw new IllegalArgumentException("The routing of the countNode doesn't contain the current nodeId");
            }

            final SingleBucketBuilder singleBucketBuilder = new SingleBucketBuilder(new Streamer[]{DataTypes.LONG});
            CountContext countContext = new CountContext(
                    countOperation,
                    singleBucketBuilder,
                    indexShardMap,
                    node.whereClause()
            );
            context.directResultFuture = singleBucketBuilder.result();
            context.contextBuilder.addSubContext(node.executionNodeId(), countContext);
            return null;
        }

        @Override
        public Void visitMergeNode(MergeNode node, PreparerContext context) {
            RamAccountingContext ramAccountingContext = RamAccountingContext.forExecutionNode(circuitBreaker, node);
            ResultProvider downstream = resultProviderFactory.createDownstream(node, node.jobId());
            Tuple<PageDownstream, FlatProjectorChain> pageDownstreamProjectorChain =
                    pageDownstreamFactory.createMergeNodePageDownstream(
                            node,
                            downstream,
                            ramAccountingContext,
                            Optional.of(threadPool.executor(ThreadPool.Names.SEARCH)));
            StreamerVisitor.Context streamerContext = streamerVisitor.processPlanNode(node);
            PageDownstreamContext pageDownstreamContext = new PageDownstreamContext(
                    node.name(),
                    pageDownstreamProjectorChain.v1(),
                    streamerContext.inputStreamers(),
                    ramAccountingContext,
                    node.numUpstreams());

            context.contextBuilder.addSubContext(node.executionNodeId(), pageDownstreamContext);

            FlatProjectorChain flatProjectorChain = pageDownstreamProjectorChain.v2();
            if (flatProjectorChain != null) {
                flatProjectorChain.startProjections(pageDownstreamContext);
            }
            return null;
        }

        @Override
        public Void visitCollectNode(CollectNode node, PreparerContext context) {
            RamAccountingContext ramAccountingContext = RamAccountingContext.forExecutionNode(circuitBreaker, node);
            ResultProvider downstream = resultProviderFactory.createDownstream(node, node.jobId());

            if (ExecutionNodes.hasDirectResponseDownstream(node.downstreamNodes())) {
                context.directResultFuture = downstream.result();
            }
            final JobCollectContext jobCollectContext = new JobCollectContext(
                    context.jobId,
                    node,
                    collectOperation,
                    ramAccountingContext,
                    downstream
            );
            context.contextBuilder.addSubContext(node.executionNodeId(), jobCollectContext);
            return null;
        }

        @Override
        public Void visitNestedLoopNode(NestedLoopNode node, PreparerContext context) {
            RamAccountingContext ramAccountingContext = RamAccountingContext.forExecutionNode(circuitBreaker, node);

            ResultProvider downstream = resultProviderFactory.createDownstream(node, node.jobId());

            NestedLoopContext nestedLoopContext = new NestedLoopContext(
                    node,
                    downstream,
                    ramAccountingContext,
                    pageDownstreamFactory,
                    threadPool,
                    streamerVisitor);

            context.contextBuilder.addSubContext(node.executionNodeId(), nestedLoopContext);
            return null;
        }
    }
}

<code block>


package io.crate.executor;

import java.util.ArrayList;
import java.util.Collection;
import java.util.List;
import java.util.UUID;

public class Job {

    private final UUID id;
    private List<Task> tasks = new ArrayList<>();

    public Job() {
        this(UUID.randomUUID());
    }

    public Job(UUID id) {
        this.id = id;
    }

    public UUID id() {
        return id;
    }

    public void addTasks(Collection<? extends Task> tasks) {
        this.tasks.addAll(tasks);
    }

    public List<Task> tasks() {
        return tasks;
    }
}

<code block>


package io.crate.executor.transport;

import com.google.common.base.Optional;
import com.google.common.util.concurrent.FutureCallback;
import com.google.common.util.concurrent.Futures;
import com.google.common.util.concurrent.ListenableFuture;
import com.google.common.util.concurrent.SettableFuture;
import io.crate.Streamer;
import io.crate.action.job.ContextPreparer;
import io.crate.action.job.JobRequest;
import io.crate.action.job.JobResponse;
import io.crate.action.job.TransportJobAction;
import io.crate.breaker.RamAccountingContext;
import io.crate.core.collections.Bucket;
import io.crate.executor.JobTask;
import io.crate.executor.TaskResult;
import io.crate.jobs.JobContextService;
import io.crate.jobs.JobExecutionContext;
import io.crate.jobs.PageDownstreamContext;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.*;
import io.crate.operation.projectors.FlatProjectorChain;
import io.crate.planner.node.*;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.MergeNode;
import org.elasticsearch.action.ActionListener;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.common.breaker.CircuitBreaker;
import org.elasticsearch.common.collect.Tuple;
import org.elasticsearch.common.logging.ESLogger;
import org.elasticsearch.common.logging.Loggers;
import org.elasticsearch.threadpool.ThreadPool;

import javax.annotation.Nonnull;
import javax.annotation.Nullable;
import java.util.*;


public class ExecutionNodesTask extends JobTask {

    private static final ESLogger LOGGER = Loggers.getLogger(ExecutionNodesTask.class);

    private final TransportJobAction transportJobAction;
    private final ClusterService clusterService;
    private ContextPreparer contextPreparer;
    private final JobContextService jobContextService;
    private final PageDownstreamFactory pageDownstreamFactory;
    private final ThreadPool threadPool;
    private TransportCloseContextNodeAction transportCloseContextNodeAction;
    private final StreamerVisitor streamerVisitor;
    private final CircuitBreaker circuitBreaker;

    private final List<List<ExecutionNode>> groupedExecutionNodes = new ArrayList<>();
    private final List<MergeNode> finalMergeNodes = new ArrayList<>();
    private final List<SettableFuture<TaskResult>> results = new ArrayList<>();
    private boolean hasDirectResponse;
    private boolean rowCountResult = false;

    protected ExecutionNodesTask(UUID jobId,
                                 ClusterService clusterService,
                                 ContextPreparer contextPreparer,
                                 JobContextService jobContextService,
                                 PageDownstreamFactory pageDownstreamFactory,
                                 ThreadPool threadPool,
                                 TransportJobAction transportJobAction,
                                 TransportCloseContextNodeAction transportCloseContextNodeAction,
                                 StreamerVisitor streamerVisitor,
                                 CircuitBreaker circuitBreaker) {
        super(jobId);
        this.clusterService = clusterService;
        this.contextPreparer = contextPreparer;
        this.jobContextService = jobContextService;
        this.pageDownstreamFactory = pageDownstreamFactory;
        this.threadPool = threadPool;
        this.transportCloseContextNodeAction = transportCloseContextNodeAction;
        this.streamerVisitor = streamerVisitor;
        this.circuitBreaker = circuitBreaker;
        this.transportJobAction = transportJobAction;
    }



    public void addFinalMergeNode(MergeNode finalMergeNode) {
        finalMergeNode.jobId(jobId());
        finalMergeNodes.add(finalMergeNode);
    }

    public void addExecutionNode(int group, ExecutionNode executionNode) {
        executionNode.jobId(jobId());
        while (group >= groupedExecutionNodes.size()) {
            results.add(SettableFuture.<TaskResult>create());
            groupedExecutionNodes.add(new ArrayList<ExecutionNode>());
        }
        List<ExecutionNode> executionNodes = groupedExecutionNodes.get(group);

        if (ExecutionNodes.hasDirectResponseDownstream(executionNode.downstreamNodes())) {
            hasDirectResponse = true;
        }
        executionNodes.add(executionNode);
    }

    public void rowCountResult(boolean rowCountResult) {
        this.rowCountResult = rowCountResult;
    }

    @Override
    public void start() {
        assert finalMergeNodes.size() == groupedExecutionNodes.size() : "groupedExecutionNodes and finalMergeNodes sizes must match";

        Map<String, Collection<ExecutionNode>> nodesByServer = ExecutionNodeGrouper.groupByServer(clusterService.state().nodes().localNodeId(), groupedExecutionNodes);
        RowDownstream rowDownstream;
        if (rowCountResult) {
            rowDownstream = new RowCountResultRowDownstream(results);
        } else {
            rowDownstream = new QueryResultRowDownstream(results);
        }
        Streamer<?>[] streamers = streamerVisitor.processExecutionNode(finalMergeNodes.get(0)).inputStreamers();
        List<PageDownstreamContext> pageDownstreamContexts = new ArrayList<>(groupedExecutionNodes.size());

        for (int i = 0; i < groupedExecutionNodes.size(); i++) {
            RamAccountingContext ramAccountingContext = RamAccountingContext.forExecutionNode(
                    circuitBreaker, finalMergeNodes.get(i));

            PageDownstreamContext pageDownstreamContext = createPageDownstreamContext(ramAccountingContext, streamers,
                    finalMergeNodes.get(i), groupedExecutionNodes.get(i), rowDownstream);
            if (nodesByServer.size() == 0) {
                pageDownstreamContext.finish();
                continue;
            }
            if (!hasDirectResponse) {
                createLocalContextAndStartOperation(pageDownstreamContext, nodesByServer, finalMergeNodes.get(i).executionNodeId());
            }
            pageDownstreamContexts.add(pageDownstreamContext);
        }
        if (nodesByServer.size() == 0) {
            return;
        }
        addCloseContextCallback(transportCloseContextNodeAction, groupedExecutionNodes, nodesByServer.keySet());
        sendJobRequests(streamers, pageDownstreamContexts, nodesByServer);
    }

    private PageDownstreamContext createPageDownstreamContext(
            RamAccountingContext ramAccountingContext,
            Streamer<?>[] streamers,
            MergeNode mergeNode,
            List<ExecutionNode> executionNodes,
            RowDownstream rowDownstream) {
        Tuple<PageDownstream, FlatProjectorChain> pageDownstreamProjectorChain = pageDownstreamFactory.createMergeNodePageDownstream(
                mergeNode,
                rowDownstream,
                ramAccountingContext,
                Optional.of(threadPool.executor(ThreadPool.Names.SEARCH))
        );
        PageDownstreamContext pageDownstreamContext = new PageDownstreamContext(
                mergeNode.name(),
                pageDownstreamProjectorChain.v1(),
                streamers,
                ramAccountingContext,
                executionNodes.get(executionNodes.size() - 1).executionNodes().size()
        );
        FlatProjectorChain flatProjectorChain = pageDownstreamProjectorChain.v2();
        if (flatProjectorChain != null) {
            flatProjectorChain.startProjections(pageDownstreamContext);
        }
        return pageDownstreamContext;
    }

    private void sendJobRequests(Streamer<?>[] streamers,
                                 List<PageDownstreamContext> pageDownstreamContexts,
                                 Map<String, Collection<ExecutionNode>> nodesByServer) {
        int idx = 0;
        for (Map.Entry<String, Collection<ExecutionNode>> entry : nodesByServer.entrySet()) {
            String serverNodeId = entry.getKey();
            if (TableInfo.NULL_NODE_ID.equals(serverNodeId)) {
                continue; 
            }
            Collection<ExecutionNode> executionNodes = entry.getValue();

            JobRequest request = new JobRequest(jobId(), executionNodes);
            if (hasDirectResponse) {
                transportJobAction.execute(serverNodeId, request,
                        new DirectResponseListener(idx, streamers, pageDownstreamContexts));
            } else {
                transportJobAction.execute(serverNodeId, request,
                        new FailureOnlyResponseListener(results));
            }
            idx++;
        }
    }


    private void createLocalContextAndStartOperation(PageDownstreamContext finalLocalMerge,
                                                     Map<String, Collection<ExecutionNode>> nodesByServer,
                                                     int localMergeExecutionNodeId) {
        String localNodeId = clusterService.localNode().id();
        Collection<ExecutionNode> localExecutionNodes = nodesByServer.remove(localNodeId);

        JobExecutionContext.Builder builder = jobContextService.newBuilder(jobId());
        builder.addSubContext(localMergeExecutionNodeId, finalLocalMerge);

        if (localExecutionNodes == null || localExecutionNodes.isEmpty()) {

            jobContextService.createContext(builder);
        } else {
            for (ExecutionNode executionNode : localExecutionNodes) {
                contextPreparer.prepare(jobId(), executionNode, builder);
            }
            JobExecutionContext context = jobContextService.createContext(builder);
            context.start();
        }
    }

    private void addCloseContextCallback(TransportCloseContextNodeAction transportCloseContextNodeAction,
                                         final List<List<ExecutionNode>> groupedExecutionNodes,
                                         final Set<String> server) {
        if (server.isEmpty()) {
            return;
        }
        final ContextCloser contextCloser = new ContextCloser(transportCloseContextNodeAction);
        Futures.addCallback(Futures.allAsList(results), new FutureCallback<List<TaskResult>>() {
            @Override
            public void onSuccess(@Nullable List<TaskResult> result) {

            }

            @Override
            public void onFailure(@Nonnull Throwable t) {

                for (List<ExecutionNode> executionNodeGroup : groupedExecutionNodes) {
                    for (ExecutionNode executionNode : executionNodeGroup) {
                        contextCloser.process(executionNode, server);
                    }
                }
            }
        });
    }

    @Override
    public List<? extends ListenableFuture<TaskResult>> result() {
        if (results.size() != groupedExecutionNodes.size()) {
            for (int i = 0; i < groupedExecutionNodes.size(); i++) {
                results.add(SettableFuture.<TaskResult>create());
            }
        }
        return results;
    }

    @Override
    public void upstreamResult(List<? extends ListenableFuture<TaskResult>> result) {
        throw new UnsupportedOperationException("ExecutionNodesTask doesn't support upstreamResult");
    }

    static boolean hasDirectResponse(List<List<ExecutionNode>> groupedExecutionNodes) {
        for (List<ExecutionNode> executionNodeGroup : groupedExecutionNodes) {
            for (ExecutionNode executionNode : executionNodeGroup) {
                if (ExecutionNodes.hasDirectResponseDownstream(executionNode.downstreamNodes())) {
                    return true;
                }
            }
        }
        return false;
    }

    private static class DirectResponseListener implements ActionListener<JobResponse> {

        private final int bucketIdx;
        private final Streamer<?>[] streamer;
        private final List<PageDownstreamContext> pageDownstreamContexts;

        public DirectResponseListener(int bucketIdx, Streamer<?>[] streamer, List<PageDownstreamContext> pageDownstreamContexts) {
            this.bucketIdx = bucketIdx;
            this.streamer = streamer;
            this.pageDownstreamContexts = pageDownstreamContexts;
        }

        @Override
        public void onResponse(JobResponse jobResponse) {
            jobResponse.streamers(streamer);
            for (int i = 0; i < pageDownstreamContexts.size(); i++) {
                PageDownstreamContext pageDownstreamContext = pageDownstreamContexts.get(i);
                Bucket bucket = jobResponse.directResponse().get(i);
                if (bucket == null) {
                    pageDownstreamContext.failure(bucketIdx, new IllegalStateException("expected directResponse but didn't get one"));
                }
                pageDownstreamContext.setBucket(bucketIdx, bucket, true, new PageResultListener() {
                    @Override
                    public void needMore(boolean needMore) {

                    }

                    @Override
                    public int buckedIdx() {
                        return bucketIdx;
                    }
                });
            }
        }

        @Override
        public void onFailure(Throwable e) {
            for (PageDownstreamContext pageDownstreamContext : pageDownstreamContexts) {
                pageDownstreamContext.failure(bucketIdx, e);
            }
        }
    }

    private static class FailureOnlyResponseListener implements ActionListener<JobResponse> {

        private final List<SettableFuture<TaskResult>> results;

        public FailureOnlyResponseListener(List<SettableFuture<TaskResult>> results) {
            this.results = results;
        }

        @Override
        public void onResponse(JobResponse jobResponse) {
            if (jobResponse.directResponse().size() > 0) {
                for (SettableFuture<TaskResult> result : results) {
                    result.setException(new IllegalStateException("Got a directResponse but didn't expect one"));
                }
            }
        }

        @Override
        public void onFailure(Throwable e) {

            LOGGER.warn(e.getMessage(), e);
        }
    }

    private static class ContextCloser extends ExecutionNodeVisitor<Set<String>, Void> {

        private final TransportCloseContextNodeAction transportCloseContextNodeAction;

        public ContextCloser(TransportCloseContextNodeAction transportCloseContextNodeAction) {
            this.transportCloseContextNodeAction = transportCloseContextNodeAction;
        }

        @Override
        public Void visitCollectNode(final CollectNode node, Set<String> nodeIds) {
            if (!node.keepContextForFetcher()) {
                return null;
            }

            LOGGER.trace("closing job context {} on {} nodes", node.jobId(), nodeIds.size());
            for (final String nodeId : nodeIds) {
                transportCloseContextNodeAction.execute(
                        nodeId,
                        new NodeCloseContextRequest(node.jobId(), node.executionNodeId()),
                        new ActionListener<NodeCloseContextResponse>() {

                    @Override
                    public void onResponse(NodeCloseContextResponse nodeCloseContextResponse) {
                    }

                    @Override
                    public void onFailure(Throwable e) {
                        LOGGER.warn("Closing job context {} failed on node {} with: {}", node.jobId(), nodeId, e.getMessage());
                    }
                });
            }
            return null;
        }
    }
}

<code block>


package io.crate.executor.transport;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.Iterables;
import com.google.common.util.concurrent.ListenableFuture;
import io.crate.action.job.ContextPreparer;
import io.crate.action.sql.DDLStatementDispatcher;
import io.crate.breaker.CrateCircuitBreakerService;
import io.crate.executor.*;
import io.crate.executor.task.DDLTask;
import io.crate.executor.task.NoopTask;
import io.crate.executor.transport.task.CreateTableTask;
import io.crate.executor.transport.task.DropTableTask;
import io.crate.executor.transport.task.KillTask;
import io.crate.executor.transport.task.SymbolBasedUpsertByIdTask;
import io.crate.executor.transport.task.elasticsearch.*;
import io.crate.jobs.JobContextService;
import io.crate.metadata.Functions;
import io.crate.metadata.ReferenceResolver;
import io.crate.operation.ImplementationSymbolVisitor;
import io.crate.operation.PageDownstreamFactory;
import io.crate.operation.projectors.ProjectionToProjectorVisitor;
import io.crate.planner.*;
import io.crate.planner.node.PlanNode;
import io.crate.planner.node.PlanNodeVisitor;
import io.crate.planner.node.StreamerVisitor;
import io.crate.planner.node.ddl.*;
import io.crate.planner.node.dml.*;
import io.crate.planner.node.dql.*;
import io.crate.planner.node.dql.join.NestedLoop;
import io.crate.planner.node.management.KillPlan;
import org.elasticsearch.action.bulk.BulkRetryCoordinatorPool;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.common.breaker.CircuitBreaker;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.inject.Provider;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.threadpool.ThreadPool;

import java.util.*;

public class TransportExecutor implements Executor, TaskExecutor {

    private final Functions functions;
    private final TaskCollectingVisitor planVisitor;
    private Provider<DDLStatementDispatcher> ddlAnalysisDispatcherProvider;
    private final NodeVisitor nodeVisitor;
    private final ThreadPool threadPool;

    private final ClusterService clusterService;
    private final JobContextService jobContextService;
    private final ContextPreparer contextPreparer;
    private final TransportActionProvider transportActionProvider;
    private final BulkRetryCoordinatorPool bulkRetryCoordinatorPool;

    private final ProjectionToProjectorVisitor globalProjectionToProjectionVisitor;


    private final CircuitBreaker circuitBreaker;

    private final PageDownstreamFactory pageDownstreamFactory;

    private final StreamerVisitor streamerVisitor;

    private final ExecutionNodesPlanVisitor executionNodesPlanVisitor;

    @Inject
    public TransportExecutor(Settings settings,
                             JobContextService jobContextService,
                             ContextPreparer contextPreparer,
                             TransportActionProvider transportActionProvider,
                             ThreadPool threadPool,
                             Functions functions,
                             ReferenceResolver referenceResolver,
                             PageDownstreamFactory pageDownstreamFactory,
                             Provider<DDLStatementDispatcher> ddlAnalysisDispatcherProvider,
                             ClusterService clusterService,
                             CrateCircuitBreakerService breakerService,
                             BulkRetryCoordinatorPool bulkRetryCoordinatorPool,
                             StreamerVisitor streamerVisitor) {
        this.jobContextService = jobContextService;
        this.contextPreparer = contextPreparer;
        this.transportActionProvider = transportActionProvider;
        this.pageDownstreamFactory = pageDownstreamFactory;
        this.threadPool = threadPool;
        this.functions = functions;
        this.ddlAnalysisDispatcherProvider = ddlAnalysisDispatcherProvider;
        this.clusterService = clusterService;
        this.bulkRetryCoordinatorPool = bulkRetryCoordinatorPool;
        this.streamerVisitor = streamerVisitor;
        nodeVisitor = new NodeVisitor();
        planVisitor = new TaskCollectingVisitor();
        circuitBreaker = breakerService.getBreaker(CrateCircuitBreakerService.QUERY_BREAKER);
        ImplementationSymbolVisitor globalImplementationSymbolVisitor = new ImplementationSymbolVisitor(
                referenceResolver, functions, RowGranularity.CLUSTER);
        globalProjectionToProjectionVisitor = new ProjectionToProjectorVisitor(
                clusterService,
                threadPool,
                settings,
                transportActionProvider,
                bulkRetryCoordinatorPool,
                globalImplementationSymbolVisitor);
        executionNodesPlanVisitor = new ExecutionNodesPlanVisitor();
    }

    @Override
    public Job newJob(Plan plan) {
        final Job job = new Job();
        List<? extends Task> tasks = planVisitor.process(plan, job);
        job.addTasks(tasks);
        return job;
    }

    @Override
    public List<? extends ListenableFuture<TaskResult>> execute(Job job) {
        assert job.tasks().size() > 0;
        return execute(job.tasks());

    }

    @Override
    public List<Task> newTasks(PlanNode planNode, UUID jobId) {
        return planNode.accept(nodeVisitor, jobId);
    }

    @Override
    public List<? extends ListenableFuture<TaskResult>> execute(Collection<Task> tasks) {
        Task lastTask = null;
        assert tasks.size() > 0 : "need at least one task to execute";
        for (Task task : tasks) {

            if (lastTask != null) {
                task.upstreamResult(lastTask.result());
            }
            task.start();
            lastTask = task;
        }
        assert lastTask != null;
        return lastTask.result();
    }

    class TaskCollectingVisitor extends PlanVisitor<Job, List<? extends Task>> {

        @Override
        public List<Task> visitIterablePlan(IterablePlan plan, Job job) {
            List<Task> tasks = new ArrayList<>();
            for (PlanNode planNode : plan) {
                tasks.addAll(planNode.accept(nodeVisitor, job.id()));
            }
            return tasks;
        }

        @Override
        public List<Task> visitNoopPlan(NoopPlan plan, Job job) {
            return ImmutableList.<Task>of(NoopTask.INSTANCE);
        }

        @Override
        protected List<? extends Task> visitPlan(Plan plan, Job job) {
            ExecutionNodesTask task = executionNodesPlanVisitor.process(plan, job);
            return ImmutableList.of(task);
        }

        @Override
        public List<? extends Task> visitUpsert(Upsert plan, Job job) {
            if (plan.nodes().size() == 1 && plan.nodes().get(0) instanceof IterablePlan) {
                return process(plan.nodes().get(0), job);
            }

            ExecutionNodesTask task = executionNodesPlanVisitor.process(plan, job);
            task.rowCountResult(true);
            return ImmutableList.<Task>of(task);
        }

        @Override
        public List<? extends Task> visitInsertByQuery(InsertFromSubQuery node, Job job) {
            List<? extends Task> tasks = process(node.innerPlan(), job);
            if (node.handlerMergeNode().isPresent()) {

                Task previousTask = Iterables.getLast(tasks);
                if (previousTask instanceof ExecutionNodesTask) {
                    ((ExecutionNodesTask) previousTask).addFinalMergeNode(node.handlerMergeNode().get());
                } else {
                    ArrayList<Task> tasks2 = new ArrayList<>(tasks);
                    tasks2.addAll(nodeVisitor.visitMergeNode(node.handlerMergeNode().get(), job.id()));
                    return tasks2;
                }
            }
            return tasks;
        }

        @Override
        public List<Task> visitKillPlan(KillPlan killPlan, Job job) {
            return ImmutableList.<Task>of(new KillTask(
                    clusterService,
                    transportActionProvider.transportKillAllNodeAction(),
                    job.id()));
        }

    }

    class NodeVisitor extends PlanNodeVisitor<UUID, ImmutableList<Task>> {

        private ImmutableList<Task> singleTask(Task task) {
            return ImmutableList.of(task);
        }

        @Override
        public ImmutableList<Task> visitGenericDDLNode(GenericDDLNode node, UUID jobId) {
            return singleTask(new DDLTask(jobId, ddlAnalysisDispatcherProvider.get(), node));
        }

        @Override
        public ImmutableList<Task> visitESGetNode(ESGetNode node, UUID jobId) {
            return singleTask(new ESGetTask(
                    jobId,
                    functions,
                    globalProjectionToProjectionVisitor,
                    transportActionProvider.transportMultiGetAction(),
                    transportActionProvider.transportGetAction(),
                    node,
                    jobContextService));
        }

        @Override
        public ImmutableList<Task> visitESDeleteByQueryNode(ESDeleteByQueryNode node, UUID jobId) {
            return singleTask(new ESDeleteByQueryTask(
                    jobId,
                    node,
                    transportActionProvider.transportDeleteByQueryAction(),
                    jobContextService));
        }

        @Override
        public ImmutableList<Task> visitESDeleteNode(ESDeleteNode node, UUID jobId) {
            return singleTask(new ESDeleteTask(
                    jobId,
                    node,
                    transportActionProvider.transportDeleteAction(),
                    jobContextService));
        }

        @Override
        public ImmutableList<Task> visitCreateTableNode(CreateTableNode node, UUID jobId) {
            return singleTask(new CreateTableTask(
                            jobId,
                            clusterService,
                            transportActionProvider.transportCreateIndexAction(),
                            transportActionProvider.transportDeleteIndexAction(),
                            transportActionProvider.transportPutIndexTemplateAction(),
                            node)
            );
        }

        @Override
        public ImmutableList<Task> visitESCreateTemplateNode(ESCreateTemplateNode node, UUID jobId) {
            return singleTask(new ESCreateTemplateTask(jobId,
                    node,
                    transportActionProvider.transportPutIndexTemplateAction()));
        }

        @Override
        public ImmutableList<Task> visitSymbolBasedUpsertByIdNode(SymbolBasedUpsertByIdNode node, UUID jobId) {
            return singleTask(new SymbolBasedUpsertByIdTask(jobId,
                    clusterService,
                    clusterService.state().metaData().settings(),
                    transportActionProvider.symbolBasedTransportShardUpsertActionDelegate(),
                    transportActionProvider.transportCreateIndexAction(),
                    transportActionProvider.transportBulkCreateIndicesAction(),
                    bulkRetryCoordinatorPool,
                    node,
                    jobContextService));
        }

        @Override
        public ImmutableList<Task> visitDropTableNode(DropTableNode node, UUID jobId) {
            return singleTask(new DropTableTask(jobId,
                    transportActionProvider.transportDeleteIndexTemplateAction(),
                    transportActionProvider.transportDeleteIndexAction(),
                    node));
        }

        @Override
        public ImmutableList<Task> visitESDeletePartitionNode(ESDeletePartitionNode node, UUID jobId) {
            return singleTask(new ESDeletePartitionTask(jobId,
                    transportActionProvider.transportDeleteIndexAction(),
                    node));
        }

        @Override
        public ImmutableList<Task> visitESClusterUpdateSettingsNode(ESClusterUpdateSettingsNode node, UUID jobId) {
            return singleTask(new ESClusterUpdateSettingsTask(
                    jobId,
                    transportActionProvider.transportClusterUpdateSettingsAction(),
                    node));
        }

        @Override
        protected ImmutableList<Task> visitPlanNode(PlanNode node, UUID jobId) {
            throw new UnsupportedOperationException(
                    String.format("Can't generate job/task for planNode %s", node));
        }
    }

    class ExecutionNodesPlanVisitor extends PlanVisitor<ExecutionNodesPlanVisitor.Context, Void> {

        class Context {
            boolean isRootPlan = true;
            ExecutionNodesTask executionNodesTask;

            public Context(ExecutionNodesTask executionNodesTask) {
                this.executionNodesTask = executionNodesTask;
            }
        }

        public ExecutionNodesTask process(Plan plan, Job job) {
            ExecutionNodesTask executionNodesTask = new ExecutionNodesTask(
                    job.id(),
                    clusterService,
                    contextPreparer,
                    jobContextService,
                    pageDownstreamFactory,
                    threadPool,
                    transportActionProvider.transportJobInitAction(),
                    transportActionProvider.transportCloseContextNodeAction(),
                    streamerVisitor,
                    circuitBreaker);
            Context context = new Context(executionNodesTask);
            process(plan, context);
            return executionNodesTask;
        }

        @Override
        public Void visitQueryThenFetch(QueryThenFetch plan, Context context) {
            context.executionNodesTask.addExecutionNode(0, plan.collectNode());
            if (context.isRootPlan && plan.mergeNode() != null) {
                context.executionNodesTask.addFinalMergeNode(plan.mergeNode());
            }
            return null;
        }

        @Override
        public Void visitQueryAndFetch(QueryAndFetch plan, Context context) {
            context.executionNodesTask.addExecutionNode(0, plan.collectNode());
            if (context.isRootPlan && plan.localMergeNode() != null) {
                context.executionNodesTask.addFinalMergeNode(plan.localMergeNode());
            }
            return null;
        }

        @Override
        public Void visitDistributedGroupBy(DistributedGroupBy plan, Context context) {
            context.executionNodesTask.addExecutionNode(0, plan.collectNode());
            context.executionNodesTask.addExecutionNode(0, plan.reducerMergeNode());
            if (context.isRootPlan && plan.localMergeNode() != null) {
                context.executionNodesTask.addFinalMergeNode(plan.localMergeNode());
            }
            return null;
        }

        @Override
        public Void visitNonDistributedGroupBy(NonDistributedGroupBy plan, Context context) {
            context.executionNodesTask.addExecutionNode(0, plan.collectNode());
            if (context.isRootPlan && plan.localMergeNode() != null) {
                context.executionNodesTask.addFinalMergeNode(plan.localMergeNode());
            }
            return null;
        }

        @Override
        public Void visitGlobalAggregate(GlobalAggregate plan, Context context) {
            context.executionNodesTask.addExecutionNode(0, plan.collectNode());
            if (context.isRootPlan && plan.mergeNode() != null) {
                context.executionNodesTask.addFinalMergeNode(plan.mergeNode());
            }
            return null;
        }

        @Override
        public Void visitCollectAndMerge(CollectAndMerge plan, Context context) {
            context.executionNodesTask.addExecutionNode(0, plan.collectNode());
            if (context.isRootPlan && plan.localMergeNode() != null) {
                context.executionNodesTask.addFinalMergeNode(plan.localMergeNode());
            }
            return null;
        }

        @Override
        public Void visitCountPlan(CountPlan plan, Context context) {
            context.executionNodesTask.addExecutionNode(0, plan.countNode());
            if (context.isRootPlan && plan.mergeNode() != null) {
                context.executionNodesTask.addFinalMergeNode(plan.mergeNode());
            }
            return null;
        }

        @Override
        public Void visitUpsert(Upsert plan, Context context) {
            for (int i = 0; i < plan.nodes().size(); i++) {
                Plan subPlan = plan.nodes().get(i);
                assert subPlan instanceof CollectAndMerge;
                context.executionNodesTask.addExecutionNode(i, ((CollectAndMerge) subPlan).collectNode());
                context.executionNodesTask.addFinalMergeNode(((CollectAndMerge) subPlan).localMergeNode());
            }
            return null;
        }

        @Override
        public Void visitNestedLoop(NestedLoop plan, Context context) {
            boolean isRootPlan = context.isRootPlan;
            if (isRootPlan) {
                context.isRootPlan = false;
            }
            process(plan.left().plan(), context);
            process(plan.right().plan(), context);

            if (plan.nestedLoopNode().leftMergeNode() != null) {
                plan.nestedLoopNode().leftMergeNode().jobId(context.executionNodesTask.jobId());
            }
            if (plan.nestedLoopNode().rightMergeNode() != null) {
                plan.nestedLoopNode().rightMergeNode().jobId(context.executionNodesTask.jobId());
            }

            context.executionNodesTask.addExecutionNode(0, plan.nestedLoopNode());
            if (isRootPlan && plan.localMergeNode() != null) {
                context.executionNodesTask.addFinalMergeNode(plan.localMergeNode());
            }
            return null;
        }

        @Override
        protected Void visitPlan(Plan plan, Context context) {
            throw new UnsupportedOperationException(String.format(Locale.ENGLISH,
                    "Plan %s not supported", plan.getClass().getCanonicalName()));
        }
    }
}

<code block>


package io.crate.executor.transport.distributed;

import io.crate.Streamer;
import io.crate.core.collections.Bucket;
import io.crate.exceptions.UnknownUpstreamFailure;
import io.crate.executor.transport.StreamBucket;
import org.elasticsearch.common.io.ThrowableObjectInputStream;
import org.elasticsearch.common.io.ThrowableObjectOutputStream;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;
import org.elasticsearch.transport.TransportRequest;

import javax.annotation.Nullable;
import java.io.IOException;
import java.util.UUID;

public class DistributedResultRequest extends TransportRequest {

    private int executionNodeId;
    private byte executionNodeInputId = 0;
    private int bucketIdx;

    private Streamer<?>[] streamers;
    private Bucket rows;
    private UUID jobId;
    private boolean isLast = true;

    private Throwable throwable = null;

    public DistributedResultRequest() {
    }

    public DistributedResultRequest(UUID jobId,
                                    int executionNodeId,
                                    byte executionNodeInputId,
                                    int bucketIdx,
                                    Streamer<?>[] streamers) {
        this.jobId = jobId;
        this.executionNodeId = executionNodeId;
        this.executionNodeInputId = executionNodeInputId;
        this.bucketIdx = bucketIdx;
        this.streamers = streamers;
    }

    public UUID jobId() {
        return jobId;
    }

    public int executionNodeId() {
        return executionNodeId;
    }

    public byte executionNodeInputId() {
        return executionNodeInputId;
    }

    public int bucketIdx() {
        return bucketIdx;
    }

    public void streamers(Streamer<?>[] streamers) {
        if (rows instanceof StreamBucket) {
            assert streamers != null;
            ((StreamBucket) rows).streamers(streamers);
        }
        this.streamers = streamers;
    }

    public boolean rowsCanBeRead(){
        if (rows instanceof StreamBucket){
            return streamers != null;
        }
        return true;
    }

    public Bucket rows() {
        return rows;
    }

    public void rows(Bucket rows) {
        this.rows = rows;
    }

    public boolean isLast() {
        return isLast;
    }

    public void isLast(boolean isLast) {
        this.isLast = isLast;
    }

    public void throwable(Throwable throwable) {
        this.throwable = throwable;
    }

    @Nullable
    public Throwable throwable() {
        return throwable;
    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        super.readFrom(in);
        jobId = new UUID(in.readLong(), in.readLong());
        executionNodeId = in.readVInt();
        executionNodeInputId = in.readByte();
        bucketIdx = in.readVInt();
        isLast = in.readBoolean();

        boolean failure = in.readBoolean();
        if (failure) {
            ThrowableObjectInputStream tis = new ThrowableObjectInputStream(in);
            try {
                throwable = (Throwable) tis.readObject();
            } catch (ClassNotFoundException e) {
                throwable = new UnknownUpstreamFailure();
            }
        } else {
            StreamBucket bucket = new StreamBucket(streamers);
            bucket.readFrom(in);
            rows = bucket;
        }
    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        super.writeTo(out);
        out.writeLong(jobId.getMostSignificantBits());
        out.writeLong(jobId.getLeastSignificantBits());
        out.writeVInt(executionNodeId);
        out.writeByte(executionNodeInputId);
        out.writeVInt(bucketIdx);
        out.writeBoolean(isLast);

        boolean failure = throwable != null;
        out.writeBoolean(failure);
        if (failure) {
            ThrowableObjectOutputStream too = new ThrowableObjectOutputStream(out);
            too.writeObject(throwable);
        } else {

            StreamBucket.writeBucket(out, streamers, rows);
        }
    }
}

<code block>


package io.crate.executor.transport.distributed;

import io.crate.Constants;
import io.crate.Streamer;
import io.crate.core.collections.Bucket;
import io.crate.core.collections.Row;
import org.elasticsearch.action.ActionListener;
import org.elasticsearch.common.logging.ESLogger;
import org.elasticsearch.common.logging.Loggers;

import java.io.IOException;
import java.util.Collection;
import java.util.Deque;
import java.util.UUID;
import java.util.concurrent.CancellationException;
import java.util.concurrent.ConcurrentLinkedDeque;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicInteger;

public class DistributingDownstream extends ResultProviderBase {

    private static final ESLogger LOGGER = Loggers.getLogger(DistributingDownstream.class);

    private final UUID jobId;
    private final TransportDistributedResultAction transportDistributedResultAction;
    private final MultiBucketBuilder bucketBuilder;
    private Downstream[] downstreams;
    private final AtomicInteger finishedDownstreams = new AtomicInteger(0);

    public DistributingDownstream(UUID jobId,
                                  int targetExecutionNodeId,
                                  byte targetInputId,
                                  int bucketIdx,
                                  Collection<String> downstreamNodeIds,
                                  TransportDistributedResultAction transportDistributedResultAction,
                                  Streamer<?>[] streamers) {
        this.jobId = jobId;
        this.transportDistributedResultAction = transportDistributedResultAction;

        downstreams = new Downstream[downstreamNodeIds.size()];
        bucketBuilder = new MultiBucketBuilder(streamers, downstreams.length);

        int idx = 0;
        for (String downstreamNodeId : downstreamNodeIds) {
            downstreams[idx] = new Downstream(downstreamNodeId, jobId, targetExecutionNodeId,
                    targetInputId, bucketIdx, streamers);
            idx++;
        }
    }

    @Override
    public boolean setNextRow(Row row) {
        if (allDownstreamsFinished()) {
            return false;
        }
        try {
            int downstreamIdx = bucketBuilder.getBucket(row);

            if (downstreams[downstreamIdx].wantMore.get()) {
                bucketBuilder.setNextRow(downstreamIdx, row);
                sendRequestIfNeeded(downstreamIdx);
            }
        } catch (IOException e) {
            fail(e);
            return false;
        }
        return true;
    }

    protected void sendRequestIfNeeded(int downstreamIdx) {
        int size = bucketBuilder.size(downstreamIdx);
        if (size >= Constants.PAGE_SIZE || remainingUpstreams.get() <= 0) {
            Downstream downstream = downstreams[downstreamIdx];
            downstream.bucketQueue.add(bucketBuilder.build(downstreamIdx));
            sendRequest(downstream);
        }
    }

    protected void onAllUpstreamsFinished() {
        for (int i = 0; i < downstreams.length; i++) {
            sendRequestIfNeeded(i);
        }
    }

    private void forwardFailures(Throwable throwable) {
        for (Downstream downstream : downstreams) {
            downstream.request.throwable(throwable);
            sendRequest(downstream.request, downstream);
        }
    }

    private boolean allDownstreamsFinished() {
        return finishedDownstreams.get() == downstreams.length;
    }

    private void sendRequest(Downstream downstream) {
        if (downstream.requestPending.compareAndSet(false, true)) {
            DistributedResultRequest request = downstream.request;
            Deque<Bucket> queue = downstream.bucketQueue;
            int size = queue.size();
            if (size > 0) {
                request.rows(queue.poll());
            } else {
                request.rows(Bucket.EMPTY);
            }
            request.isLast(!(size > 1 || remainingUpstreams.get() > 0));
            sendRequest(request, downstream);
        }
    }

    private void sendRequest(final DistributedResultRequest request, final Downstream downstream) {
        if (LOGGER.isTraceEnabled()) {
            LOGGER.trace("[{}] sending distributing collect request to {}, isLast? {} ...",
                    jobId.toString(),
                    downstream.node, request.isLast());
        }
        try {
            transportDistributedResultAction.pushResult(
                    downstream.node,
                    request,
                    new DistributedResultResponseActionListener(downstream)
            );
        } catch (IllegalArgumentException e) {
            LOGGER.error(e.getMessage(), e);
            downstream.wantMore.set(false);
        }
    }

    @Override
    public Bucket doFinish() {
        onAllUpstreamsFinished();
        return null;
    }

    @Override
    public Throwable doFail(Throwable t) {
        if (t instanceof CancellationException) {

            LOGGER.debug("{} killed", getClass().getSimpleName());
        } else {
            forwardFailures(t);
        }
        return t;
    }

    static class Downstream {

        final AtomicBoolean wantMore = new AtomicBoolean(true);
        final AtomicBoolean requestPending = new AtomicBoolean(false);
        final Deque<Bucket> bucketQueue = new ConcurrentLinkedDeque<>();
        final DistributedResultRequest request;
        final String node;

        public Downstream(String node,
                          UUID jobId,
                          int targetExecutionNodeId,
                          byte targetInputId,
                          int bucketIdx,
                          Streamer<?>[] streamers) {
            this.node = node;
            this.request = new DistributedResultRequest(jobId, targetExecutionNodeId, targetInputId, bucketIdx, streamers);
        }
    }

    private class DistributedResultResponseActionListener implements ActionListener<DistributedResultResponse> {
        private final Downstream downstream;

        public DistributedResultResponseActionListener(Downstream downstream) {
            this.downstream = downstream;
        }

        @Override
        public void onResponse(DistributedResultResponse response) {
            if (LOGGER.isTraceEnabled()) {
                LOGGER.trace("[{}] successfully sent distributing collect request to {}, needMore? {}",
                        jobId,
                        downstream.node,
                        response.needMore());
            }

            downstream.wantMore.set(response.needMore());
            if (!response.needMore()) {
                finishedDownstreams.incrementAndGet();

                downstream.bucketQueue.clear();
            } else {

                downstream.requestPending.set(false);
                sendRequest(downstream);
            }
        }

        @Override
        public void onFailure(Throwable exp) {
            LOGGER.error("[{}] Exception sending distributing collect request to {}", exp, jobId, downstream.node);
            downstream.wantMore.set(false);
            downstream.bucketQueue.clear();
            finishedDownstreams.incrementAndGet();
        }
    }
}

<code block>


package io.crate.executor.transport.distributed;

import io.crate.executor.transport.DefaultTransportResponseHandler;
import io.crate.executor.transport.NodeAction;
import io.crate.executor.transport.NodeActionRequestHandler;
import io.crate.executor.transport.Transports;
import io.crate.jobs.*;
import io.crate.operation.PageResultListener;
import io.crate.planner.node.ExecutionNode;
import org.elasticsearch.action.ActionListener;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.logging.ESLogger;
import org.elasticsearch.common.logging.Loggers;
import org.elasticsearch.threadpool.ThreadPool;
import org.elasticsearch.transport.TransportService;

import java.util.Locale;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;


public class TransportDistributedResultAction implements NodeAction<DistributedResultRequest, DistributedResultResponse> {

    private static final ESLogger LOGGER = Loggers.getLogger(TransportDistributedResultAction.class);

    public final static String DISTRIBUTED_RESULT_ACTION = "crate/sql/node/merge/add_rows";
    private final static String EXECUTOR_NAME = ThreadPool.Names.SEARCH;

    private final Transports transports;
    private final JobContextService jobContextService;
    private final ScheduledExecutorService scheduler;

    @Inject
    public TransportDistributedResultAction(Transports transports,
                                            JobContextService jobContextService,
                                            ThreadPool threadPool,
                                            TransportService transportService) {
        this.transports = transports;
        this.jobContextService = jobContextService;
        scheduler = threadPool.scheduler();
        transportService.registerHandler(DISTRIBUTED_RESULT_ACTION, new NodeActionRequestHandler<DistributedResultRequest, DistributedResultResponse>(this) {
            @Override
            public DistributedResultRequest newInstance() {
                return new DistributedResultRequest();
            }
        });
    }

    public void pushResult(String node, DistributedResultRequest request, ActionListener<DistributedResultResponse> listener) {
        transports.executeLocalOrWithTransport(this, node, request, listener,
                new DefaultTransportResponseHandler<DistributedResultResponse>(listener, EXECUTOR_NAME) {
                    @Override
                    public DistributedResultResponse newInstance() {
                        return new DistributedResultResponse();
                    }
                });
    }

    @Override
    public String actionName() {
        return DISTRIBUTED_RESULT_ACTION;
    }

    @Override
    public String executorName() {
        return EXECUTOR_NAME;
    }

    @Override
    public void nodeOperation(DistributedResultRequest request,
                              ActionListener<DistributedResultResponse> listener) {
        nodeOperation(request, listener, 0);
    }

    private void nodeOperation(final DistributedResultRequest request,
                               final ActionListener<DistributedResultResponse> listener,
                               final int retry) {
        if (request.executionNodeId() == ExecutionNode.NO_EXECUTION_NODE) {
            listener.onFailure(new IllegalStateException("request must contain a valid executionNodeId"));
            return;
        }
        JobExecutionContext context = jobContextService.getContextOrNull(request.jobId());
        if (context == null) {
            retryOrFailureResponse(request, listener, retry);
            return;
        }

        DownstreamExecutionSubContext executionContext;
        try {
            executionContext = context.getSubContextOrNull(request.executionNodeId());
        } catch (ClassCastException e) {
            listener.onFailure(new IllegalStateException(String.format(Locale.ENGLISH,
                    "Found execution context for %d but it's not a downstream context", request.executionNodeId())));
            return;
        }
        if (executionContext == null) {

            listener.onFailure(new IllegalStateException(String.format(Locale.ENGLISH,
                    "Couldn't find execution context for %d", request.executionNodeId())));
            return;
        }

        PageDownstreamContext pageDownstreamContext = executionContext.pageDownstreamContext(request.executionNodeInputId());
        if (pageDownstreamContext == null) {
            listener.onFailure(new IllegalStateException(String.format(Locale.ENGLISH,
                    "Couldn't find pageDownstreamContext for input %d", request.executionNodeInputId())));
            return;
        }

        Throwable throwable = request.throwable();
        if (throwable == null) {
            request.streamers(pageDownstreamContext.streamer());
            pageDownstreamContext.setBucket(
                    request.bucketIdx(),
                    request.rows(),
                    request.isLast(),
                    new SendResponsePageResultListener(listener, request));
        } else {
            pageDownstreamContext.failure(request.bucketIdx(), throwable);
            listener.onResponse(new DistributedResultResponse(false));
        }
    }

    private void retryOrFailureResponse(DistributedResultRequest request,
                                        ActionListener<DistributedResultResponse> listener,
                                        int retry) {
        if (retry > 20) {
            listener.onFailure(new IllegalStateException(
                    String.format("Couldn't find JobExecutionContext for %s", request.jobId())));
        } else {
            scheduler.schedule(new NodeOperationRunnable(request, listener, retry), (retry + 1) * 2, TimeUnit.MILLISECONDS);
        }
    }

    private static class SendResponsePageResultListener implements PageResultListener {
        private final ActionListener<DistributedResultResponse> listener;
        private final DistributedResultRequest request;

        public SendResponsePageResultListener(ActionListener<DistributedResultResponse> listener, DistributedResultRequest request) {
            this.listener = listener;
            this.request = request;
        }

        @Override
        public void needMore(boolean needMore) {
            LOGGER.trace("sending needMore response, need more? {}", needMore);
            listener.onResponse(new DistributedResultResponse(needMore));
        }

        @Override
        public int buckedIdx() {
            return request.bucketIdx();
        }
    }

    private class NodeOperationRunnable implements Runnable {
        private final DistributedResultRequest request;
        private final ActionListener<DistributedResultResponse> listener;
        private final int retry;

        public NodeOperationRunnable(DistributedResultRequest request, ActionListener<DistributedResultResponse> listener, int retry) {
            this.request = request;
            this.listener = listener;
            this.retry = retry;
        }

        @Override
        public void run() {
            nodeOperation(request, listener, retry + 1);
        }
    }
}

<code block>


package io.crate.planner;

import io.crate.planner.node.dml.InsertFromSubQuery;
import io.crate.planner.node.dql.*;
import io.crate.planner.node.dml.Upsert;
import io.crate.planner.node.dql.join.NestedLoop;
import io.crate.planner.node.management.KillPlan;
import org.elasticsearch.common.Nullable;

public class PlanVisitor<C, R> {

    public R process(Plan plan, @Nullable C context) {
        return plan.accept(this, context);
    }

    protected R visitPlan(Plan plan, C context) {
        return null;
    }

    public R visitIterablePlan(IterablePlan plan, C context) {
        return visitPlan(plan, context);
    }

    public R visitNoopPlan(NoopPlan plan, C context) {
        return visitPlan(plan, context);
    }

    public R visitGlobalAggregate(GlobalAggregate plan, C context) {
        return visitPlan(plan, context);
    }

    public R visitQueryAndFetch(QueryAndFetch plan, C context){
        return visitPlan(plan, context);
    }

    public R visitQueryThenFetch(QueryThenFetch plan, C context){
        return visitPlan(plan, context);
    }

    public R visitNonDistributedGroupBy(NonDistributedGroupBy plan, C context) {
        return visitPlan(plan, context);
    }

    public R visitUpsert(Upsert plan, C context) {
        return visitPlan(plan, context);
    }

    public R visitDistributedGroupBy(DistributedGroupBy plan, C context) {
        return visitPlan(plan, context);
    }

    public R visitInsertByQuery(InsertFromSubQuery plan, C context) {
        return visitPlan(plan, context);
    }

    public R visitCollectAndMerge(CollectAndMerge plan, C context) {
        return visitPlan(plan, context);
    }

    public R visitCountPlan(CountPlan plan, C context) {
        return visitPlan(plan, context);
    }

    public R visitKillPlan(KillPlan plan, C context) {
        return visitPlan(plan, context);
    }

    public R visitNestedLoop(NestedLoop plan, C context) {
        return visitPlan(plan, context);
    }
}

<code block>


package io.crate.planner;

import com.google.common.base.MoreObjects;
import com.google.common.base.Predicates;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableSet;
import com.google.common.collect.Iterables;
import io.crate.analyze.OrderBy;
import io.crate.analyze.WhereClause;
import io.crate.metadata.PartitionName;
import io.crate.metadata.Routing;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.consumer.OrderByPositionVisitor;
import io.crate.planner.node.dql.AbstractDQLPlanNode;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.DQLPlanNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.dql.join.NestedLoopNode;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.InputColumn;
import io.crate.planner.symbol.Symbol;
import io.crate.planner.symbol.Symbols;
import io.crate.types.DataType;

import javax.annotation.Nullable;
import java.util.*;

public class PlanNodeBuilder {

    public static CollectNode distributingCollect(TableInfo tableInfo,
                                                  Planner.Context plannerContext,
                                                  WhereClause whereClause,
                                                  List<Symbol> toCollect,
                                                  List<String> downstreamNodes,
                                                  ImmutableList<Projection> projections) {
        Routing routing = tableInfo.getRouting(whereClause, null);
        plannerContext.allocateJobSearchContextIds(routing);
        CollectNode node = new CollectNode(
                plannerContext.nextExecutionNodeId(),
                "distributing collect",
                routing);
        node.whereClause(whereClause);
        node.maxRowGranularity(tableInfo.rowGranularity());
        node.downstreamNodes(downstreamNodes);
        node.toCollect(toCollect);
        node.projections(projections);

        node.isPartitioned(tableInfo.isPartitioned());
        setOutputTypes(node);
        return node;
    }

    public static MergeNode distributedMerge(CollectNode collectNode,
                                             Planner.Context plannerContext,
                                             List<Projection> projections) {
        MergeNode node = new MergeNode(
                plannerContext.nextExecutionNodeId(),
                "distributed merge",
                collectNode.executionNodes().size());
        node.projections(projections);

        assert collectNode.hasDistributingDownstreams();
        node.executionNodes(ImmutableSet.copyOf(collectNode.downstreamNodes()));
        connectTypes(collectNode, node);
        return node;
    }

    public static MergeNode localMerge(List<Projection> projections,
                                       DQLPlanNode previousNode,
                                       Planner.Context plannerContext) {
        MergeNode node = new MergeNode(
                plannerContext.nextExecutionNodeId(),
                "localMerge",
                previousNode.executionNodes().size());
        node.projections(projections);
        connectTypes(previousNode, node);
        return node;
    }


    public static MergeNode sortedLocalMerge(List<Projection> projections,
                                             OrderBy orderBy,
                                             List<Symbol> sourceSymbols,
                                             @Nullable List<Symbol> orderBySymbols,
                                             DQLPlanNode previousNode,
                                             Planner.Context plannerContext) {
        int[] orderByIndices = OrderByPositionVisitor.orderByPositions(
                MoreObjects.firstNonNull(orderBySymbols, orderBy.orderBySymbols()),
                sourceSymbols
        );
        MergeNode node = MergeNode.sortedMergeNode(
                plannerContext.nextExecutionNodeId(),
                "sortedLocalMerge",
                previousNode.executionNodes().size(),
                orderByIndices,
                orderBy.reverseFlags(),
                orderBy.nullsFirst()
        );
        node.projections(projections);
        connectTypes(previousNode, node);
        return node;
    }

    public static <PN extends AbstractDQLPlanNode> NestedLoopNode localNestedLoopNode(
            List<Projection> projections,
            Set<String> executionNodes,
            PN leftPreviousNode,
            PN rightPreviousNode,
            List<Symbol> leftSymbols,
            List<Symbol> rightSymbols,
            @Nullable OrderBy leftOrderBy,
            @Nullable OrderBy rightOrderBy,
            Planner.Context plannerContext) {
        NestedLoopNode node = new NestedLoopNode(
                plannerContext.nextExecutionNodeId(),
                "localNestedLoopNode"
        );
        node.projections(projections);
        node.executionNodes(executionNodes);


        leftPreviousNode.downstreamExecutionNodeId(node.executionNodeId());
        rightPreviousNode.downstreamExecutionNodeId(node.executionNodeId());
        leftPreviousNode.downstreamNodes(executionNodes);
        rightPreviousNode.downstreamNodes(executionNodes);
        leftPreviousNode.downstreamInputId((byte) 0);
        rightPreviousNode.downstreamInputId((byte) 1);

        MergeNode leftMergeNode;
        MergeNode rightMergeNode;
        if (leftOrderBy != null) {
            leftMergeNode = sortedLocalMerge(
                    ImmutableList.<Projection>of(), leftOrderBy, leftSymbols,
                    null, leftPreviousNode, plannerContext);
        } else {
            leftMergeNode = localMerge(
                    ImmutableList.<Projection>of(), leftPreviousNode, plannerContext);
        }
        if (rightOrderBy != null) {
            rightMergeNode = sortedLocalMerge(
                    ImmutableList.<Projection>of(), rightOrderBy, rightSymbols,
                    null, rightPreviousNode, plannerContext);
        } else {
            rightMergeNode = localMerge(
                    ImmutableList.<Projection>of(), rightPreviousNode, plannerContext);
        }



        leftMergeNode.downstreamExecutionNodeId(node.executionNodeId());
        leftMergeNode.downstreamNodes(node.executionNodes());
        leftMergeNode.executionNodes(node.executionNodes());
        rightMergeNode.downstreamExecutionNodeId(node.executionNodeId());
        rightMergeNode.downstreamNodes(node.executionNodes());
        rightMergeNode.executionNodes(node.executionNodes());

        connectTypes(leftPreviousNode, leftMergeNode);
        connectTypes(rightPreviousNode, rightMergeNode);

        node.leftMergeNode(leftMergeNode);
        node.rightMergeNode(rightMergeNode);
        connectTypes(leftMergeNode, rightMergeNode, node);
        return node;
    }


    public static void setOutputTypes(CollectNode node) {
        if (node.projections().isEmpty()) {
            node.outputTypes(Symbols.extractTypes(node.toCollect()));
        } else {
            node.outputTypes(Planner.extractDataTypes(node.projections(), Symbols.extractTypes(node.toCollect())));
        }
    }


    public static void connectTypes(DQLPlanNode previousNode, DQLPlanNode nextNode) {
        nextNode.inputTypes(previousNode.outputTypes());
        nextNode.outputTypes(Planner.extractDataTypes(nextNode.projections(), nextNode.inputTypes()));
    }

    public static void connectTypes(@Nullable DQLPlanNode left, @Nullable DQLPlanNode right, NestedLoopNode nextNode) {
        List<DataType> outputTypes = new ArrayList<>();
        if (left != null) {
            outputTypes.addAll(left.inputTypes());
            nextNode.leftInputTypes(left.outputTypes());
        }
        if (right != null) {
            nextNode.rightInputTypes(right.inputTypes());
            outputTypes.addAll(right.inputTypes());
        }
        nextNode.outputTypes(Planner.extractDataTypes(nextNode.projections(), outputTypes));
    }

    public static CollectNode collect(TableInfo tableInfo,
                                      Planner.Context plannerContext,
                                      WhereClause whereClause,
                                      List<Symbol> toCollect,
                                      ImmutableList<Projection> projections,
                                      @Nullable String partitionIdent,
                                      @Nullable String routingPreference,
                                      @Nullable OrderBy orderBy,
                                      @Nullable Integer limit) {
        assert !Iterables.any(toCollect, Predicates.instanceOf(InputColumn.class)) : "cannot collect inputcolumns";
        Routing routing = tableInfo.getRouting(whereClause, routingPreference);
        if (partitionIdent != null && routing.hasLocations()) {
            routing = filterRouting(routing, PartitionName.fromPartitionIdent(
                    tableInfo.ident().schema(), tableInfo.ident().name(), partitionIdent).stringValue());
        }
        plannerContext.allocateJobSearchContextIds(routing);
        CollectNode node = new CollectNode(
                plannerContext.nextExecutionNodeId(),
                "collect",
                routing,
                toCollect,
                projections);
        node.whereClause(whereClause);
        node.maxRowGranularity(tableInfo.rowGranularity());
        node.isPartitioned(tableInfo.isPartitioned());
        setOutputTypes(node);
        node.orderBy(orderBy);
        node.limit(limit);
        return node;
    }

    private static Routing filterRouting(Routing routing, String includeTableName) {
        assert routing.hasLocations();
        assert includeTableName != null;
        Map<String, Map<String, List<Integer>>> newLocations = new TreeMap<>();

        for (Map.Entry<String, Map<String, List<Integer>>> entry : routing.locations().entrySet()) {
            Map<String, List<Integer>> tableMap = new TreeMap<>();
            for (Map.Entry<String, List<Integer>> tableEntry : entry.getValue().entrySet()) {
                if (includeTableName.equals(tableEntry.getKey())) {
                    tableMap.put(tableEntry.getKey(), tableEntry.getValue());
                }
            }
            if (tableMap.size()>0){
                newLocations.put(entry.getKey(), tableMap);
            }

        }
        if (newLocations.size()>0) {
            return new Routing(newLocations);
        } else {
            return new Routing();
        }

    }

    public static CollectNode collect(TableInfo tableInfo,
                                      Planner.Context plannerContext,
                                      WhereClause whereClause,
                                      List<Symbol> toCollect,
                                      ImmutableList<Projection> projections) {
        return collect(tableInfo, plannerContext, whereClause, toCollect, projections, null, null, null, null);
    }

    public static CollectNode collect(TableInfo tableInfo,
                                      Planner.Context plannerContext,
                                      WhereClause whereClause,
                                      List<Symbol> toCollect,
                                      ImmutableList<Projection> projections,
                                      @Nullable String partitionIdent,
                                      @Nullable String routingPreference) {
        return collect(tableInfo, plannerContext, whereClause, toCollect, projections, partitionIdent, routingPreference, null, null);
    }

    public static CollectNode collect(TableInfo tableInfo,
                                      Planner.Context plannerContext,
                                      WhereClause whereClause,
                                      List<Symbol> toCollect,
                                      ImmutableList<Projection> projections,
                                      @Nullable String partitionIdent) {
        return collect(tableInfo, plannerContext, whereClause, toCollect, projections, partitionIdent, null);
    }

    public static CollectNode collect(TableInfo tableInfo,
                                      Planner.Context plannerContext,
                                      WhereClause whereClause,
                                      List<Symbol> toCollect,
                                      ImmutableList<Projection> projections,
                                      @Nullable OrderBy orderBy,
                                      @Nullable Integer limit) {
        return collect(tableInfo, plannerContext, whereClause, toCollect, projections, null, null, orderBy, limit);
    }
}

<code block>


package io.crate.planner.node;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.Lists;
import io.crate.Streamer;
import io.crate.exceptions.ResourceUnknownException;
import io.crate.metadata.Functions;
import io.crate.operation.aggregation.AggregationFunction;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.dql.join.NestedLoopNode;
import io.crate.planner.projection.AggregationProjection;
import io.crate.planner.projection.GroupProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.ProjectionType;
import io.crate.planner.symbol.Aggregation;
import io.crate.planner.symbol.InputColumn;
import io.crate.planner.symbol.Symbol;
import io.crate.planner.symbol.SymbolType;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import io.crate.types.UndefinedType;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.inject.Singleton;

import java.util.ArrayList;
import java.util.Collections;
import java.util.List;


@Singleton
public class StreamerVisitor {

    public static class Context {
        private List<Streamer<?>> inputStreamers = new ArrayList<>();
        private List<Streamer<?>> outputStreamers = new ArrayList<>();

        public Context() {
        }

        public Streamer<?>[] inputStreamers() {
            return inputStreamers.toArray(new Streamer<?>[inputStreamers.size()]);
        }

        public Streamer<?>[] outputStreamers() {
            return outputStreamers.toArray(new Streamer<?>[outputStreamers.size()]);
        }
    }

    private final Functions functions;
    private final PlanNodeStreamerVisitor planNodeStreamerVisitor;
    private final ExecutionNodeStreamerVisitor executionNodeStreamerVisitor;

    @Inject
    public StreamerVisitor(Functions functions) {
        this.functions = functions;
        this.planNodeStreamerVisitor = new PlanNodeStreamerVisitor();
        this.executionNodeStreamerVisitor = new ExecutionNodeStreamerVisitor();
    }

    public Context processPlanNode(PlanNode node) {
        Context context = new Context();
        planNodeStreamerVisitor.process(node, context);
        return context;
    }

    public Context processExecutionNode(ExecutionNode executionNode) {
        Context context = new Context();
        executionNodeStreamerVisitor.process(executionNode, context);
        return context;
    }

    private class PlanNodeStreamerVisitor extends PlanNodeVisitor<Context, Void> {

        @Override
        public Void visitCollectNode(CollectNode node, Context context) {
            extractFromCollectNode(node, context);
            return null;
        }

        @Override
        public Void visitMergeNode(MergeNode node, Context context) {
            extractFromMergeNode(node, context);
            return null;
        }
    }

    private class ExecutionNodeStreamerVisitor extends ExecutionNodeVisitor<Context, Void> {

        @Override
        public Void visitMergeNode(MergeNode node, Context context) {
            extractFromMergeNode(node, context);
            return null;
        }

        @Override
        public Void visitCollectNode(CollectNode collectNode, Context context) {
            extractFromCollectNode(collectNode, context);
            return null;
        }

        @Override
        public Void visitNestedLoopNode(NestedLoopNode node, Context context) {
            setOutputStreamers(node.outputTypes(), ImmutableList.<DataType>of(), node.projections(), context);
            return null;
        }

        @Override
        protected Void visitExecutionNode(ExecutionNode node, Context context) {
            throw new UnsupportedOperationException(String.format("Got unsupported ExecutionNode %s", node.getClass().getName()));
        }
    }


    private Streamer<?> resolveStreamer(Aggregation aggregation, Aggregation.Step step) {
        Streamer<?> streamer;
        AggregationFunction<?, ?> aggFunction = (AggregationFunction<?, ?>)functions.get(aggregation.functionIdent());
        if (aggFunction == null) {
            throw new ResourceUnknownException("unknown aggregation function");
        }
        switch (step) {
            case ITER:
                assert aggFunction.info().ident().argumentTypes().size() == 1;
                streamer = aggFunction.info().ident().argumentTypes().get(0).streamer();
                break;
            case PARTIAL:
                streamer = aggFunction.partialType().streamer();
                break;
            case FINAL:
                streamer = aggFunction.info().returnType().streamer();
                break;
            default:
                throw new UnsupportedOperationException("step not supported");
        }
        return streamer;
    }

    private void extractFromCollectNode(CollectNode node, Context context) {

        List<Aggregation> aggregations = ImmutableList.of();
        List<Projection> projections = Lists.reverse(node.projections());
        for(Projection projection : projections){
            if (projection.projectionType() == ProjectionType.AGGREGATION) {
                aggregations = ((AggregationProjection)projection).aggregations();
                break;
            } else if (projection.projectionType() == ProjectionType.GROUP) {
                aggregations = ((GroupProjection)projection).values();
                break;
            }
        }


        int aggIdx = 0;
        Aggregation aggregation;
        for (DataType outputType : node.outputTypes()) {
            if (outputType == null || outputType == UndefinedType.INSTANCE) {

                try {
                    aggregation = aggregations.get(aggIdx);
                    if (aggregation != null) {
                        context.outputStreamers.add(resolveStreamer(aggregation, aggregation.toStep()));
                    }
                } catch (IndexOutOfBoundsException e) {

                    context.outputStreamers.add(UndefinedType.INSTANCE.streamer());
                }
                aggIdx++;
            } else {

                context.outputStreamers.add(outputType.streamer());
            }
        }
    }

    private void extractFromMergeNode(MergeNode node, Context context) {
        if (node.projections().isEmpty()) {
            for (DataType dataType : node.inputTypes()) {
                if (dataType != null) {
                    context.inputStreamers.add(dataType.streamer());
                } else {
                    throw new IllegalStateException("Can't resolve Streamer from null dataType");
                }
            }
            return;
        }

        Projection firstProjection = node.projections().get(0);
        setInputStreamers(node.inputTypes(), firstProjection, context);
        setOutputStreamers(node.outputTypes(), node.inputTypes(), node.projections(), context);
    }

    private void setOutputStreamers(List<DataType> outputTypes,
                                    List<DataType> inputTypes,
                                    List<Projection> projections, Context context) {
        final Streamer<?>[] streamers = new Streamer[outputTypes.size()];

        int idx = 0;
        for (DataType outputType : outputTypes) {
            if (outputType == UndefinedType.INSTANCE) {
                resolveStreamer(streamers, projections, idx, projections.size() - 1, inputTypes);
                if (streamers[idx] == null) {
                    streamers[idx] = outputType.streamer();
                }
            } else {
                streamers[idx] = outputType.streamer();
            }
            idx++;
        }

        for (Streamer<?> streamer : streamers) {
            if (streamer == null) {
                throw new IllegalStateException("Could not resolve all output streamers");
            }
        }
        Collections.addAll(context.outputStreamers, streamers);
    }


    private void resolveStreamer(Streamer<?>[] streamers,
                                 List<Projection> projections,
                                 int columnIdx,
                                 int projectionIdx,
                                 List<DataType> inputTypes) {
        final Projection projection = projections.get(projectionIdx);
        final Symbol symbol = projection.outputs().get(columnIdx);

        if (!symbol.valueType().equals(DataTypes.UNDEFINED)) {
            streamers[columnIdx] = symbol.valueType().streamer();
        } else if (symbol.symbolType() == SymbolType.AGGREGATION) {
            Aggregation aggregation = (Aggregation)symbol;
            streamers[columnIdx] = resolveStreamer(aggregation, aggregation.toStep());
        } else if (symbol.symbolType() == SymbolType.INPUT_COLUMN) {
            columnIdx = ((InputColumn)symbol).index();
            if (projectionIdx > 0) {
                projectionIdx--;
                resolveStreamer(streamers, projections, columnIdx, projectionIdx, inputTypes);
            } else {
                streamers[columnIdx] = inputTypes.get(((InputColumn)symbol).index()).streamer();
            }
        }
    }

    private void setInputStreamers(List<DataType> inputTypes, Projection projection, Context context) {
        List<Aggregation> aggregations;
        switch (projection.projectionType()) {
            case TOPN:
            case FETCH:
                aggregations = ImmutableList.of();
                break;
            case GROUP:
                aggregations = ((GroupProjection)projection).values();
                break;
            case AGGREGATION:
                aggregations = ((AggregationProjection)projection).aggregations();
                break;
            default:
                throw new UnsupportedOperationException("projectionType not supported");
        }

        int idx = 0;
        for (DataType inputType : inputTypes) {
            if (inputType != null && inputType != UndefinedType.INSTANCE) {
                context.inputStreamers.add(inputType.streamer());
            } else {
                Aggregation aggregation = aggregations.get(idx);
                context.inputStreamers.add(resolveStreamer(aggregation, aggregation.fromStep()));
                idx++;
            }
        }
    }
}

<code block>


package io.crate.planner.node;

import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.CountNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.dql.join.NestedLoopNode;

public class ExecutionNodeVisitor<C, R> {

    public R process(ExecutionNode node, C context) {
        return node.accept(this, context);
    }

    protected R visitExecutionNode(ExecutionNode node, C context) {
        return null;
    }

    public R visitCollectNode(CollectNode node, C context) {
        return visitExecutionNode(node, context);
    }

    public R visitMergeNode(MergeNode node, C context) {
        return visitExecutionNode(node, context);
    }

    public R visitCountNode(CountNode node, C context) {
        return visitExecutionNode(node, context);
    }

    public R visitNestedLoopNode(NestedLoopNode node, C context) {
        return visitExecutionNode(node, context);
    }
}

<code block>


package io.crate.planner.node;

import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.CountNode;
import io.crate.planner.node.dql.FileUriCollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.dql.join.NestedLoopNode;
import org.elasticsearch.common.io.stream.Streamable;

import java.util.List;
import java.util.Set;
import java.util.UUID;

public interface ExecutionNode extends Streamable {

    String DIRECT_RETURN_DOWNSTREAM_NODE = "_response";

    int NO_EXECUTION_NODE = Integer.MAX_VALUE;

    interface ExecutionNodeFactory<T extends ExecutionNode> {
        T create();
    }

    enum Type {
        COLLECT(CollectNode.FACTORY),
        COUNT(CountNode.FACTORY),
        FILE_URI_COLLECT(FileUriCollectNode.FACTORY),
        MERGE(MergeNode.FACTORY),
        NESTED_LOOP(NestedLoopNode.FACTORY);

        private final ExecutionNodeFactory factory;

        Type(ExecutionNodeFactory factory) {
            this.factory = factory;
        }

        public ExecutionNodeFactory factory() {
            return factory;
        }
    }

    Type type();

    String name();

    int executionNodeId();

    Set<String> executionNodes();

    List<String> downstreamNodes();

    int downstreamExecutionNodeId();

    byte downstreamInputId();

    UUID jobId();

    void jobId(UUID jobId);


    <C, R> R accept(ExecutionNodeVisitor<C, R> visitor, C context);
}

<code block>


package io.crate.planner.node.dql;

import io.crate.planner.PlanAndPlannedAnalyzedRelation;
import io.crate.planner.PlanVisitor;
import io.crate.planner.projection.Projection;

public class CountPlan extends PlanAndPlannedAnalyzedRelation {

    private final CountNode countNode;
    private final MergeNode mergeNode;

    public CountPlan(CountNode countNode, MergeNode mergeNode) {
        this.countNode = countNode;
        this.mergeNode = mergeNode;
    }

    public CountNode countNode() {
        return countNode;
    }

    public MergeNode mergeNode() {
        return mergeNode;
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitCountPlan(this, context);
    }

    @Override
    public void addProjection(Projection projection) {
        mergeNode.addProjection(projection);
    }

    @Override
    public boolean resultIsDistributed() {
        return false;
    }

    @Override
    public DQLPlanNode resultNode() {
        return mergeNode;
    }
}

<code block>


package io.crate.planner.node.dql;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.Sets;
import io.crate.analyze.WhereClause;
import io.crate.metadata.Routing;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.node.ExecutionNodeVisitor;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;

import java.io.IOException;
import java.util.List;
import java.util.Set;
import java.util.UUID;

public class CountNode implements ExecutionNode {

    public static final ExecutionNodeFactory<CountNode> FACTORY = new ExecutionNodeFactory<CountNode>() {
        @Override
        public CountNode create() {
            return new CountNode();
        }
    };
    private UUID jobId;
    private int executionNodeId;
    private Routing routing;
    private WhereClause whereClause;

    CountNode() {}

    public CountNode(int executionNodeId, Routing routing, WhereClause whereClause) {
        this.executionNodeId = executionNodeId;
        this.routing = routing;
        this.whereClause = whereClause;
    }

    @Override
    public Type type() {
        return Type.COUNT;
    }

    @Override
    public String name() {
        return "count";
    }

    @Override
    public UUID jobId() {
        return jobId;
    }

    @Override
    public void jobId(UUID jobId) {
        this.jobId = jobId;
    }

    public Routing routing() {
        return routing;
    }

    public WhereClause whereClause() {
        return whereClause;
    }

    @Override
    public int executionNodeId() {
        return executionNodeId;
    }

    @Override
    public Set<String> executionNodes() {
        if (routing.isNullRouting()) {
            return routing.nodes();
        } else {
            return Sets.filter(routing.nodes(), TableInfo.IS_NOT_NULL_NODE_ID);
        }
    }

    @Override
    public List<String> downstreamNodes() {
        return ImmutableList.of(ExecutionNode.DIRECT_RETURN_DOWNSTREAM_NODE);
    }

    @Override
    public int downstreamExecutionNodeId() {
        return ExecutionNode.NO_EXECUTION_NODE;
    }

    @Override
    public byte downstreamInputId() {
        return 0;
    }

    @Override
    public <C, R> R accept(ExecutionNodeVisitor<C, R> visitor, C context) {
        return visitor.visitCountNode(this, context);
    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        jobId = new UUID(in.readLong(), in.readLong());
        executionNodeId = in.readVInt();
        routing = new Routing();
        routing.readFrom(in);
        whereClause = new WhereClause(in);
    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        assert jobId != null : "jobId must not be null";
        out.writeLong(jobId.getMostSignificantBits());
        out.writeLong(jobId.getLeastSignificantBits());
        out.writeVInt(executionNodeId);
        routing.writeTo(out);
        whereClause.writeTo(out);
    }
}

<code block>


package io.crate.planner.node.dql;

import com.google.common.base.MoreObjects;
import com.google.common.base.Optional;
import com.google.common.collect.ImmutableList;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.projection.Projection;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;
import org.elasticsearch.common.io.stream.Streamable;

import javax.annotation.Nullable;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import java.util.Set;
import java.util.UUID;

public abstract class AbstractDQLPlanNode implements DQLPlanNode, Streamable, ExecutionNode {

    private UUID jobId;
    private int executionNodeId;
    @Nullable
    private List<String> downstreamNodes;
    private int downstreamExecutionNodeId = NO_EXECUTION_NODE;
    private byte downstreamInputId = 0;
    private String name;
    protected List<Projection> projections = ImmutableList.of();
    protected List<DataType> outputTypes = ImmutableList.of();
    protected List<DataType> inputTypes = ImmutableList.of();

    public AbstractDQLPlanNode() {

    }

    protected AbstractDQLPlanNode(int executionNodeId, String name) {
        this.executionNodeId = executionNodeId;
        this.name = name;
    }

    @Override
    public String name() {
        return name;
    }

    @Override
    public UUID jobId() {
        return jobId;
    }

    @Override
    public void jobId(UUID jobId) {
        this.jobId = jobId;
    }

    @Override
    public int executionNodeId() {
        return executionNodeId;
    }

    @Nullable
    public List<String> downstreamNodes() {
        return downstreamNodes;
    }

    public void downstreamNodes(List<String> downStreamNodes) {
        this.downstreamNodes = downStreamNodes;
    }

    public void downstreamNodes(Set<String> downStreamNodes) {
        this.downstreamNodes = ImmutableList.copyOf(downStreamNodes);
    }

    @Override
    public int downstreamExecutionNodeId() {
        return downstreamExecutionNodeId;
    }

    public void downstreamExecutionNodeId(int downstreamExecutionNodeId) {
        this.downstreamExecutionNodeId = downstreamExecutionNodeId;
    }

    @Override
    public byte downstreamInputId() {
        return downstreamInputId;
    }

    public void downstreamInputId(byte downstreamInputId) {
        this.downstreamInputId = downstreamInputId;
    }


    @Override
    public boolean hasProjections() {
        return projections != null && projections.size() > 0;
    }

    @Override
    public List<Projection> projections() {
        return projections;
    }

    public void projections(List<Projection> projections) {
        this.projections = projections;
    }

    @Override
    public void addProjection(Projection projection) {
        List<Projection> projections = new ArrayList<>(this.projections);
        projections.add(projection);
        this.projections = ImmutableList.copyOf(projections);
    }

    public Optional<Projection> finalProjection() {
        if (projections.size() == 0) {
            return Optional.absent();
        } else {
            return Optional.of(projections.get(projections.size()-1));
        }
    }


    public void outputTypes(List<DataType> outputTypes) {
        this.outputTypes = outputTypes;
    }

    public List<DataType> outputTypes() {
        return outputTypes;
    }

    @Override
    public void inputTypes(List<DataType> dataTypes) {
        this.inputTypes = dataTypes;
    }

    @Override
    public List<DataType> inputTypes() {
        return inputTypes;
    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        name = in.readString();
        jobId = new UUID(in.readLong(), in.readLong());
        executionNodeId = in.readVInt();
        int numDownStreams = in.readVInt();
        downstreamNodes = new ArrayList<>(numDownStreams);
        for (int i = 0; i < numDownStreams; i++) {
            downstreamNodes.add(in.readString());
        }
        downstreamExecutionNodeId = in.readVInt();
        downstreamInputId = in.readByte();

        int numInputCols = in.readVInt();
        inputTypes = new ArrayList<>(numInputCols);
        for (int i = 0; i < numInputCols; i++) {
            inputTypes.add(DataTypes.fromStream(in));
        }
        int numOutputCols = in.readVInt();
        outputTypes = new ArrayList<>(numOutputCols);
        for (int i = 0; i < numOutputCols; i++) {
            outputTypes.add(DataTypes.fromStream(in));
        }

        int numProjections = in.readVInt();
        if (numProjections > 0) {
            projections = new ArrayList<>(numProjections);
            for (int i = 0; i < numProjections; i++) {
                projections.add(Projection.fromStream(in));
            }
        }

    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        out.writeString(name);
        assert jobId != null : "jobId must not be null";
        out.writeLong(jobId.getMostSignificantBits());
        out.writeLong(jobId.getLeastSignificantBits());
        out.writeVInt(executionNodeId);

        if (downstreamNodes != null) {
            out.writeVInt(downstreamNodes.size());
            for (String downstreamNode : downstreamNodes) {
                out.writeString(downstreamNode);
            }
        } else {
            out.writeVInt(0);
        }

        out.writeVInt(downstreamExecutionNodeId);
        out.writeByte(downstreamInputId);

        out.writeVInt(inputTypes.size());
        for (DataType inputType : inputTypes) {
            DataTypes.toStream(inputType, out);
        }
        out.writeVInt(outputTypes.size());
        for (DataType outputType : outputTypes) {
            DataTypes.toStream(outputType, out);
        }

        if (hasProjections()) {
            out.writeVInt(projections.size());
            for (Projection p : projections) {
                Projection.toStream(p, out);
            }
        } else {
            out.writeVInt(0);
        }
    }

    @Override
    public boolean equals(Object o) {
        if (this == o) return true;
        if (o == null || getClass() != o.getClass()) return false;

        AbstractDQLPlanNode node = (AbstractDQLPlanNode) o;

        return !(name != null ? !name.equals(node.name) : node.name != null);

    }

    @Override
    public int hashCode() {
        return name != null ? name.hashCode() : 0;
    }

    @Override
    public String toString() {
        return MoreObjects.toStringHelper(this)
                .add("name", name)
                .add("projections", projections)
                .add("outputTypes", outputTypes)
                .toString();
    }
}

<code block>


package io.crate.planner.node.dql;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableSet;
import com.google.common.collect.Sets;
import io.crate.analyze.EvaluatingNormalizer;
import io.crate.analyze.OrderBy;
import io.crate.analyze.WhereClause;
import io.crate.metadata.Routing;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.node.ExecutionNodeVisitor;
import io.crate.planner.node.PlanNodeVisitor;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.Symbol;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;

import javax.annotation.Nullable;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import java.util.Set;


public class CollectNode extends AbstractDQLPlanNode {

    public static final ExecutionNodeFactory<CollectNode> FACTORY = new ExecutionNodeFactory<CollectNode>() {
        @Override
        public CollectNode create() {
            return new CollectNode();
        }
    };
    private Routing routing;
    private List<Symbol> toCollect;
    private WhereClause whereClause = WhereClause.MATCH_ALL;
    private RowGranularity maxRowGranularity = RowGranularity.CLUSTER;

    private boolean isPartitioned = false;
    private boolean keepContextForFetcher = false;
    private @Nullable String handlerSideCollect = null;

    private @Nullable Integer limit = null;
    private @Nullable OrderBy orderBy = null;

    protected CollectNode() {
        super();
    }

    public CollectNode(int executionNodeId, String name) {
        super(executionNodeId, name);
    }

    public CollectNode(int executionNodeId, String name, Routing routing) {
        this(executionNodeId, name, routing, ImmutableList.<Symbol>of(), ImmutableList.<Projection>of());
    }

    public CollectNode(int executionNodeId, String name, Routing routing, List<Symbol> toCollect, List<Projection> projections) {
        super(executionNodeId, name);
        this.routing = routing;
        this.toCollect = toCollect;
        this.projections = projections;
        downstreamNodes(ImmutableList.of(ExecutionNode.DIRECT_RETURN_DOWNSTREAM_NODE));
    }

    @Override
    public Type type() {
        return Type.COLLECT;
    }


    @Override
    public Set<String> executionNodes() {
        if (routing != null) {
            if (routing.isNullRouting()) {
                return routing.nodes();
            } else {
                return Sets.filter(routing.nodes(), TableInfo.IS_NOT_NULL_NODE_ID);
            }
        } else {
            return ImmutableSet.of();
        }
    }

    public @Nullable Integer limit() {
        return limit;
    }

    public void limit(Integer limit) {
        this.limit = limit;
    }

    public @Nullable OrderBy orderBy() {
        return orderBy;
    }

    public void orderBy(@Nullable OrderBy orderBy) {
        this.orderBy = orderBy;
    }


    public boolean hasDistributingDownstreams() {
        List<String> downstreamNodes = downstreamNodes();
        if (downstreamNodes != null && downstreamNodes.size() > 0) {
            if (downstreamNodes.size() == 1
                    && downstreamNodes.get(0).equals(ExecutionNode.DIRECT_RETURN_DOWNSTREAM_NODE)) {
                return false;
            }
            return true;
        }
        return false;
    }

    public WhereClause whereClause() {
        return whereClause;
    }

    public void whereClause(WhereClause whereClause) {
        assert whereClause != null;
        this.whereClause = whereClause;
    }

    public Routing routing() {
        return routing;
    }

    public List<Symbol> toCollect() {
        return toCollect;
    }

    public void toCollect(List<Symbol> toCollect) {
        assert toCollect != null;
        this.toCollect = toCollect;
    }

    public boolean isRouted() {
        return routing != null && routing.hasLocations();
    }


    public boolean isPartitioned() {
        return isPartitioned;
    }

    public void isPartitioned(boolean isPartitioned) {
        this.isPartitioned = isPartitioned;
    }

    public RowGranularity maxRowGranularity() {
        return maxRowGranularity;
    }

    public void maxRowGranularity(RowGranularity newRowGranularity) {
        if (maxRowGranularity.compareTo(newRowGranularity) < 0) {
            maxRowGranularity = newRowGranularity;
        }
    }

    @Override
    public <C, R> R accept(PlanNodeVisitor<C, R> visitor, C context) {
        return visitor.visitCollectNode(this, context);
    }

    @Override
    public <C, R> R accept(ExecutionNodeVisitor<C, R> visitor, C context) {
        return visitor.visitCollectNode(this, context);
    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        super.readFrom(in);

        int numCols = in.readVInt();
        if (numCols > 0) {
            toCollect = new ArrayList<>(numCols);
            for (int i = 0; i < numCols; i++) {
                toCollect.add(Symbol.fromStream(in));
            }
        } else {
            toCollect = ImmutableList.of();
        }

        maxRowGranularity = RowGranularity.fromStream(in);

        if (in.readBoolean()) {
            routing = new Routing();
            routing.readFrom(in);
        }

        whereClause = new WhereClause(in);
        keepContextForFetcher = in.readBoolean();

        if( in.readBoolean()) {
            limit = in.readVInt();
        }

        if (in.readBoolean()) {
            orderBy = OrderBy.fromStream(in);
        }
        isPartitioned = in.readBoolean();
        handlerSideCollect = in.readOptionalString();
    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        super.writeTo(out);

        int numCols = toCollect.size();
        out.writeVInt(numCols);
        for (int i = 0; i < numCols; i++) {
            Symbol.toStream(toCollect.get(i), out);
        }

        RowGranularity.toStream(maxRowGranularity, out);

        if (routing != null) {
            out.writeBoolean(true);
            routing.writeTo(out);
        } else {
            out.writeBoolean(false);
        }
        whereClause.writeTo(out);
        out.writeBoolean(keepContextForFetcher);

        if (limit != null ) {
            out.writeBoolean(true);
            out.writeVInt(limit);
        } else {
            out.writeBoolean(false);
        }
        if (orderBy != null) {
            out.writeBoolean(true);
            OrderBy.toStream(orderBy, out);
        } else {
            out.writeBoolean(false);
        }
        out.writeBoolean(isPartitioned);
        out.writeOptionalString(handlerSideCollect);
    }


    public CollectNode normalize(EvaluatingNormalizer normalizer) {
        assert whereClause() != null;
        CollectNode result = this;
        List<Symbol> newToCollect = normalizer.normalize(toCollect());
        boolean changed = newToCollect != toCollect();
        WhereClause newWhereClause = whereClause().normalize(normalizer);
        if (newWhereClause != whereClause()) {
            changed = changed || newWhereClause != whereClause();
        }
        if (changed) {
            result = new CollectNode(executionNodeId(), name(), routing, newToCollect, projections);
            result.downstreamNodes(downstreamNodes());
            result.maxRowGranularity = maxRowGranularity;
            result.jobId(jobId());
            result.keepContextForFetcher = keepContextForFetcher;
            result.handlerSideCollect = handlerSideCollect;
            result.isPartitioned(isPartitioned);
            result.whereClause(newWhereClause);
        }
        return result;
    }

    public void keepContextForFetcher(boolean keepContextForFetcher) {
        this.keepContextForFetcher = keepContextForFetcher;
    }

    public boolean keepContextForFetcher() {
        return keepContextForFetcher;
    }

    public void handlerSideCollect(String handlerSideCollect) {
        this.handlerSideCollect = handlerSideCollect;
    }

    @Nullable
    public String handlerSideCollect() {
        return handlerSideCollect;
    }
}
<code block>


package io.crate.planner.node.dql;

import com.google.common.base.MoreObjects;
import com.google.common.base.Preconditions;
import com.google.common.collect.ImmutableSet;
import io.crate.planner.node.ExecutionNodeVisitor;
import io.crate.planner.node.PlanNodeVisitor;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;

import javax.annotation.Nullable;
import java.io.IOException;
import java.util.Arrays;
import java.util.HashSet;
import java.util.Set;


public class MergeNode extends AbstractDQLPlanNode {

    public static final ExecutionNodeFactory<MergeNode> FACTORY = new ExecutionNodeFactory<MergeNode>() {
        @Override
        public MergeNode create() {
            return new MergeNode();
        }
    };

    private int numUpstreams;
    private Set<String> executionNodes;


    private boolean sortedInputOutput = false;
    private int[] orderByIndices;
    private boolean[] reverseFlags;
    private Boolean[] nullsFirst;

    public MergeNode() {
        numUpstreams = 0;
    }

    public MergeNode(int executionNodeId, String name, int numUpstreams) {
        super(executionNodeId, name);
        this.numUpstreams = numUpstreams;
    }

    public static MergeNode sortedMergeNode(int executionNodeId,
                                            String name,
                                            int numUpstreams,
                                            int[] orderByIndices,
                                            boolean[] reverseFlags,
                                            Boolean[] nullsFirst) {
        Preconditions.checkArgument(
                orderByIndices.length == reverseFlags.length && reverseFlags.length == nullsFirst.length,
                "ordering parameters must be of the same length");
        MergeNode mergeNode = new MergeNode(executionNodeId, name, numUpstreams);
        mergeNode.sortedInputOutput = true;
        mergeNode.orderByIndices = orderByIndices;
        mergeNode.reverseFlags = reverseFlags;
        mergeNode.nullsFirst = nullsFirst;
        return mergeNode;
    }

    @Override
    public Type type() {
        return Type.MERGE;
    }

    @Override
    public Set<String> executionNodes() {
        if (executionNodes == null) {
            return ImmutableSet.of();
        } else {
            return executionNodes;
        }
    }

    public void executionNodes(Set<String> executionNodes) {
        this.executionNodes = executionNodes;
    }

    public int numUpstreams() {
        return numUpstreams;
    }

    public boolean sortedInputOutput() {
        return sortedInputOutput;
    }

    @Nullable
    public int[] orderByIndices() {
        return orderByIndices;
    }

    @Nullable
    public boolean[] reverseFlags() {
        return reverseFlags;
    }

    @Nullable
    public Boolean[] nullsFirst() {
        return nullsFirst;
    }

    @Override
    public <C, R> R accept(PlanNodeVisitor<C, R> visitor, C context) {
        return visitor.visitMergeNode(this, context);
    }

    @Override
    public <C, R> R accept(ExecutionNodeVisitor<C, R> visitor, C context) {
        return visitor.visitMergeNode(this, context);
    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        super.readFrom(in);
        numUpstreams = in.readVInt();

        int numExecutionNodes = in.readVInt();

        if (numExecutionNodes > 0) {
            executionNodes = new HashSet<>(numExecutionNodes);
            for (int i = 0; i < numExecutionNodes; i++) {
                executionNodes.add(in.readString());
            }
        }

        sortedInputOutput = in.readBoolean();
        if (sortedInputOutput) {
            int orderByIndicesLength = in.readVInt();
            orderByIndices = new int[orderByIndicesLength];
            reverseFlags = new boolean[orderByIndicesLength];
            nullsFirst = new Boolean[orderByIndicesLength];
            for (int i = 0; i < orderByIndicesLength; i++) {
                orderByIndices[i] = in.readVInt();
                reverseFlags[i] = in.readBoolean();
                nullsFirst[i] = in.readOptionalBoolean();
            }
        }
    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        super.writeTo(out);
        out.writeVInt(numUpstreams);

        if (executionNodes == null) {
            out.writeVInt(0);
        } else {
            out.writeVInt(executionNodes.size());
            for (String node : executionNodes) {
                out.writeString(node);
            }
        }

        out.writeBoolean(sortedInputOutput);
        if (sortedInputOutput) {
            out.writeVInt(orderByIndices.length);
            for (int i = 0; i < orderByIndices.length; i++) {
                out.writeVInt(orderByIndices[i]);
                out.writeBoolean(reverseFlags[i]);
                out.writeOptionalBoolean(nullsFirst[i]);
            }
        }
    }

    @Override
    public String toString() {
        MoreObjects.ToStringHelper helper = MoreObjects.toStringHelper(this)
                .add("executionNodeId", executionNodeId())
                .add("name", name())
                .add("projections", projections)
                .add("outputTypes", outputTypes)
                .add("jobId", jobId())
                .add("numUpstreams", numUpstreams)
                .add("executionNodes", executionNodes)
                .add("inputTypes", inputTypes())
                .add("sortedInputOutput", sortedInputOutput);
        if (sortedInputOutput) {
            helper.add("orderByIndices", Arrays.toString(orderByIndices))
                  .add("reverseFlags", Arrays.toString(reverseFlags))
                  .add("nullsFirst", Arrays.toString(nullsFirst));
        }
        return helper.toString();
    }

}

<code block>
package io.crate.planner.node.dql;


import io.crate.planner.PlanAndPlannedAnalyzedRelation;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.PlanVisitor;
import io.crate.planner.projection.Projection;

import javax.annotation.Nullable;

public class QueryAndFetch extends PlanAndPlannedAnalyzedRelation {

    private final CollectNode collectNode;
    @Nullable
    private MergeNode localMergeNode;

    public QueryAndFetch(CollectNode collectNode, @Nullable MergeNode localMergeNode){
        this.collectNode = collectNode;
        this.localMergeNode = localMergeNode;
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitQueryAndFetch(this, context);
    }

    public CollectNode collectNode() {
        return collectNode;
    }

    @Nullable
    public MergeNode localMergeNode(){
        return localMergeNode;
    }

    @Override
    public void addProjection(Projection projection) {
        DQLPlanNode node = resultNode();
        node.addProjection(projection);
        if (node instanceof CollectNode) {
            PlanNodeBuilder.setOutputTypes((CollectNode)node);
        } else if (node instanceof MergeNode) {
            PlanNodeBuilder.connectTypes(collectNode, node);
        }
    }

    @Override
    public boolean resultIsDistributed() {
        return localMergeNode == null;
    }

    @Override
    public DQLPlanNode resultNode() {
        return localMergeNode == null ? collectNode : localMergeNode;
    }
}

<code block>


package io.crate.planner.node.dql.join;

import com.google.common.base.MoreObjects;
import com.google.common.collect.ImmutableSet;
import io.crate.planner.node.ExecutionNodeVisitor;
import io.crate.planner.node.PlanNodeVisitor;
import io.crate.planner.node.dql.AbstractDQLPlanNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.elasticsearch.common.Nullable;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;

import java.io.IOException;
import java.util.ArrayList;
import java.util.HashSet;
import java.util.List;
import java.util.Set;

public class NestedLoopNode extends AbstractDQLPlanNode {

    public static final ExecutionNodeFactory<NestedLoopNode> FACTORY = new ExecutionNodeFactory<NestedLoopNode>() {
        @Override
        public NestedLoopNode create() {
            return new NestedLoopNode();
        }
    };

    private Set<String> executionNodes;

    private List<DataType> leftInputTypes;
    private List<DataType> rightInputTypes;

    @Nullable
    private MergeNode leftMergeNode;
    @Nullable
    private MergeNode rightMergeNode;

    public NestedLoopNode() {}

    public NestedLoopNode(int executionNodeId, String name) {
        super(executionNodeId, name);
    }

    @Override
    public Type type() {
        return Type.NESTED_LOOP;
    }

    @Override
    public Set<String> executionNodes() {
        if (executionNodes == null) {
            return ImmutableSet.of();
        } else {
            return executionNodes;
        }
    }

    public void leftMergeNode(MergeNode leftMergeNode) {
        this.leftMergeNode = leftMergeNode;
    }

    @Nullable
    public MergeNode leftMergeNode() {
        return leftMergeNode;
    }

    public void rightMergeNode(MergeNode rightMergeNode) {
        this.rightMergeNode = rightMergeNode;
    }

    @Nullable
    public MergeNode rightMergeNode() {
        return rightMergeNode;
    }

    public void executionNodes(Set<String> executionNodes) {
        this.executionNodes = executionNodes;
    }

    public List<DataType> leftInputTypes() {
        return leftInputTypes;
    }

    public void leftInputTypes(List<DataType> leftInputTypes) {
        this.leftInputTypes = leftInputTypes;
    }

    public List<DataType> rightInputTypes() {
        return rightInputTypes;
    }

    public void rightInputTypes(List<DataType> rightInputTypes) {
        this.rightInputTypes = rightInputTypes;
    }

    @Override
    public List<DataType> inputTypes() {
        throw new UnsupportedOperationException("inputsTypes not supported. " +
                "Use leftInputTypes() or rightInputTypes()");
    }

    @Override
    public void inputTypes(List<DataType> inputTypes) {
        throw new UnsupportedOperationException("inputsTypes not supported. " +
                "Use leftInputTypes() or rightInputTypes()");
    }

    @Override
    public <C, R> R accept(ExecutionNodeVisitor<C, R> visitor, C context) {
        return visitor.visitNestedLoopNode(this, context);
    }


    @Override
    public <C, R> R accept(PlanNodeVisitor<C, R> visitor, C context) {
        return visitor.visitNestedLoopNode(this, context);
    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        super.readFrom(in);

        int leftNumCols = in.readVInt();
        if (leftNumCols > 0) {
            leftInputTypes = new ArrayList<>(leftNumCols);
            for (int i = 0; i < leftNumCols; i++) {
                leftInputTypes.add(DataTypes.fromStream(in));
            }
        }

        int rightNumCols = in.readVInt();
        if (rightNumCols > 0) {
            rightInputTypes = new ArrayList<>(rightNumCols);
            for (int i = 0; i < rightNumCols; i++) {
                rightInputTypes.add(DataTypes.fromStream(in));
            }
        }
        int numExecutionNodes = in.readVInt();
        if (numExecutionNodes > 0) {
            executionNodes = new HashSet<>(numExecutionNodes);
            for (int i = 0; i < numExecutionNodes; i++) {
                executionNodes.add(in.readString());
            }
        }
        if (in.readBoolean()) {
            leftMergeNode = new MergeNode();
            leftMergeNode.readFrom(in);
        }
        if (in.readBoolean()) {
            rightMergeNode = new MergeNode();
            rightMergeNode.readFrom(in);
        }
    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        super.writeTo(out);

        int leftNumCols = leftInputTypes().size();
        out.writeVInt(leftNumCols);
        for (DataType inputType : leftInputTypes) {
            DataTypes.toStream(inputType, out);
        }

        int rightNumCols = rightInputTypes().size();
        out.writeVInt(rightNumCols);
        for (DataType inputType : rightInputTypes) {
            DataTypes.toStream(inputType, out);
        }

        if (executionNodes == null) {
            out.writeVInt(0);
        } else {
            out.writeVInt(executionNodes.size());
            for (String node : executionNodes) {
                out.writeString(node);
            }
        }

        if (leftMergeNode == null) {
            out.writeBoolean(false);
        } else {
            out.writeBoolean(true);
            leftMergeNode.writeTo(out);
        }
        if (rightMergeNode == null) {
            out.writeBoolean(false);
        } else {
            out.writeBoolean(true);
            rightMergeNode.writeTo(out);
        }
    }

    @Override
    public String toString() {
        MoreObjects.ToStringHelper helper = MoreObjects.toStringHelper(this)
                .add("executionNodeId", executionNodeId())
                .add("name", name())
                .add("outputTypes", outputTypes)
                .add("jobId", jobId())
                .add("executionNodes", executionNodes)
                .add("leftInputTypes", leftInputTypes)
                .add("rightInputTypes", rightInputTypes);
        return helper.toString();
    }
}

<code block>

package io.crate.planner.node.dql.join;

import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.planner.PlanAndPlannedAnalyzedRelation;
import io.crate.planner.PlanVisitor;
import io.crate.planner.node.dql.DQLPlanNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.Projection;
import org.elasticsearch.common.Nullable;


public class NestedLoop extends PlanAndPlannedAnalyzedRelation {


    private final PlannedAnalyzedRelation left;
    private final PlannedAnalyzedRelation right;
    private final NestedLoopNode nestedLoopNode;
    @Nullable
    private MergeNode localMergeNode;

    private boolean leftOuterLoop = true;


    public NestedLoop(PlannedAnalyzedRelation left,
                      PlannedAnalyzedRelation right,
                      NestedLoopNode nestedLoopNode,
                      boolean leftOuterLoop) {
        this.leftOuterLoop = leftOuterLoop;
        this.left = left;
        this.right = right;
        this.nestedLoopNode = nestedLoopNode;
    }

    public PlannedAnalyzedRelation left() {
        return left;
    }

    public PlannedAnalyzedRelation right() {
        return right;
    }

    public PlannedAnalyzedRelation inner() {
        return leftOuterLoop() ? right : left;
    }

    public PlannedAnalyzedRelation outer() {
        return leftOuterLoop() ? left : right;
    }

    public boolean leftOuterLoop() {
        return leftOuterLoop;
    }

    public void localMergeNode(MergeNode localMergeNode) {
        this.localMergeNode = localMergeNode;
    }

    public NestedLoopNode nestedLoopNode() {
        return nestedLoopNode;
    }

    @Nullable
    public MergeNode localMergeNode() {
        return localMergeNode;
    }

    @Override
    public void addProjection(Projection projection) {
    }

    @Override
    public boolean resultIsDistributed() {
        return false;
    }

    @Override
    public DQLPlanNode resultNode() {
        return localMergeNode == null ? outer().resultNode() : localMergeNode;
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitNestedLoop(this, context);
    }
}

<code block>


package io.crate.integrationtests;

import io.crate.Constants;
import io.crate.action.sql.SQLResponse;
import io.crate.testing.SQLTransportExecutor;
import org.elasticsearch.common.settings.ImmutableSettings;

import java.util.Arrays;
import java.util.HashMap;
import java.util.Map;

import static com.google.common.collect.Maps.newHashMap;
import static org.hamcrest.MatcherAssert.assertThat;
import static org.hamcrest.Matchers.is;

public class Setup {

    private final SQLTransportExecutor transportExecutor;

    public Setup(SQLTransportExecutor transportExecutor) {
        this.transportExecutor = transportExecutor;
    }

    public void setUpLocations() throws Exception {
        transportExecutor.exec("create table locations (" +
            " id string primary key," +
            " name string," +
            " date timestamp," +
            " kind string," +
            " position integer," +
            " description string," +
            " race object," +
            " index name_description_ft using fulltext(name, description) with (analyzer='english')" +
            ") clustered by(id) into 2 shards with(number_of_replicas=0)");

        String insertStmt = "insert into locations " +
                "(id, name, date, kind, position, description, race) " +
                "values (?, ?, ?, ?, ?, ?, ?)";
        Object[][] rows = new Object[][]{
                new Object[]{"1", "North West Ripple", "1979-10-12",
                        "Galaxy", 1, "Relative to life on NowWhat, living on an affluent " +
                        "world in the North West ripple of the Galaxy is said to be easier " +
                        "by a factor of about seventeen million.", null
                },
                new Object[]{
                        "2", "Outer Eastern Rim", "1979-10-12", "Galaxy", 2, "The Outer Eastern Rim " +
                        "of the Galaxy where the Guide has supplanted the Encyclopedia Galactica " +
                        "among its more relaxed civilisations.", null
                },
                new Object[]{
                        "3","Galactic Sector QQ7 Active J Gamma", "2013-05-01",  "Galaxy",  4,
                        "Galactic Sector QQ7 Active J Gamma contains the Sun Zarss, " +
                        "the planet Preliumtarn of the famed Sevorbeupstry and " +
                        "Quentulus Quazgar Mountains.", null
                },
                new Object[]{
                        "4", "Aldebaran", "2013-07-16",  "Star System",  1,
                        "Max Quordlepleen claims that the only thing left after the end " +
                        "of the Universe will be the sweets trolley and a fine selection " +
                        "of Aldebaran liqueurs.", null
                },
                new Object[]{
                        "5",  "Algol", "2013-07-16",  "Star System",  2,
                        "Algol is the home of the Algolian Suntiger, " +
                        "the tooth of which is one of the ingredients of the " +
                        "Pan Galactic Gargle Blaster.", null
                },
                new Object[]{
                        "6",  "Alpha Centauri", "1979-10-12",  "Star System",  3,
                        "4.1 light-years northwest of earth", null
                },
                new Object[]{
                        "7",  "Altair", "2013-07-16",  "Star System",  4,
                        "The Altairian dollar is one of three freely convertible currencies in the galaxy, " +
                        "though by the time of the novels it had apparently recently collapsed.",
                        null
                },
                new Object[]{
                        "8",  "Allosimanius Syneca", "2013-07-16",  "Planet",  1,
                        "Allosimanius Syneca is a planet noted for ice, snow, " +
                        "mind-hurtling beauty and stunning cold.", null
                },
                new Object[]{
                        "9",  "Argabuthon", "2013-07-16",  "Planet",  2,
                        "It is also the home of Prak, a man placed into solitary confinement " +
                        "after an overdose of truth drug caused him to tell the Truth in its absolute " +
                        "and final form, causing anyone to hear it to go insane.", null,
                },
                new Object[]{
                        "10",  "Arkintoofle Minor", "1979-10-12",  "Planet",  3,
                        "Motivated by the fact that the only thing in the Universe that " +
                        "travels faster than light is bad news, the Hingefreel people native " +
                        "to Arkintoofle Minor constructed a starship driven by bad news.", null
                },
                new Object[]{
                        "11",  "Bartledan", "2013-07-16",  "Planet",  4,
                        "An Earthlike planet on which Arthur Dent lived for a short time, " +
                                "Bartledan is inhabited by Bartledanians, a race that appears human but only physically.",
                        new HashMap<String, Object>(){{
                            put("name", "Bartledannians");
                            put("description", "Similar to humans, but do not breathe");
                            put("interests", "netball");
                        }}
                },
                new Object[]{
                        "12",  "", "2013-07-16",  "Planet",  5,  "This Planet doesn't really exist", null
                },
                new Object[]{
                        "13",  "End of the Galaxy", "2013-07-16",  "Galaxy",  6,  "The end of the Galaxy.%", null
                }
        };
        transportExecutor.exec(insertStmt, rows);
        transportExecutor.refresh("locations");
    }

    public void groupBySetup() throws Exception {
        groupBySetup("integer");
    }

    public void groupBySetup(String numericType) throws Exception {
        transportExecutor.exec(String.format("create table characters (" +
            " race string," +
            " gender string," +
            " age %s," +
            " birthdate timestamp," +
            " name string," +
            " details object as (job string)," +
            " details_ignored object(ignored)" +
            ")", numericType));
        transportExecutor.ensureYellowOrGreen();

        Map<String, String> details = newHashMap();
        details.put("job", "Sandwitch Maker");
        transportExecutor.exec("insert into characters (race, gender, age, birthdate, name, details) values (?, ?, ?, ?, ?, ?)",
            new Object[]{"Human", "male", 34, "1975-10-01", "Arthur Dent", details});

        details = newHashMap();
        details.put("job", "Mathematician");
        transportExecutor.exec("insert into characters (race, gender, age, birthdate, name, details) values (?, ?, ?, ?, ?, ?)",
            new Object[]{"Human", "female", 32, "1978-10-11", "Trillian", details});
        transportExecutor.exec("insert into characters (race, gender, age, birthdate, name, details) values (?, ?, ?, ?, ?, ?)",
            new Object[]{"Human", "female", 43, "1970-01-01", "Anjie", null});
        transportExecutor.exec("insert into characters (race, gender, age, name) values (?, ?, ?, ?)",
            new Object[]{"Human", "male", 112, "Ford Perfect"});

        transportExecutor.exec("insert into characters (race, gender, name) values ('Android', 'male', 'Marving')");
        transportExecutor.exec("insert into characters (race, gender, name) values ('Vogon', 'male', 'Jeltz')");
        transportExecutor.exec("insert into characters (race, gender, name) values ('Vogon', 'male', 'Kwaltz')");
        transportExecutor.refresh("characters");
    }

    public void setUpEmployees() {
        transportExecutor.exec("create table employees (" +
            " name string, " +
            " department string," +
            " hired timestamp, " +
            " age short," +
            " income double, " +
            " good boolean" +
            ") with (number_of_replicas=0)");
        transportExecutor.ensureGreen();
        transportExecutor.exec("insert into employees (name, department, hired, age, income, good) values (?, ?, ?, ?, ?, ?)",
            new Object[]{"dilbert", "engineering", "1985-01-01", 47, 4000.0, true});
        transportExecutor.exec("insert into employees (name, department, hired, age, income, good) values (?, ?, ?, ?, ?, ?)",
            new Object[]{"wally", "engineering", "2000-01-01", 54, 6000.0, true});
        transportExecutor.exec("insert into employees (name, department, hired, age, income, good) values (?, ?, ?, ?, ?, ?)",
            new Object[]{"pointy haired boss", "management", "2010-10-10", 45, Double.MAX_VALUE, false});

        transportExecutor.exec("insert into employees (name, department, hired, age, income, good) values (?, ?, ?, ?, ?, ?)",
            new Object[]{"catbert", "HR", "1990-01-01", 12, 999999999.99, false});
        transportExecutor.exec("insert into employees (name, department, income) values (?, ?, ?)",
            new Object[]{"ratbert", "HR", 0.50});
        transportExecutor.exec("insert into employees (name, department, age) values (?, ?, ?)",
            new Object[]{"asok", "internship", 28});
        transportExecutor.refresh("employees");
    }

    public void setUpObjectTable() {
        transportExecutor.exec("create table ot (" +
                "  title string," +
                "  author object(dynamic) as (" +
                "    name object(strict) as (" +
                "      first_name string," +
                "      last_name string" +
                "    )," +
                "    age integer" +
                "  )," +
                "  details object(ignored) as (" +
                "    num_pages integer" +
                "  )" +
                ") with (number_of_replicas = 0)");
        transportExecutor.exec("insert into ot (title, author, details) values (?, ?, ?)",
                new Object[]{
                        "The Hitchhiker's Guide to the Galaxy",
                        new HashMap<String, Object>() {{
                            put("name", new HashMap<String, Object>() {{
                                put("first_name", "Douglas");
                                put("last_name", "Adams");
                            }});
                            put("age", 49);
                        }},
                        new HashMap<String, Object>() {{
                            put("num_pages", 224);
                        }}
                }
        );
        transportExecutor.refresh("ot");
    }

    public void setUpObjectMappingWithUnknownTypes() throws Exception {
        transportExecutor.prepareCreate("ut")
                .setSettings(ImmutableSettings.builder().put("number_of_replicas", 0).put("number_of_shards", 2).build())
                .addMapping(Constants.DEFAULT_MAPPING_TYPE, new HashMap<String, Object>(){{
                    put("properties", new HashMap<String, Object>(){{
                        put("name", new HashMap<String, Object>(){{
                            put("type", "string");
                            put("store", "false");
                            put("index", "not_analyzed");
                        }});
                        put("location", new HashMap<String, Object>(){{
                            put("type", "geo_shape");
                        }});
                        put("o", new HashMap<String, Object>(){{
                            put("type", "object");
                        }});
                        put("population", new HashMap<String, Object>(){{
                            put("type", "long");
                            put("store", "false");
                            put("index", "not_analyzed");
                        }});
                    }});
                }}).execute().actionGet();
        transportExecutor.client().prepareIndex("ut", Constants.DEFAULT_MAPPING_TYPE, "id1")
                .setSource("{\"name\":\"Berlin\",\"location\":{\"type\": \"point\", \"coordinates\": [52.5081, 13.4416]}, \"population\":3500000}")
                .execute().actionGet();
        transportExecutor.client().prepareIndex("ut", Constants.DEFAULT_MAPPING_TYPE, "id2")
                .setSource("{\"name\":\"Dornbirn\",\"location\":{\"type\": \"point\", \"coordinates\": [47.3904,9.7562]}, \"population\":46080}")
                .execute().actionGet();
        transportExecutor.refresh("ut");
    }

    public void setUpArrayTables() {
        transportExecutor.exec("create table any_table (" +
                "  id int primary key," +
                "  temps array(double)," +
                "  names array(string)," +
                "  tags array(string)" +
                ") with (number_of_replicas=0)");
        transportExecutor.ensureGreen();
        SQLResponse response = transportExecutor.exec("insert into any_table (id, temps, names, tags) values (?,?,?,?), (?,?,?,?), (?,?,?,?), (?,?,?,?)",
                        1, Arrays.asList(0L, 0L, 0L), Arrays.asList("Dornbirn", "Berlin", "St. Margrethen"), Arrays.asList("cool"),
                        2, Arrays.asList(0, 1, -1), Arrays.asList("Dornbirn", "Dornbirn", "Dornbirn"), Arrays.asList("cool", null),
                        3, Arrays.asList(42, -42), Arrays.asList("Hangelsberg", "Berlin"), Arrays.asList("kuhl", "cool"),
                        4, null, null, Arrays.asList("kuhl", null)
                );
        assertThat(response.rowCount(), is(4L));
        transportExecutor.refresh("any_table");
    }

    public void partitionTableSetup() {
        transportExecutor.exec("create table parted (" +
                "id int primary key," +
                "date timestamp primary key," +
                "o object(ignored)" +
                ") partitioned by (date) with (number_of_replicas=0)");
        transportExecutor.ensureGreen();
        transportExecutor.exec("insert into parted (id, date) values (1, '2014-01-01')");
        transportExecutor.exec("insert into parted (id, date) values (2, '2014-01-01')");
        transportExecutor.exec("insert into parted (id, date) values (3, '2014-02-01')");
        transportExecutor.exec("insert into parted (id, date) values (4, '2014-02-01')");
        transportExecutor.exec("insert into parted (id, date) values (5, '2014-02-01')");
        transportExecutor.refresh("parted");
    }

    public void createTestTableWithPrimaryKey() {
        transportExecutor.exec("create table test (" +
                "  pk_col string primary key, " +
                "  message string" +
                ") with (number_of_replicas=0)");
        transportExecutor.ensureGreen();
    }

    public void setUpCharacters() {
        transportExecutor.exec("create table characters (id int primary key, name string, female boolean, details object)");
        transportExecutor.ensureGreen();
        transportExecutor.exec("insert into characters (id, name, female) values (?, ?, ?)",
                new Object[][]{
                        new Object[] { 1, "Arthur", false},
                        new Object[] { 2, "Ford", false},
                        new Object[] { 3, "Trillian", true},
                        new Object[] { 4, "Arthur", true}
                }
        );
        transportExecutor.refresh("characters");
    }

    public void setUpPartitionedTableWithName() {
        transportExecutor.exec("create table parted (id int, name string, date timestamp) partitioned by (date)");
        transportExecutor.ensureGreen();
        transportExecutor.exec("insert into parted (id, name, date) values (?, ?, ?), (?, ?, ?), (?, ?, ?)",
                new Object[]{
                        1, "Trillian", null,
                        2, null, 0L,
                        3, "Ford", 1396388720242L
                });
        transportExecutor.ensureGreen();
        transportExecutor.refresh("parted");
    }
}

<code block>


package io.crate.operation.join;

import com.carrotsearch.randomizedtesting.annotations.Repeat;
import io.crate.core.collections.Bucket;
import io.crate.core.collections.Row;
import io.crate.core.collections.Row1;
import io.crate.operation.Input;
import io.crate.operation.RowDownstreamHandle;
import io.crate.operation.RowUpstream;
import io.crate.operation.collect.CollectExpression;
import io.crate.operation.collect.InputCollectExpression;
import io.crate.operation.projectors.SimpleTopNProjector;
import io.crate.test.integration.CrateUnitTest;
import io.crate.testing.CollectingProjector;
import io.crate.testing.TestingHelpers;
import org.junit.Test;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collections;
import java.util.List;
import java.util.concurrent.TimeUnit;

import static org.hamcrest.core.Is.is;

public class NestedLoopOperationTest extends CrateUnitTest {

    private Bucket executeNestedLoop(List<Row> leftRows, List<Row> rightRows) throws Exception {
        NestedLoopOperation nestedLoopOperation = new NestedLoopOperation();

        RowUpstream dummyUpstream = new RowUpstream() {};

        final RowDownstreamHandle left = nestedLoopOperation.registerUpstream(dummyUpstream);
        final RowDownstreamHandle right = nestedLoopOperation.registerUpstream(dummyUpstream);

        CollectingProjector collectingProjector = new CollectingProjector();
        nestedLoopOperation.downstream(collectingProjector);

        Thread t1 = sendRowsThreaded("left", left, leftRows);
        Thread t2 = sendRowsThreaded("right", right, rightRows);
        t1.join();
        t2.join();
        return collectingProjector.result().get(2, TimeUnit.SECONDS);
    }

    private List<Row> asRows(Object ...rows) {
        List<Row> result = new ArrayList<>(rows.length);
        for (Object row : rows) {
            result.add(new Row1(row));
        }
        return result;
    }

    @Test
    public void testLeftSideEmpty() throws Exception {
        Bucket rows = executeNestedLoop(Collections.<Row>emptyList(), asRows("small", "medium"));
        assertThat(rows.size(), is(0));
    }

    @Test
    public void testRightSideIsEmpty() throws Exception {
        Bucket rows = executeNestedLoop(asRows("small", "medium"), Collections.<Row>emptyList());
        assertThat(rows.size(), is(0));
    }

    @Test
    @Repeat(iterations = 5)
    public void testNestedLoopOperation() throws Exception {
        List<Row> leftRows = asRows("green", "blue", "red");
        List<Row> rightRows = asRows("small", "medium");

        Bucket rows = executeNestedLoop(leftRows, rightRows);
        assertThat(TestingHelpers.printedTable(rows), is("" +
                "green| small\n" +
                "green| medium\n" +
                "blue| small\n" +
                "blue| medium\n" +
                "red| small\n" +
                "red| medium\n"));
    }

    @Test
    @Repeat (iterations = 5)
    public void testNestedLoopWithTopNDownstream() throws Exception {
        RowUpstream dummyUpstream = new RowUpstream() {};
        NestedLoopOperation nestedLoopOperation = new NestedLoopOperation();
        final RowDownstreamHandle left = nestedLoopOperation.registerUpstream(dummyUpstream);
        final RowDownstreamHandle right = nestedLoopOperation.registerUpstream(dummyUpstream);

        InputCollectExpression<Object> firstCol = new InputCollectExpression<>(0);
        InputCollectExpression<Object> secondCol = new InputCollectExpression<>(1);
        SimpleTopNProjector topNProjector = new SimpleTopNProjector(
                Arrays.<Input<?>>asList(firstCol, secondCol),
                new CollectExpression[] { firstCol, secondCol },
                3,
                1
        );
        nestedLoopOperation.downstream(topNProjector);
        CollectingProjector collectingProjector = new CollectingProjector();
        topNProjector.downstream(collectingProjector);

        Thread leftT = sendRowsThreaded("left", left, asRows("green", "blue", "red"));
        Thread rightT = sendRowsThreaded("right", right, asRows("small", "medium"));

        Bucket rows = collectingProjector.result().get(2, TimeUnit.SECONDS);
        assertThat(TestingHelpers.printedTable(rows), is("" +
                "green| medium\n" +
                "blue| small\n" +
                "blue| medium\n"));

        leftT.join();
        rightT.join();
    }

    private Thread sendRowsThreaded(String name, final RowDownstreamHandle downstreamHandle, final List<Row> rows) {
        Thread t = new Thread() {
            @Override
            public void run() {
                try {
                    for (Row row : rows) {
                        downstreamHandle.setNextRow(row);
                    }
                    downstreamHandle.finish();
                } catch (Throwable t) {
                    t.printStackTrace();
                }
            }
        };
        t.setName(name);
        t.setDaemon(true);
        t.start();
        return t;
    }

}

<code block>


package io.crate.executor.transport;

import com.google.common.collect.ImmutableList;
import io.crate.analyze.QuerySpec;
import io.crate.analyze.WhereClause;
import io.crate.analyze.where.DocKeys;
import io.crate.integrationtests.SQLTransportIntegrationTest;
import io.crate.integrationtests.Setup;
import io.crate.metadata.ColumnIdent;
import io.crate.metadata.ReferenceIdent;
import io.crate.metadata.ReferenceInfo;
import io.crate.metadata.TableIdent;
import io.crate.metadata.doc.DocSchemaInfo;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.dql.ESGetNode;
import io.crate.planner.symbol.Literal;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.testing.TestingHelpers;
import io.crate.types.DataTypes;
import org.elasticsearch.cluster.ClusterService;
import org.junit.After;
import org.junit.Before;

import java.util.ArrayList;
import java.util.List;

public class BaseTransportExecutorTest extends SQLTransportIntegrationTest {

    Setup setup = new Setup(sqlExecutor);

    static {
        ClassLoader.getSystemClassLoader().setDefaultAssertionStatus(true);
    }

    TransportExecutor executor;
    DocSchemaInfo docSchemaInfo;
    ClusterService clusterService;

    TableIdent charactersIdent = new TableIdent(null, "characters");
    TableIdent booksIdent = new TableIdent(null, "books");

    Reference idRef = new Reference(new ReferenceInfo(
            new ReferenceIdent(charactersIdent, "id"), RowGranularity.DOC, DataTypes.INTEGER));
    Reference nameRef = new Reference(new ReferenceInfo(
            new ReferenceIdent(charactersIdent, "name"), RowGranularity.DOC, DataTypes.STRING));
    Reference femaleRef = TestingHelpers.createReference(charactersIdent.name(), new ColumnIdent("female"), DataTypes.BOOLEAN);

    TableIdent partedTable = new TableIdent("doc", "parted");
    Reference partedIdRef = new Reference(new ReferenceInfo(
            new ReferenceIdent(partedTable, "id"), RowGranularity.DOC, DataTypes.INTEGER));
    Reference partedNameRef = new Reference(new ReferenceInfo(
            new ReferenceIdent(partedTable, "name"), RowGranularity.DOC, DataTypes.STRING));
    Reference partedDateRef = new Reference(new ReferenceInfo(
            new ReferenceIdent(partedTable, "date"), RowGranularity.PARTITION, DataTypes.TIMESTAMP));

    public static ESGetNode newGetNode(TableInfo tableInfo, List<Symbol> outputs, List<String> singleStringKeys, int executionNodeId) {
        QuerySpec querySpec = new QuerySpec();
        querySpec.outputs(outputs);
        List<List<Symbol>> keys = new ArrayList<>(singleStringKeys.size());
        for (String v : singleStringKeys) {
            keys.add(ImmutableList.<Symbol>of(Literal.newLiteral(v)));
        }
        WhereClause whereClause = new WhereClause(null, new DocKeys(keys, false, -1, null), null);
        querySpec.where(whereClause);
        return new ESGetNode(executionNodeId, tableInfo, querySpec);
    }

    @Before
    public void transportSetUp() {
        String[] nodeNames = internalCluster().getNodeNames();
        String handlerNodeName = nodeNames[randomIntBetween(0, nodeNames.length-1)];
        executor = internalCluster().getInstance(TransportExecutor.class, handlerNodeName);
        docSchemaInfo = internalCluster().getInstance(DocSchemaInfo.class, handlerNodeName);
        clusterService = internalCluster().getInstance(ClusterService.class, handlerNodeName);
    }

    @After
    public void transportTearDown() {
        executor = null;
        docSchemaInfo = null;
    }
}

<code block>


package io.crate.executor.transport;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.Lists;
import com.google.common.collect.Sets;
import com.google.common.util.concurrent.ListenableFuture;
import io.crate.Constants;
import io.crate.analyze.OrderBy;
import io.crate.analyze.WhereClause;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.core.collections.Bucket;
import io.crate.executor.Job;
import io.crate.executor.RowCountResult;
import io.crate.executor.Task;
import io.crate.executor.TaskResult;
import io.crate.executor.transport.task.KillTask;
import io.crate.executor.transport.task.SymbolBasedUpsertByIdTask;
import io.crate.executor.transport.task.elasticsearch.ESDeleteByQueryTask;
import io.crate.executor.transport.task.elasticsearch.ESGetTask;
import io.crate.metadata.*;
import io.crate.metadata.doc.DocSysColumns;
import io.crate.metadata.doc.DocTableInfo;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.aggregation.impl.CountAggregation;
import io.crate.operation.operator.EqOperator;
import io.crate.operation.projectors.TopN;
import io.crate.operation.scalar.DateTruncFunction;
import io.crate.planner.*;
import io.crate.planner.node.dml.ESDeleteByQueryNode;
import io.crate.planner.node.dml.SymbolBasedUpsertByIdNode;
import io.crate.planner.node.dml.Upsert;
import io.crate.planner.node.dql.*;
import io.crate.planner.node.dql.join.NestedLoop;
import io.crate.planner.node.dql.join.NestedLoopNode;
import io.crate.planner.node.management.KillPlan;
import io.crate.planner.projection.*;
import io.crate.planner.projection.builder.ProjectionBuilder;
import io.crate.planner.symbol.*;
import io.crate.testing.TestingHelpers;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.cluster.routing.operation.plain.Preference;
import org.elasticsearch.common.collect.MapBuilder;
import org.elasticsearch.search.SearchHits;
import org.junit.Before;
import org.junit.Rule;
import org.junit.Test;
import org.junit.rules.ExpectedException;

import java.util.*;

import static io.crate.testing.TestingHelpers.isRow;
import static java.util.Arrays.asList;
import static org.hamcrest.Matchers.*;
import static org.hamcrest.core.Is.is;

public class TransportExecutorTest extends BaseTransportExecutorTest {

    @Rule
    public ExpectedException expectedException = ExpectedException.none();

    @Before
    public void setup() {
    }

    protected ESGetNode newGetNode(String tableName, List<Symbol> outputs, String singleStringKey, int executionNodeId) {
        return newGetNode(tableName, outputs, Collections.singletonList(singleStringKey), executionNodeId);
    }

    protected ESGetNode newGetNode(String tableName, List<Symbol> outputs, List<String> singleStringKeys, int executionNodeId) {
        return newGetNode(docSchemaInfo.getTableInfo(tableName), outputs, singleStringKeys, executionNodeId);
    }

    @Test
    public void testESGetTask() throws Exception {
        setup.setUpCharacters();


        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef);
        Planner.Context ctx = new Planner.Context(clusterService());
        ESGetNode node = newGetNode("characters", outputs, "2", ctx.nextExecutionNodeId());
        Plan plan = new IterablePlan(node);
        Job job = executor.newJob(plan);


        assertThat(job.tasks().size(), is(1));
        Task task = job.tasks().get(0);
        assertThat(task, instanceOf(ESGetTask.class));


        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, contains(isRow(2, "Ford")));
    }

    @Test
    public void testESGetTaskWithDynamicReference() throws Exception {
        setup.setUpCharacters();

        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, new DynamicReference(
                new ReferenceIdent(new TableIdent(null, "characters"), "foo"), RowGranularity.DOC));
        Planner.Context ctx = new Planner.Context(clusterService());
        ESGetNode node = newGetNode("characters", outputs, "2", ctx.nextExecutionNodeId());
        Plan plan = new IterablePlan(node);
        Job job = executor.newJob(plan);
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, contains(isRow(2, null)));
    }

    @Test
    public void testESMultiGet() throws Exception {
        setup.setUpCharacters();
        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef);
        Planner.Context ctx = new Planner.Context(clusterService());
        ESGetNode node = newGetNode("characters", outputs, asList("1", "2"), ctx.nextExecutionNodeId());
        Plan plan = new IterablePlan(node);
        Job job = executor.newJob(plan);
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();

        assertThat(objects.size(), is(2));
    }

    @Test
    public void testQTFTask() throws Exception {

        setup.setUpCharacters();
        DocTableInfo characters = docSchemaInfo.getTableInfo("characters");
        ReferenceInfo docIdRefInfo = characters.getReferenceInfo(new ColumnIdent(DocSysColumns.DOCID.name()));
        List<Symbol> collectSymbols = Lists.<Symbol>newArrayList(new Reference(docIdRefInfo));
        List<Symbol> outputSymbols = Lists.<Symbol>newArrayList(idRef, nameRef);

        Planner.Context ctx = new Planner.Context(clusterService());

        CollectNode collectNode = PlanNodeBuilder.collect(
                characters,
                ctx,
                WhereClause.MATCH_ALL,
                collectSymbols,
                ImmutableList.<Projection>of(),
                null,
                Constants.DEFAULT_SELECT_LIMIT
        );
        collectNode.keepContextForFetcher(true);

        FetchProjection fetchProjection = getFetchProjection((DocTableInfo) characters, (List<Symbol>) collectSymbols, (List<Symbol>) outputSymbols, (CollectNode) collectNode, ctx);

        MergeNode localMergeNode = PlanNodeBuilder.localMerge(
                ImmutableList.<Projection>of(fetchProjection),
                collectNode,
                ctx);

        Plan plan = new QueryThenFetch(collectNode, localMergeNode);

        Job job = executor.newJob(plan);
        assertThat(job.tasks().size(), is(1));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, containsInAnyOrder(
                isRow(1, "Arthur"),
                isRow(4, "Arthur"),
                isRow(2, "Ford"),
                isRow(3, "Trillian")
        ));
    }

    @Test
    public void testQTFTaskWithFilter() throws Exception {

        setup.setUpCharacters();
        DocTableInfo characters = docSchemaInfo.getTableInfo("characters");
        ReferenceInfo docIdRefInfo = characters.getReferenceInfo(new ColumnIdent(DocSysColumns.DOCID.name()));
        List<Symbol> collectSymbols = Lists.<Symbol>newArrayList(new Reference(docIdRefInfo));
        List<Symbol> outputSymbols = Lists.<Symbol>newArrayList(idRef, nameRef);

        Function whereClause = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.STRING, DataTypes.STRING)),
                DataTypes.BOOLEAN),
                Arrays.<Symbol>asList(nameRef, Literal.newLiteral("Ford")));

        Planner.Context ctx = new Planner.Context(clusterService());

        CollectNode collectNode = PlanNodeBuilder.collect(
                characters,
                ctx,
                new WhereClause(whereClause),
                collectSymbols,
                ImmutableList.<Projection>of(),
                null,
                Constants.DEFAULT_SELECT_LIMIT
        );
        collectNode.keepContextForFetcher(true);

        FetchProjection fetchProjection = getFetchProjection(characters, collectSymbols, outputSymbols, collectNode, ctx);

        MergeNode localMergeNode = PlanNodeBuilder.localMerge(
                ImmutableList.<Projection>of(fetchProjection),
                collectNode,
                ctx);

        Plan plan = new QueryThenFetch(collectNode, localMergeNode);

        Job job = executor.newJob(plan);
        assertThat(job.tasks().size(), is(1));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, contains(isRow(2, "Ford")));
    }

    private FetchProjection getFetchProjection(DocTableInfo characters, List<Symbol> collectSymbols, List<Symbol> outputSymbols, CollectNode collectNode, Planner.Context ctx) {
        Map<Integer, List<String>> executionNodes = new HashMap<>();
        executionNodes.put(collectNode.executionNodeId(), new ArrayList<>(collectNode.executionNodes()));
        return new FetchProjection(
                ctx.jobSearchContextIdToExecutionNodeId(),
                new InputColumn(0, DataTypes.STRING), collectSymbols, outputSymbols,
                characters.partitionedByColumns(),
                executionNodes,
                5,
                false,
                ctx.jobSearchContextIdToNode(),
                ctx.jobSearchContextIdToShard()
        );
    }

    @Test
    public void testQTFTaskOrdered() throws Exception {

        setup.setUpCharacters();
        DocTableInfo characters = docSchemaInfo.getTableInfo("characters");

        OrderBy orderBy = new OrderBy(Arrays.<Symbol>asList(nameRef, femaleRef),
                new boolean[]{false, false},
                new Boolean[]{false, false});

        ReferenceInfo docIdRefInfo = characters.getReferenceInfo(new ColumnIdent(DocSysColumns.DOCID.name()));

        List<Symbol> collectSymbols = Lists.<Symbol>newArrayList(new Reference(docIdRefInfo), nameRef, femaleRef);
        List<Symbol> outputSymbols = Lists.<Symbol>newArrayList(idRef, nameRef);

        MergeProjection mergeProjection = new MergeProjection(
                collectSymbols,
                orderBy.orderBySymbols(),
                orderBy.reverseFlags(),
                orderBy.nullsFirst()
        );
        Planner.Context ctx = new Planner.Context(clusterService());

        CollectNode collectNode = PlanNodeBuilder.collect(
                characters,
                ctx,
                WhereClause.MATCH_ALL,
                collectSymbols,
                ImmutableList.<Projection>of(),
                orderBy,
                Constants.DEFAULT_SELECT_LIMIT
        );
        collectNode.projections(ImmutableList.<Projection>of(mergeProjection));
        collectNode.keepContextForFetcher(true);

        FetchProjection fetchProjection = getFetchProjection(characters, collectSymbols, outputSymbols, collectNode, ctx);

        MergeNode localMergeNode = PlanNodeBuilder.sortedLocalMerge(
                ImmutableList.<Projection>of(fetchProjection),
                orderBy,
                collectSymbols,
                null,
                collectNode,
                ctx);

        Plan plan = new QueryThenFetch(collectNode, localMergeNode);

        Job job = executor.newJob(plan);
        assertThat(job.tasks().size(), is(1));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, contains(
                isRow(1, "Arthur"),
                isRow(4, "Arthur"),
                isRow(2, "Ford"),
                isRow(3, "Trillian")
        ));
    }

    @Test
    public void testQTFTaskWithFunction() throws Exception {

        execute("create table searchf (id int primary key, date timestamp) with (number_of_replicas=0)");
        ensureGreen();
        execute("insert into searchf (id, date) values (1, '1980-01-01'), (2, '1980-01-02')");
        refresh();

        Reference id_ref = new Reference(new ReferenceInfo(
                new ReferenceIdent(
                        new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "searchf"),
                        "id"),
                RowGranularity.DOC,
                DataTypes.INTEGER
        ));
        Reference date_ref = new Reference(new ReferenceInfo(
                new ReferenceIdent(
                        new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "searchf"),
                        "date"),
                RowGranularity.DOC,
                DataTypes.TIMESTAMP
        ));
        Function function = new Function(new FunctionInfo(
                new FunctionIdent(DateTruncFunction.NAME, Arrays.<DataType>asList(DataTypes.STRING, DataTypes.TIMESTAMP)),
                DataTypes.TIMESTAMP
        ), Arrays.asList(Literal.newLiteral("month"), date_ref));
        Function whereClause = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.INTEGER, DataTypes.INTEGER)),
                DataTypes.BOOLEAN),
                Arrays.asList(id_ref, Literal.newLiteral(2))
        );

        DocTableInfo searchf = docSchemaInfo.getTableInfo("searchf");
        ReferenceInfo docIdRefInfo = searchf.getReferenceInfo(new ColumnIdent(DocSysColumns.DOCID.name()));

        Planner.Context ctx = new Planner.Context(clusterService());
        List<Symbol> collectSymbols = ImmutableList.<Symbol>of(new Reference(docIdRefInfo));
        CollectNode collectNode = PlanNodeBuilder.collect(
                searchf,
                ctx,
                new WhereClause(whereClause),
                collectSymbols,
                ImmutableList.<Projection>of(),
                null,
                Constants.DEFAULT_SELECT_LIMIT
        );
        collectNode.keepContextForFetcher(true);

        TopNProjection topN = new TopNProjection(2, TopN.NO_OFFSET);
        topN.outputs(Collections.<Symbol>singletonList(new InputColumn(0)));

        FetchProjection fetchProjection = getFetchProjection(searchf, collectSymbols, Arrays.asList(id_ref, function), collectNode, ctx);

        MergeNode mergeNode = PlanNodeBuilder.localMerge(
                ImmutableList.of(topN, fetchProjection),
                collectNode,
                ctx);
        Plan plan = new QueryThenFetch(collectNode, mergeNode);

        Job job = executor.newJob(plan);
        assertThat(job.tasks().size(), is(1));

        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, contains(isRow(2, 315532800000L)));
    }

    @Test
    public void testQTFTaskPartitioned() throws Exception {
        setup.setUpPartitionedTableWithName();
        DocTableInfo parted = docSchemaInfo.getTableInfo("parted");
        Planner.Context ctx = new Planner.Context(clusterService());

        ReferenceInfo docIdRefInfo = parted.getReferenceInfo(new ColumnIdent(DocSysColumns.DOCID.name()));
        List<Symbol> collectSymbols = Lists.<Symbol>newArrayList(new Reference(docIdRefInfo));
        List<Symbol> outputSymbols =  Arrays.<Symbol>asList(partedIdRef, partedNameRef, partedDateRef);

        CollectNode collectNode = PlanNodeBuilder.collect(
                parted,
                ctx,
                WhereClause.MATCH_ALL,
                collectSymbols,
                ImmutableList.<Projection>of(),
                null,
                Constants.DEFAULT_SELECT_LIMIT
        );
        collectNode.keepContextForFetcher(true);

        FetchProjection fetchProjection = getFetchProjection(parted, collectSymbols, outputSymbols, collectNode, ctx);

        MergeNode localMergeNode = PlanNodeBuilder.localMerge(
                ImmutableList.<Projection>of(fetchProjection),
                collectNode,
                ctx);

        Plan plan = new QueryThenFetch(collectNode, localMergeNode);
        Job job = executor.newJob(plan);

        assertThat(job.tasks().size(), is(1));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, containsInAnyOrder(
                isRow(3, "Ford", 1396388720242L),
                isRow(1, "Trillian", null),
                isRow(2, null, 0L)
        ));
    }

    @Test
    public void testESDeleteByQueryTask() throws Exception {
        setup.setUpCharacters();

        Function whereClause = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.STRING, DataTypes.STRING)),
                DataTypes.BOOLEAN),
                Arrays.<Symbol>asList(idRef, Literal.newLiteral(2)));

        ESDeleteByQueryNode node = new ESDeleteByQueryNode(
                1,
                ImmutableList.of(new String[]{"characters"}),
                ImmutableList.of(new WhereClause(whereClause)));
        Plan plan = new IterablePlan(node);
        Job job = executor.newJob(plan);
        ESDeleteByQueryTask task = (ESDeleteByQueryTask) job.tasks().get(0);

        task.start();
        TaskResult taskResult = task.result().get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(-1L)));


        execute("select * from characters where id = 2");
        assertThat(response.rowCount(), is(0L));
    }

    @Test
    public void testInsertWithSymbolBasedUpsertByIdTask() throws Exception {
        execute("create table characters (id int primary key, name string)");
        ensureGreen();


        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(),
                false,
                false,
                null,
                new Reference[]{idRef, nameRef});
        updateNode.add("characters", "99", "99", null, null, new Object[]{99, new BytesRef("Marvin")});

        Plan plan = new IterablePlan(updateNode);
        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));

        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(1L)));


        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef);
        ESGetNode getNode = newGetNode("characters", outputs, "99", ctx.nextExecutionNodeId());
        plan = new IterablePlan(getNode);
        job = executor.newJob(plan);
        result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();

        assertThat(objects, contains(isRow(99, "Marvin")));
    }

    @Test
    public void testInsertIntoPartitionedTableWithSymbolBasedUpsertByIdTask() throws Exception {
        execute("create table parted (" +
                "  id int, " +
                "  name string, " +
                "  date timestamp" +
                ") partitioned by (date)");
        ensureGreen();


        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(),
                true,
                false,
                null,
                new Reference[]{idRef, nameRef});

        PartitionName partitionName = new PartitionName("parted", Arrays.asList(new BytesRef("13959981214861")));
        updateNode.add(partitionName.stringValue(), "123", "123", null, null, new Object[]{0L, new BytesRef("Trillian")});

        Plan plan = new IterablePlan(updateNode);
        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));

        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket indexResult = taskResult.rows();
        assertThat(indexResult, contains(isRow(1L)));

        refresh();

        assertTrue(
                client().admin().indices().prepareExists(partitionName.stringValue())
                        .execute().actionGet().isExists()
        );
        assertTrue(
                client().admin().indices().prepareAliasesExist("parted")
                        .execute().actionGet().exists()
        );
        SearchHits hits = client().prepareSearch(partitionName.stringValue())
                .setTypes(Constants.DEFAULT_MAPPING_TYPE)
                .addFields("id", "name")
                .setQuery(new MapBuilder<String, Object>()
                                .put("match_all", new HashMap<String, Object>())
                                .map()
                ).execute().actionGet().getHits();
        assertThat(hits.getTotalHits(), is(1L));
        assertThat((Integer) hits.getHits()[0].field("id").getValues().get(0), is(0));
        assertThat((String) hits.getHits()[0].field("name").getValues().get(0), is("Trillian"));
    }

    @Test
    public void testInsertMultiValuesWithSymbolBasedUpsertByIdTask() throws Exception {
        execute("create table characters (id int primary key, name string)");
        ensureGreen();


        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(),
                false,
                false,
                null,
                new Reference[]{idRef, nameRef});

        updateNode.add("characters", "99", "99", null, null, new Object[]{99, new BytesRef("Marvin")});
        updateNode.add("characters", "42", "42", null, null, new Object[]{42, new BytesRef("Deep Thought")});

        Plan plan = new IterablePlan(updateNode);
        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));

        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(2L)));


        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef);
        ESGetNode getNode = newGetNode("characters", outputs, Arrays.asList("99", "42"), ctx.nextExecutionNodeId());
        plan = new IterablePlan(getNode);
        job = executor.newJob(plan);
        result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();

        assertThat(objects, contains(
                isRow(99, "Marvin"),
                isRow(42, "Deep Thought")
        ));
    }

    @Test
    public void testUpdateWithSymbolBasedUpsertByIdTask() throws Exception {
        setup.setUpCharacters();


        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(), false, false, new String[]{nameRef.ident().columnIdent().fqn()}, null);
        updateNode.add("characters", "1", "1", new Symbol[]{Literal.newLiteral("Vogon lyric fan")}, null);
        Plan plan = new IterablePlan(updateNode);

        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(1L)));


        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef);
        ESGetNode getNode = newGetNode("characters", outputs, "1", ctx.nextExecutionNodeId());
        plan = new IterablePlan(getNode);
        job = executor.newJob(plan);
        result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();

        assertThat(objects, contains(isRow(1, "Vogon lyric fan")));
    }

    @Test
    public void testInsertOnDuplicateWithSymbolBasedUpsertByIdTask() throws Exception {
        setup.setUpCharacters();

        Object[] missingAssignments = new Object[]{5, new BytesRef("Zaphod Beeblebrox"), false};
        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(),
                false,
                false,
                new String[]{nameRef.ident().columnIdent().fqn()},
                new Reference[]{idRef, nameRef, femaleRef});

        updateNode.add("characters", "5", "5", new Symbol[]{Literal.newLiteral("Zaphod Beeblebrox")}, null, missingAssignments);
        Plan plan = new IterablePlan(updateNode);
        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(1L)));


        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef, femaleRef);
        ESGetNode getNode = newGetNode("characters", outputs, "5", ctx.nextExecutionNodeId());
        plan = new IterablePlan(getNode);
        job = executor.newJob(plan);
        result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();
        assertThat(objects, contains(isRow(5, "Zaphod Beeblebrox", false)));

    }

    @Test
    public void testUpdateOnDuplicateWithSymbolBasedUpsertByIdTask() throws Exception {
        setup.setUpCharacters();

        Object[] missingAssignments = new Object[]{1, new BytesRef("Zaphod Beeblebrox"), true};
        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(),
                false,
                false,
                new String[]{femaleRef.ident().columnIdent().fqn()},
                new Reference[]{idRef, nameRef, femaleRef});
        updateNode.add("characters", "1", "1", new Symbol[]{Literal.newLiteral(true)}, null, missingAssignments);
        Plan plan = new IterablePlan(updateNode);
        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(1L)));


        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef, femaleRef);
        ESGetNode getNode = newGetNode("characters", outputs, "1", ctx.nextExecutionNodeId());
        plan = new IterablePlan(getNode);
        job = executor.newJob(plan);
        result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();

        assertThat(objects, contains(isRow(1, "Arthur", true)));
    }

    @Test
    public void testBulkUpdateByQueryTask() throws Exception {
        setup.setUpCharacters();


        List<Plan> childNodes = new ArrayList<>();
        Planner.Context plannerContext = new Planner.Context(clusterService());

        TableInfo tableInfo = docSchemaInfo.getTableInfo("characters");
        Reference uidReference = new Reference(
                new ReferenceInfo(
                        new ReferenceIdent(tableInfo.ident(), "_uid"),
                        RowGranularity.DOC, DataTypes.STRING));


        Function whereClause1 = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.BOOLEAN, DataTypes.BOOLEAN)),
                DataTypes.BOOLEAN),
                Arrays.<Symbol>asList(femaleRef, Literal.newLiteral(true)));

        UpdateProjection updateProjection = new UpdateProjection(
                new InputColumn(0, DataTypes.STRING),
                new String[]{"name"},
                new Symbol[]{Literal.newLiteral("Zaphod Beeblebrox")},
                null);

        CollectNode collectNode1 = PlanNodeBuilder.collect(
                tableInfo,
                plannerContext,
                new WhereClause(whereClause1),
                ImmutableList.<Symbol>of(uidReference),
                ImmutableList.<Projection>of(updateProjection),
                null,
                Preference.PRIMARY.type()
        );
        MergeNode mergeNode1 = PlanNodeBuilder.localMerge(
                ImmutableList.<Projection>of(CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION), collectNode1,
                plannerContext);
        childNodes.add(new CollectAndMerge(collectNode1, mergeNode1));


        Function whereClause2 = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.BOOLEAN, DataTypes.BOOLEAN)),
                DataTypes.BOOLEAN),
                Arrays.<Symbol>asList(femaleRef, Literal.newLiteral(true)));

        CollectNode collectNode2 = PlanNodeBuilder.collect(
                tableInfo,
                plannerContext,
                new WhereClause(whereClause2),
                ImmutableList.<Symbol>of(uidReference),
                ImmutableList.<Projection>of(updateProjection),
                null,
                Preference.PRIMARY.type()
        );
        MergeNode mergeNode2 = PlanNodeBuilder.localMerge(
                ImmutableList.<Projection>of(CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION), collectNode2,
                plannerContext);
        childNodes.add(new CollectAndMerge(collectNode2, mergeNode2));

        Upsert plan = new Upsert(childNodes);
        Job job = executor.newJob(plan);

        assertThat(job.tasks().size(), is(1));
        assertThat(job.tasks().get(0), instanceOf(ExecutionNodesTask.class));
        List<? extends ListenableFuture<TaskResult>> results = executor.execute(job);
        assertThat(results.size(), is(2));

        for (int i = 0; i < results.size(); i++) {
            TaskResult result = results.get(i).get();
            assertThat(result, instanceOf(RowCountResult.class));

            assertThat(((RowCountResult)result).rowCount(), is(2L));
        }
    }

    @Test
    public void testKillTask() throws Exception {
        Job job = executor.newJob(KillPlan.INSTANCE);
        assertThat(job.tasks(), hasSize(1));
        assertThat(job.tasks().get(0), instanceOf(KillTask.class));

        List<? extends ListenableFuture<TaskResult>> results = executor.execute(job);
        assertThat(results, hasSize(1));
        results.get(0).get();
    }

    @Test
    public void testNestedLoopWithOrderedQAF() throws Exception {





        setup.setUpCharacters();
        setup.setUpLocations();
        setup.setUpEmployees();

        DocTableInfo characters = docSchemaInfo.getTableInfo("characters");
        DocTableInfo locations = docSchemaInfo.getTableInfo("locations");
        DocTableInfo employees = docSchemaInfo.getTableInfo("employees");

        Reference outerLeftIdRef = new Reference(characters.getReferenceInfo(new ColumnIdent("id")));
        Reference outerLeftNameRef = new Reference(characters.getReferenceInfo(new ColumnIdent("name")));

        Reference innerLeftIdRef = new Reference(locations.getReferenceInfo(new ColumnIdent("id")));
        Reference innerLeftNameRef = new Reference(locations.getReferenceInfo(new ColumnIdent("name")));
        Reference innerLeftPosRef = new Reference(locations.getReferenceInfo(new ColumnIdent("position")));

        Reference innerRightNameRef = new Reference(employees.getReferenceInfo(new ColumnIdent("name")));
        Reference innerRightDepartmentRef = new Reference(employees.getReferenceInfo(new ColumnIdent("department")));

        List<Symbol> outerLeftCollectSymbols = Lists.<Symbol>newArrayList(outerLeftIdRef, outerLeftNameRef);
        List<Symbol> innerLeftCollectSymbols = Lists.<Symbol>newArrayList(innerLeftPosRef, innerLeftIdRef, innerLeftNameRef);
        List<Symbol> innerRightCollectSymbols = Lists.<Symbol>newArrayList(innerRightNameRef, innerRightDepartmentRef);
        List<Symbol> outputSymbols = Lists.newArrayList(outerLeftCollectSymbols);
        outputSymbols.addAll(innerLeftCollectSymbols);
        outputSymbols.addAll(innerRightCollectSymbols);

        ProjectionBuilder projectionBuilder = new ProjectionBuilder(null);
        Planner.Context ctx = new Planner.Context(clusterService());
        String localNodeId = clusterService.localNode().id();
        Set<String> localExecutionNode = Sets.newHashSet(localNodeId);


        CollectNode outerLeftCollectNode = PlanNodeBuilder.distributingCollect(
                characters,
                ctx,
                WhereClause.MATCH_ALL,
                outerLeftCollectSymbols,
                Lists.newArrayList(localExecutionNode),
                ImmutableList.<Projection>of());
        OrderBy outerLeftOrderBy = new OrderBy(ImmutableList.<Symbol>of(outerLeftIdRef),
                new boolean[]{false}, new Boolean[]{false});
        outerLeftCollectNode.orderBy(outerLeftOrderBy);
        MergeProjection outerLeftMergeProjection = projectionBuilder.mergeProjection(
                outerLeftCollectSymbols,
                outerLeftOrderBy);
        outerLeftCollectNode.projections(ImmutableList.<Projection>of(outerLeftMergeProjection));


        CollectNode innerLeftCollectNode = PlanNodeBuilder.distributingCollect(
                locations,
                ctx,
                WhereClause.MATCH_ALL,
                innerLeftCollectSymbols,
                Lists.newArrayList(localExecutionNode),
                ImmutableList.<Projection>of());
        OrderBy innerLeftOrderBy = new OrderBy(ImmutableList.<Symbol>of(innerLeftPosRef, innerLeftIdRef),
                new boolean[]{false, false}, new Boolean[]{false, false});
        innerLeftCollectNode.orderBy(innerLeftOrderBy);
        MergeProjection innerLeftMergeProjection = projectionBuilder.mergeProjection(
                innerLeftCollectSymbols,
                innerLeftOrderBy);
        innerLeftCollectNode.projections(ImmutableList.<Projection>of(innerLeftMergeProjection));


        CollectNode innerRightCollectNode = PlanNodeBuilder.distributingCollect(
                employees,
                ctx,
                WhereClause.MATCH_ALL,
                innerRightCollectSymbols,
                Lists.newArrayList(localExecutionNode),
                ImmutableList.<Projection>of());
        OrderBy innerRightOrderBy = new OrderBy(ImmutableList.<Symbol>of(innerRightNameRef),
                new boolean[]{false}, new Boolean[]{false});
        innerRightCollectNode.orderBy(innerRightOrderBy);
        MergeProjection innerRightMergeProjection = projectionBuilder.mergeProjection(
                innerRightCollectSymbols,
                innerRightOrderBy);
        innerRightCollectNode.projections(ImmutableList.<Projection>of(innerRightMergeProjection));


        NestedLoopNode innerNestedLoopNode = PlanNodeBuilder.localNestedLoopNode(
                ImmutableList.<Projection>of(),
                localExecutionNode,
                innerLeftCollectNode,
                innerRightCollectNode,
                innerLeftCollectSymbols,
                innerRightCollectSymbols,
                innerLeftOrderBy,
                innerRightOrderBy,
                ctx);

        PlannedAnalyzedRelation innerPlan = new NestedLoop(
                new QueryThenFetch(innerLeftCollectNode, null),
                new QueryThenFetch(innerRightCollectNode, null),
                innerNestedLoopNode,
                false);



        List<Symbol> outerRightCollectSymbols = Lists.newArrayList(innerLeftCollectSymbols);
        outerRightCollectSymbols.addAll(innerRightCollectSymbols);
        OrderBy outerRightOrderBy = new OrderBy(ImmutableList.<Symbol>of(innerLeftPosRef, innerLeftIdRef, innerRightNameRef),
                new boolean[]{false, false, false}, new Boolean[]{false, false, false});
        NestedLoopNode outerNestedLoopNode = PlanNodeBuilder.localNestedLoopNode(
                ImmutableList.<Projection>of(),
                localExecutionNode,
                outerLeftCollectNode,
                innerNestedLoopNode,
                outerLeftCollectSymbols,
                outerRightCollectSymbols,
                outerLeftOrderBy,
                outerRightOrderBy,
                ctx);
        innerNestedLoopNode.downstreamExecutionNodeId(outerNestedLoopNode.executionNodeId());
        innerNestedLoopNode.downstreamNodes(Lists.newArrayList(outerNestedLoopNode.executionNodes()));


        MergeNode localMergeNode = PlanNodeBuilder.sortedLocalMerge(
                ImmutableList.<Projection>of(),
                innerLeftOrderBy,
                outputSymbols,
                null,
                outerNestedLoopNode,
                ctx);
        localMergeNode.executionNodes(localExecutionNode);
        outerNestedLoopNode.downstreamExecutionNodeId(localMergeNode.executionNodeId());
        outerNestedLoopNode.downstreamNodes(Lists.newArrayList(localMergeNode.executionNodes()));


        NestedLoop nestedLoopPlan = new NestedLoop(
                new QueryThenFetch(outerLeftCollectNode, null),
                innerPlan,
                outerNestedLoopNode,
                false);
        nestedLoopPlan.localMergeNode(localMergeNode);

        Job job = executor.newJob(nestedLoopPlan);
        assertThat(job.tasks().size(), is(1));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows.size(), is(312));

        assertThat(TestingHelpers.printedTable(rows), startsWith("" +
                "1| Arthur| 1| 1| North West Ripple| asok| internship\n" +
                "1| Arthur| 1| 1| North West Ripple| catbert| HR\n" +
                "1| Arthur| 1| 1| North West Ripple| dilbert| engineering\n" +
                "1| Arthur| 1| 1| North West Ripple| pointy haired boss| management\n" +
                "1| Arthur| 1| 1| North West Ripple| ratbert| HR\n" +
                "1| Arthur| 1| 1| North West Ripple| wally| engineering\n" +
                "1| Arthur| 1| 4| Aldebaran| asok| internship\n"));

        assertThat(TestingHelpers.printedTable(rows), endsWith("" +
                "4| Arthur| 6| 13| End of the Galaxy| wally| engineering\n"));
    }

}

<code block>


package io.crate.executor.transport.distributed;

import io.crate.Constants;
import io.crate.Streamer;
import io.crate.core.collections.Row1;
import io.crate.test.integration.CrateUnitTest;
import io.crate.testing.TestingHelpers;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.action.ActionListener;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;
import org.mockito.ArgumentCaptor;
import org.mockito.Captor;
import org.mockito.MockitoAnnotations;

import java.util.Arrays;
import java.util.List;
import java.util.UUID;
import java.util.concurrent.CancellationException;

import static org.hamcrest.Matchers.is;
import static org.mockito.Matchers.any;
import static org.mockito.Matchers.eq;
import static org.mockito.Mockito.*;

public class DistributingDownstreamTest extends CrateUnitTest {

    private TransportDistributedResultAction distributedResultAction;
    private DistributingDownstream downstream;

    @Captor
    public ArgumentCaptor<ActionListener<DistributedResultResponse>> listenerArgumentCaptor;
    private int originalPageSize;

    @Before
    public void before() throws Exception {
        originalPageSize = Constants.PAGE_SIZE;
        MockitoAnnotations.initMocks(this);

        List<String> downstreamNodes = Arrays.asList("n1", "n2");
        distributedResultAction = mock(TransportDistributedResultAction.class);
        Streamer<?>[] streamers = {DataTypes.STRING.streamer()};
        downstream = new DistributingDownstream(
                UUID.randomUUID(),
                1,
                (byte) 0,
                0,
                downstreamNodes,
                distributedResultAction,
                streamers
        );
        downstream.registerUpstream(null);
    }

    @After
    public void after() throws Exception {
        Constants.PAGE_SIZE = originalPageSize;
    }

    @Test
    public void testBucketing() throws Exception {
        ArgumentCaptor<DistributedResultRequest> r1Captor = ArgumentCaptor.forClass(DistributedResultRequest.class);
        doNothing().when(distributedResultAction).pushResult(eq("n1"), r1Captor.capture(), any(ActionListener.class));

        ArgumentCaptor<DistributedResultRequest> r2Captor = ArgumentCaptor.forClass(DistributedResultRequest.class);
        doNothing().when(distributedResultAction).pushResult(eq("n2"), r2Captor.capture(), any(ActionListener.class));


        downstream.setNextRow(new Row1(new BytesRef("Trillian")));
        downstream.setNextRow(new Row1(new BytesRef("Marvin")));
        downstream.setNextRow(new Row1(new BytesRef("Arthur")));
        downstream.setNextRow(new Row1(new BytesRef("Slartibartfast")));

        downstream.finish();

        assertRows(r2Captor, "Trillian\nMarvin\n");
        assertRows(r1Captor, "Arthur\nSlartibartfast\n");
    }

    @Test
    public void testOperationIsStoppedOnFailureResponse() throws Exception {
        Constants.PAGE_SIZE = 2;

        ArgumentCaptor<DistributedResultRequest> captor = ArgumentCaptor.forClass(DistributedResultRequest.class);
        doNothing().when(distributedResultAction).pushResult(any(String.class), captor.capture(), listenerArgumentCaptor.capture());

        int iterations = 0;
        int expected = -1;
        while (true) {
            if (!downstream.setNextRow(new Row1(new BytesRef("Trillian")))) {
                break;
            }
            List<ActionListener<DistributedResultResponse>> allValues = listenerArgumentCaptor.getAllValues();
            if (allValues.size() == 1) {
                ActionListener<DistributedResultResponse> distributedResultResponseActionListener = allValues.get(0);
                distributedResultResponseActionListener.onFailure(new IllegalStateException("epic fail"));
                expected = iterations + 1;
            }
            iterations++;
        }
        assertThat(iterations, is(expected));
    }

    @Test
    public void testRequestsAreSentWithoutRows() throws Exception {
        ArgumentCaptor<DistributedResultRequest> captor = ArgumentCaptor.forClass(DistributedResultRequest.class);
        doNothing().when(distributedResultAction).pushResult(any(String.class), captor.capture(), any(ActionListener.class));

        downstream.finish();
        assertThat(captor.getAllValues().size(), is(2));
        for (DistributedResultRequest distributedResultRequest : captor.getAllValues()) {
            assertThat(distributedResultRequest.rows().size(), is(0));
        }

    }

    @Test
    public void testNoRequestsSendWhenCancelled() throws Exception {
        downstream.setNextRow(new Row1(new BytesRef("LateNightSprintFinishingAwesomeness")));
        downstream.fail(new CancellationException());

        verify(distributedResultAction, never()).pushResult(any(String.class), any(DistributedResultRequest.class), any(ActionListener.class));

    }

    private void assertRows(ArgumentCaptor<DistributedResultRequest> r2Captor, String expectedRows) {
        List<DistributedResultRequest> allRequestsForNodeN1 = r2Captor.getAllValues();
        assertThat(allRequestsForNodeN1.size(), is(1));
        DistributedResultRequest n1Request = allRequestsForNodeN1.get(0);
        assertThat(n1Request.isLast(), is(true));
        assertThat(n1Request.rowsCanBeRead(), is(true));

        assertThat(TestingHelpers.printedTable(n1Request.rows()), is(expectedRows));
    }
}
<code block>


package io.crate.executor.transport.merge;

import io.crate.Streamer;
import io.crate.core.collections.ArrayBucket;
import io.crate.executor.transport.distributed.DistributedResultRequest;
import io.crate.test.integration.CrateUnitTest;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.common.io.stream.BytesStreamInput;
import org.elasticsearch.common.io.stream.BytesStreamOutput;
import org.junit.Test;

import java.util.UUID;

import static io.crate.testing.TestingHelpers.isNullRow;
import static io.crate.testing.TestingHelpers.isRow;
import static org.hamcrest.Matchers.contains;

public class DistributedResultRequestTest extends CrateUnitTest {

    @Test
    public void testStreaming() throws Exception {
        Streamer<?>[] streamers = new Streamer[]{DataTypes.STRING.streamer()};

        Object[][] rows = new Object[][]{
                {new BytesRef("ab")},{null},{new BytesRef("cd")}
        };
        UUID uuid = UUID.randomUUID();

        DistributedResultRequest r1 = new DistributedResultRequest(uuid, 1, (byte) 0, 1, streamers);
        r1.rows(new ArrayBucket(rows));

        BytesStreamOutput out = new BytesStreamOutput();
        r1.writeTo(out);
        BytesStreamInput in = new BytesStreamInput(out.bytes());
        DistributedResultRequest r2 = new DistributedResultRequest();
        r2.readFrom(in);
        r2.streamers(streamers);
        assertTrue(r2.rowsCanBeRead());

        assertEquals(r1.rows().size(), r2.rows().size());

        assertThat(r2.rows(), contains(isRow("ab"), isNullRow(), isRow("cd")));
    }
}

<code block>


package io.crate.planner.node.dql;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.Sets;
import io.crate.planner.node.dql.join.NestedLoopNode;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.TopNProjection;
import io.crate.test.integration.CrateUnitTest;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.elasticsearch.common.io.stream.BytesStreamInput;
import org.elasticsearch.common.io.stream.BytesStreamOutput;
import org.hamcrest.core.Is;
import org.junit.Test;

import java.util.Arrays;
import java.util.UUID;

import static org.hamcrest.CoreMatchers.is;

public class NestedLoopNodeTest extends CrateUnitTest {

    @Test
    public void testSerialization() throws Exception {

        NestedLoopNode node = new NestedLoopNode(1, "nestedLoop");
        node.jobId(UUID.randomUUID());
        node.executionNodes(Sets.newHashSet("node1", "node2"));
        node.leftInputTypes(Arrays.<DataType>asList(DataTypes.UNDEFINED, DataTypes.STRING));
        node.rightInputTypes(Arrays.<DataType>asList(DataTypes.BOOLEAN, DataTypes.INTEGER, DataTypes.DOUBLE));
        node.downstreamNodes(Sets.newHashSet("node3", "node4"));
        node.downstreamExecutionNodeId(5);
        TopNProjection topNProjection = new TopNProjection(10, 0);
        node.projections(ImmutableList.<Projection>of(topNProjection));

        BytesStreamOutput output = new BytesStreamOutput();
        node.writeTo(output);

        BytesStreamInput input = new BytesStreamInput(output.bytes());
        NestedLoopNode node2 = new NestedLoopNode();
        node2.readFrom(input);

        assertThat(node.downstreamExecutionNodeId(), is(node2.downstreamExecutionNodeId()));
        assertThat(node.downstreamNodes(), is(node2.downstreamNodes()));
        assertThat(node.executionNodes(), Is.is(node2.executionNodes()));
        assertThat(node.jobId(), Is.is(node2.jobId()));
        assertThat(node.leftInputTypes(), is(node2.leftInputTypes()));
        assertThat(node.rightInputTypes(), is(node2.rightInputTypes()));
        assertThat(node.executionNodeId(), is(node2.executionNodeId()));
        assertThat(node.name(), is(node2.name()));
        assertThat(node.outputTypes(), is(node2.outputTypes()));
    }
}

<code block>


package io.crate.action.sql;

import com.google.common.cache.CacheBuilder;
import com.google.common.cache.CacheLoader;
import com.google.common.cache.LoadingCache;
import com.google.common.util.concurrent.FutureCallback;
import com.google.common.util.concurrent.Futures;
import com.google.common.util.concurrent.ListenableFuture;
import io.crate.Constants;
import io.crate.analyze.Analysis;
import io.crate.analyze.Analyzer;
import io.crate.exceptions.*;
import io.crate.executor.Executor;
import io.crate.executor.Job;
import io.crate.executor.TaskResult;
import io.crate.metadata.PartitionName;
import io.crate.metadata.TableIdent;
import io.crate.operation.collect.StatsTables;
import io.crate.planner.Plan;
import io.crate.planner.PlanPrinter;
import io.crate.planner.Planner;
import io.crate.planner.symbol.Field;
import io.crate.sql.parser.ParsingException;
import io.crate.sql.parser.SqlParser;
import io.crate.sql.tree.Statement;
import io.crate.types.DataType;
import org.elasticsearch.action.ActionListener;
import org.elasticsearch.action.support.ActionFilters;
import org.elasticsearch.action.support.TransportAction;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.common.collect.Tuple;
import org.elasticsearch.common.inject.Provider;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.index.engine.DocumentAlreadyExistsException;
import org.elasticsearch.index.mapper.MapperParsingException;
import org.elasticsearch.indices.IndexAlreadyExistsException;
import org.elasticsearch.indices.IndexMissingException;
import org.elasticsearch.indices.InvalidIndexNameException;
import org.elasticsearch.indices.InvalidIndexTemplateException;
import org.elasticsearch.rest.RestStatus;
import org.elasticsearch.threadpool.ThreadPool;
import org.elasticsearch.transport.NodeDisconnectedException;

import javax.annotation.Nonnull;
import javax.annotation.Nullable;
import java.io.PrintWriter;
import java.io.StringWriter;
import java.util.List;
import java.util.UUID;
import java.util.concurrent.CancellationException;

import static com.google.common.base.MoreObjects.firstNonNull;

public abstract class TransportBaseSQLAction<TRequest extends SQLBaseRequest, TResponse extends SQLBaseResponse>
        extends TransportAction<TRequest, TResponse> {

    private static final DataType[] EMPTY_TYPES = new DataType[0];
    private static final String[] EMPTY_NAMES = new String[0];


    private final LoadingCache<String, Statement> statementCache = CacheBuilder.newBuilder()
            .maximumSize(100)
            .build(
                    new CacheLoader<String, Statement>() {
                        @Override
                        public Statement load(@Nonnull String statement) throws Exception {
                            return SqlParser.createStatement(statement);
                        }
                    }
            );

    private final ClusterService clusterService;
    protected final Analyzer analyzer;
    protected final Planner planner;
    private final Provider<Executor> executorProvider;
    private final StatsTables statsTables;
    private volatile boolean disabled;

    public TransportBaseSQLAction(ClusterService clusterService,
                                  Settings settings,
                                  String actionName,
                                  ThreadPool threadPool,
                                  Analyzer analyzer,
                                  Planner planner,
                                  Provider<Executor> executorProvider,
                                  StatsTables statsTables,
                                  ActionFilters actionFilters) {
        super(settings, actionName, threadPool, actionFilters);
        this.clusterService = clusterService;
        this.analyzer = analyzer;
        this.planner = planner;
        this.executorProvider = executorProvider;
        this.statsTables = statsTables;
    }

    public abstract Analysis getAnalysis(Statement statement, TRequest request);



    protected abstract TResponse emptyResponse(TRequest request, String[] outputNames, @Nullable DataType[] types);


    protected abstract TResponse createResponseFromResult(String[] outputNames,
                                                          DataType[] outputTypes,
                                                          List<TaskResult> result,
                                                          boolean expectsAffectedRows,
                                                          TRequest request);


    private TResponse createResponseFromResult(@Nullable List<TaskResult> result, Analysis analysis, TRequest request) {
        String[] outputNames;
        DataType[] outputTypes;
        if (analysis.expectsAffectedRows()) {
            outputNames = EMPTY_NAMES;
            outputTypes = EMPTY_TYPES;
        } else {
            assert analysis.rootRelation() != null;
            outputNames = new String[analysis.rootRelation().fields().size()];
            outputTypes = new DataType[analysis.rootRelation().fields().size()];
            for (int i = 0; i < analysis.rootRelation().fields().size(); i++) {
                Field field = analysis.rootRelation().fields().get(i);
                outputNames[i] = field.path().outputName();
                outputTypes[i] = field.valueType();
            }
        }
        if (result == null) {
            return emptyResponse(request, outputNames, outputTypes);
        } else {
            return createResponseFromResult(outputNames, outputTypes, result, analysis.expectsAffectedRows(), request);
        }

    }

    @Override
    protected void doExecute(TRequest request, ActionListener<TResponse> listener) {
        logger.debug("{}", request);
        statsTables.activeRequestsInc();
        if (disabled) {
            sendResponse(listener, new NodeDisconnectedException(clusterService.localNode(), actionName));
            return;
        }
        try {
            Statement statement = statementCache.get(request.stmt());
            Analysis analysis = getAnalysis(statement, request);
            processAnalysis(analysis, request, listener);
        } catch (Throwable e) {
            logger.debug("Error executing SQLRequest", e);
            sendResponse(listener, buildSQLActionException(e));
        }
    }

    private void sendResponse(ActionListener<TResponse> listener, Throwable throwable) {
        listener.onFailure(throwable);
        statsTables.activeRequestsDec();
    }

    private void sendResponse(ActionListener<TResponse> listener, TResponse response) {
        listener.onResponse(response);
        statsTables.activeRequestsDec();
    }

    private void processAnalysis(Analysis analysis, TRequest request, ActionListener<TResponse> listener) {
        final Plan plan = planner.plan(analysis);
        tracePlan(plan);
        executePlan(analysis, plan, listener, request);
    }

    private void executePlan(final Analysis analysis,
                             final Plan plan,
                             final ActionListener<TResponse> listener,
                             final TRequest request) {
        Executor executor = executorProvider.get();
        Job job = executor.newJob(plan);

        final UUID jobId = job.id();
        assert jobId != null;
        statsTables.jobStarted(jobId, request.stmt());
        List<? extends ListenableFuture<TaskResult>> resultFutureList = executor.execute(job);
        Futures.addCallback(Futures.allAsList(resultFutureList), new FutureCallback<List<TaskResult>>() {
                    @Override
                    public void onSuccess(@Nullable List<TaskResult> result) {
                        TResponse response;
                        try {
                            response = createResponseFromResult(result, analysis, request);
                        } catch (Throwable e) {
                            sendResponse(listener, buildSQLActionException(e));
                            return;
                        }
                        statsTables.jobFinished(jobId, null);
                        sendResponse(listener, response);
                    }

                    @Override
                    public void onFailure(@Nonnull Throwable t) {
                        String message;
                        if (t instanceof CancellationException) {
                            message = Constants.KILLED_MESSAGE;
                            logger.debug("KILLED: [{}]", request.stmt());
                        } else {
                            message = Exceptions.messageOf(t);
                            logger.debug("Error processing SQLRequest", t);
                        }
                        statsTables.jobFinished(jobId, message);
                        sendResponse(listener, buildSQLActionException(t));
                    }
                }

        );
    }

    private void tracePlan(Plan plan) {
        if (logger.isTraceEnabled()) {
            PlanPrinter printer = new PlanPrinter();
            logger.trace(printer.print(plan));
        }
    }



    public Throwable esToCrateException(Throwable e) {
        e = Exceptions.unwrap(e);

        if (e instanceof IllegalArgumentException || e instanceof ParsingException) {
            return new SQLParseException(e.getMessage(), (Exception) e);
        } else if (e instanceof UnsupportedOperationException) {
            return new UnsupportedFeatureException(e.getMessage(), (Exception) e);
        } else if (e instanceof DocumentAlreadyExistsException) {
            return new DuplicateKeyException(
                    "A document with the same primary key exists already", e);
        } else if (e instanceof IndexAlreadyExistsException) {
            return new TableAlreadyExistsException(((IndexAlreadyExistsException) e).index().name(), e);
        } else if ((e instanceof InvalidIndexNameException)) {
            if (e.getMessage().contains("already exists as alias")) {

                return new TableAlreadyExistsException(((InvalidIndexNameException) e).index().getName(),
                        e);
            }
            return new InvalidTableNameException(((InvalidIndexNameException) e).index().getName(), e);
        } else if (e instanceof InvalidIndexTemplateException) {
            Tuple<String, String> schemaAndTable = PartitionName.schemaAndTableName(((InvalidIndexTemplateException) e).name());
            return new InvalidTableNameException(new TableIdent(schemaAndTable.v1(), schemaAndTable.v2()).fqn(), e);
        } else if (e instanceof IndexMissingException) {
            return new TableUnknownException(((IndexMissingException) e).index().name(), e);
        } else if (e instanceof org.elasticsearch.common.breaker.CircuitBreakingException) {
            return new CircuitBreakingException(e.getMessage());
        } else if (e instanceof CancellationException) {
            return new JobKilledException();
        }
        return e;
    }


    public SQLActionException buildSQLActionException(Throwable e) {
        if (e instanceof SQLActionException) {
            return (SQLActionException) e;
        }
        e = esToCrateException(e);

        int errorCode = 5000;
        RestStatus restStatus = RestStatus.INTERNAL_SERVER_ERROR;
        String message = e.getMessage();
        StringWriter stackTrace = new StringWriter();
        e.printStackTrace(new PrintWriter(stackTrace));

        if (e instanceof CrateException) {
            CrateException crateException = (CrateException) e;
            if (e instanceof ValidationException) {
                errorCode = 4000 + crateException.errorCode();
                restStatus = RestStatus.BAD_REQUEST;
            } else if (e instanceof ResourceUnknownException) {
                errorCode = 4040 + crateException.errorCode();
                restStatus = RestStatus.NOT_FOUND;
            } else if (e instanceof ConflictException) {
                errorCode = 4090 + crateException.errorCode();
                restStatus = RestStatus.CONFLICT;
            } else if (e instanceof UnhandledServerException) {
                errorCode = 5000 + crateException.errorCode();
            }
        } else if (e instanceof ParsingException) {
            errorCode = 4000;
            restStatus = RestStatus.BAD_REQUEST;
        } else if (e instanceof MapperParsingException) {
            errorCode = 4000;
            restStatus = RestStatus.BAD_REQUEST;
        }

        if (e instanceof NullPointerException && message == null) {
            StackTraceElement[] stackTrace1 = e.getStackTrace();
            if (stackTrace1.length > 0) {
                message = String.format("NPE in %s", stackTrace1[0]);
            }
        } else if (e instanceof ArrayIndexOutOfBoundsException) {

            StackTraceElement[] stackTrace1 = e.getStackTrace();
            if (stackTrace1.length > 0) {
                message = String.format("ArrayIndexOutOfBoundsException in %s", stackTrace1[0]);
            }
        }
        if (logger.isTraceEnabled()) {
            message = firstNonNull(message, stackTrace.toString());
        } else if (Constants.DEBUG_MODE) {

            Throwable t;
            if (e instanceof CrateException && e.getCause() != null) {


                t = e.getCause();
            } else {
                t = e;
            }
            StringWriter stringWriter = new StringWriter();
            t.printStackTrace(new PrintWriter(stringWriter));
            stackTrace = stringWriter;
            message = firstNonNull(message, stackTrace.toString());
        }
        return new SQLActionException(message, errorCode, restStatus, stackTrace.toString());
    }

    public void enable() {
        disabled = false;
    }

    public void disable() {
        disabled = true;
    }
}

<code block>


package io.crate.executor;

import java.util.ArrayList;
import java.util.Collection;
import java.util.List;
import java.util.UUID;

public class Job {

    private final UUID id;
    private List<Task> tasks = new ArrayList<>();

    public Job() {
        this(UUID.randomUUID());
    }

    public Job(UUID id) {
        this.id = id;
    }

    public UUID id() {
        return id;
    }

    public void addTasks(Collection<Task> tasks) {
        this.tasks.addAll(tasks);
    }

    public List<Task> tasks() {
        return tasks;
    }
}

<code block>


package io.crate.executor.transport;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.Iterables;
import com.google.common.util.concurrent.ListenableFuture;
import io.crate.action.job.ContextPreparer;
import io.crate.action.sql.DDLStatementDispatcher;
import io.crate.breaker.CrateCircuitBreakerService;
import io.crate.executor.*;
import io.crate.executor.task.DDLTask;
import io.crate.executor.task.NoopTask;
import io.crate.executor.transport.task.CreateTableTask;
import io.crate.executor.transport.task.DropTableTask;
import io.crate.executor.transport.task.KillTask;
import io.crate.executor.transport.task.SymbolBasedUpsertByIdTask;
import io.crate.executor.transport.task.elasticsearch.*;
import io.crate.jobs.JobContextService;
import io.crate.metadata.Functions;
import io.crate.metadata.ReferenceResolver;
import io.crate.operation.ImplementationSymbolVisitor;
import io.crate.operation.PageDownstreamFactory;
import io.crate.operation.projectors.ProjectionToProjectorVisitor;
import io.crate.planner.*;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.node.PlanNode;
import io.crate.planner.node.PlanNodeVisitor;
import io.crate.planner.node.StreamerVisitor;
import io.crate.planner.node.ddl.*;
import io.crate.planner.node.dml.*;
import io.crate.planner.node.dql.*;
import io.crate.planner.node.management.KillPlan;
import org.elasticsearch.action.bulk.BulkRetryCoordinatorPool;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.common.Nullable;
import org.elasticsearch.common.breaker.CircuitBreaker;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.inject.Provider;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.threadpool.ThreadPool;

import java.util.ArrayList;
import java.util.Collection;
import java.util.List;
import java.util.UUID;

public class TransportExecutor implements Executor, TaskExecutor {

    private final Functions functions;
    private final TaskCollectingVisitor planVisitor;
    private Provider<DDLStatementDispatcher> ddlAnalysisDispatcherProvider;
    private final NodeVisitor nodeVisitor;
    private final ThreadPool threadPool;

    private final ClusterService clusterService;
    private final JobContextService jobContextService;
    private final ContextPreparer contextPreparer;
    private final TransportActionProvider transportActionProvider;
    private final BulkRetryCoordinatorPool bulkRetryCoordinatorPool;

    private final ProjectionToProjectorVisitor globalProjectionToProjectionVisitor;


    private final CircuitBreaker circuitBreaker;

    private final PageDownstreamFactory pageDownstreamFactory;

    private final StreamerVisitor streamerVisitor;

    @Inject
    public TransportExecutor(Settings settings,
                             JobContextService jobContextService,
                             ContextPreparer contextPreparer,
                             TransportActionProvider transportActionProvider,
                             ThreadPool threadPool,
                             Functions functions,
                             ReferenceResolver referenceResolver,
                             PageDownstreamFactory pageDownstreamFactory,
                             Provider<DDLStatementDispatcher> ddlAnalysisDispatcherProvider,
                             ClusterService clusterService,
                             CrateCircuitBreakerService breakerService,
                             BulkRetryCoordinatorPool bulkRetryCoordinatorPool,
                             StreamerVisitor streamerVisitor) {
        this.jobContextService = jobContextService;
        this.contextPreparer = contextPreparer;
        this.transportActionProvider = transportActionProvider;
        this.pageDownstreamFactory = pageDownstreamFactory;
        this.threadPool = threadPool;
        this.functions = functions;
        this.ddlAnalysisDispatcherProvider = ddlAnalysisDispatcherProvider;
        this.clusterService = clusterService;
        this.bulkRetryCoordinatorPool = bulkRetryCoordinatorPool;
        this.streamerVisitor = streamerVisitor;
        this.nodeVisitor = new NodeVisitor();
        this.planVisitor = new TaskCollectingVisitor();
        this.circuitBreaker = breakerService.getBreaker(CrateCircuitBreakerService.QUERY_BREAKER);
        ImplementationSymbolVisitor globalImplementationSymbolVisitor = new ImplementationSymbolVisitor(
                referenceResolver, functions, RowGranularity.CLUSTER);
        this.globalProjectionToProjectionVisitor = new ProjectionToProjectorVisitor(
                clusterService,
                threadPool,
                settings,
                transportActionProvider,
                bulkRetryCoordinatorPool,
                globalImplementationSymbolVisitor);
    }

    @Override
    public Job newJob(Plan plan) {
        final Job job = new Job();
        List<Task> tasks = planVisitor.process(plan, job);
        job.addTasks(tasks);
        return job;
    }

    @Override
    public List<? extends ListenableFuture<TaskResult>> execute(Job job) {
        assert job.tasks().size() > 0;
        return execute(job.tasks());

    }

    @Override
    public List<Task> newTasks(PlanNode planNode, UUID jobId) {
        return planNode.accept(nodeVisitor, jobId);
    }

    @Override
    public List<? extends ListenableFuture<TaskResult>> execute(Collection<Task> tasks) {
        Task lastTask = null;
        assert tasks.size() > 0 : "need at least one task to execute";
        for (Task task : tasks) {

            if (lastTask != null) {
                task.upstreamResult(lastTask.result());
            }
            task.start();
            lastTask = task;
        }
        assert lastTask != null;
        return lastTask.result();
    }

    class TaskCollectingVisitor extends PlanVisitor<Job, List<Task>> {

        @Override
        public List<Task> visitIterablePlan(IterablePlan plan, Job job) {
            List<Task> tasks = new ArrayList<>();
            for (PlanNode planNode : plan) {
                tasks.addAll(planNode.accept(nodeVisitor, job.id()));
            }
            return tasks;
        }

        @Override
        public List<Task> visitNoopPlan(NoopPlan plan, Job job) {
            return ImmutableList.<Task>of(NoopTask.INSTANCE);
        }

        @Override
        public List<Task> visitGlobalAggregate(GlobalAggregate plan, Job job) {
            return ImmutableList.of(createExecutableNodesTask(job, plan.collectNode(), plan.mergeNode()));
        }

        @Override
        public List<Task> visitCollectAndMerge(CollectAndMerge plan, Job job) {
            return ImmutableList.of(createExecutableNodesTask(job, plan.collectNode(), plan.localMergeNode()));
        }

        @Override
        public List<Task> visitQueryAndFetch(QueryAndFetch plan, Job job) {
            return ImmutableList.of(createExecutableNodesTask(job, plan.collectNode(), plan.localMergeNode()));
        }

        @Override
        public List<Task> visitCountPlan(CountPlan countPlan, Job job) {
            return ImmutableList.of(createExecutableNodesTask(job, countPlan.countNode(), countPlan.mergeNode()));
        }

        private Task createExecutableNodesTask(Job job, ExecutionNode executionNode, @Nullable MergeNode localMergeNode) {
            return createExecutableNodesTask(job,
                    ImmutableList.<List<ExecutionNode>>of(ImmutableList.of(executionNode)),
                    localMergeNode == null ? null : ImmutableList.of(localMergeNode));
        }

        private ExecutionNodesTask createExecutableNodesTask(Job job, List<List<ExecutionNode>> groupedExecutionNodes, @Nullable List<MergeNode> localMergeNodes) {
            for (List<ExecutionNode> executionNodeGroup : groupedExecutionNodes) {
                for (ExecutionNode executionNode : executionNodeGroup) {
                    executionNode.jobId(job.id());
                }
            }
            if (localMergeNodes != null) {
                for (MergeNode localMergeNode : localMergeNodes) {
                    localMergeNode.jobId(job.id());
                }
            }
            return new ExecutionNodesTask(
                    job.id(),
                    clusterService,
                    contextPreparer,
                    jobContextService,
                    pageDownstreamFactory,
                    threadPool,
                    transportActionProvider.transportJobInitAction(),
                    transportActionProvider.transportCloseContextNodeAction(),
                    streamerVisitor,
                    circuitBreaker,
                    localMergeNodes,
                    groupedExecutionNodes
            );
        }

        @Override
        public List<Task> visitNonDistributedGroupBy(NonDistributedGroupBy plan, Job job) {
            return ImmutableList.of(createExecutableNodesTask(job, plan.collectNode(), plan.localMergeNode()));
        }

        @Override
        public List<Task> visitUpsert(Upsert plan, Job job) {
            if (plan.nodes().size() == 1 && plan.nodes().get(0) instanceof IterablePlan) {
                return process(plan.nodes().get(0), job);
            }

            List<List<ExecutionNode>> groupedExecutionNodes = new ArrayList<>(plan.nodes().size());
            List<MergeNode> mergeNodes = new ArrayList<>(plan.nodes().size());
            for (Plan subPlan : plan.nodes()) {
                assert subPlan instanceof CollectAndMerge;
                groupedExecutionNodes.add(ImmutableList.<ExecutionNode>of(((CollectAndMerge) subPlan).collectNode()));
                mergeNodes.add(((CollectAndMerge) subPlan).localMergeNode());
            }
            ExecutionNodesTask task = createExecutableNodesTask(job, groupedExecutionNodes, mergeNodes);
            task.rowCountResult(true);
            return ImmutableList.<Task>of(task);
        }

        @Override
        public List<Task> visitDistributedGroupBy(DistributedGroupBy plan, Job job) {
            plan.collectNode().jobId(job.id());
            plan.reducerMergeNode().jobId(job.id());
            MergeNode localMergeNode = plan.localMergeNode();
            List<MergeNode> mergeNodes = null;
            if (localMergeNode != null) {
                localMergeNode.jobId(job.id());
                mergeNodes = ImmutableList.of(localMergeNode);
            }
            return ImmutableList.<Task>of(
                    createExecutableNodesTask(job,
                            ImmutableList.<List<ExecutionNode>>of(
                                    ImmutableList.<ExecutionNode>of(
                                            plan.collectNode(),
                                            plan.reducerMergeNode())),
                            mergeNodes));
        }

        @Override
        public List<Task> visitInsertByQuery(InsertFromSubQuery node, Job job) {
            List<Task> tasks = process(node.innerPlan(), job);
            if(node.handlerMergeNode().isPresent()) {

                Task previousTask = Iterables.getLast(tasks);
                if (previousTask instanceof ExecutionNodesTask) {
                    ((ExecutionNodesTask) previousTask).mergeNodes(ImmutableList.of(node.handlerMergeNode().get()));
                } else {
                    ArrayList<Task> tasks2 = new ArrayList<>(tasks);
                    tasks2.addAll(nodeVisitor.visitMergeNode(node.handlerMergeNode().get(), job.id()));
                    return tasks2;
                }
            }
            return tasks;
        }

        @Override
        public List<Task> visitQueryThenFetch(QueryThenFetch plan, Job job) {
            return ImmutableList.of(createExecutableNodesTask(job, plan.collectNode(), plan.mergeNode()));
        }

        @Override
        public List<Task> visitKillPlan(KillPlan killPlan, Job job) {
            return ImmutableList.<Task>of(new KillTask(
                    clusterService,
                    transportActionProvider.transportKillAllNodeAction(),
                    job.id()));
        }
    }

    class NodeVisitor extends PlanNodeVisitor<UUID, ImmutableList<Task>> {

        private ImmutableList<Task> singleTask(Task task) {
            return ImmutableList.of(task);
        }

        @Override
        public ImmutableList<Task> visitGenericDDLNode(GenericDDLNode node, UUID jobId) {
            return singleTask(new DDLTask(jobId, ddlAnalysisDispatcherProvider.get(), node));
        }

        @Override
        public ImmutableList<Task> visitESGetNode(ESGetNode node, UUID jobId) {
            return singleTask(new ESGetTask(
                    jobId,
                    functions,
                    globalProjectionToProjectionVisitor,
                    transportActionProvider.transportMultiGetAction(),
                    transportActionProvider.transportGetAction(),
                    node,
                    jobContextService));
        }

        @Override
        public ImmutableList<Task> visitESDeleteByQueryNode(ESDeleteByQueryNode node, UUID jobId) {
            return singleTask(new ESDeleteByQueryTask(
                    jobId,
                    node,
                    transportActionProvider.transportDeleteByQueryAction(),
                    jobContextService));
        }

        @Override
        public ImmutableList<Task> visitESDeleteNode(ESDeleteNode node, UUID jobId) {
            return singleTask(new ESDeleteTask(
                    jobId,
                    node,
                    transportActionProvider.transportDeleteAction(),
                    jobContextService));
        }

        @Override
        public ImmutableList<Task> visitCreateTableNode(CreateTableNode node, UUID jobId) {
            return singleTask(new CreateTableTask(
                            jobId,
                            clusterService,
                            transportActionProvider.transportCreateIndexAction(),
                            transportActionProvider.transportDeleteIndexAction(),
                            transportActionProvider.transportPutIndexTemplateAction(),
                            node)
            );
        }

        @Override
        public ImmutableList<Task> visitESCreateTemplateNode(ESCreateTemplateNode node, UUID jobId) {
            return singleTask(new ESCreateTemplateTask(jobId,
                    node,
                    transportActionProvider.transportPutIndexTemplateAction()));
        }

        @Override
        public ImmutableList<Task> visitSymbolBasedUpsertByIdNode(SymbolBasedUpsertByIdNode node, UUID jobId) {
            return singleTask(new SymbolBasedUpsertByIdTask(jobId,
                    clusterService,
                    clusterService.state().metaData().settings(),
                    transportActionProvider.symbolBasedTransportShardUpsertActionDelegate(),
                    transportActionProvider.transportCreateIndexAction(),
                    transportActionProvider.transportBulkCreateIndicesAction(),
                    bulkRetryCoordinatorPool,
                    node,
                    jobContextService));
        }

        @Override
        public ImmutableList<Task> visitDropTableNode(DropTableNode node, UUID jobId) {
            return singleTask(new DropTableTask(jobId,
                    transportActionProvider.transportDeleteIndexTemplateAction(),
                    transportActionProvider.transportDeleteIndexAction(),
                    node));
        }

        @Override
        public ImmutableList<Task> visitESDeletePartitionNode(ESDeletePartitionNode node, UUID jobId) {
            return singleTask(new ESDeletePartitionTask(jobId,
                    transportActionProvider.transportDeleteIndexAction(),
                    node));
        }

        @Override
        public ImmutableList<Task> visitESClusterUpdateSettingsNode(ESClusterUpdateSettingsNode node, UUID jobId) {
            return singleTask(new ESClusterUpdateSettingsTask(
                    jobId,
                    transportActionProvider.transportClusterUpdateSettingsAction(),
                    node));
        }

        @Override
        protected ImmutableList<Task> visitPlanNode(PlanNode node, UUID jobId) {
            throw new UnsupportedOperationException(
                    String.format("Can't generate job/task for planNode %s", node));
        }
    }
}

<code block>


package io.crate.planner;

import com.google.common.collect.Lists;
import io.crate.planner.node.PlanNode;

import java.util.ArrayList;
import java.util.Iterator;


public class IterablePlan implements Iterable<PlanNode>, Plan {

    private ArrayList<PlanNode> nodes;

    public IterablePlan(PlanNode... nodes) {
        this.nodes = Lists.newArrayList(nodes);
    }

    public void add(PlanNode node) {
        nodes.add(node);
    }

    @Override
    public Iterator<PlanNode> iterator() {
        return nodes.iterator();
    }

    public boolean isEmpty() {
        return nodes.isEmpty();
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitIterablePlan(this, context);
    }
}

<code block>


package io.crate.planner;


public class NoopPlan implements Plan {

    public static final NoopPlan INSTANCE = new NoopPlan();

    private NoopPlan() {
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitNoopPlan(this, context);
    }
}

<code block>


package io.crate.planner;

import com.google.common.base.MoreObjects;
import com.google.common.base.Predicates;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableSet;
import com.google.common.collect.Iterables;
import io.crate.analyze.OrderBy;
import io.crate.analyze.WhereClause;
import io.crate.metadata.PartitionName;
import io.crate.metadata.Routing;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.consumer.OrderByPositionVisitor;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.DQLPlanNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.InputColumn;
import io.crate.planner.symbol.Symbol;
import io.crate.planner.symbol.Symbols;

import javax.annotation.Nullable;
import java.util.List;
import java.util.Map;
import java.util.TreeMap;

public class PlanNodeBuilder {

    public static CollectNode distributingCollect(TableInfo tableInfo,
                                                  Planner.Context plannerContext,
                                                  WhereClause whereClause,
                                                  List<Symbol> toCollect,
                                                  List<String> downstreamNodes,
                                                  ImmutableList<Projection> projections) {
        Routing routing = tableInfo.getRouting(whereClause, null);
        plannerContext.allocateJobSearchContextIds(routing);
        CollectNode node = new CollectNode(
                plannerContext.nextExecutionNodeId(),
                "distributing collect",
                routing);
        node.whereClause(whereClause);
        node.maxRowGranularity(tableInfo.rowGranularity());
        node.downstreamNodes(downstreamNodes);
        node.toCollect(toCollect);
        node.projections(projections);

        node.isPartitioned(tableInfo.isPartitioned());
        setOutputTypes(node);
        return node;
    }

    public static MergeNode distributedMerge(CollectNode collectNode,
                                             Planner.Context plannerContext,
                                             List<Projection> projections) {
        MergeNode node = new MergeNode(
                plannerContext.nextExecutionNodeId(),
                "distributed merge",
                collectNode.executionNodes().size());
        node.projections(projections);

        assert collectNode.hasDistributingDownstreams();
        node.executionNodes(ImmutableSet.copyOf(collectNode.downstreamNodes()));
        connectTypes(collectNode, node);
        return node;
    }

    public static MergeNode localMerge(List<Projection> projections,
                                       DQLPlanNode previousNode,
                                       Planner.Context plannerContext) {
        MergeNode node = new MergeNode(
                plannerContext.nextExecutionNodeId(),
                "localMerge",
                previousNode.executionNodes().size());
        node.projections(projections);
        connectTypes(previousNode, node);
        return node;
    }


    public static MergeNode sortedLocalMerge(List<Projection> projections,
                                             OrderBy orderBy,
                                             List<Symbol> sourceSymbols,
                                             @Nullable List<Symbol> orderBySymbols,
                                             DQLPlanNode previousNode,
                                             Planner.Context plannerContext) {
        int[] orderByIndices = OrderByPositionVisitor.orderByPositions(
                MoreObjects.firstNonNull(orderBySymbols, orderBy.orderBySymbols()),
                sourceSymbols
        );
        MergeNode node = MergeNode.sortedMergeNode(
                plannerContext.nextExecutionNodeId(),
                "sortedLocalMerge",
                previousNode.executionNodes().size(),
                orderByIndices,
                orderBy.reverseFlags(),
                orderBy.nullsFirst()
        );
        node.projections(projections);
        connectTypes(previousNode, node);
        return node;
    }


    public static void setOutputTypes(CollectNode node) {
        if (node.projections().isEmpty()) {
            node.outputTypes(Symbols.extractTypes(node.toCollect()));
        } else {
            node.outputTypes(Planner.extractDataTypes(node.projections(), Symbols.extractTypes(node.toCollect())));
        }
    }


    public static void connectTypes(DQLPlanNode previousNode, DQLPlanNode nextNode) {
        nextNode.inputTypes(previousNode.outputTypes());
        nextNode.outputTypes(Planner.extractDataTypes(nextNode.projections(), nextNode.inputTypes()));
    }

    public static CollectNode collect(TableInfo tableInfo,
                                      Planner.Context plannerContext,
                                      WhereClause whereClause,
                                      List<Symbol> toCollect,
                                      ImmutableList<Projection> projections,
                                      @Nullable String partitionIdent,
                                      @Nullable String routingPreference,
                                      @Nullable OrderBy orderBy,
                                      @Nullable Integer limit) {
        assert !Iterables.any(toCollect, Predicates.instanceOf(InputColumn.class)) : "cannot collect inputcolumns";
        Routing routing = tableInfo.getRouting(whereClause, routingPreference);
        if (partitionIdent != null && routing.hasLocations()) {
            routing = filterRouting(routing, PartitionName.fromPartitionIdent(
                    tableInfo.ident().schema(), tableInfo.ident().name(), partitionIdent).stringValue());
        }
        plannerContext.allocateJobSearchContextIds(routing);
        CollectNode node = new CollectNode(
                plannerContext.nextExecutionNodeId(),
                "collect",
                routing,
                toCollect,
                projections);
        node.whereClause(whereClause);
        node.maxRowGranularity(tableInfo.rowGranularity());
        node.isPartitioned(tableInfo.isPartitioned());
        setOutputTypes(node);
        node.orderBy(orderBy);
        node.limit(limit);
        return node;
    }

    private static Routing filterRouting(Routing routing, String includeTableName) {
        assert routing.hasLocations();
        assert includeTableName != null;
        Map<String, Map<String, List<Integer>>> newLocations = new TreeMap<>();

        for (Map.Entry<String, Map<String, List<Integer>>> entry : routing.locations().entrySet()) {
            Map<String, List<Integer>> tableMap = new TreeMap<>();
            for (Map.Entry<String, List<Integer>> tableEntry : entry.getValue().entrySet()) {
                if (includeTableName.equals(tableEntry.getKey())) {
                    tableMap.put(tableEntry.getKey(), tableEntry.getValue());
                }
            }
            if (tableMap.size()>0){
                newLocations.put(entry.getKey(), tableMap);
            }

        }
        if (newLocations.size()>0) {
            return new Routing(newLocations);
        } else {
            return new Routing();
        }

    }

    public static CollectNode collect(TableInfo tableInfo,
                                      Planner.Context plannerContext,
                                      WhereClause whereClause,
                                      List<Symbol> toCollect,
                                      ImmutableList<Projection> projections) {
        return collect(tableInfo, plannerContext, whereClause, toCollect, projections, null, null, null, null);
    }

    public static CollectNode collect(TableInfo tableInfo,
                                      Planner.Context plannerContext,
                                      WhereClause whereClause,
                                      List<Symbol> toCollect,
                                      ImmutableList<Projection> projections,
                                      @Nullable String partitionIdent,
                                      @Nullable String routingPreference) {
        return collect(tableInfo, plannerContext, whereClause, toCollect, projections, partitionIdent, routingPreference, null, null);
    }

    public static CollectNode collect(TableInfo tableInfo,
                                      Planner.Context plannerContext,
                                      WhereClause whereClause,
                                      List<Symbol> toCollect,
                                      ImmutableList<Projection> projections,
                                      @Nullable String partitionIdent) {
        return collect(tableInfo, plannerContext, whereClause, toCollect, projections, partitionIdent, null);
    }

    public static CollectNode collect(TableInfo tableInfo,
                                      Planner.Context plannerContext,
                                      WhereClause whereClause,
                                      List<Symbol> toCollect,
                                      ImmutableList<Projection> projections,
                                      @Nullable OrderBy orderBy,
                                      @Nullable Integer limit) {
        return collect(tableInfo, plannerContext, whereClause, toCollect, projections, null, null, orderBy, limit);
    }
}

<code block>


package io.crate.planner;

import com.carrotsearch.hppc.IntObjectOpenHashMap;
import com.carrotsearch.hppc.procedures.ObjectProcedure;
import com.google.common.base.Preconditions;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.Lists;
import io.crate.analyze.*;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.analyze.relations.TableRelation;
import io.crate.analyze.where.DocKeys;
import io.crate.core.collections.TreeMapBuilder;
import io.crate.exceptions.UnhandledServerException;
import io.crate.metadata.*;
import io.crate.metadata.doc.DocSysColumns;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.aggregation.impl.CountAggregation;
import io.crate.planner.consumer.ConsumerContext;
import io.crate.planner.consumer.ConsumingPlanner;
import io.crate.planner.consumer.UpdateConsumer;
import io.crate.planner.node.ddl.*;
import io.crate.planner.node.dml.ESDeleteByQueryNode;
import io.crate.planner.node.dml.ESDeleteNode;
import io.crate.planner.node.dml.SymbolBasedUpsertByIdNode;
import io.crate.planner.node.dml.Upsert;
import io.crate.planner.node.dql.CollectAndMerge;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.FileUriCollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.management.KillPlan;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.SourceIndexWriterProjection;
import io.crate.planner.projection.WriterProjection;
import io.crate.planner.symbol.InputColumn;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.cluster.node.DiscoveryNodes;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.inject.Singleton;
import org.elasticsearch.common.settings.ImmutableSettings;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.index.shard.ShardId;

import javax.annotation.Nullable;
import java.io.IOException;
import java.util.*;
import java.util.concurrent.atomic.AtomicInteger;

import static com.google.common.base.MoreObjects.firstNonNull;

@Singleton
public class Planner extends AnalyzedStatementVisitor<Planner.Context, Plan> {

    private final ConsumingPlanner consumingPlanner;
    private final ClusterService clusterService;
    private UpdateConsumer updateConsumer;

    public static class Context {

        private final IntObjectOpenHashMap<ShardId> jobSearchContextIdToShard = new IntObjectOpenHashMap<>();
        private final IntObjectOpenHashMap<String> jobSearchContextIdToNode = new IntObjectOpenHashMap<>();
        private final ClusterService clusterService;
        private int jobSearchContextIdBaseSeq = 0;
        private int executionNodeId = 0;

        public Context(ClusterService clusterService) {
            this.clusterService = clusterService;
        }

        public ClusterService clusterService() {
            return clusterService;
        }


        public void allocateJobSearchContextIds(Routing routing) {
            if (routing.jobSearchContextIdBase() > -1 || routing.hasLocations() == false
                    || routing.numShards() == 0) {
                return;
            }
            int jobSearchContextId = jobSearchContextIdBaseSeq;
            jobSearchContextIdBaseSeq += routing.numShards();
            routing.jobSearchContextIdBase(jobSearchContextId);
            for (Map.Entry<String, Map<String, List<Integer>>> nodeEntry : routing.locations().entrySet()) {
                String nodeId = nodeEntry.getKey();
                Map<String, List<Integer>> nodeRouting = nodeEntry.getValue();
                if (nodeRouting != null) {
                    for (Map.Entry<String, List<Integer>> entry : nodeRouting.entrySet()) {
                        for (Integer shardId : entry.getValue()) {
                            jobSearchContextIdToShard.put(jobSearchContextId, new ShardId(entry.getKey(), shardId));
                            jobSearchContextIdToNode.put(jobSearchContextId, nodeId);
                            jobSearchContextId++;
                        }
                    }
                }
            }
        }

        @Nullable
        public ShardId shardId(int jobSearchContextId) {
            return jobSearchContextIdToShard.get(jobSearchContextId);
        }

        public IntObjectOpenHashMap<ShardId> jobSearchContextIdToShard() {
            return jobSearchContextIdToShard;
        }

        @Nullable
        public String nodeId(int jobSearchContextId) {
            return jobSearchContextIdToNode.get(jobSearchContextId);
        }

        public IntObjectOpenHashMap<String> jobSearchContextIdToNode() {
            return jobSearchContextIdToNode;
        }

        public int nextExecutionNodeId() {
            return executionNodeId++;
        }
    }

    @Inject
    public Planner(ClusterService clusterService, ConsumingPlanner consumingPlanner, UpdateConsumer updateConsumer) {
        this.clusterService = clusterService;
        this.updateConsumer = updateConsumer;
        this.consumingPlanner = consumingPlanner;
    }


    public Plan plan(Analysis analysis) {
        AnalyzedStatement analyzedStatement = analysis.analyzedStatement();
        return process(analyzedStatement, new Context(clusterService));
    }

    @Override
    protected Plan visitAnalyzedStatement(AnalyzedStatement analyzedStatement, Context context) {
        throw new UnsupportedOperationException(String.format("AnalyzedStatement \"%s\" not supported.", analyzedStatement));
    }

    @Override
    protected Plan visitSelectStatement(SelectAnalyzedStatement statement, Context context) {
        return consumingPlanner.plan(statement.relation(), context);
    }

    @Override
    protected Plan visitInsertFromValuesStatement(InsertFromValuesAnalyzedStatement statement, Context context) {
        Preconditions.checkState(!statement.sourceMaps().isEmpty(), "no values given");
        return processInsertStatement(statement, context);
    }

    @Override
    protected Plan visitInsertFromSubQueryStatement(InsertFromSubQueryAnalyzedStatement statement, Context context) {
        return consumingPlanner.plan(statement, context);
    }

    @Override
    protected Plan visitUpdateStatement(UpdateAnalyzedStatement statement, Context context) {
        ConsumerContext consumerContext = new ConsumerContext(statement, context);
        if (updateConsumer.consume(statement, consumerContext)) {
            return ((PlannedAnalyzedRelation) consumerContext.rootRelation()).plan();
        }
        throw new IllegalArgumentException("Couldn't plan Update statement");
    }

    @Override
    protected Plan visitDeleteStatement(DeleteAnalyzedStatement analyzedStatement, Context context) {
        IterablePlan plan = new IterablePlan();
        TableRelation tableRelation = analyzedStatement.analyzedRelation();
        List<WhereClause> whereClauses = new ArrayList<>(analyzedStatement.whereClauses().size());
        List<DocKeys.DocKey> docKeys = new ArrayList<>(analyzedStatement.whereClauses().size());
        for (WhereClause whereClause : analyzedStatement.whereClauses()) {
            if (whereClause.noMatch()) {
                continue;
            }
            if (whereClause.docKeys().isPresent() && whereClause.docKeys().get().size() == 1) {
                docKeys.add(whereClause.docKeys().get().getOnlyKey());
            } else if (!whereClause.noMatch()) {
                whereClauses.add(whereClause);
            }
        }
        if (!docKeys.isEmpty()) {
            plan.add(new ESDeleteNode(context.nextExecutionNodeId(), tableRelation.tableInfo(), docKeys));
        } else if (!whereClauses.isEmpty()) {
            createESDeleteByQueryNode(tableRelation.tableInfo(), whereClauses, plan, context);
        }

        if (plan.isEmpty()) {
            return NoopPlan.INSTANCE;
        }
        return plan;
    }

    @Override
    protected Plan visitCopyStatement(final CopyAnalyzedStatement analysis, Context context) {
        switch (analysis.mode()) {
            case FROM:
                return copyFromPlan(analysis, context);
            case TO:
                return copyToPlan(analysis, context);
            default:
                throw new UnsupportedOperationException("mode not supported: " + analysis.mode());
        }
    }

    private Plan copyToPlan(CopyAnalyzedStatement analysis, Context context) {
        TableInfo tableInfo = analysis.table();
        WriterProjection projection = new WriterProjection();
        projection.uri(analysis.uri());
        projection.isDirectoryUri(analysis.directoryUri());
        projection.settings(analysis.settings());

        List<Symbol> outputs;
        if (analysis.selectedColumns() != null && !analysis.selectedColumns().isEmpty()) {
            outputs = new ArrayList<>(analysis.selectedColumns().size());
            List<Symbol> columnSymbols = new ArrayList<>(analysis.selectedColumns().size());
            for (int i = 0; i < analysis.selectedColumns().size(); i++) {
                outputs.add(DocReferenceConverter.convertIfPossible(analysis.selectedColumns().get(i), analysis.table()));
                columnSymbols.add(new InputColumn(i, null));
            }
            projection.inputs(columnSymbols);
        } else {
            Reference sourceRef;
            if (analysis.table().isPartitioned() && analysis.partitionIdent() == null) {

                sourceRef = new Reference(analysis.table().getReferenceInfo(DocSysColumns.DOC));
                Map<ColumnIdent, Symbol> overwrites = new HashMap<>();
                for (ReferenceInfo referenceInfo : analysis.table().partitionedByColumns()) {
                    overwrites.put(referenceInfo.ident().columnIdent(), new Reference(referenceInfo));
                }
                projection.overwrites(overwrites);
            } else {
                sourceRef = new Reference(analysis.table().getReferenceInfo(DocSysColumns.RAW));
            }
            outputs = ImmutableList.<Symbol>of(sourceRef);
        }
        CollectNode collectNode = PlanNodeBuilder.collect(
                tableInfo,
                context,
                WhereClause.MATCH_ALL,
                outputs,
                ImmutableList.<Projection>of(projection),
                analysis.partitionIdent()
        );

        MergeNode mergeNode = PlanNodeBuilder.localMerge(
                ImmutableList.<Projection>of(CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION), collectNode, context);
        return new CollectAndMerge(collectNode, mergeNode);
    }

    private Plan copyFromPlan(CopyAnalyzedStatement analysis, Context context) {


        TableInfo table = analysis.table();
        int clusteredByPrimaryKeyIdx = table.primaryKey().indexOf(analysis.table().clusteredBy());
        List<String> partitionedByNames;
        String partitionIdent = null;

        List<BytesRef> partitionValues;
        if (analysis.partitionIdent() == null) {

            if (table.isPartitioned()) {
                partitionedByNames = Lists.newArrayList(
                        Lists.transform(table.partitionedBy(), ColumnIdent.GET_FQN_NAME_FUNCTION));
            } else {
                partitionedByNames = Collections.emptyList();
            }
            partitionValues = ImmutableList.of();
        } else {
            assert table.isPartitioned() : "table must be partitioned if partitionIdent is set";

            PartitionName partitionName = PartitionName.fromPartitionIdent(table.ident().schema(), table.ident().name(), analysis.partitionIdent());
            partitionValues = partitionName.values();

            partitionIdent = partitionName.ident();
            partitionedByNames = Collections.emptyList();
        }

        SourceIndexWriterProjection sourceIndexWriterProjection = new SourceIndexWriterProjection(
                table.ident(),
                partitionIdent,
                new Reference(table.getReferenceInfo(DocSysColumns.RAW)),
                table.primaryKey(),
                table.partitionedBy(),
                partitionValues,
                table.clusteredBy(),
                clusteredByPrimaryKeyIdx,
                analysis.settings(),
                null,
                partitionedByNames.size() > 0 ? partitionedByNames.toArray(new String[partitionedByNames.size()]) : null,
                table.isPartitioned() 
        );
        List<Projection> projections = Arrays.<Projection>asList(sourceIndexWriterProjection);
        partitionedByNames.removeAll(Lists.transform(table.primaryKey(), ColumnIdent.GET_FQN_NAME_FUNCTION));
        int referencesSize = table.primaryKey().size() + partitionedByNames.size() + 1;
        referencesSize = clusteredByPrimaryKeyIdx == -1 ? referencesSize + 1 : referencesSize;

        List<Symbol> toCollect = new ArrayList<>(referencesSize);

        for (ColumnIdent primaryKey : table.primaryKey()) {
            toCollect.add(new Reference(table.getReferenceInfo(primaryKey)));
        }


        for (String partitionedColumn : partitionedByNames) {
            toCollect.add(
                    new Reference(table.getReferenceInfo(ColumnIdent.fromPath(partitionedColumn)))
            );
        }

        if (clusteredByPrimaryKeyIdx == -1) {
            toCollect.add(
                    new Reference(table.getReferenceInfo(table.clusteredBy())));
        }

        if (table.isPartitioned() && analysis.partitionIdent() == null) {
            toCollect.add(new Reference(table.getReferenceInfo(DocSysColumns.DOC)));
        } else {
            toCollect.add(new Reference(table.getReferenceInfo(DocSysColumns.RAW)));
        }

        DiscoveryNodes allNodes = clusterService.state().nodes();
        FileUriCollectNode collectNode = new FileUriCollectNode(
                context.nextExecutionNodeId(),
                "copyFrom",
                generateRouting(allNodes, analysis.settings().getAsInt("num_readers", allNodes.getSize())),
                analysis.uri(),
                toCollect,
                projections,
                analysis.settings().get("compression", null),
                analysis.settings().getAsBoolean("shared", null)
        );
        PlanNodeBuilder.setOutputTypes(collectNode);

        return new CollectAndMerge(collectNode, PlanNodeBuilder.localMerge(
                ImmutableList.<Projection>of(CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION), collectNode, context));
    }

    private Routing generateRouting(DiscoveryNodes allNodes, int maxNodes) {
        final AtomicInteger counter = new AtomicInteger(maxNodes);
        final Map<String, Map<String, List<Integer>>> locations = new TreeMap<>();
        allNodes.dataNodes().keys().forEach(new ObjectProcedure<String>() {
            @Override
            public void apply(String value) {
                if (counter.getAndDecrement() > 0) {
                    locations.put(value, TreeMapBuilder.<String, List<Integer>>newMapBuilder().map());
                }
            }
        });
        return new Routing(locations);
    }

    @Override
    protected Plan visitDDLAnalyzedStatement(AbstractDDLAnalyzedStatement statement, Context context) {
        return new IterablePlan(new GenericDDLNode(statement));
    }

    @Override
    public Plan visitDropBlobTableStatement(DropBlobTableAnalyzedStatement analysis, Context context) {
        if (analysis.noop()) {
            return NoopPlan.INSTANCE;
        }
        return visitDDLAnalyzedStatement(analysis, context);
    }

    @Override
    protected Plan visitDropTableStatement(DropTableAnalyzedStatement analysis, Context context) {
        if (analysis.noop()) {
            return NoopPlan.INSTANCE;
        }
        return new IterablePlan(new DropTableNode(analysis.table(), analysis.dropIfExists()));
    }

    @Override
    protected Plan visitCreateTableStatement(CreateTableAnalyzedStatement analysis, Context context) {
        if (analysis.noOp()) {
            return NoopPlan.INSTANCE;
        }
        TableIdent tableIdent = analysis.tableIdent();

        CreateTableNode createTableNode;
        if (analysis.isPartitioned()) {
            createTableNode = CreateTableNode.createPartitionedTableNode(
                    tableIdent,
                    analysis.ifNotExists(),
                    analysis.tableParameter().settings().getByPrefix("index."),
                    analysis.mapping(),
                    analysis.templateName(),
                    analysis.templatePrefix()
            );
        } else {
            createTableNode = CreateTableNode.createTableNode(
                    tableIdent,
                    analysis.ifNotExists(),
                    analysis.tableParameter().settings(),
                    analysis.mapping()
            );
        }
        return new IterablePlan(createTableNode);
    }

    @Override
    protected Plan visitCreateAnalyzerStatement(CreateAnalyzerAnalyzedStatement analysis, Context context) {
        Settings analyzerSettings;
        try {
            analyzerSettings = analysis.buildSettings();
        } catch (IOException ioe) {
            throw new UnhandledServerException("Could not build analyzer Settings", ioe);
        }

        ESClusterUpdateSettingsNode node = new ESClusterUpdateSettingsNode(analyzerSettings);
        return new IterablePlan(node);
    }

    @Override
    public Plan visitSetStatement(SetAnalyzedStatement analysis, Context context) {
        ESClusterUpdateSettingsNode node = null;
        if (analysis.isReset()) {

            if (analysis.settingsToRemove() != null) {
                node = new ESClusterUpdateSettingsNode(analysis.settingsToRemove(), analysis.settingsToRemove());
            }
        } else {
            if (analysis.settings() != null) {
                if (analysis.isPersistent()) {
                    node = new ESClusterUpdateSettingsNode(analysis.settings());
                } else {
                    node = new ESClusterUpdateSettingsNode(ImmutableSettings.EMPTY, analysis.settings());
                }
            }
        }
        return node != null ? new IterablePlan(node) : NoopPlan.INSTANCE;
    }

    @Override
    public Plan visitKillAnalyzedStatement(KillAnalyzedStatement analysis, Context context) {
        return KillPlan.INSTANCE;
    }

    private void createESDeleteByQueryNode(TableInfo tableInfo,
                                           List<WhereClause> whereClauses,
                                           IterablePlan plan,
                                           Context context) {

        List<String[]> indicesList = new ArrayList<>(whereClauses.size());
        for (WhereClause whereClause : whereClauses) {
            String[] indices = indices(tableInfo, whereClauses.get(0));
            if (indices.length > 0) {
                if (!whereClause.hasQuery() && tableInfo.isPartitioned()) {
                    plan.add(new ESDeletePartitionNode(indices));
                } else {
                    indicesList.add(indices);
                }
            }
        }



        if (!indicesList.isEmpty()) {
            plan.add(new ESDeleteByQueryNode(context.nextExecutionNodeId(), indicesList, whereClauses));
        }
    }

    private Upsert processInsertStatement(InsertFromValuesAnalyzedStatement analysis, Context context) {
        String[] onDuplicateKeyAssignmentsColumns = null;
        if (analysis.onDuplicateKeyAssignmentsColumns().size() > 0) {
            onDuplicateKeyAssignmentsColumns = analysis.onDuplicateKeyAssignmentsColumns().get(0);
        }
        SymbolBasedUpsertByIdNode upsertByIdNode = new SymbolBasedUpsertByIdNode(
                context.nextExecutionNodeId(),
                analysis.tableInfo().isPartitioned(),
                analysis.isBulkRequest(),
                onDuplicateKeyAssignmentsColumns,
                analysis.columns().toArray(new Reference[analysis.columns().size()])
        );
        if (analysis.tableInfo().isPartitioned()) {
            List<String> partitions = analysis.generatePartitions();
            String[] indices = partitions.toArray(new String[partitions.size()]);
            for (int i = 0; i < indices.length; i++) {
                Symbol[] onDuplicateKeyAssignments = null;
                if (analysis.onDuplicateKeyAssignmentsColumns().size() > i) {
                    onDuplicateKeyAssignments = analysis.onDuplicateKeyAssignments().get(i);
                }
                upsertByIdNode.add(
                        indices[i],
                        analysis.ids().get(i),
                        analysis.routingValues().get(i),
                        onDuplicateKeyAssignments,
                        null,
                        analysis.sourceMaps().get(i));
            }
        } else {
            for (int i = 0; i < analysis.ids().size(); i++) {
                Symbol[] onDuplicateKeyAssignments = null;
                if (analysis.onDuplicateKeyAssignments().size() > i) {
                    onDuplicateKeyAssignments = analysis.onDuplicateKeyAssignments().get(i);
                }
                upsertByIdNode.add(
                        analysis.tableInfo().ident().esName(),
                        analysis.ids().get(i),
                        analysis.routingValues().get(i),
                        onDuplicateKeyAssignments,
                        null,
                        analysis.sourceMaps().get(i));
            }
        }

        return new Upsert(ImmutableList.<Plan>of(new IterablePlan(upsertByIdNode)));
    }

    static List<DataType> extractDataTypes(List<Projection> projections, @Nullable List<DataType> inputTypes) {
        if (projections.size() == 0) {
            return inputTypes;
        }
        int projectionIdx = projections.size() - 1;
        Projection lastProjection = projections.get(projectionIdx);
        List<DataType> types = new ArrayList<>(lastProjection.outputs().size());
        List<DataType> dataTypes = firstNonNull(inputTypes, ImmutableList.<DataType>of());

        for (int c = 0; c < lastProjection.outputs().size(); c++) {
            types.add(resolveType(projections, projectionIdx, c, dataTypes));
        }
        return types;
    }

    private static DataType resolveType(List<Projection> projections, int projectionIdx, int columnIdx, List<DataType> inputTypes) {
        Projection projection = projections.get(projectionIdx);
        Symbol symbol = projection.outputs().get(columnIdx);
        DataType type = symbol.valueType();
        if (type == null || (type.equals(DataTypes.UNDEFINED) && symbol instanceof InputColumn)) {
            if (projectionIdx > 0) {
                if (symbol instanceof InputColumn) {
                    columnIdx = ((InputColumn) symbol).index();
                }
                return resolveType(projections, projectionIdx - 1, columnIdx, inputTypes);
            } else {
                assert symbol instanceof InputColumn; 
                return inputTypes.get(((InputColumn) symbol).index());
            }
        }

        return type;
    }



    public static String[] indices(TableInfo tableInfo, WhereClause whereClause) {
        String[] indices;

        if (whereClause.noMatch()) {
            indices = org.elasticsearch.common.Strings.EMPTY_ARRAY;
        } else if (!tableInfo.isPartitioned()) {

            indices = new String[]{tableInfo.ident().esName()};
        } else if (whereClause.partitions().isEmpty()) {
            if (whereClause.noMatch()) {
                return new String[0];
            }


            indices = new String[tableInfo.partitions().size()];
            int i = 0;
            for (PartitionName partitionName: tableInfo.partitions()) {
                indices[i] = partitionName.stringValue();
                i++;
            }
        } else {
            indices = whereClause.partitions().toArray(new String[whereClause.partitions().size()]);
        }
        return indices;
    }
}


<code block>


package io.crate.planner;

public interface Plan {

    <C, R> R accept(PlanVisitor<C, R> visitor, C context);
}

<code block>


package io.crate.planner.consumer;

import com.google.common.collect.ImmutableList;
import io.crate.analyze.UpdateAnalyzedStatement;
import io.crate.analyze.VersionRewriter;
import io.crate.analyze.WhereClause;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.analyze.relations.TableRelation;
import io.crate.analyze.where.DocKeys;
import io.crate.metadata.PartitionName;
import io.crate.metadata.ReferenceIdent;
import io.crate.metadata.ReferenceInfo;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.aggregation.impl.CountAggregation;
import io.crate.planner.*;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dml.SymbolBasedUpsertByIdNode;
import io.crate.planner.node.dml.Upsert;
import io.crate.planner.node.dql.CollectAndMerge;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.UpdateProjection;
import io.crate.planner.symbol.InputColumn;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.planner.symbol.ValueSymbolVisitor;
import io.crate.types.DataTypes;
import org.elasticsearch.cluster.routing.operation.plain.Preference;
import org.elasticsearch.common.collect.Tuple;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.inject.Singleton;

import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;
import java.util.Map;

@Singleton
public class UpdateConsumer implements Consumer {

    private final Visitor visitor;

    @Inject
    public UpdateConsumer() {
        visitor = new Visitor();
    }

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        PlannedAnalyzedRelation plannedAnalyzedRelation = visitor.process(rootRelation, context);
        if (plannedAnalyzedRelation == null) {
            return false;
        }
        context.rootRelation(plannedAnalyzedRelation);
        return true;
    }

    class Visitor extends AnalyzedRelationVisitor<ConsumerContext, PlannedAnalyzedRelation> {

        @Override
        public PlannedAnalyzedRelation visitUpdateAnalyzedStatement(UpdateAnalyzedStatement statement, ConsumerContext context) {
            assert statement.sourceRelation() instanceof TableRelation : "sourceRelation of update statement must be a TableRelation";
            TableRelation tableRelation = (TableRelation) statement.sourceRelation();
            TableInfo tableInfo = tableRelation.tableInfo();

            if (tableInfo.schemaInfo().systemSchema() || tableInfo.rowGranularity() != RowGranularity.DOC) {
                return null;
            }

            List<Plan> childNodes = new ArrayList<>(statement.nestedStatements().size());
            SymbolBasedUpsertByIdNode upsertByIdNode = null;
            for (UpdateAnalyzedStatement.NestedAnalyzedStatement nestedAnalysis : statement.nestedStatements()) {
                WhereClause whereClause = nestedAnalysis.whereClause();
                if (whereClause.noMatch()){
                    continue;
                }
                if (whereClause.docKeys().isPresent()) {
                    if (upsertByIdNode == null) {
                        Tuple<String[], Symbol[]> assignments = convertAssignments(nestedAnalysis.assignments());
                        upsertByIdNode = new SymbolBasedUpsertByIdNode(context.plannerContext().nextExecutionNodeId(), false, statement.nestedStatements().size() > 1, assignments.v1(), null);
                        childNodes.add(new IterablePlan(upsertByIdNode));
                    }
                    upsertById(nestedAnalysis, tableInfo, whereClause, upsertByIdNode);
                } else {
                    Plan plan = upsertByQuery(nestedAnalysis, context, tableInfo, whereClause);
                    if (plan != null) {
                        childNodes.add(plan);
                    }
                }
            }
            if (childNodes.size() > 0){
                return new Upsert(childNodes);
            } else {
                return new NoopPlannedAnalyzedRelation(statement);
            }
        }

        private Plan upsertByQuery(UpdateAnalyzedStatement.NestedAnalyzedStatement nestedAnalysis,
                                   ConsumerContext consumerContext,
                                   TableInfo tableInfo,
                                   WhereClause whereClause) {

            Symbol versionSymbol = null;
            if(whereClause.hasVersions()){
                versionSymbol = VersionRewriter.get(whereClause.query());
                whereClause = new WhereClause(whereClause.query(), whereClause.docKeys().orNull(), whereClause.partitions());
            }


            if (!whereClause.noMatch() || !(tableInfo.isPartitioned() && whereClause.partitions().isEmpty())) {

                Reference uidReference = new Reference(
                        new ReferenceInfo(
                                new ReferenceIdent(tableInfo.ident(), "_uid"),
                                RowGranularity.DOC, DataTypes.STRING));

                Tuple<String[], Symbol[]> assignments = convertAssignments(nestedAnalysis.assignments());

                Long version = null;
                if (versionSymbol != null){
                    version = ValueSymbolVisitor.LONG.process(versionSymbol);
                }

                UpdateProjection updateProjection = new UpdateProjection(
                        new InputColumn(0, DataTypes.STRING),
                        assignments.v1(),
                        assignments.v2(),
                        version);

                CollectNode collectNode = PlanNodeBuilder.collect(
                        tableInfo,
                        consumerContext.plannerContext(),
                        whereClause,
                        ImmutableList.<Symbol>of(uidReference),
                        ImmutableList.<Projection>of(updateProjection),
                        null,
                        Preference.PRIMARY.type()
                );
                MergeNode mergeNode = PlanNodeBuilder.localMerge(
                        ImmutableList.<Projection>of(CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION), collectNode,
                        consumerContext.plannerContext());
                return new CollectAndMerge(collectNode, mergeNode);
            } else {
                return null;
            }
        }

        private void upsertById(UpdateAnalyzedStatement.NestedAnalyzedStatement nestedAnalysis,
                                             TableInfo tableInfo,
                                             WhereClause whereClause,
                                             SymbolBasedUpsertByIdNode upsertByIdNode) {
            String[] indices = Planner.indices(tableInfo, whereClause);
            assert tableInfo.isPartitioned() || indices.length == 1;

            Tuple<String[], Symbol[]> assignments = convertAssignments(nestedAnalysis.assignments());


            for (DocKeys.DocKey key : whereClause.docKeys().get()) {
                String index;
                if (key.partitionValues().isPresent()) {
                    index = new PartitionName(tableInfo.ident(), key.partitionValues().get()).stringValue();
                } else {
                    index = indices[0];
                }
                upsertByIdNode.add(
                        index,
                        key.id(),
                        key.routing(),
                        assignments.v2(),
                        key.version().orNull());
            }
        }


        private Tuple<String[], Symbol[]> convertAssignments(Map<Reference, Symbol> assignments) {
            String[] assignmentColumns = new String[assignments.size()];
            Symbol[] assignmentSymbols = new Symbol[assignments.size()];
            Iterator<Reference> it = assignments.keySet().iterator();
            int i = 0;
            while(it.hasNext()) {
                Reference key = it.next();
                assignmentColumns[i] = key.ident().columnIdent().fqn();
                assignmentSymbols[i] = assignments.get(key);
                i++;
            }
            return new Tuple<>(assignmentColumns, assignmentSymbols);
        }

        @Override
        protected PlannedAnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, ConsumerContext context) {
            return null;
        }
    }
}

<code block>

package io.crate.planner.consumer;

import com.google.common.collect.ImmutableList;
import io.crate.analyze.*;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.Routing;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.GroupByConsumer;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.dql.NonDistributedGroupBy;
import io.crate.planner.projection.GroupProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.builder.ProjectionBuilder;
import io.crate.planner.projection.builder.SplitPoints;
import io.crate.planner.symbol.Aggregation;
import io.crate.planner.symbol.Symbol;

import java.util.ArrayList;
import java.util.List;

public class NonDistributedGroupByConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        Context ctx = new Context(context);
        context.rootRelation(VISITOR.process(context.rootRelation(), ctx));
        return ctx.result;
    }

    private static class Context {
        ConsumerContext consumerContext;
        boolean result = false;

        public Context(ConsumerContext context) {
            this.consumerContext = context;
        }
    }

    private static class Visitor extends AnalyzedRelationVisitor<Context, AnalyzedRelation> {

        @Override
        public AnalyzedRelation visitQueriedTable(QueriedTable table, Context context) {
            if (table.querySpec().groupBy() == null) {
                return table;
            }
            TableInfo tableInfo = table.tableRelation().tableInfo();

            if (table.querySpec().where().hasVersions()) {
                context.consumerContext.validationException(new VersionInvalidException());
                return table;
            }

            Routing routing = tableInfo.getRouting(table.querySpec().where(), null);

            if (GroupByConsumer.requiresDistribution(tableInfo, routing) && !(tableInfo.schemaInfo().systemSchema())) {
                return table;
            }

            context.result = true;
            return nonDistributedGroupBy(table, context);
        }

        @Override
        public AnalyzedRelation visitInsertFromQuery(InsertFromSubQueryAnalyzedStatement insertFromSubQueryAnalyzedStatement, Context context) {
            InsertFromSubQueryConsumer.planInnerRelation(insertFromSubQueryAnalyzedStatement, context, this);
            return insertFromSubQueryAnalyzedStatement;

        }

        @Override
        protected AnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, Context context) {
            return relation;
        }


        private AnalyzedRelation nonDistributedGroupBy(QueriedTable table, Context context) {
            TableInfo tableInfo = table.tableRelation().tableInfo();

            GroupByConsumer.validateGroupBySymbols(table.tableRelation(), table.querySpec().groupBy());
            List<Symbol> groupBy = table.querySpec().groupBy();

            ProjectionBuilder projectionBuilder = new ProjectionBuilder(table.querySpec());
            SplitPoints splitPoints = projectionBuilder.getSplitPoints();


            GroupProjection groupProjection = projectionBuilder.groupProjection(
                    splitPoints.leaves(),
                    table.querySpec().groupBy(),
                    splitPoints.aggregates(),
                    Aggregation.Step.ITER,
                    Aggregation.Step.PARTIAL);

            CollectNode collectNode = PlanNodeBuilder.collect(
                    tableInfo,
                    context.consumerContext.plannerContext(),
                    table.querySpec().where(),
                    splitPoints.leaves(),
                    ImmutableList.<Projection>of(groupProjection)
            );

            List<Symbol> collectOutputs = new ArrayList<>(
                    groupBy.size() +
                            splitPoints.aggregates().size());
            collectOutputs.addAll(groupBy);
            collectOutputs.addAll(splitPoints.aggregates());


            OrderBy orderBy = table.querySpec().orderBy();
            if (orderBy != null) {
                table.tableRelation().validateOrderBy(orderBy);
            }

            List<Projection> projections = new ArrayList<>();
            projections.add(projectionBuilder.groupProjection(
                    collectOutputs,
                    table.querySpec().groupBy(),
                    splitPoints.aggregates(),
                    Aggregation.Step.PARTIAL,
                    Aggregation.Step.FINAL
            ));

            HavingClause havingClause = table.querySpec().having();
            if (havingClause != null) {
                if (havingClause.noMatch()) {
                    return new NoopPlannedAnalyzedRelation(table);
                } else if (havingClause.hasQuery()){
                    projections.add(projectionBuilder.filterProjection(
                            collectOutputs,
                            havingClause.query()
                    ));
                }
            }


            boolean outputsMatch = table.querySpec().outputs().size() == collectOutputs.size() &&
                                    collectOutputs.containsAll(table.querySpec().outputs());
            if (context.consumerContext.rootRelation() == table || !outputsMatch){
                projections.add(projectionBuilder.topNProjection(
                        collectOutputs,
                        orderBy,
                        table.querySpec().offset(),
                        table.querySpec().limit(),
                        table.querySpec().outputs()
                ));
            }
            MergeNode localMergeNode = PlanNodeBuilder.localMerge(projections, collectNode,
                    context.consumerContext.plannerContext());
            return new NonDistributedGroupBy(collectNode, localMergeNode);
        }
    }

}

<code block>


package io.crate.planner.consumer;

import io.crate.analyze.QueriedTable;
import io.crate.analyze.QuerySpec;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.Routing;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.aggregation.impl.CountAggregation;
import io.crate.planner.Planner;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.CountNode;
import io.crate.planner.node.dql.CountPlan;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.Function;
import io.crate.planner.symbol.Symbol;
import io.crate.types.DataType;
import io.crate.types.DataTypes;

import java.util.Collections;
import java.util.List;

import static com.google.common.base.MoreObjects.firstNonNull;

public class CountConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        AnalyzedRelation analyzedRelation = VISITOR.process(rootRelation, context);
        if (analyzedRelation != null) {
            context.rootRelation(analyzedRelation);
            return true;
        }
        return false;
    }

    private static class Visitor extends AnalyzedRelationVisitor<ConsumerContext, PlannedAnalyzedRelation> {

        @Override
        public PlannedAnalyzedRelation visitQueriedTable(QueriedTable table, ConsumerContext context) {
            QuerySpec querySpec = table.querySpec();
            if (!querySpec.hasAggregates() || querySpec.groupBy()!=null) {
                return null;
            }
            TableInfo tableInfo = table.tableRelation().tableInfo();
            if (tableInfo.schemaInfo().systemSchema()) {
                return null;
            }
            if (!hasOnlyGlobalCount(querySpec.outputs())) {
                return null;
            }
            if(querySpec.where().hasVersions()){
                context.validationException(new VersionInvalidException());
                return null;
            }

            if (firstNonNull(querySpec.limit(), 1) < 1 ||
                    querySpec.offset() > 0){
                return new NoopPlannedAnalyzedRelation(table);
            }

            Routing routing = tableInfo.getRouting(querySpec.where(), null);
            Planner.Context plannerContext = context.plannerContext();
            CountNode countNode = new CountNode(plannerContext.nextExecutionNodeId(), routing, querySpec.where());
            MergeNode mergeNode = new MergeNode(
                    plannerContext.nextExecutionNodeId(),
                    "count-merge",
                    countNode.executionNodes().size());
            mergeNode.inputTypes(Collections.<DataType>singletonList(DataTypes.LONG));
            mergeNode.projections(Collections.<Projection>singletonList(
                    CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION));
            return new CountPlan(countNode, mergeNode);
        }

        @Override
        protected PlannedAnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, ConsumerContext context) {
            return null;
        }

        private boolean hasOnlyGlobalCount(List<Symbol> symbols) {
            if (symbols.size() != 1) {
                return false;
            }
            Symbol symbol = symbols.get(0);
            if (!(symbol instanceof Function)) {
                return false;
            }
            Function function = (Function) symbol;
            return function.info().equals(CountAggregation.COUNT_STAR_FUNCTION);
        }
    }
}

<code block>


package io.crate.planner.consumer;

import com.google.common.collect.ImmutableList;
import io.crate.Constants;
import io.crate.analyze.*;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.TableRelation;
import io.crate.exceptions.UnsupportedFeatureException;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.DocReferenceConverter;
import io.crate.metadata.FunctionIdent;
import io.crate.metadata.Functions;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.aggregation.impl.SumAggregation;
import io.crate.operation.predicate.MatchPredicate;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.node.dql.QueryAndFetch;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.AggregationProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.TopNProjection;
import io.crate.planner.symbol.*;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import io.crate.types.LongType;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;

import static com.google.common.base.MoreObjects.firstNonNull;

public class QueryAndFetchConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        Context ctx = new Context(context);
        context.rootRelation(VISITOR.process(context.rootRelation(), ctx));
        return ctx.result;
    }

    private static class Context {
        ConsumerContext consumerContext;
        boolean result = false;

        public Context(ConsumerContext context) {
            this.consumerContext = context;
        }
    }

    private static class Visitor extends AnalyzedRelationVisitor<Context, AnalyzedRelation> {

        @Override
        public AnalyzedRelation visitQueriedTable(QueriedTable table, Context context) {
            TableRelation tableRelation = table.tableRelation();
            if(table.querySpec().where().hasVersions()){
                context.consumerContext.validationException(new VersionInvalidException());
                return table;
            }
            TableInfo tableInfo = tableRelation.tableInfo();

            if (tableInfo.schemaInfo().systemSchema() && table.querySpec().where().hasQuery()) {
                ensureNoLuceneOnlyPredicates(table.querySpec().where().query());
            }
            if (table.querySpec().hasAggregates()) {
                context.result = true;
                return GlobalAggregateConsumer.globalAggregates(table, tableRelation, table.querySpec().where(), context.consumerContext);
            } else {
               context.result = true;
               return normalSelect(table, table.querySpec().where(), tableRelation, context);
            }
        }

        @Override
        public AnalyzedRelation visitInsertFromQuery(InsertFromSubQueryAnalyzedStatement insertFromSubQueryAnalyzedStatement,
                                                     Context context) {
            InsertFromSubQueryConsumer.planInnerRelation(insertFromSubQueryAnalyzedStatement, context, this);
            return insertFromSubQueryAnalyzedStatement;
        }

        @Override
        protected AnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, Context context) {
            return relation;
        }

        private void ensureNoLuceneOnlyPredicates(Symbol query) {
            NoPredicateVisitor noPredicateVisitor = new NoPredicateVisitor();
            noPredicateVisitor.process(query, null);
        }

        private static class NoPredicateVisitor extends SymbolVisitor<Void, Void> {
            @Override
            public Void visitFunction(Function symbol, Void context) {
                if (symbol.info().ident().name().equals(MatchPredicate.NAME)) {
                    throw new UnsupportedFeatureException("Cannot use match predicate on system tables");
                }
                for (Symbol argument : symbol.arguments()) {
                    process(argument, context);
                }
                return null;
            }
        }

        private AnalyzedRelation normalSelect(QueriedTable table,
                                              WhereClause whereClause,
                                              TableRelation tableRelation,
                                              Context context){
            QuerySpec querySpec = table.querySpec();
            TableInfo tableInfo = tableRelation.tableInfo();

            List<Symbol> outputSymbols;
            if (tableInfo.schemaInfo().systemSchema()) {
                outputSymbols = tableRelation.resolve(querySpec.outputs());
            } else {
                outputSymbols = new ArrayList<>(querySpec.outputs().size());
                for (Symbol symbol : querySpec.outputs()) {
                    outputSymbols.add(DocReferenceConverter.convertIfPossible(tableRelation.resolve(symbol), tableInfo));
                }
            }
            CollectNode collectNode;
            MergeNode mergeNode = null;
            OrderBy orderBy = querySpec.orderBy();
            if (context.consumerContext.rootRelation() != table) {

                assert !querySpec.isLimited() : "insert from sub query with limit or order by is not supported. " +
                        "Analyzer should have thrown an exception already.";

                ImmutableList<Projection> projections = ImmutableList.<Projection>of();
                collectNode = PlanNodeBuilder.collect(tableInfo,
                        context.consumerContext.plannerContext(),
                        whereClause, outputSymbols, projections);
            } else if (querySpec.isLimited() || orderBy != null) {

                List<Symbol> toCollect;
                List<Symbol> orderByInputColumns = null;
                if (orderBy != null){
                    List<Symbol> orderBySymbols = tableRelation.resolve(orderBy.orderBySymbols());
                    toCollect = new ArrayList<>(outputSymbols.size() + orderBySymbols.size());
                    toCollect.addAll(outputSymbols);

                    for (Symbol orderBySymbol : orderBySymbols) {
                        if (!toCollect.contains(orderBySymbol)) {
                            toCollect.add(orderBySymbol);
                        }
                    }
                    orderByInputColumns = new ArrayList<>();
                    for (Symbol symbol : orderBySymbols) {
                        orderByInputColumns.add(new InputColumn(toCollect.indexOf(symbol), symbol.valueType()));
                    }
                } else {
                    toCollect = new ArrayList<>(outputSymbols.size());
                    toCollect.addAll(outputSymbols);
                }

                List<Symbol> allOutputs = new ArrayList<>(toCollect.size());        
                for (int i = 0; i < toCollect.size(); i++) {
                    allOutputs.add(new InputColumn(i, toCollect.get(i).valueType()));
                }
                List<Symbol> finalOutputs = new ArrayList<>(outputSymbols.size());  
                for (int i = 0; i < outputSymbols.size(); i++) {
                    finalOutputs.add(new InputColumn(i, outputSymbols.get(i).valueType()));
                }



                TopNProjection tnp;
                int limit = firstNonNull(querySpec.limit(), Constants.DEFAULT_SELECT_LIMIT);
                if (orderBy == null){
                    tnp = new TopNProjection(querySpec.offset() + limit, 0);
                } else {
                    tnp = new TopNProjection(querySpec.offset() + limit, 0,
                            orderByInputColumns,
                            orderBy.reverseFlags(),
                            orderBy.nullsFirst()
                    );
                }
                tnp.outputs(allOutputs);
                collectNode = PlanNodeBuilder.collect(tableInfo,
                        context.consumerContext.plannerContext(),
                        whereClause, toCollect, ImmutableList.<Projection>of(tnp));


                tnp = new TopNProjection(limit, querySpec.offset());
                tnp.outputs(finalOutputs);
                if (orderBy == null) {

                    mergeNode = PlanNodeBuilder.localMerge(ImmutableList.<Projection>of(tnp), collectNode,
                            context.consumerContext.plannerContext());
                } else {


                    mergeNode = PlanNodeBuilder.sortedLocalMerge(
                            ImmutableList.<Projection>of(tnp),
                            orderBy,
                            allOutputs,
                            orderByInputColumns,
                            collectNode,
                            context.consumerContext.plannerContext()
                    );
                }
            } else {
                collectNode = PlanNodeBuilder.collect(tableInfo,
                        context.consumerContext.plannerContext(),
                        whereClause, outputSymbols, ImmutableList.<Projection>of());
                mergeNode = PlanNodeBuilder.localMerge(
                        ImmutableList.<Projection>of(), collectNode,
                        context.consumerContext.plannerContext());
            }
            return new QueryAndFetch(collectNode, mergeNode);
        }
    }
}

<code block>


package io.crate.planner.consumer;

import com.google.common.collect.ImmutableList;
import io.crate.analyze.*;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.analyze.relations.TableRelation;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.FunctionInfo;
import io.crate.metadata.ReferenceInfo;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.GlobalAggregate;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.AggregationProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.TopNProjection;
import io.crate.planner.projection.builder.ProjectionBuilder;
import io.crate.planner.projection.builder.SplitPoints;
import io.crate.planner.symbol.*;

import java.util.ArrayList;
import java.util.Collection;
import java.util.List;

import static com.google.common.base.MoreObjects.firstNonNull;


public class GlobalAggregateConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();
    private static final AggregationOutputValidator AGGREGATION_OUTPUT_VALIDATOR = new AggregationOutputValidator();

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        AnalyzedRelation analyzedRelation = VISITOR.process(rootRelation, context);
        if (analyzedRelation != null) {
            context.rootRelation(analyzedRelation);
            return true;
        }
        return false;
    }

    private static class Visitor extends AnalyzedRelationVisitor<ConsumerContext, PlannedAnalyzedRelation> {

        @Override
        public PlannedAnalyzedRelation visitQueriedTable(QueriedTable table, ConsumerContext context) {
            if (table.querySpec().groupBy()!=null || !table.querySpec().hasAggregates()) {
                return null;
            }
            if (firstNonNull(table.querySpec().limit(), 1) < 1 || table.querySpec().offset() > 0){
                return new NoopPlannedAnalyzedRelation(table);
            }

            if (table.querySpec().where().hasVersions()){
                context.validationException(new VersionInvalidException());
                return null;
            }
            return globalAggregates(table, table.tableRelation(),  table.querySpec().where(), context);
        }

        @Override
        public PlannedAnalyzedRelation visitInsertFromQuery(InsertFromSubQueryAnalyzedStatement insertFromSubQueryAnalyzedStatement, ConsumerContext context) {
            InsertFromSubQueryConsumer.planInnerRelation(insertFromSubQueryAnalyzedStatement, context, this);
            return null;
        }

        @Override
        protected PlannedAnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, ConsumerContext context) {
            return null;
        }
    }

    private static boolean noGroupBy(List<Symbol> groupBy) {
        return groupBy == null || groupBy.isEmpty();
    }

    public static PlannedAnalyzedRelation globalAggregates(QueriedTable table,
                                                           TableRelation tableRelation,
                                                           WhereClause whereClause,
                                                           ConsumerContext context) {
        assert noGroupBy(table.querySpec().groupBy()) : "must not have group by clause for global aggregate queries";
        validateAggregationOutputs(tableRelation, table.querySpec().outputs());


        ProjectionBuilder projectionBuilder = new ProjectionBuilder(table.querySpec());
        SplitPoints splitPoints = projectionBuilder.getSplitPoints();

        AggregationProjection ap = projectionBuilder.aggregationProjection(
                splitPoints.leaves(),
                splitPoints.aggregates(),
                Aggregation.Step.ITER,
                Aggregation.Step.PARTIAL);

        CollectNode collectNode = PlanNodeBuilder.collect(
                tableRelation.tableInfo(),
                context.plannerContext(),
                whereClause,
                splitPoints.leaves(),
                ImmutableList.<Projection>of(ap)
        );


        List<Projection> projections = new ArrayList<>();
        projections.add(projectionBuilder.aggregationProjection(
                splitPoints.aggregates(),
                splitPoints.aggregates(),
                Aggregation.Step.PARTIAL,
                Aggregation.Step.FINAL));

        HavingClause havingClause = table.querySpec().having();
        if(havingClause != null){
            if (havingClause.noMatch()) {
                return new NoopPlannedAnalyzedRelation(table);
            } else if (havingClause.hasQuery()){
                projections.add(projectionBuilder.filterProjection(
                        splitPoints.aggregates(),
                        havingClause.query()
                ));
            }
        }

        TopNProjection topNProjection = projectionBuilder.topNProjection(
                splitPoints.aggregates(),
                null, 0, 1,
                table.querySpec().outputs()
                );
        projections.add(topNProjection);
        MergeNode localMergeNode = PlanNodeBuilder.localMerge(projections, collectNode,
                context.plannerContext());
        return new GlobalAggregate(collectNode, localMergeNode);
    }

    private static void validateAggregationOutputs(TableRelation tableRelation, Collection<? extends Symbol> outputSymbols) {
        OutputValidatorContext context = new OutputValidatorContext(tableRelation);
        for (Symbol outputSymbol : outputSymbols) {
            context.insideAggregation = false;
            AGGREGATION_OUTPUT_VALIDATOR.process(outputSymbol, context);
        }
    }

    private static class OutputValidatorContext {
        private final TableRelation tableRelation;
        private boolean insideAggregation = false;

        public OutputValidatorContext(TableRelation tableRelation) {
            this.tableRelation = tableRelation;
        }
    }

    private static class AggregationOutputValidator extends SymbolVisitor<OutputValidatorContext, Void> {

        @Override
        public Void visitFunction(Function symbol, OutputValidatorContext context) {
            context.insideAggregation = context.insideAggregation || symbol.info().type().equals(FunctionInfo.Type.AGGREGATE);
            for (Symbol argument : symbol.arguments()) {
                process(argument, context);
            }
            context.insideAggregation = false;
            return null;
        }

        @Override
        public Void visitReference(Reference symbol, OutputValidatorContext context) {
            if (context.insideAggregation) {
                ReferenceInfo.IndexType indexType = symbol.info().indexType();
                if (indexType == ReferenceInfo.IndexType.ANALYZED) {
                    throw new IllegalArgumentException(String.format(
                            "Cannot select analyzed column '%s' within grouping or aggregations", SymbolFormatter.format(symbol)));
                } else if (indexType == ReferenceInfo.IndexType.NO) {
                    throw new IllegalArgumentException(String.format(
                            "Cannot select non-indexed column '%s' within grouping or aggregations", SymbolFormatter.format(symbol)));
                }
            }
            return null;
        }

        @Override
        public Void visitField(Field field, OutputValidatorContext context) {
            return process(context.tableRelation.resolveField(field), context);
        }

        @Override
        protected Void visitSymbol(Symbol symbol, OutputValidatorContext context) {
            return null;
        }
    }
}

<code block>


package io.crate.planner.consumer;


import io.crate.analyze.OrderBy;
import io.crate.analyze.QueriedTable;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.ESGetNode;

public class ESGetConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        PlannedAnalyzedRelation relation = VISITOR.process(rootRelation, context);
        if (relation == null) {
            return false;
        }
        context.rootRelation(relation);
        return true;
    }

    private static class Visitor extends AnalyzedRelationVisitor<ConsumerContext, PlannedAnalyzedRelation> {

        @Override
        public PlannedAnalyzedRelation visitQueriedTable(QueriedTable table, ConsumerContext context) {
            if (table.querySpec().hasAggregates() || table.querySpec().groupBy()!=null) {
                return null;
            }
            TableInfo tableInfo = table.tableRelation().tableInfo();
            if (tableInfo.isAlias()
                    || tableInfo.schemaInfo().systemSchema()
                    || tableInfo.rowGranularity() != RowGranularity.DOC) {
                return null;
            }

            if (!table.querySpec().where().docKeys().isPresent()) {
                return null;
            }

            if(table.querySpec().where().docKeys().get().withVersions()){
                context.validationException(new VersionInvalidException());
                return null;
            }
            Integer limit = table.querySpec().limit();
            if (limit != null){
                if (limit == 0){
                    return new NoopPlannedAnalyzedRelation(table);
                }
            }

            OrderBy orderBy = table.querySpec().orderBy();
            if (orderBy != null){
                table.tableRelation().validateOrderBy(orderBy);
            }
            return new ESGetNode(context.plannerContext().nextExecutionNodeId(), tableInfo, table.querySpec());
        }

        @Override
        protected PlannedAnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, ConsumerContext context) {
            return null;
        }
    }
}

<code block>


package io.crate.planner.consumer;

import com.google.common.collect.ImmutableList;
import io.crate.Constants;
import io.crate.analyze.OrderBy;
import io.crate.analyze.QueriedTable;
import io.crate.analyze.QuerySpec;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.ColumnIdent;
import io.crate.metadata.DocReferenceConverter;
import io.crate.metadata.ReferenceInfo;
import io.crate.metadata.ScoreReferenceDetector;
import io.crate.metadata.doc.DocSysColumns;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.projectors.FetchProjector;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.dql.QueryThenFetch;
import io.crate.planner.projection.FetchProjection;
import io.crate.planner.projection.MergeProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.TopNProjection;
import io.crate.planner.projection.builder.ProjectionBuilder;
import io.crate.planner.projection.builder.SplitPoints;
import io.crate.planner.symbol.*;
import io.crate.types.DataTypes;

import java.util.ArrayList;
import java.util.Collections;
import java.util.List;

public class QueryThenFetchConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();
    private static final OutputOrderReferenceCollector OUTPUT_ORDER_REFERENCE_COLLECTOR = new OutputOrderReferenceCollector();
    private static final ReferencesCollector REFERENCES_COLLECTOR = new ReferencesCollector();
    private static final ScoreReferenceDetector SCORE_REFERENCE_DETECTOR = new ScoreReferenceDetector();
    private static final ColumnIdent DOC_ID_COLUMN_IDENT = new ColumnIdent(DocSysColumns.DOCID.name());
    private static final InputColumn DEFAULT_DOC_ID_INPUT_COLUMN = new InputColumn(0, DataTypes.STRING);

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        PlannedAnalyzedRelation plannedAnalyzedRelation = VISITOR.process(rootRelation, context);
        if (plannedAnalyzedRelation == null) {
            return false;
        }
        context.rootRelation(plannedAnalyzedRelation);
        return true;
    }

    private static class Visitor extends AnalyzedRelationVisitor<ConsumerContext, PlannedAnalyzedRelation> {

        @Override
        public PlannedAnalyzedRelation visitQueriedTable(QueriedTable table, ConsumerContext context) {
            QuerySpec querySpec = table.querySpec();
            if (querySpec.hasAggregates() || querySpec.groupBy()!=null) {
                return null;
            }
            TableInfo tableInfo = table.tableRelation().tableInfo();
            if (tableInfo.schemaInfo().systemSchema() || tableInfo.rowGranularity() != RowGranularity.DOC) {
                return null;
            }

            if(querySpec.where().hasVersions()){
                context.validationException(new VersionInvalidException());
                return null;
            }

            if (querySpec.where().noMatch()) {
                return new NoopPlannedAnalyzedRelation(table);
            }

            boolean outputsAreAllOrdered = false;
            boolean needFetchProjection = REFERENCES_COLLECTOR.collect(querySpec.outputs()).containsAnyReference();
            List<Projection> collectProjections = new ArrayList<>();
            List<Projection> mergeProjections = new ArrayList<>();
            List<Symbol> collectSymbols = new ArrayList<>();
            List<Symbol> outputSymbols = new ArrayList<>();
            ReferenceInfo docIdRefInfo = tableInfo.getReferenceInfo(DOC_ID_COLUMN_IDENT);

            ProjectionBuilder projectionBuilder = new ProjectionBuilder(querySpec);
            SplitPoints splitPoints = projectionBuilder.getSplitPoints();


            OrderBy orderBy = querySpec.orderBy();
            if (orderBy != null) {
                table.tableRelation().validateOrderBy(orderBy);




                OutputOrderReferenceContext outputOrderContext =
                        OUTPUT_ORDER_REFERENCE_COLLECTOR.collect(splitPoints.leaves());
                outputOrderContext.collectOrderBy = true;
                OUTPUT_ORDER_REFERENCE_COLLECTOR.collect(orderBy.orderBySymbols(), outputOrderContext);
                outputsAreAllOrdered = outputOrderContext.outputsAreAllOrdered();
                if (outputsAreAllOrdered) {
                    collectSymbols = splitPoints.toCollect();
                } else {
                    collectSymbols.addAll(orderBy.orderBySymbols());
                }
            }

            needFetchProjection = needFetchProjection & !outputsAreAllOrdered;

            if (needFetchProjection) {
                collectSymbols.add(0, new Reference(docIdRefInfo));
                for (Symbol symbol : querySpec.outputs()) {

                    if (SCORE_REFERENCE_DETECTOR.detect(symbol) && !collectSymbols.contains(symbol)) {
                        collectSymbols.add(symbol);
                    }
                    outputSymbols.add(DocReferenceConverter.convertIfPossible(symbol, tableInfo));
                }
            } else {

                collectSymbols = splitPoints.toCollect();
            }
            if (orderBy != null) {
                MergeProjection mergeProjection = projectionBuilder.mergeProjection(
                        collectSymbols,
                        orderBy);
                collectProjections.add(mergeProjection);
            }

            Integer limit = querySpec.limit();

            if ( limit == null && context.rootRelation() == table) {
                limit = Constants.DEFAULT_SELECT_LIMIT;
            }

            CollectNode collectNode = PlanNodeBuilder.collect(
                    tableInfo,
                    context.plannerContext(),
                    querySpec.where(),
                    collectSymbols,
                    ImmutableList.<Projection>of(),
                    orderBy,
                    limit == null ? null : limit + querySpec.offset()
            );


            collectNode.keepContextForFetcher(needFetchProjection);
            collectNode.projections(collectProjections);



            TopNProjection topNProjection;
            if (needFetchProjection) {
                topNProjection = projectionBuilder.topNProjection(
                        collectSymbols,
                        null,
                        querySpec.offset(),
                        querySpec.limit(),
                        null);
                mergeProjections.add(topNProjection);



                int bulkSize = FetchProjector.NO_BULK_REQUESTS;
                if (topNProjection.limit() > Constants.DEFAULT_SELECT_LIMIT) {
                    bulkSize = Constants.DEFAULT_SELECT_LIMIT;
                }

                FetchProjection fetchProjection = new FetchProjection(
                        collectNode.executionNodeId(),
                        DEFAULT_DOC_ID_INPUT_COLUMN, collectSymbols, outputSymbols,
                        tableInfo.partitionedByColumns(),
                        collectNode.executionNodes(),
                        bulkSize,
                        querySpec.isLimited(),
                        context.plannerContext().jobSearchContextIdToNode(),
                        context.plannerContext().jobSearchContextIdToShard()
                );
                mergeProjections.add(fetchProjection);
            } else {
                topNProjection = projectionBuilder.topNProjection(
                        collectSymbols,
                        null,
                        querySpec.offset(),
                        querySpec.limit(),
                        querySpec.outputs());
                mergeProjections.add(topNProjection);
            }

            MergeNode localMergeNode;
            if (orderBy != null) {
                localMergeNode = PlanNodeBuilder.sortedLocalMerge(
                        mergeProjections,
                        orderBy,
                        collectSymbols,
                        null,
                        collectNode,
                        context.plannerContext());
            } else {
                localMergeNode = PlanNodeBuilder.localMerge(
                        mergeProjections,
                        collectNode,
                        context.plannerContext());
            }


            if (limit != null && limit + querySpec.offset() > Constants.PAGE_SIZE) {
                collectNode.downstreamNodes(Collections.singletonList(context.plannerContext().clusterService().localNode().id()));
                collectNode.downstreamExecutionNodeId(localMergeNode.executionNodeId());
            }
            return new QueryThenFetch(collectNode, localMergeNode);
        }

        @Override
        protected PlannedAnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, ConsumerContext context) {
            return null;
        }
    }

    static class OutputOrderReferenceContext {

        private List<Reference> outputReferences = new ArrayList<>();
        private List<Reference> orderByReferences = new ArrayList<>();
        public boolean collectOrderBy = false;

        public void addReference(Reference reference) {
            if (collectOrderBy) {
                orderByReferences.add(reference);
            } else {
                outputReferences.add(reference);
            }
        }

        public boolean outputsAreAllOrdered() {
            return orderByReferences.containsAll(outputReferences);
        }

    }

    static class OutputOrderReferenceCollector extends SymbolVisitor<OutputOrderReferenceContext, Void> {

        public OutputOrderReferenceContext collect(List<Symbol> symbols) {
            OutputOrderReferenceContext context = new OutputOrderReferenceContext();
            collect(symbols, context);
            return context;
        }

        public void collect(List<Symbol> symbols, OutputOrderReferenceContext context) {
            for (Symbol symbol : symbols) {
                process(symbol, context);
            }
        }

        @Override
        public Void visitAggregation(Aggregation aggregation, OutputOrderReferenceContext context) {
            for (Symbol symbol : aggregation.inputs()) {
                process(symbol, context);
            }
            return null;
        }

        @Override
        public Void visitReference(Reference symbol, OutputOrderReferenceContext context) {
            context.addReference(symbol);
            return null;
        }

        @Override
        public Void visitDynamicReference(DynamicReference symbol, OutputOrderReferenceContext context) {
            return visitReference(symbol, context);
        }

        @Override
        public Void visitFunction(Function function, OutputOrderReferenceContext context) {
            for (Symbol symbol : function.arguments()) {
                process(symbol, context);
            }
            return null;
        }
    }

    static class ReferencesCollectorContext {
        private List<Reference> outputReferences = new ArrayList<>();

        public void addReference(Reference reference) {
            outputReferences.add(reference);
        }

        public boolean containsAnyReference() {
            return !outputReferences.isEmpty();
        }
    }

    static class ReferencesCollector extends SymbolVisitor<ReferencesCollectorContext, Void> {

        public ReferencesCollectorContext collect(List<Symbol> symbols) {
            ReferencesCollectorContext context = new ReferencesCollectorContext();
            collect(symbols, context);
            return context;
        }

        public void collect(List<Symbol> symbols, ReferencesCollectorContext context) {
            for (Symbol symbol : symbols) {
                process(symbol, context);
            }
        }

        @Override
        public Void visitAggregation(Aggregation aggregation, ReferencesCollectorContext context) {
            for (Symbol symbol : aggregation.inputs()) {
                process(symbol, context);
            }
            return null;
        }

        @Override
        public Void visitReference(Reference symbol, ReferencesCollectorContext context) {
            context.addReference(symbol);
            return null;
        }

        @Override
        public Void visitDynamicReference(DynamicReference symbol, ReferencesCollectorContext context) {
            return visitReference(symbol, context);
        }

        @Override
        public Void visitFunction(Function function, ReferencesCollectorContext context) {
            for (Symbol symbol : function.arguments()) {
                process(symbol, context);
            }
            return null;
        }

    }
}

<code block>


package io.crate.planner.consumer;

import com.google.common.base.MoreObjects;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.Lists;
import com.google.common.collect.Sets;
import io.crate.Constants;
import io.crate.analyze.HavingClause;
import io.crate.analyze.InsertFromSubQueryAnalyzedStatement;
import io.crate.analyze.OrderBy;
import io.crate.analyze.QueriedTable;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.Routing;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.DistributedGroupBy;
import io.crate.planner.node.dql.GroupByConsumer;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.GroupProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.TopNProjection;
import io.crate.planner.projection.builder.ProjectionBuilder;
import io.crate.planner.projection.builder.SplitPoints;
import io.crate.planner.symbol.Aggregation;
import io.crate.planner.symbol.Symbol;

import java.util.ArrayList;
import java.util.LinkedList;
import java.util.List;

public class DistributedGroupByConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        Context ctx = new Context(context);
        context.rootRelation(VISITOR.process(context.rootRelation(), ctx));
        return ctx.result;
    }

    private static class Context {
        ConsumerContext consumerContext;
        boolean result = false;

        public Context(ConsumerContext context) {
            this.consumerContext = context;
        }
    }

    private static class Visitor extends AnalyzedRelationVisitor<Context, AnalyzedRelation> {

        @Override
        public AnalyzedRelation visitQueriedTable(QueriedTable table, Context context) {
            List<Symbol> groupBy = table.querySpec().groupBy();
            if (groupBy == null) {
                return table;
            }

            TableInfo tableInfo = table.tableRelation().tableInfo();
            if(table.querySpec().where().hasVersions()){
                context.consumerContext.validationException(new VersionInvalidException());
                return table;
            }

            Routing routing = tableInfo.getRouting(table.querySpec().where(), null);

            GroupByConsumer.validateGroupBySymbols(table.tableRelation(), table.querySpec().groupBy());

            ProjectionBuilder projectionBuilder = new ProjectionBuilder(table.querySpec());

            SplitPoints splitPoints = projectionBuilder.getSplitPoints();


            GroupProjection groupProjection = projectionBuilder.groupProjection(
                    splitPoints.leaves(),
                    table.querySpec().groupBy(),
                    splitPoints.aggregates(),
                    Aggregation.Step.ITER,
                    Aggregation.Step.PARTIAL);

            CollectNode collectNode = PlanNodeBuilder.distributingCollect(
                    tableInfo,
                    context.consumerContext.plannerContext(),
                    table.querySpec().where(),
                    splitPoints.leaves(),
                    Lists.newArrayList(routing.nodes()),
                    ImmutableList.<Projection>of(groupProjection)
            );



            List<Symbol> collectOutputs = new ArrayList<>(
                    groupBy.size() +
                            splitPoints.aggregates().size());
            collectOutputs.addAll(groupBy);
            collectOutputs.addAll(splitPoints.aggregates());

            List<Projection> reducerProjections = new LinkedList<>();
            reducerProjections.add(projectionBuilder.groupProjection(
                    collectOutputs,
                    table.querySpec().groupBy(),
                    splitPoints.aggregates(),
                    Aggregation.Step.PARTIAL,
                    Aggregation.Step.FINAL)
            );

            OrderBy orderBy = table.querySpec().orderBy();
            if (orderBy != null) {
                table.tableRelation().validateOrderBy(orderBy);
            }

            HavingClause havingClause = table.querySpec().having();
            if (havingClause != null) {
                if (havingClause.noMatch()) {
                    return new NoopPlannedAnalyzedRelation(table);
                } else if (havingClause.hasQuery()) {
                    reducerProjections.add(projectionBuilder.filterProjection(
                            collectOutputs,
                            havingClause.query()
                    ));
                }
            }

            boolean isRootRelation = context.consumerContext.rootRelation() == table;
            if (isRootRelation) {
                reducerProjections.add(projectionBuilder.topNProjection(
                        collectOutputs,
                        orderBy,
                        0,
                        MoreObjects.firstNonNull(table.querySpec().limit(),
                                Constants.DEFAULT_SELECT_LIMIT) + table.querySpec().offset(),
                        table.querySpec().outputs()));
            }
            MergeNode mergeNode = PlanNodeBuilder.distributedMerge(
                    collectNode,
                    context.consumerContext.plannerContext(),
                    reducerProjections
            );


            MergeNode localMergeNode = null;
            String localNodeId = context.consumerContext.plannerContext().clusterService().state().nodes().localNodeId();
            if(isRootRelation) {
                TopNProjection topN = projectionBuilder.topNProjection(
                        table.querySpec().outputs(),
                        orderBy,
                        table.querySpec().offset(),
                        table.querySpec().limit(),
                        null);
                localMergeNode = PlanNodeBuilder.localMerge(ImmutableList.<Projection>of(topN),
                        mergeNode, context.consumerContext.plannerContext());
                localMergeNode.executionNodes(Sets.newHashSet(localNodeId));

                mergeNode.downstreamNodes(localMergeNode.executionNodes());
                mergeNode.downstreamExecutionNodeId(localMergeNode.executionNodeId());
            } else {
                mergeNode.downstreamNodes(Sets.newHashSet(localNodeId));
                mergeNode.downstreamExecutionNodeId(mergeNode.executionNodeId() + 1);
            }
            context.result = true;

            collectNode.downstreamExecutionNodeId(mergeNode.executionNodeId());
            return new DistributedGroupBy(
                    collectNode,
                    mergeNode,
                    localMergeNode
            );
        }

        @Override
        public AnalyzedRelation visitInsertFromQuery(InsertFromSubQueryAnalyzedStatement insertFromSubQueryAnalyzedStatement, Context context) {
            InsertFromSubQueryConsumer.planInnerRelation(insertFromSubQueryAnalyzedStatement, context, this);
            return insertFromSubQueryAnalyzedStatement;
        }

        @Override
        protected AnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, Context context) {
            return relation;
        }

    }
}

<code block>


package io.crate.planner.consumer;

import com.google.common.collect.ImmutableList;
import io.crate.Constants;
import io.crate.analyze.HavingClause;
import io.crate.analyze.InsertFromSubQueryAnalyzedStatement;
import io.crate.analyze.OrderBy;
import io.crate.analyze.QueriedTable;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.TableRelation;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.projectors.TopN;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.GroupByConsumer;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.dql.NonDistributedGroupBy;
import io.crate.planner.projection.FilterProjection;
import io.crate.planner.projection.GroupProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.builder.ProjectionBuilder;
import io.crate.planner.projection.builder.SplitPoints;
import io.crate.planner.symbol.Aggregation;
import io.crate.planner.symbol.Symbol;

import java.util.ArrayList;
import java.util.List;

import static com.google.common.base.MoreObjects.firstNonNull;

public class ReduceOnCollectorGroupByConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        Context ctx = new Context(context);
        context.rootRelation(VISITOR.process(context.rootRelation(), ctx));
        return ctx.result;
    }

    private static class Context {
        ConsumerContext consumerContext;
        boolean result = false;

        public Context(ConsumerContext context) {
            this.consumerContext = context;
        }
    }

    private static class Visitor extends AnalyzedRelationVisitor<Context, AnalyzedRelation> {

        @Override
        public AnalyzedRelation visitQueriedTable(QueriedTable table, Context context) {
            if (table.querySpec().groupBy() == null) {
                return table;
            }

            if (!GroupByConsumer.groupedByClusteredColumnOrPrimaryKeys(
                    table.tableRelation(), table.querySpec().where(), table.querySpec().groupBy())) {
                return table;
            }

            if (table.querySpec().where().hasVersions()) {
                context.consumerContext.validationException(new VersionInvalidException());
                return table;
            }
            context.result = true;
            return optimizedReduceOnCollectorGroupBy(table, table.tableRelation(), context.consumerContext);
        }

        @Override
        public AnalyzedRelation visitInsertFromQuery(InsertFromSubQueryAnalyzedStatement insertFromSubQueryAnalyzedStatement, Context context) {
            InsertFromSubQueryConsumer.planInnerRelation(insertFromSubQueryAnalyzedStatement, context, this);
            return insertFromSubQueryAnalyzedStatement;
        }

        @Override
        protected AnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, Context context) {
            return relation;
        }


        private AnalyzedRelation optimizedReduceOnCollectorGroupBy(QueriedTable table, TableRelation tableRelation, ConsumerContext context) {
            assert GroupByConsumer.groupedByClusteredColumnOrPrimaryKeys(
                    tableRelation, table.querySpec().where(), table.querySpec().groupBy()) : "not grouped by clustered column or primary keys";
            TableInfo tableInfo = tableRelation.tableInfo();
            GroupByConsumer.validateGroupBySymbols(tableRelation, table.querySpec().groupBy());
            List<Symbol> groupBy = table.querySpec().groupBy();

            boolean ignoreSorting = context.rootRelation() != table
                    && table.querySpec().limit() == null
                    && table.querySpec().offset() == TopN.NO_OFFSET;

            ProjectionBuilder projectionBuilder = new ProjectionBuilder(table.querySpec());
            SplitPoints splitPoints = projectionBuilder.getSplitPoints();


            List<Symbol> collectOutputs = new ArrayList<>(
                    groupBy.size() +
                            splitPoints.aggregates().size());
            collectOutputs.addAll(groupBy);
            collectOutputs.addAll(splitPoints.aggregates());

            OrderBy orderBy = table.querySpec().orderBy();
            if (orderBy != null) {
                table.tableRelation().validateOrderBy(orderBy);
            }

            List<Projection> projections = new ArrayList<>();
            GroupProjection groupProjection = projectionBuilder.groupProjection(
                    splitPoints.leaves(),
                    table.querySpec().groupBy(),
                    splitPoints.aggregates(),
                    Aggregation.Step.ITER,
                    Aggregation.Step.FINAL
            );
            groupProjection.setRequiredGranularity(RowGranularity.SHARD);
            projections.add(groupProjection);

            HavingClause havingClause = table.querySpec().having();
            if (havingClause != null) {
                if (havingClause.noMatch()) {
                    return new NoopPlannedAnalyzedRelation(table);
                } else if (havingClause.hasQuery()) {
                    FilterProjection fp = projectionBuilder.filterProjection(
                            collectOutputs,
                            havingClause.query()
                    );
                    fp.requiredGranularity(RowGranularity.SHARD);
                    projections.add(fp);
                }
            }


            boolean outputsMatch = table.querySpec().outputs().size() == collectOutputs.size() &&
                    collectOutputs.containsAll(table.querySpec().outputs());
            boolean collectorTopN = table.querySpec().limit() != null || table.querySpec().offset() > 0 || !outputsMatch;

            if (collectorTopN) {
                projections.add(projectionBuilder.topNProjection(
                        collectOutputs,
                        orderBy,
                        0, 
                        firstNonNull(table.querySpec().limit(), Constants.DEFAULT_SELECT_LIMIT) + table.querySpec().offset(),
                        table.querySpec().outputs()
                ));
            }

            CollectNode collectNode = PlanNodeBuilder.collect(
                    tableInfo,
                    context.plannerContext(),
                    table.querySpec().where(),
                    splitPoints.leaves(),
                    ImmutableList.copyOf(projections)
            );


            List<Projection> handlerProjections = new ArrayList<>();
            MergeNode localMergeNode;
            if (!ignoreSorting && collectorTopN && orderBy != null && orderBy.isSorted()) {


                handlerProjections.add(
                        projectionBuilder.topNProjection(
                                table.querySpec().outputs(),
                                null, 
                                table.querySpec().offset(),
                                firstNonNull(table.querySpec().limit(), Constants.DEFAULT_SELECT_LIMIT),
                                table.querySpec().outputs()
                        )
                );
                localMergeNode = PlanNodeBuilder.sortedLocalMerge(
                        handlerProjections, orderBy, table.querySpec().outputs(), null,
                        collectNode, context.plannerContext());
            } else {
                handlerProjections.add(
                        projectionBuilder.topNProjection(
                                collectorTopN ? table.querySpec().outputs() : collectOutputs,
                                orderBy,
                                table.querySpec().offset(),
                                firstNonNull(table.querySpec().limit(), Constants.DEFAULT_SELECT_LIMIT),
                                table.querySpec().outputs()
                        )
                );

                localMergeNode = PlanNodeBuilder.localMerge(handlerProjections, collectNode,
                        context.plannerContext());
            }
            return new NonDistributedGroupBy(collectNode, localMergeNode);
        }


    }
}

<code block>


package io.crate.planner.consumer;


import com.google.common.collect.ImmutableList;
import io.crate.analyze.InsertFromSubQueryAnalyzedStatement;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.operation.aggregation.impl.CountAggregation;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.node.dml.InsertFromSubQuery;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.AggregationProjection;
import io.crate.planner.projection.ColumnIndexWriterProjection;
import io.crate.planner.projection.Projection;
import org.elasticsearch.common.settings.ImmutableSettings;


public class InsertFromSubQueryConsumer implements Consumer {

    private final Visitor visitor;

    public InsertFromSubQueryConsumer(){
        visitor = new Visitor();
    }

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        Context ctx = new Context(context);
        context.rootRelation(visitor.process(context.rootRelation(), ctx));
        return ctx.result;
    }

    private static class Context {
        ConsumerContext consumerContext;
        boolean result = false;

        public Context(ConsumerContext context){
            this.consumerContext = context;
        }
    }

    private static class Visitor extends AnalyzedRelationVisitor<Context, AnalyzedRelation> {

        @Override
        public AnalyzedRelation visitInsertFromQuery(InsertFromSubQueryAnalyzedStatement insertFromSubQueryAnalyzedStatement, Context context) {

            ColumnIndexWriterProjection indexWriterProjection = new ColumnIndexWriterProjection(
                    insertFromSubQueryAnalyzedStatement.tableInfo().ident(),
                    null,
                    insertFromSubQueryAnalyzedStatement.tableInfo().primaryKey(),
                    insertFromSubQueryAnalyzedStatement.columns(),
                    insertFromSubQueryAnalyzedStatement.onDuplicateKeyAssignments(),
                    insertFromSubQueryAnalyzedStatement.primaryKeyColumnIndices(),
                    insertFromSubQueryAnalyzedStatement.partitionedByIndices(),
                    insertFromSubQueryAnalyzedStatement.routingColumn(),
                    insertFromSubQueryAnalyzedStatement.routingColumnIndex(),
                    ImmutableSettings.EMPTY,
                    insertFromSubQueryAnalyzedStatement.tableInfo().isPartitioned()
            );

            AnalyzedRelation innerRelation = insertFromSubQueryAnalyzedStatement.subQueryRelation();
            if (innerRelation instanceof PlannedAnalyzedRelation) {
                PlannedAnalyzedRelation analyzedRelation = (PlannedAnalyzedRelation)innerRelation;
                analyzedRelation.addProjection(indexWriterProjection);

                MergeNode mergeNode = null;
                if (analyzedRelation.resultIsDistributed()) {

                    AggregationProjection aggregationProjection = CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION;
                    mergeNode = PlanNodeBuilder.localMerge(
                            ImmutableList.<Projection>of(aggregationProjection),
                            analyzedRelation.resultNode(),
                            context.consumerContext.plannerContext());
                }
                context.result = true;
                return new InsertFromSubQuery(((PlannedAnalyzedRelation) innerRelation).plan(), mergeNode);
            } else {
                return insertFromSubQueryAnalyzedStatement;
            }
        }

        @Override
        protected AnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, Context context) {
            return relation;
        }
    }

    public static <C, R> void planInnerRelation(InsertFromSubQueryAnalyzedStatement insertFromSubQueryAnalyzedStatement,
                                                C context, AnalyzedRelationVisitor<C,R> visitor) {
        if (insertFromSubQueryAnalyzedStatement.subQueryRelation() instanceof PlannedAnalyzedRelation) {

            return;
        }
        R innerRelation = visitor.process(insertFromSubQueryAnalyzedStatement.subQueryRelation(), context);
        if (innerRelation != null && innerRelation instanceof PlannedAnalyzedRelation) {
            insertFromSubQueryAnalyzedStatement.subQueryRelation((PlannedAnalyzedRelation)innerRelation);
        }
    }


}

<code block>


package io.crate.planner.node;

import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.exceptions.ColumnUnknownException;
import io.crate.metadata.Path;
import io.crate.planner.NoopPlan;
import io.crate.planner.Plan;
import io.crate.planner.node.dql.DQLPlanNode;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.Field;

import javax.annotation.Nullable;
import java.util.List;

public class NoopPlannedAnalyzedRelation implements PlannedAnalyzedRelation {

    private final AnalyzedRelation relation;

    public NoopPlannedAnalyzedRelation(AnalyzedRelation relation) {
        this.relation = relation;
    }

    @Override
    public Plan plan() {
        return NoopPlan.INSTANCE;
    }

    @Override
    public <C, R> R accept(AnalyzedRelationVisitor<C, R> visitor, C context) {
        return visitor.visitPlanedAnalyzedRelation(this, context);
    }

    @Nullable
    @Override
    public Field getField(Path path) {
        return relation.getField(path);
    }

    @Override
    public Field getWritableField(Path path) throws UnsupportedOperationException, ColumnUnknownException {
        return relation.getWritableField(path);
    }

    @Override
    public List<Field> fields() {
        return relation.fields();
    }

    @Override
    public void addProjection(Projection projection) {
        throw new UnsupportedOperationException("addingProjection not supported");
    }

    @Override
    public boolean resultIsDistributed() {
        throw new UnsupportedOperationException("resultIsDistributed is not supported");
    }

    @Override
    public DQLPlanNode resultNode() {
        throw new UnsupportedOperationException("resultNode is not supported");
    }

}

<code block>


package io.crate.planner.node;

import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.CountNode;
import io.crate.planner.node.dql.FileUriCollectNode;
import io.crate.planner.node.dql.MergeNode;
import org.elasticsearch.common.io.stream.Streamable;

import java.util.List;
import java.util.Set;
import java.util.UUID;

public interface ExecutionNode extends Streamable {

    String DIRECT_RETURN_DOWNSTREAM_NODE = "_response";

    int NO_EXECUTION_NODE = Integer.MAX_VALUE;

    interface ExecutionNodeFactory<T extends ExecutionNode> {
        T create();
    }

    enum Type {
        COLLECT(CollectNode.FACTORY),
        COUNT(CountNode.FACTORY),
        FILE_URI_COLLECT(FileUriCollectNode.FACTORY),
        MERGE(MergeNode.FACTORY);

        private final ExecutionNodeFactory factory;

        Type(ExecutionNodeFactory factory) {
            this.factory = factory;
        }

        public ExecutionNodeFactory factory() {
            return factory;
        }
    }

    Type type();

    String name();

    int executionNodeId();

    Set<String> executionNodes();

    List<String> downstreamNodes();

    int downstreamExecutionNodeId();

    UUID jobId();

    void jobId(UUID jobId);


    <C, R> R accept(ExecutionNodeVisitor<C, R> visitor, C context);
}

<code block>


package io.crate.planner.node.dql;

import io.crate.planner.PlanAndPlannedAnalyzedRelation;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.PlanVisitor;
import io.crate.planner.projection.Projection;

import javax.annotation.Nullable;

public class DistributedGroupBy extends PlanAndPlannedAnalyzedRelation {

    private final CollectNode collectNode;
    private final MergeNode reducerMergeNode;
    private MergeNode localMergeNode;

    public DistributedGroupBy(CollectNode collectNode, MergeNode reducerMergeNode, @Nullable MergeNode localMergeNode) {
        this.collectNode = collectNode;
        this.reducerMergeNode = reducerMergeNode;
        this.localMergeNode = localMergeNode;
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitDistributedGroupBy(this, context);
    }

    public CollectNode collectNode() {
        return collectNode;
    }

    public MergeNode reducerMergeNode() {
        return reducerMergeNode;
    }

    public MergeNode localMergeNode() {
        return localMergeNode;
    }

    @Override
    public void addProjection(Projection projection) {
        DQLPlanNode node = resultNode();
        node.addProjection(projection);
        if (node instanceof CollectNode) {
            PlanNodeBuilder.setOutputTypes((CollectNode)node);
        } else if (node instanceof MergeNode) {
            PlanNodeBuilder.connectTypes(reducerMergeNode, node);
        }
    }

    @Override
    public boolean resultIsDistributed() {
        return localMergeNode == null;
    }

    @Override
    public DQLPlanNode resultNode() {
        return localMergeNode != null ? localMergeNode : reducerMergeNode;
    }
}

<code block>


package io.crate.planner.node.dql;

import io.crate.planner.PlanAndPlannedAnalyzedRelation;
import io.crate.planner.PlanVisitor;
import io.crate.planner.projection.Projection;

public class CountPlan extends PlanAndPlannedAnalyzedRelation{

    private final CountNode countNode;
    private final MergeNode mergeNode;

    public CountPlan(CountNode countNode, MergeNode mergeNode) {
        this.countNode = countNode;
        this.mergeNode = mergeNode;
    }

    public CountNode countNode() {
        return countNode;
    }

    public MergeNode mergeNode() {
        return mergeNode;
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitCountPlan(this, context);
    }

    @Override
    public void addProjection(Projection projection) {
        mergeNode.addProjection(projection);
    }

    @Override
    public boolean resultIsDistributed() {
        return false;
    }

    @Override
    public DQLPlanNode resultNode() {
        return mergeNode;
    }
}

<code block>


package io.crate.planner.node.dql;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.Sets;
import io.crate.analyze.WhereClause;
import io.crate.metadata.Routing;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.node.ExecutionNodeVisitor;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;

import java.io.IOException;
import java.util.List;
import java.util.Set;
import java.util.UUID;

public class CountNode implements ExecutionNode {

    public static final ExecutionNodeFactory<CountNode> FACTORY = new ExecutionNodeFactory<CountNode>() {
        @Override
        public CountNode create() {
            return new CountNode();
        }
    };
    private UUID jobId;
    private int executionNodeId;
    private Routing routing;
    private WhereClause whereClause;

    CountNode() {}

    public CountNode(int executionNodeId, Routing routing, WhereClause whereClause) {
        this.executionNodeId = executionNodeId;
        this.routing = routing;
        this.whereClause = whereClause;
    }

    @Override
    public Type type() {
        return Type.COUNT;
    }

    @Override
    public String name() {
        return "count";
    }

    @Override
    public UUID jobId() {
        return jobId;
    }

    @Override
    public void jobId(UUID jobId) {
        this.jobId = jobId;
    }

    public Routing routing() {
        return routing;
    }

    public WhereClause whereClause() {
        return whereClause;
    }

    @Override
    public int executionNodeId() {
        return executionNodeId;
    }

    @Override
    public Set<String> executionNodes() {
        if (routing.isNullRouting()) {
            return routing.nodes();
        } else {
            return Sets.filter(routing.nodes(), TableInfo.IS_NOT_NULL_NODE_ID);
        }
    }

    @Override
    public List<String> downstreamNodes() {
        return ImmutableList.of(ExecutionNode.DIRECT_RETURN_DOWNSTREAM_NODE);
    }

    @Override
    public int downstreamExecutionNodeId() {
        return ExecutionNode.NO_EXECUTION_NODE;
    }

    @Override
    public <C, R> R accept(ExecutionNodeVisitor<C, R> visitor, C context) {
        return visitor.visitCountNode(this, context);
    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        jobId = new UUID(in.readLong(), in.readLong());
        executionNodeId = in.readVInt();
        routing = new Routing();
        routing.readFrom(in);
        whereClause = new WhereClause(in);
    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        assert jobId != null : "jobId must not be null";
        out.writeLong(jobId.getMostSignificantBits());
        out.writeLong(jobId.getLeastSignificantBits());
        out.writeVInt(executionNodeId);
        routing.writeTo(out);
        whereClause.writeTo(out);
    }
}

<code block>


package io.crate.planner.node.dql;

import com.google.common.base.MoreObjects;
import com.google.common.base.Optional;
import com.google.common.collect.ImmutableList;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.projection.Projection;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;
import org.elasticsearch.common.io.stream.Streamable;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import java.util.UUID;

public abstract class AbstractDQLPlanNode implements DQLPlanNode, Streamable, ExecutionNode {

    private UUID jobId;
    private int executionNodeId;
    private String name;
    protected List<Projection> projections = ImmutableList.of();
    protected List<DataType> outputTypes = ImmutableList.of();
    private List<DataType> inputTypes;

    public AbstractDQLPlanNode() {

    }

    protected AbstractDQLPlanNode(int executionNodeId, String name) {
        this.executionNodeId = executionNodeId;
        this.name = name;
    }

    public String name() {
        return name;
    }

    @Override
    public UUID jobId() {
        return jobId;
    }

    @Override
    public void jobId(UUID jobId) {
        this.jobId = jobId;
    }

    @Override
    public int executionNodeId() {
        return executionNodeId;
    }

    public boolean hasProjections() {
        return projections != null && projections.size() > 0;
    }

    @Override
    public List<Projection> projections() {
        return projections;
    }

    public void projections(List<Projection> projections) {
        this.projections = projections;
    }

    @Override
    public void addProjection(Projection projection) {
        List<Projection> projections = new ArrayList<>(this.projections);
        projections.add(projection);
        this.projections = ImmutableList.copyOf(projections);
    }

    public Optional<Projection> finalProjection() {
        if (projections.size() == 0) {
            return Optional.absent();
        } else {
            return Optional.of(projections.get(projections.size()-1));
        }
    }


    public void outputTypes(List<DataType> outputTypes) {
        this.outputTypes = outputTypes;
    }

    public List<DataType> outputTypes() {
        return outputTypes;
    }

    @Override
    public void inputTypes(List<DataType> dataTypes) {
        this.inputTypes = dataTypes;
    }

    @Override
    public List<DataType> inputTypes() {
        return inputTypes;
    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        name = in.readString();
        jobId = new UUID(in.readLong(), in.readLong());
        executionNodeId = in.readVInt();

        int numCols = in.readVInt();
        if (numCols > 0) {
            outputTypes = new ArrayList<>(numCols);
            for (int i = 0; i < numCols; i++) {
                outputTypes.add(DataTypes.fromStream(in));
            }
        }

        int numProjections = in.readVInt();
        if (numProjections > 0) {
            projections = new ArrayList<>(numProjections);
            for (int i = 0; i < numProjections; i++) {
                projections.add(Projection.fromStream(in));
            }
        }

    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        out.writeString(name);
        assert jobId != null : "jobId must not be null";
        out.writeLong(jobId.getMostSignificantBits());
        out.writeLong(jobId.getLeastSignificantBits());
        out.writeVInt(executionNodeId);

        int numCols = outputTypes.size();
        out.writeVInt(numCols);
        for (int i = 0; i < numCols; i++) {
            DataTypes.toStream(outputTypes.get(i), out);
        }

        if (hasProjections()) {
            out.writeVInt(projections.size());
            for (Projection p : projections) {
                Projection.toStream(p, out);
            }
        } else {
            out.writeVInt(0);
        }
    }

    @Override
    public boolean equals(Object o) {
        if (this == o) return true;
        if (o == null || getClass() != o.getClass()) return false;

        AbstractDQLPlanNode node = (AbstractDQLPlanNode) o;

        return !(name != null ? !name.equals(node.name) : node.name != null);

    }

    @Override
    public int hashCode() {
        return name != null ? name.hashCode() : 0;
    }

    @Override
    public String toString() {
        return MoreObjects.toStringHelper(this)
                .add("name", name)
                .add("projections", projections)
                .add("outputTypes", outputTypes)
                .toString();
    }
}

<code block>


package io.crate.planner.node.dql;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableSet;
import com.google.common.collect.Sets;
import io.crate.analyze.EvaluatingNormalizer;
import io.crate.analyze.OrderBy;
import io.crate.analyze.WhereClause;
import io.crate.metadata.Routing;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.node.ExecutionNodeVisitor;
import io.crate.planner.node.PlanNodeVisitor;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.Symbol;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;

import javax.annotation.Nullable;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import java.util.Set;


public class CollectNode extends AbstractDQLPlanNode {

    public static final ExecutionNodeFactory<CollectNode> FACTORY = new ExecutionNodeFactory<CollectNode>() {
        @Override
        public CollectNode create() {
            return new CollectNode();
        }
    };
    private Routing routing;
    private List<Symbol> toCollect;
    private WhereClause whereClause = WhereClause.MATCH_ALL;
    private RowGranularity maxRowGranularity = RowGranularity.CLUSTER;

    @Nullable
    private List<String> downstreamNodes;

    private int downstreamExecutionNodeId = ExecutionNode.NO_EXECUTION_NODE;

    private boolean isPartitioned = false;
    private boolean keepContextForFetcher = false;
    private @Nullable String handlerSideCollect = null;

    private @Nullable Integer limit = null;
    private @Nullable OrderBy orderBy = null;

    protected CollectNode() {
        super();
    }

    public CollectNode(int executionNodeId, String name) {
        super(executionNodeId, name);
    }

    public CollectNode(int executionNodeId, String name, Routing routing) {
        this(executionNodeId, name, routing, ImmutableList.<Symbol>of(), ImmutableList.<Projection>of());
    }

    public CollectNode(int executionNodeId, String name, Routing routing, List<Symbol> toCollect, List<Projection> projections) {
        super(executionNodeId, name);
        this.routing = routing;
        this.toCollect = toCollect;
        this.projections = projections;
        this.downstreamNodes = ImmutableList.of(ExecutionNode.DIRECT_RETURN_DOWNSTREAM_NODE);
    }

    @Override
    public Type type() {
        return Type.COLLECT;
    }


    @Override
    public Set<String> executionNodes() {
        if (routing != null) {
            if (routing.isNullRouting()) {
                return routing.nodes();
            } else {
                return Sets.filter(routing.nodes(), TableInfo.IS_NOT_NULL_NODE_ID);
            }
        } else {
            return ImmutableSet.of();
        }
    }

    public @Nullable Integer limit() {
        return limit;
    }

    public void limit(Integer limit) {
        this.limit = limit;
    }

    public @Nullable OrderBy orderBy() {
        return orderBy;
    }

    public void orderBy(@Nullable OrderBy orderBy) {
        this.orderBy = orderBy;
    }

    @Nullable
    public List<String> downstreamNodes() {
        return downstreamNodes;
    }


    public boolean hasDistributingDownstreams() {
        if (downstreamNodes != null && downstreamNodes.size() > 0) {
            if (downstreamNodes.size() == 1
                    && downstreamNodes.get(0).equals(ExecutionNode.DIRECT_RETURN_DOWNSTREAM_NODE)) {
                return false;
            }
            return true;
        }
        return false;
    }

    public void downstreamNodes(List<String> downStreamNodes) {
        this.downstreamNodes = downStreamNodes;
    }

    public void downstreamExecutionNodeId(int executionNodeId) {
        this.downstreamExecutionNodeId = executionNodeId;
    }

    public int downstreamExecutionNodeId() {
        return downstreamExecutionNodeId;
    }

    public WhereClause whereClause() {
        return whereClause;
    }

    public void whereClause(WhereClause whereClause) {
        assert whereClause != null;
        this.whereClause = whereClause;
    }

    public Routing routing() {
        return routing;
    }

    public List<Symbol> toCollect() {
        return toCollect;
    }

    public void toCollect(List<Symbol> toCollect) {
        assert toCollect != null;
        this.toCollect = toCollect;
    }

    public boolean isRouted() {
        return routing != null && routing.hasLocations();
    }


    public boolean isPartitioned() {
        return isPartitioned;
    }

    public void isPartitioned(boolean isPartitioned) {
        this.isPartitioned = isPartitioned;
    }

    public RowGranularity maxRowGranularity() {
        return maxRowGranularity;
    }

    public void maxRowGranularity(RowGranularity newRowGranularity) {
        if (maxRowGranularity.compareTo(newRowGranularity) < 0) {
            maxRowGranularity = newRowGranularity;
        }
    }

    @Override
    public <C, R> R accept(PlanNodeVisitor<C, R> visitor, C context) {
        return visitor.visitCollectNode(this, context);
    }

    @Override
    public <C, R> R accept(ExecutionNodeVisitor<C, R> visitor, C context) {
        return visitor.visitCollectNode(this, context);
    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        super.readFrom(in);
        downstreamExecutionNodeId = in.readVInt();

        int numCols = in.readVInt();
        if (numCols > 0) {
            toCollect = new ArrayList<>(numCols);
            for (int i = 0; i < numCols; i++) {
                toCollect.add(Symbol.fromStream(in));
            }
        } else {
            toCollect = ImmutableList.of();
        }

        maxRowGranularity = RowGranularity.fromStream(in);

        if (in.readBoolean()) {
            routing = new Routing();
            routing.readFrom(in);
        }

        whereClause = new WhereClause(in);

        int numDownStreams = in.readVInt();
        downstreamNodes = new ArrayList<>(numDownStreams);
        for (int i = 0; i < numDownStreams; i++) {
            downstreamNodes.add(in.readString());
        }
        keepContextForFetcher = in.readBoolean();

        if( in.readBoolean()) {
            limit = in.readVInt();
        }

        if (in.readBoolean()) {
            orderBy = OrderBy.fromStream(in);
        }
        isPartitioned = in.readBoolean();
        handlerSideCollect = in.readOptionalString();
    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        super.writeTo(out);
        out.writeVInt(downstreamExecutionNodeId);

        int numCols = toCollect.size();
        out.writeVInt(numCols);
        for (int i = 0; i < numCols; i++) {
            Symbol.toStream(toCollect.get(i), out);
        }

        RowGranularity.toStream(maxRowGranularity, out);

        if (routing != null) {
            out.writeBoolean(true);
            routing.writeTo(out);
        } else {
            out.writeBoolean(false);
        }
        whereClause.writeTo(out);

        if (downstreamNodes != null) {
            out.writeVInt(downstreamNodes.size());
            for (String downstreamNode : downstreamNodes) {
                out.writeString(downstreamNode);
            }
        } else {
            out.writeVInt(0);
        }
        out.writeBoolean(keepContextForFetcher);
        if (limit != null ) {
            out.writeBoolean(true);
            out.writeVInt(limit);
        } else {
            out.writeBoolean(false);
        }
        if (orderBy != null) {
            out.writeBoolean(true);
            OrderBy.toStream(orderBy, out);
        } else {
            out.writeBoolean(false);
        }
        out.writeBoolean(isPartitioned);
        out.writeOptionalString(handlerSideCollect);
    }


    public CollectNode normalize(EvaluatingNormalizer normalizer) {
        assert whereClause() != null;
        CollectNode result = this;
        List<Symbol> newToCollect = normalizer.normalize(toCollect());
        boolean changed = newToCollect != toCollect();
        WhereClause newWhereClause = whereClause().normalize(normalizer);
        if (newWhereClause != whereClause()) {
            changed = changed || newWhereClause != whereClause();
        }
        if (changed) {
            result = new CollectNode(executionNodeId(), name(), routing, newToCollect, projections);
            result.downstreamNodes = downstreamNodes;
            result.maxRowGranularity = maxRowGranularity;
            result.jobId(jobId());
            result.keepContextForFetcher = keepContextForFetcher;
            result.handlerSideCollect = handlerSideCollect;
            result.isPartitioned(isPartitioned);
            result.whereClause(newWhereClause);
        }
        return result;
    }

    public void keepContextForFetcher(boolean keepContextForFetcher) {
        this.keepContextForFetcher = keepContextForFetcher;
    }

    public boolean keepContextForFetcher() {
        return keepContextForFetcher;
    }

    public void handlerSideCollect(String handlerSideCollect) {
        this.handlerSideCollect = handlerSideCollect;
    }

    @Nullable
    public String handlerSideCollect() {
        return handlerSideCollect;
    }
}
<code block>


package io.crate.planner.node.dql;

import com.google.common.base.MoreObjects;
import com.google.common.base.Preconditions;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableSet;
import io.crate.planner.node.ExecutionNodeVisitor;
import io.crate.planner.node.PlanNodeVisitor;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;

import javax.annotation.Nullable;
import java.io.IOException;
import java.util.*;


public class MergeNode extends AbstractDQLPlanNode {

    public static final ExecutionNodeFactory<MergeNode> FACTORY = new ExecutionNodeFactory<MergeNode>() {
        @Override
        public MergeNode create() {
            return new MergeNode();
        }
    };

    private List<DataType> inputTypes;
    private int numUpstreams;
    private Set<String> executionNodes;


    private boolean sortedInputOutput = false;
    private int[] orderByIndices;
    private boolean[] reverseFlags;
    private Boolean[] nullsFirst;
    private int downstreamExecutionNodeId = NO_EXECUTION_NODE;
    private List<String> downstreamNodes = ImmutableList.of();

    public MergeNode() {
        numUpstreams = 0;
    }

    public MergeNode(int executionNodeId, String name, int numUpstreams) {
        super(executionNodeId, name);
        this.numUpstreams = numUpstreams;
    }

    public static MergeNode sortedMergeNode(int executionNodeId,
                                            String name,
                                            int numUpstreams,
                                            int[] orderByIndices,
                                            boolean[] reverseFlags,
                                            Boolean[] nullsFirst) {
        Preconditions.checkArgument(
                orderByIndices.length == reverseFlags.length && reverseFlags.length == nullsFirst.length,
                "ordering parameters must be of the same length");
        MergeNode mergeNode = new MergeNode(executionNodeId, name, numUpstreams);
        mergeNode.sortedInputOutput = true;
        mergeNode.orderByIndices = orderByIndices;
        mergeNode.reverseFlags = reverseFlags;
        mergeNode.nullsFirst = nullsFirst;
        return mergeNode;
    }

    @Override
    public Type type() {
        return Type.MERGE;
    }

    @Override
    public Set<String> executionNodes() {
        if (executionNodes == null) {
            return ImmutableSet.of();
        } else {
            return executionNodes;
        }
    }

    @Override
    public List<String> downstreamNodes() {
        return downstreamNodes;
    }

    public void downstreamNodes(Set<String> nodes) {
        downstreamNodes = ImmutableList.copyOf(nodes);
    }

    @Override
    public int downstreamExecutionNodeId() {
        return downstreamExecutionNodeId;
    }

    public void executionNodes(Set<String> executionNodes) {
        this.executionNodes = executionNodes;
    }

    public int numUpstreams() {
        return numUpstreams;
    }

    public List<DataType> inputTypes() {
        return inputTypes;
    }

    public void inputTypes(List<DataType> inputTypes) {
        this.inputTypes = inputTypes;
    }

    public boolean sortedInputOutput() {
        return sortedInputOutput;
    }

    @Nullable
    public int[] orderByIndices() {
        return orderByIndices;
    }

    @Nullable
    public boolean[] reverseFlags() {
        return reverseFlags;
    }

    @Nullable
    public Boolean[] nullsFirst() {
        return nullsFirst;
    }

    @Override
    public <C, R> R accept(PlanNodeVisitor<C, R> visitor, C context) {
        return visitor.visitMergeNode(this, context);
    }

    @Override
    public <C, R> R accept(ExecutionNodeVisitor<C, R> visitor, C context) {
        return visitor.visitMergeNode(this, context);
    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        super.readFrom(in);
        downstreamExecutionNodeId = in.readVInt();

        int numDownstreamNodes = in.readVInt();
        downstreamNodes = new ArrayList<>(numDownstreamNodes);
        for (int i = 0; i < numDownstreamNodes; i++) {
            downstreamNodes.add(in.readString());
        }

        numUpstreams = in.readVInt();

        int numCols = in.readVInt();
        if (numCols > 0) {
            inputTypes = new ArrayList<>(numCols);
            for (int i = 0; i < numCols; i++) {
                inputTypes.add(DataTypes.fromStream(in));
            }
        }
        int numExecutionNodes = in.readVInt();

        if (numExecutionNodes > 0) {
            executionNodes = new HashSet<>(numExecutionNodes);
            for (int i = 0; i < numExecutionNodes; i++) {
                executionNodes.add(in.readString());
            }
        }

        sortedInputOutput = in.readBoolean();
        if (sortedInputOutput) {
            int orderByIndicesLength = in.readVInt();
            orderByIndices = new int[orderByIndicesLength];
            reverseFlags = new boolean[orderByIndicesLength];
            nullsFirst = new Boolean[orderByIndicesLength];
            for (int i = 0; i < orderByIndicesLength; i++) {
                orderByIndices[i] = in.readVInt();
                reverseFlags[i] = in.readBoolean();
                nullsFirst[i] = in.readOptionalBoolean();
            }
        }
    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        super.writeTo(out);
        out.writeVInt(downstreamExecutionNodeId);

        out.writeVInt(downstreamNodes.size());
        for (String downstreamNode : downstreamNodes) {
            out.writeString(downstreamNode);
        }

        out.writeVInt(numUpstreams);

        int numCols = inputTypes.size();
        out.writeVInt(numCols);
        for (DataType inputType : inputTypes) {
            DataTypes.toStream(inputType, out);
        }

        if (executionNodes == null) {
            out.writeVInt(0);
        } else {
            out.writeVInt(executionNodes.size());
            for (String node : executionNodes) {
                out.writeString(node);
            }
        }

        out.writeBoolean(sortedInputOutput);
        if (sortedInputOutput) {
            out.writeVInt(orderByIndices.length);
            for (int i = 0; i < orderByIndices.length; i++) {
                out.writeVInt(orderByIndices[i]);
                out.writeBoolean(reverseFlags[i]);
                out.writeOptionalBoolean(nullsFirst[i]);
            }
        }
    }

    @Override
    public String toString() {
        MoreObjects.ToStringHelper helper = MoreObjects.toStringHelper(this)
                .add("executionNodeId", executionNodeId())
                .add("name", name())
                .add("projections", projections)
                .add("outputTypes", outputTypes)
                .add("jobId", jobId())
                .add("numUpstreams", numUpstreams)
                .add("executionNodes", executionNodes)
                .add("inputTypes", inputTypes)
                .add("sortedInputOutput", sortedInputOutput);
        if (sortedInputOutput) {
            helper.add("orderByIndices", Arrays.toString(orderByIndices))
                  .add("reverseFlags", Arrays.toString(reverseFlags))
                  .add("nullsFirst", Arrays.toString(nullsFirst));
        }
        return helper.toString();
    }

    public void downstreamExecutionNodeId(int downstreamExecutionNodeId) {
        this.downstreamExecutionNodeId = downstreamExecutionNodeId;
    }
}

<code block>
package io.crate.planner.node.dql;


import io.crate.planner.PlanAndPlannedAnalyzedRelation;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.PlanVisitor;
import io.crate.planner.projection.Projection;

import javax.annotation.Nullable;

public class QueryAndFetch extends PlanAndPlannedAnalyzedRelation {

    private final CollectNode collectNode;
    private MergeNode localMergeNode;

    public QueryAndFetch(CollectNode collectNode, @Nullable MergeNode localMergeNode){
        this.collectNode = collectNode;
        this.localMergeNode = localMergeNode;
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitQueryAndFetch(this, context);
    }

    public CollectNode collectNode() {
        return collectNode;
    }

    public MergeNode localMergeNode(){
        return localMergeNode;
    }

    @Override
    public void addProjection(Projection projection) {
        DQLPlanNode node = resultNode();
        node.addProjection(projection);
        if (node instanceof CollectNode) {
            PlanNodeBuilder.setOutputTypes((CollectNode)node);
        } else if (node instanceof MergeNode) {
            PlanNodeBuilder.connectTypes(collectNode, node);
        }
    }

    @Override
    public boolean resultIsDistributed() {
        return localMergeNode == null;
    }

    @Override
    public DQLPlanNode resultNode() {
        return localMergeNode == null ? collectNode : localMergeNode;
    }
}

<code block>


package io.crate.planner.node.dql;

import com.google.common.base.MoreObjects;
import com.google.common.collect.ImmutableList;
import io.crate.analyze.OrderBy;
import io.crate.analyze.QuerySpec;
import io.crate.analyze.where.DocKeys;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.node.PlanNodeVisitor;
import io.crate.planner.symbol.Symbol;
import io.crate.planner.symbol.Symbols;
import org.elasticsearch.common.Nullable;

import java.util.List;


public class ESGetNode extends ESDQLPlanNode implements DQLPlanNode {

    private final TableInfo tableInfo;
    private final QuerySpec querySpec;
    private final List<Symbol> sortSymbols;
    private final boolean[] reverseFlags;
    private final Boolean[] nullsFirst;
    private final int executionNodeId;

    private final static boolean[] EMPTY_REVERSE_FLAGS = new boolean[0];
    private final static Boolean[] EMPTY_NULLS_FIRST = new Boolean[0];
    private final DocKeys docKeys;

    public ESGetNode(int executionNodeId,
                     TableInfo tableInfo,
                     QuerySpec querySpec) {

        assert querySpec.where().docKeys().isPresent();
        this.tableInfo = tableInfo;
        this.querySpec = querySpec;
        this.outputs = querySpec.outputs();
        this.docKeys = querySpec.where().docKeys().get();
        this.executionNodeId = executionNodeId;


        outputTypes(Symbols.extractTypes(outputs));

        OrderBy orderBy = querySpec.orderBy();
        if (orderBy != null && orderBy.isSorted()){
            this.sortSymbols = orderBy.orderBySymbols();
            this.reverseFlags = orderBy.reverseFlags();
            this.nullsFirst = orderBy.nullsFirst();
        } else {
            this.sortSymbols = ImmutableList.<Symbol>of();
            this.reverseFlags = EMPTY_REVERSE_FLAGS;
            this.nullsFirst = EMPTY_NULLS_FIRST;
        }
    }

    @Override
    public <C, R> R accept(PlanNodeVisitor<C, R> visitor, C context) {
        return visitor.visitESGetNode(this, context);
    }

    public TableInfo tableInfo() {
        return tableInfo;
    }

    public QuerySpec querySpec() {
        return querySpec;
    }

    public DocKeys docKeys() {
        return docKeys;
    }

    @Nullable
    public Integer limit() {
        return querySpec().limit();
    }

    public int offset() {
        return querySpec().offset();
    }

    public List<Symbol> sortSymbols() {
        return sortSymbols;
    }

    public boolean[] reverseFlags() {
        return reverseFlags;
    }

    public Boolean[] nullsFirst() {
        return nullsFirst;
    }

    public int executionNodeId() {
        return executionNodeId;
    }

    @Override
    public String toString() {
        return MoreObjects.toStringHelper(this)
                .add("docKeys", docKeys)
                .add("outputs", outputs)
                .toString();
    }
}

<code block>

package io.crate.planner.node.dql;


import io.crate.planner.PlanAndPlannedAnalyzedRelation;
import io.crate.planner.PlanVisitor;
import io.crate.planner.projection.Projection;

public class NonDistributedGroupBy extends PlanAndPlannedAnalyzedRelation {

    private final CollectNode collectNode;
    private MergeNode localMergeNode;

    public NonDistributedGroupBy(CollectNode collectNode, MergeNode localMergeNode){
        this.collectNode = collectNode;
        this.localMergeNode = localMergeNode;
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitNonDistributedGroupBy(this, context);
    }

    public MergeNode localMergeNode() {
        return localMergeNode;
    }

    public CollectNode collectNode() {
        return collectNode;
    }

    @Override
    public void addProjection(Projection projection) {
        this.resultNode().addProjection(projection);
    }

    @Override
    public boolean resultIsDistributed() {
        return localMergeNode == null;
    }

    @Override
    public DQLPlanNode resultNode() {
        return localMergeNode == null ? collectNode : localMergeNode;
    }
}

<code block>


package io.crate.planner.node.dql;

import com.google.common.base.Optional;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableSet;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.exceptions.ColumnUnknownException;
import io.crate.metadata.Path;
import io.crate.planner.IterablePlan;
import io.crate.planner.Plan;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.Field;
import io.crate.planner.symbol.Symbol;
import io.crate.planner.symbol.ValueSymbolVisitor;
import io.crate.types.DataType;

import javax.annotation.Nullable;
import java.util.List;
import java.util.Set;

public abstract class ESDQLPlanNode implements DQLPlanNode, PlannedAnalyzedRelation {

    private static final char COMMA = ',';

    protected List<Symbol> outputs;
    private List<DataType> inputTypes;
    private List<DataType> outputTypes;

    public List<Symbol> outputs() {
        return outputs;
    }

    @Nullable
    public static String noCommaStringRouting(Optional<Set<Symbol>> clusteredBy) {
        if (clusteredBy.isPresent()){
            StringBuilder sb = new StringBuilder();
            boolean first = true;
            for (Symbol symbol : clusteredBy.get()) {
                String s = ValueSymbolVisitor.STRING.process(symbol);
                if (s.indexOf(COMMA)>-1){
                    return null;
                }
                if (!first){
                    sb.append(COMMA);
                } else {
                    first = false;
                }
                sb.append(s);
            }
            return sb.toString();
        }
        return null;
    }

    @Override
    public boolean hasProjections() {
        return false;
    }

    @Override
    public List<Projection> projections() {
        return ImmutableList.of();
    }

    @Override
    public void inputTypes(List<DataType> dataTypes) {
        this.inputTypes = dataTypes;
    }

    @Override
    public List<DataType> inputTypes() {
        return inputTypes;
    }

    @Override
    public Set<String> executionNodes() {
        return ImmutableSet.of();
    }

    @Override
    public void outputTypes(List<DataType> outputTypes) {
        this.outputTypes = outputTypes;
    }

    @Override
    public List<DataType> outputTypes() {
        return outputTypes;
    }

    @Override
    public <C, R> R accept(AnalyzedRelationVisitor<C, R> visitor, C context) {
        return visitor.visitPlanedAnalyzedRelation(this, context);
    }

    @javax.annotation.Nullable
    @Override
    public Field getField(Path path) {
        throw new UnsupportedOperationException("getField is not supported on ESDQLPlanNode");
    }

    @Override
    public Field getWritableField(Path path) throws UnsupportedOperationException, ColumnUnknownException {
        throw new UnsupportedOperationException("getWritableField is not supported on ESDQLPlanNode");
    }

    @Override
    public List<Field> fields() {
        throw new UnsupportedOperationException("fields is not supported on ESDQLPlanNode");
    }

    @Override
    public void addProjection(Projection projection) {
        throw new UnsupportedOperationException("addProjection not supported on ESDQLPlanNode");
    }

    @Override
    public boolean resultIsDistributed() {
        throw new UnsupportedOperationException("resultIsDistributed is not supported on ESDQLPlanNode");
    }

    @Override
    public DQLPlanNode resultNode() {
        throw new UnsupportedOperationException("resultNode is not supported on ESDQLPLanNode");
    }

    @Override
    public Plan plan() {
        return new IterablePlan(this);
    }
}

<code block>


package io.crate.planner.node.dql;

import io.crate.planner.PlanAndPlannedAnalyzedRelation;
import io.crate.planner.PlanVisitor;
import io.crate.planner.projection.Projection;

public class GlobalAggregate extends PlanAndPlannedAnalyzedRelation {

    private final CollectNode collectNode;
    private MergeNode mergeNode;

    public GlobalAggregate(CollectNode collectNode, MergeNode mergeNode) {
        this.collectNode = collectNode;
        this.mergeNode = mergeNode;
    }

    public CollectNode collectNode() {
        return collectNode;
    }

    public MergeNode mergeNode() {
        return mergeNode;
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitGlobalAggregate(this, context);
    }

    @Override
    public void addProjection(Projection projection) {
        mergeNode.projections().add(projection);
    }

    @Override
    public boolean resultIsDistributed() {
        return mergeNode == null;
    }

    @Override
    public DQLPlanNode resultNode() {
        return mergeNode == null ? collectNode : mergeNode;
    }
}

<code block>


package io.crate.planner.node.dql;

import io.crate.planner.Plan;
import io.crate.planner.PlanVisitor;
import io.crate.planner.node.PlanNode;

import java.util.Arrays;
import java.util.Iterator;

public class CollectAndMerge implements Iterable<PlanNode>, Plan {

    private final CollectNode collectNode;
    private final MergeNode localMergeNode;
    private Iterable<PlanNode> nodes;

    public CollectAndMerge(CollectNode collectNode, MergeNode localMergeNode) {
        this.collectNode = collectNode;
        this.localMergeNode = localMergeNode;
        nodes = Arrays.<PlanNode>asList(collectNode, localMergeNode);
    }

    public CollectNode collectNode() {
        return collectNode;
    }

    public MergeNode localMergeNode() {
        return localMergeNode;
    }

    @Override
    public Iterator<PlanNode> iterator() {
        return nodes.iterator();
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitCollectAndMerge(this, context);
    }
}

<code block>


package io.crate.planner.node.dql;

import com.google.common.base.MoreObjects;
import io.crate.analyze.EvaluatingNormalizer;
import io.crate.analyze.WhereClause;
import io.crate.metadata.Routing;
import io.crate.operation.collect.files.FileReadingCollector;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.Symbol;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;

import javax.annotation.Nullable;
import java.io.IOException;
import java.util.List;

public class FileUriCollectNode extends CollectNode {

    public static final ExecutionNodeFactory<FileUriCollectNode> FACTORY = new ExecutionNodeFactory<FileUriCollectNode>() {
        @Override
        public FileUriCollectNode create() {
            return new FileUriCollectNode();
        }
    };
    private Symbol targetUri;
    private String compression;
    private Boolean sharedStorage;

    private FileUriCollectNode() {
        super();
    }

    public FileUriCollectNode(int executionNodeId,
                              String name,
                              Routing routing,
                              Symbol targetUri,
                              List<Symbol> toCollect,
                              List<Projection> projections,
                              String compression,
                              Boolean sharedStorage) {
        super(executionNodeId, name, routing, toCollect, projections);
        this.targetUri = targetUri;
        this.compression = compression;
        this.sharedStorage = sharedStorage;
    }

    public Symbol targetUri() {
        return targetUri;
    }

    public FileReadingCollector.FileFormat fileFormat() {
        return FileReadingCollector.FileFormat.JSON;
    }

    @Override
    public Type type() {
        return Type.FILE_URI_COLLECT;
    }

    @Override
    public FileUriCollectNode normalize(EvaluatingNormalizer normalizer) {
        List<Symbol> normalizedToCollect = normalizer.normalize(toCollect());
        Symbol normalizedTargetUri = normalizer.normalize(targetUri);
        WhereClause normalizedWhereClause = whereClause().normalize(normalizer);
        boolean changed =
                (normalizedToCollect != toCollect() )
                        || (normalizedTargetUri != targetUri)
                        || (normalizedWhereClause != whereClause());
        if (!changed) {
            return this;
        }
        FileUriCollectNode result = new FileUriCollectNode(
                executionNodeId(),
                name(),
                routing(),
                normalizedTargetUri,
                normalizedToCollect,
                projections(),
                compression(),
                sharedStorage());
        result.downstreamNodes(downstreamNodes());
        result.maxRowGranularity(maxRowGranularity());
        result.jobId(jobId());
        result.isPartitioned(isPartitioned());
        result.whereClause(normalizedWhereClause);
        return result;
    }

    @Nullable
    public String compression() {
        return compression;
    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        super.readFrom(in);
        compression = in.readOptionalString();
        sharedStorage = in.readOptionalBoolean();
        targetUri = Symbol.fromStream(in);
    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        super.writeTo(out);
        out.writeOptionalString(compression);
        out.writeOptionalBoolean(sharedStorage);
        Symbol.toStream(targetUri, out);
    }

    @Override
    public String toString() {
        return MoreObjects.toStringHelper(this)
                .add("name", name())
                .add("targetUri", targetUri)
                .add("projections", projections)
                .add("outputTypes", outputTypes)
                .add("compression", compression)
                .add("sharedStorageDefault", sharedStorage)
                .toString();
    }

    @Nullable
    public Boolean sharedStorage() {
        return sharedStorage;
    }
}


<code block>


package io.crate.planner.node.dql;

import io.crate.planner.PlanAndPlannedAnalyzedRelation;
import io.crate.planner.PlanVisitor;
import io.crate.planner.projection.Projection;

public class QueryThenFetch extends PlanAndPlannedAnalyzedRelation {

    private final CollectNode collectNode;
    private MergeNode mergeNode;

    public QueryThenFetch(CollectNode collectNode, MergeNode mergeNode) {
        this.collectNode = collectNode;
        this.mergeNode = mergeNode;
    }

    public CollectNode collectNode() {
        return collectNode;
    }

    public MergeNode mergeNode() {
        return mergeNode;
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitQueryThenFetch(this, context);
    }

    @Override
    public void addProjection(Projection projection) {
        mergeNode.projections().add(projection);
    }

    @Override
    public boolean resultIsDistributed() {
        return mergeNode == null;
    }

    @Override
    public DQLPlanNode resultNode() {
        return mergeNode == null ? collectNode : mergeNode;
    }
}

<code block>


package io.crate.planner.node.dml;


import com.google.common.base.Optional;
import io.crate.planner.Plan;
import io.crate.planner.PlanAndPlannedAnalyzedRelation;
import io.crate.planner.PlanVisitor;
import io.crate.planner.node.dql.DQLPlanNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.Projection;

import javax.annotation.Nullable;

public class InsertFromSubQuery extends PlanAndPlannedAnalyzedRelation {


    private final Optional<MergeNode> handlerMergeNode;

    private final Plan innerPlan;

    public InsertFromSubQuery(Plan innerPlan, @Nullable MergeNode handlerMergeNode) {
        this.innerPlan = innerPlan;
        this.handlerMergeNode = Optional.fromNullable(handlerMergeNode);
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitInsertByQuery(this, context);
    }

    public Plan innerPlan() {
        return innerPlan;
    }

    public Optional<MergeNode> handlerMergeNode() {
        return handlerMergeNode;
    }

    @Override
    public void addProjection(Projection projection) {
        throw new UnsupportedOperationException("addingProjection not supported");
    }

    @Override
    public boolean resultIsDistributed() {
        throw new UnsupportedOperationException("resultIsDistributed is not supported");
    }

    @Override
    public DQLPlanNode resultNode() {
        throw new UnsupportedOperationException("resultNode is not supported");
    }
}

<code block>


package io.crate.planner.node.dml;

import io.crate.planner.Plan;
import io.crate.planner.PlanAndPlannedAnalyzedRelation;
import io.crate.planner.PlanVisitor;
import io.crate.planner.node.dql.DQLPlanNode;
import io.crate.planner.projection.Projection;

import java.util.List;

public class Upsert extends PlanAndPlannedAnalyzedRelation {

    private final List<Plan> nodes;

    public Upsert(List<Plan> nodes) {
        this.nodes = nodes;
    }

    public List<Plan> nodes() {
        return nodes;
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitUpsert(this, context);
    }

    @Override
    public void addProjection(Projection projection) {
        throw new UnsupportedOperationException("adding projection not supported");
    }

    @Override
    public boolean resultIsDistributed() {
        throw new UnsupportedOperationException("resultIsDistributed is not supported");
    }

    @Override
    public DQLPlanNode resultNode() {
        throw new UnsupportedOperationException("resultNode is not supported");
    }

}

<code block>


package io.crate.planner.node.management;

import io.crate.planner.Plan;
import io.crate.planner.PlanVisitor;

public class KillPlan implements Plan {

    public static final KillPlan INSTANCE = new KillPlan();

    private KillPlan() {
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitKillPlan(this, context);
    }
}

<code block>


package io.crate.integrationtests;

import com.carrotsearch.hppc.LongArrayList;
import com.google.common.base.Predicates;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.Iterables;
import com.google.common.util.concurrent.FutureCallback;
import com.google.common.util.concurrent.Futures;
import com.google.common.util.concurrent.ListenableFuture;
import io.crate.action.job.ContextPreparer;
import io.crate.analyze.Analysis;
import io.crate.analyze.Analyzer;
import io.crate.analyze.ParameterContext;
import io.crate.analyze.WhereClause;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.core.collections.Bucket;
import io.crate.core.collections.Row;
import io.crate.executor.Job;
import io.crate.executor.TaskResult;
import io.crate.executor.transport.NodeFetchRequest;
import io.crate.executor.transport.NodeFetchResponse;
import io.crate.executor.transport.TransportExecutor;
import io.crate.executor.transport.TransportFetchNodeAction;
import io.crate.jobs.JobContextService;
import io.crate.jobs.JobExecutionContext;
import io.crate.metadata.ColumnIdent;
import io.crate.metadata.Functions;
import io.crate.metadata.ReferenceInfo;
import io.crate.metadata.doc.DocSchemaInfo;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.fetch.RowInputSymbolVisitor;
import io.crate.planner.Plan;
import io.crate.planner.Planner;
import io.crate.planner.RowGranularity;
import io.crate.planner.consumer.ConsumerContext;
import io.crate.planner.consumer.QueryThenFetchConsumer;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.dql.QueryThenFetch;
import io.crate.planner.projection.FetchProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.sql.parser.SqlParser;
import io.crate.types.DataType;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.action.ActionListener;
import org.elasticsearch.test.ElasticsearchIntegrationTest;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;

import java.util.*;
import java.util.concurrent.CountDownLatch;

import static org.hamcrest.Matchers.*;
import static org.hamcrest.core.Is.is;

@ElasticsearchIntegrationTest.ClusterScope(numDataNodes = 2, numClientNodes = 0)
public class FetchOperationIntegrationTest extends SQLTransportIntegrationTest {

    Setup setup = new Setup(sqlExecutor);
    TransportExecutor executor;
    DocSchemaInfo docSchemaInfo;

    @Before
    public void transportSetUp() {
        executor = internalCluster().getInstance(TransportExecutor.class);
        docSchemaInfo = internalCluster().getInstance(DocSchemaInfo.class);
    }

    @After
    public void transportTearDown() {
        executor = null;
        docSchemaInfo = null;
    }

    private void setUpCharacters() {
        sqlExecutor.exec("create table characters (id int primary key, name string) " +
                "clustered into 2 shards with(number_of_replicas=0)");
        sqlExecutor.ensureYellowOrGreen();
        sqlExecutor.exec("insert into characters (id, name) values (?, ?)",
                new Object[][]{
                        new Object[]{1, "Arthur"},
                        new Object[]{2, "Ford"},
                }
        );
        sqlExecutor.refresh("characters");
    }

    private Plan analyzeAndPlan(String stmt) {
        Analysis analysis = analyze(stmt);
        Planner planner = internalCluster().getInstance(Planner.class);
        return planner.plan(analysis);
    }

    private Analysis analyze(String stmt) {
        Analyzer analyzer = internalCluster().getInstance(Analyzer.class);
        return analyzer.analyze(
                SqlParser.createStatement(stmt),
                new ParameterContext(new Object[0], new Object[0][], null)
        );
    }

    private CollectNode createCollectNode(Planner.Context plannerContext, boolean keepContextForFetcher) {
        TableInfo tableInfo = docSchemaInfo.getTableInfo("characters");

        ReferenceInfo docIdRefInfo = tableInfo.getReferenceInfo(new ColumnIdent("_docid"));
        Symbol docIdRef = new Reference(docIdRefInfo);
        List<Symbol> toCollect = ImmutableList.of(docIdRef);

        List<DataType> outputTypes = new ArrayList<>(toCollect.size());
        for (Symbol symbol : toCollect) {
            outputTypes.add(symbol.valueType());
        }
        CollectNode collectNode = new CollectNode(
                plannerContext.nextExecutionNodeId(),
                "collect",
                tableInfo.getRouting(WhereClause.MATCH_ALL, null));
        collectNode.toCollect(toCollect);
        collectNode.outputTypes(outputTypes);
        collectNode.maxRowGranularity(RowGranularity.DOC);
        collectNode.keepContextForFetcher(keepContextForFetcher);
        collectNode.jobId(UUID.randomUUID());
        plannerContext.allocateJobSearchContextIds(collectNode.routing());

        return collectNode;
    }

    private List<Bucket> getBuckets(CollectNode collectNode) throws InterruptedException, java.util.concurrent.ExecutionException {
        List<Bucket> results = new ArrayList<>();
        for (String nodeName : internalCluster().getNodeNames()) {
            ContextPreparer contextPreparer = internalCluster().getInstance(ContextPreparer.class, nodeName);
            JobContextService contextService = internalCluster().getInstance(JobContextService.class, nodeName);

            JobExecutionContext.Builder builder = contextService.newBuilder(collectNode.jobId());
            ListenableFuture<Bucket> future = contextPreparer.prepare(collectNode.jobId(), collectNode, builder);
            assert future != null;

            JobExecutionContext context = contextService.createContext(builder);
            context.start();
            results.add(future.get());
        }
        return results;
    }

    @Test
    public void testCollectDocId() throws Exception {
        setUpCharacters();
        Planner.Context plannerContext = new Planner.Context(clusterService());
        CollectNode collectNode = createCollectNode(plannerContext, false);

        List<Bucket> results = getBuckets(collectNode);

        assertThat(results.size(), is(2));
        int seenJobSearchContextId = -1;
        for (Bucket rows : results) {
            assertThat(rows.size(), is(1));
            Object docIdCol = rows.iterator().next().get(0);
            assertNotNull(docIdCol);
            assertThat(docIdCol, instanceOf(Long.class));
            long docId = (long)docIdCol;

            int jobSearchContextId = (int)(docId >> 32);
            int doc = (int)docId;
            assertThat(doc, is(0));
            assertThat(jobSearchContextId, greaterThan(-1));
            if (seenJobSearchContextId == -1) {
                assertThat(jobSearchContextId, anyOf(is(0), is(1)));
                seenJobSearchContextId = jobSearchContextId;
            } else {
                assertThat(jobSearchContextId, is(seenJobSearchContextId == 0 ? 1 : 0));
            }
        }
    }

    @Test
    public void testFetchAction() throws Exception {
        setUpCharacters();

        Analysis analysis = analyze("select id, name from characters");
        QueryThenFetchConsumer queryThenFetchConsumer = internalCluster().getInstance(QueryThenFetchConsumer.class);
        Planner.Context plannerContext = new Planner.Context(clusterService());
        ConsumerContext consumerContext = new ConsumerContext(analysis.rootRelation(), plannerContext);
        queryThenFetchConsumer.consume(analysis.rootRelation(), consumerContext);

        QueryThenFetch plan = ((QueryThenFetch) ((PlannedAnalyzedRelation) consumerContext.rootRelation()).plan());
        UUID jobId = UUID.randomUUID();
        plan.collectNode().jobId(jobId);

        List<Bucket> results = getBuckets(plan.collectNode());


        TransportFetchNodeAction transportFetchNodeAction = internalCluster().getInstance(TransportFetchNodeAction.class);


        Map<String, LongArrayList> jobSearchContextDocIds = new HashMap<>();
        for (Bucket rows : results) {
            long docId = (long)rows.iterator().next().get(0);

            int jobSearchContextId = (int)(docId >> 32);
            String nodeId = plannerContext.nodeId(jobSearchContextId);
            LongArrayList docIdsPerNode = jobSearchContextDocIds.get(nodeId);
            if (docIdsPerNode == null) {
                docIdsPerNode = new LongArrayList();
                jobSearchContextDocIds.put(nodeId, docIdsPerNode);
            }
            docIdsPerNode.add(docId);
        }

        Iterable<Projection> projections = Iterables.filter(plan.mergeNode().projections(), Predicates.instanceOf(FetchProjection.class));
        FetchProjection fetchProjection = (FetchProjection )Iterables.getOnlyElement(projections);
        RowInputSymbolVisitor rowInputSymbolVisitor = new RowInputSymbolVisitor(internalCluster().getInstance(Functions.class));
        RowInputSymbolVisitor.Context context = rowInputSymbolVisitor.extractImplementations(fetchProjection.outputSymbols());

        final CountDownLatch latch = new CountDownLatch(jobSearchContextDocIds.size());
        final List<Row> rows = new ArrayList<>();
        for (Map.Entry<String, LongArrayList> nodeEntry : jobSearchContextDocIds.entrySet()) {
            NodeFetchRequest nodeFetchRequest = new NodeFetchRequest();
            nodeFetchRequest.jobId(plan.collectNode().jobId());
            nodeFetchRequest.executionNodeId(plan.collectNode().executionNodeId());
            nodeFetchRequest.toFetchReferences(context.references());
            nodeFetchRequest.closeContext(true);
            nodeFetchRequest.jobSearchContextDocIds(nodeEntry.getValue());

            transportFetchNodeAction.execute(nodeEntry.getKey(), nodeFetchRequest, new ActionListener<NodeFetchResponse>() {
                @Override
                public void onResponse(NodeFetchResponse nodeFetchResponse) {
                    for (Row row : nodeFetchResponse.rows()) {
                        rows.add(row);
                    }
                    latch.countDown();
                }

                @Override
                public void onFailure(Throwable e) {
                    latch.countDown();
                    fail(e.getMessage());
                }
            });
        }
        latch.await();

        assertThat(rows.size(), is(2));
        for (Row row : rows) {
            assertThat((Integer) row.get(0), anyOf(is(1), is(2)));
            assertThat((BytesRef) row.get(1), anyOf(is(new BytesRef("Arthur")), is(new BytesRef("Ford"))));
        }
    }

    @Test
    public void testFetchProjection() throws Exception {
        setUpCharacters();

        Plan plan = analyzeAndPlan("select id, name, substr(name, 2) from characters order by id");
        assertThat(plan, instanceOf(QueryThenFetch.class));
        QueryThenFetch qtf = (QueryThenFetch) plan;

        assertThat(qtf.collectNode().keepContextForFetcher(), is(true));
        assertThat(((FetchProjection) qtf.mergeNode().projections().get(1)).jobSearchContextIdToNode(), notNullValue());
        assertThat(((FetchProjection) qtf.mergeNode().projections().get(1)).jobSearchContextIdToShard(), notNullValue());

        Job job = executor.newJob(plan);
        ListenableFuture<List<TaskResult>> results = Futures.allAsList(executor.execute(job));

        final List<Object[]> resultingRows = new ArrayList<>();
        final CountDownLatch latch = new CountDownLatch(1);
        Futures.addCallback(results, new FutureCallback<List<TaskResult>>() {
            @Override
            public void onSuccess(List<TaskResult> resultList) {
                for (Row row : resultList.get(0).rows()) {
                    resultingRows.add(row.materialize());
                }
                latch.countDown();
            }

            @Override
            public void onFailure(Throwable t) {
                latch.countDown();
                fail(t.getMessage());
            }
        });

        latch.await();
        assertThat(resultingRows.size(), is(2));
        assertThat(resultingRows.get(0).length, is(3));
        assertThat((Integer) resultingRows.get(0)[0], is(1));
        assertThat((BytesRef) resultingRows.get(0)[1], is(new BytesRef("Arthur")));
        assertThat((BytesRef) resultingRows.get(0)[2], is(new BytesRef("rthur")));
        assertThat((Integer) resultingRows.get(1)[0], is(2));
        assertThat((BytesRef) resultingRows.get(1)[1], is(new BytesRef("Ford")));
        assertThat((BytesRef) resultingRows.get(1)[2], is(new BytesRef("ord")));
    }

    @Test
    public void testFetchProjectionWithBulkSize() throws Exception {

        setup.setUpLocations();
        sqlExecutor.refresh("locations");
        int bulkSize = 2;

        Plan plan = analyzeAndPlan("select position, name from locations order by position");
        assertThat(plan, instanceOf(QueryThenFetch.class));

        rewriteFetchProjectionToBulkSize(bulkSize, ((QueryThenFetch) plan).mergeNode());

        Job job = executor.newJob(plan);
        ListenableFuture<List<TaskResult>> results = Futures.allAsList(executor.execute(job));

        final List<Object[]> resultingRows = new ArrayList<>();
        final CountDownLatch latch = new CountDownLatch(1);
        Futures.addCallback(results, new FutureCallback<List<TaskResult>>() {
            @Override
            public void onSuccess(List<TaskResult> resultList) {
                for (Row row : resultList.get(0).rows()) {
                    resultingRows.add(row.materialize());
                }
                latch.countDown();
            }

            @Override
            public void onFailure(Throwable t) {
                latch.countDown();
                fail(t.getMessage());
            }
        });
        latch.await();

        assertThat(resultingRows.size(), is(13));
        assertThat(resultingRows.get(0).length, is(2));
        assertThat((Integer) resultingRows.get(0)[0], is(1));
        assertThat((Integer) resultingRows.get(12)[0], is(6));
    }

    private void rewriteFetchProjectionToBulkSize(int bulkSize, MergeNode mergeNode) {
        List<Projection> newProjections = new ArrayList<>(mergeNode.projections().size());
        for (Projection projection : mergeNode.projections()) {
            if (projection instanceof FetchProjection) {
                FetchProjection fetchProjection = (FetchProjection) projection;
                newProjections.add(new FetchProjection(
                        fetchProjection.executionNodeId(),
                        fetchProjection.docIdSymbol(),
                        fetchProjection.inputSymbols(),
                        fetchProjection.outputSymbols(),
                        fetchProjection.partitionedBy(),
                        fetchProjection.executionNodes(),
                        bulkSize,
                        fetchProjection.closeContexts(),
                        fetchProjection.jobSearchContextIdToNode(),
                        fetchProjection.jobSearchContextIdToShard()));
            } else {
                newProjections.add(projection);
            }
        }
        mergeNode.projections(newProjections);
    }
}
<code block>


package io.crate.operation;

import com.google.common.base.Optional;
import com.google.common.collect.Iterators;
import com.google.common.util.concurrent.Futures;
import com.google.common.util.concurrent.SettableFuture;
import io.crate.breaker.RamAccountingContext;
import io.crate.core.collections.ArrayBucket;
import io.crate.core.collections.Bucket;
import io.crate.core.collections.BucketPage;
import io.crate.executor.transport.TransportActionProvider;
import io.crate.jobs.ExecutionState;
import io.crate.metadata.*;
import io.crate.operation.aggregation.impl.AggregationImplModule;
import io.crate.operation.aggregation.impl.MinimumAggregation;
import io.crate.operation.projectors.FlatProjectorChain;
import io.crate.testing.CollectingProjector;
import io.crate.operation.projectors.TopN;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.GroupProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.TopNProjection;
import io.crate.planner.symbol.Aggregation;
import io.crate.planner.symbol.InputColumn;
import io.crate.planner.symbol.Symbol;
import io.crate.test.integration.CrateUnitTest;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.elasticsearch.action.bulk.BulkRetryCoordinatorPool;
import org.elasticsearch.client.Client;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.common.breaker.CircuitBreaker;
import org.elasticsearch.common.breaker.NoopCircuitBreaker;
import org.elasticsearch.common.collect.Tuple;
import org.elasticsearch.common.inject.AbstractModule;
import org.elasticsearch.common.inject.Injector;
import org.elasticsearch.common.inject.ModulesBuilder;
import org.elasticsearch.common.settings.ImmutableSettings;
import org.elasticsearch.threadpool.ThreadPool;
import org.hamcrest.collection.IsIterableContainingInOrder;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;
import org.mockito.Answers;

import java.util.Arrays;
import java.util.Collections;
import java.util.Iterator;
import java.util.concurrent.Executor;
import java.util.concurrent.TimeUnit;

import static io.crate.testing.TestingHelpers.isRow;
import static org.hamcrest.Matchers.contains;
import static org.mockito.Mockito.mock;

public class PageDownstreamFactoryTest extends CrateUnitTest {

    private static final RamAccountingContext ramAccountingContext =
            new RamAccountingContext("dummy", new NoopCircuitBreaker(CircuitBreaker.Name.FIELDDATA));

    private GroupProjection groupProjection;
    private Functions functions;
    private ReferenceResolver referenceResolver;
    private ThreadPool threadPool;

    @Before
    @SuppressWarnings("unchecked")
    public void prepare() {
        Injector injector = new ModulesBuilder()
                .add(new AggregationImplModule())
                .add(new AbstractModule() {
                    @Override
                    protected void configure() {
                        bind(Client.class).toInstance(mock(Client.class));
                    }
                })
                .createInjector();
        threadPool = new ThreadPool("testing");
        functions = injector.getInstance(Functions.class);
        referenceResolver = new GlobalReferenceResolver(
                Collections.<ReferenceIdent, ReferenceImplementation>emptyMap());

        FunctionIdent minAggIdent = new FunctionIdent(MinimumAggregation.NAME, Arrays.<DataType>asList(DataTypes.DOUBLE));
        FunctionInfo minAggInfo = new FunctionInfo(minAggIdent, DataTypes.DOUBLE);

        groupProjection = new GroupProjection();
        groupProjection.keys(Arrays.<Symbol>asList(new InputColumn(0, DataTypes.INTEGER)));
        groupProjection.values(Arrays.asList(
                new Aggregation(minAggInfo, Arrays.<Symbol>asList(new InputColumn(1)),
                        Aggregation.Step.PARTIAL, Aggregation.Step.FINAL)
        ));
    }

    @Override
    @After
    public void tearDown() throws Exception {
        super.tearDown();
        threadPool.shutdown();
        threadPool.awaitTermination(1, TimeUnit.SECONDS);
    }

    @Test
    public void testMergeSingleResult() throws Exception {
        TopNProjection topNProjection = new TopNProjection(3, TopN.NO_OFFSET,
                Arrays.<Symbol>asList(new InputColumn(0)), new boolean[]{false}, new Boolean[]{null});
        topNProjection.outputs(Arrays.<Symbol>asList(new InputColumn(0), new InputColumn(1)));

        MergeNode mergeNode = new MergeNode(0, "merge", 2); 
        mergeNode.projections(Arrays.asList(
                groupProjection,
                topNProjection
        ));

        Object[][] objs = new Object[20][];
        for (int i = 0; i < objs.length; i++) {
            objs[i] = new Object[]{i % 4, i + 0.5d};
        }
        Bucket rows = new ArrayBucket(objs);
        BucketPage page = new BucketPage(Futures.immediateFuture(rows));
        final PageDownstreamFactory pageDownstreamFactory = new PageDownstreamFactory(
                mock(ClusterService.class),
                threadPool,
                ImmutableSettings.EMPTY,
                mock(TransportActionProvider.class, Answers.RETURNS_DEEP_STUBS.get()),
                mock(BulkRetryCoordinatorPool.class),
                referenceResolver,
                functions
        );
        CollectingProjector collectingProjector = new CollectingProjector();
        final PageDownstream pageDownstream = getPageDownstream(mergeNode, pageDownstreamFactory, collectingProjector);
        final SettableFuture<?> future = SettableFuture.create();
        pageDownstream.nextPage(page, new PageConsumeListener() {
            @Override
            public void needMore() {
                pageDownstream.finish();
                future.set(null);
            }

            @Override
            public void finish() {
                fail("operation should want more");
            }
        });
        future.get();
        Bucket mergeResult = collectingProjector.result().get();
        assertThat(mergeResult, IsIterableContainingInOrder.contains(
                isRow(0, 0.5d),
                isRow(1, 1.5d),
                isRow(2, 2.5d)
        ));
    }

    private PageDownstream getPageDownstream(MergeNode mergeNode, PageDownstreamFactory pageDownstreamFactory, CollectingProjector collectingProjector) {
        Tuple<PageDownstream, FlatProjectorChain> downstreamFlatProjectorChainTuple =
                pageDownstreamFactory.createMergeNodePageDownstream(
                        mergeNode, collectingProjector, ramAccountingContext, Optional.<Executor>absent());
        downstreamFlatProjectorChainTuple.v2().startProjections(mock(ExecutionState.class));
        return downstreamFlatProjectorChainTuple.v1();
    }

    @Test
    public void testMergeMultipleResults() throws Exception {
        MergeNode mergeNode = new MergeNode(0, "merge", 2); 
        mergeNode.projections(Arrays.<Projection>asList(
                groupProjection
        ));
        final PageDownstreamFactory pageDownstreamFactory = new PageDownstreamFactory(
                mock(ClusterService.class),
                threadPool,
                ImmutableSettings.EMPTY,
                mock(TransportActionProvider.class, Answers.RETURNS_DEEP_STUBS.get()),
                mock(BulkRetryCoordinatorPool.class),
                referenceResolver,
                functions
        );
        CollectingProjector collectingProjector = new CollectingProjector();
        final PageDownstream pageDownstream = getPageDownstream(mergeNode, pageDownstreamFactory, collectingProjector);

        Bucket rows = new ArrayBucket(new Object[][]{{0, 100.0d}});
        BucketPage page1 = new BucketPage(Futures.immediateFuture(rows));
        Bucket otherRows = new ArrayBucket(new Object[][]{{0, 2.5d}});
        BucketPage page2 = new BucketPage(Futures.immediateFuture(otherRows));
        final Iterator<BucketPage> iterator = Iterators.forArray(page1, page2);

        final SettableFuture<?> future = SettableFuture.create();
        pageDownstream.nextPage(iterator.next(), new PageConsumeListener() {
            @Override
            public void needMore() {
                if (iterator.hasNext()) {
                    pageDownstream.nextPage(iterator.next(), this);
                } else {
                    pageDownstream.finish();
                    future.set(null);
                }
            }

            @Override
            public void finish() {
                fail("should still want more");
            }
        });
        future.get();
        Bucket mergeResult = collectingProjector.result().get();
        assertThat(mergeResult, contains(isRow(0, 2.5)));
    }

}

<code block>


package io.crate.operation.collect;

import com.google.common.collect.ImmutableList;
import io.crate.action.sql.SQLBulkRequest;
import io.crate.analyze.OrderBy;
import io.crate.analyze.WhereClause;
import io.crate.breaker.RamAccountingContext;
import io.crate.integrationtests.SQLTransportIntegrationTest;
import io.crate.jobs.JobContextService;
import io.crate.jobs.JobExecutionContext;
import io.crate.metadata.*;
import io.crate.operation.operator.EqOperator;
import io.crate.operation.projectors.ProjectionToProjectorVisitor;
import io.crate.operation.scalar.arithmetic.MultiplyFunction;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.symbol.Function;
import io.crate.planner.symbol.Literal;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.testing.CollectingProjector;
import io.crate.testing.TestingHelpers;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.action.bulk.BulkRequest;
import org.elasticsearch.action.bulk.BulkResponse;
import org.elasticsearch.action.index.IndexRequest;
import org.elasticsearch.common.breaker.CircuitBreaker;
import org.elasticsearch.common.breaker.NoopCircuitBreaker;
import org.elasticsearch.common.xcontent.XContentFactory;
import org.elasticsearch.index.IndexService;
import org.elasticsearch.indices.IndicesService;
import org.elasticsearch.test.ElasticsearchIntegrationTest;
import org.junit.Before;
import org.junit.Test;

import java.io.IOException;
import java.util.Arrays;
import java.util.List;
import java.util.UUID;

import static io.crate.testing.TestingHelpers.createReference;
import static org.hamcrest.Matchers.is;
import static org.hamcrest.Matchers.nullValue;
import static org.mockito.Matchers.any;
import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;

@ElasticsearchIntegrationTest.ClusterScope(scope = ElasticsearchIntegrationTest.Scope.SUITE, numDataNodes = 1)
public class LuceneDocCollectorTest extends SQLTransportIntegrationTest {

    private final static Integer PAGE_SIZE = 20;
    private final static String INDEX_NAME = "countries";
    private final static Integer NUMBER_OF_DOCS = 25;
    private OrderBy orderBy;
    private JobContextService jobContextService;
    private ShardCollectService shardCollectService;

    private CollectingProjector collectingProjector = new CollectingProjector();

    private static final RamAccountingContext RAM_ACCOUNTING_CONTEXT =
            new RamAccountingContext("dummy", new NoopCircuitBreaker(CircuitBreaker.Name.FIELDDATA));
    private JobCollectContext jobCollectContext;

    @Before
    public void prepare() throws Exception{
        execute("create table \""+INDEX_NAME+ "\" (" +
                " continent string, " +
                " countryName string," +
                " population integer" +
                ") clustered into 1 shards with (number_of_replicas=0)");
        refresh();
        generateData();
        IndicesService instanceFromNode = internalCluster().getDataNodeInstance(IndicesService.class);
        IndexService indexService = instanceFromNode.indexServiceSafe(INDEX_NAME);

        shardCollectService = indexService.shardInjectorSafe(0).getInstance(ShardCollectService.class);
        jobContextService = indexService.shardInjectorSafe(0).getInstance(JobContextService.class);

        ReferenceIdent ident = new ReferenceIdent(new TableIdent("doc", "countries"), "countryName");
        Reference ref = new Reference(new ReferenceInfo(ident, RowGranularity.DOC, DataTypes.STRING));
        orderBy = new OrderBy(ImmutableList.of((Symbol)ref), new boolean[]{false}, new Boolean[]{false});
    }

    private byte[] generateRowSource(String continent, String countryName, Integer population) throws IOException {
        return XContentFactory.jsonBuilder()
                .startObject()
                .field("continent", continent)
                .field("countryName", countryName)
                .field("population", population)
                .endObject()
                .bytes().toBytes();
    }

    public void generateData() throws Exception {
        BulkRequest bulkRequest = new BulkRequest();
        for (int i=0; i < NUMBER_OF_DOCS; i++) {
            IndexRequest indexRequest = new IndexRequest(INDEX_NAME, "default", String.valueOf(i));
            if (i == 0) {
                indexRequest.source(generateRowSource("Europe", "Germany", i));
            } else if (i == 1) {
                indexRequest.source(generateRowSource("Europe", "Austria", i));
            } else if (i >= 2 && i <=4) {
                indexRequest.source(generateRowSource("Europe", null, i));
            } else {
                indexRequest.source(generateRowSource("America", "USA", i));
            }
            bulkRequest.add(indexRequest);
        }
        BulkResponse response = client().bulk(bulkRequest).actionGet();
        assertFalse(response.hasFailures());
        refresh();
    }

    private LuceneDocCollector createDocCollector(OrderBy orderBy, Integer limit, List<Symbol> toCollect) throws Exception{
        return createDocCollector(orderBy, limit, toCollect, WhereClause.MATCH_ALL, PAGE_SIZE);
    }

    private LuceneDocCollector createDocCollector(OrderBy orderBy, Integer limit, List<Symbol> toCollect, WhereClause whereClause, int pageSize) throws Exception{
        CollectNode node = new CollectNode(0, "collect");
        node.whereClause(whereClause);
        node.orderBy(orderBy);
        node.limit(limit);
        UUID jobId = UUID.randomUUID();
        node.jobId(jobId);
        node.toCollect(toCollect);
        node.maxRowGranularity(RowGranularity.DOC);

        ShardProjectorChain projectorChain = mock(ShardProjectorChain.class);
        when(projectorChain.newShardDownstreamProjector(any(ProjectionToProjectorVisitor.class))).thenReturn(collectingProjector);

        JobExecutionContext.Builder builder = jobContextService.newBuilder(jobId);
        jobCollectContext = new JobCollectContext(
                jobId, node, mock(CollectOperation.class), RAM_ACCOUNTING_CONTEXT, collectingProjector);
        builder.addSubContext(node.executionNodeId(), jobCollectContext);
        jobContextService.createContext(builder);
        LuceneDocCollector collector = (LuceneDocCollector)shardCollectService.getCollector(node, projectorChain, jobCollectContext, 0);
        collector.pageSize(pageSize);
        return collector;
    }

    @Test
    public void testLimitWithoutOrder() throws Exception{
        collectingProjector.rows.clear();
        LuceneDocCollector docCollector = createDocCollector(null, 15, orderBy.orderBySymbols());
        docCollector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(15));
    }

    @Test
    public void testOrderedWithLimit() throws Exception{
        collectingProjector.rows.clear();
        LuceneDocCollector docCollector = createDocCollector(orderBy, 15, orderBy.orderBySymbols());
        docCollector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(15));
        assertThat(((BytesRef)collectingProjector.rows.get(0)[0]).utf8ToString(), is("Austria") );
        assertThat(((BytesRef)collectingProjector.rows.get(1)[0]).utf8ToString(), is("Germany") );
        assertThat(((BytesRef)collectingProjector.rows.get(2)[0]).utf8ToString(), is("USA") );
        assertThat(((BytesRef)collectingProjector.rows.get(3)[0]).utf8ToString(), is("USA") );
    }

    @Test
    public void testOrderedWithLimitHigherThanPageSize() throws Exception{
        collectingProjector.rows.clear();
        LuceneDocCollector docCollector = createDocCollector(orderBy, PAGE_SIZE + 5, orderBy.orderBySymbols());
        docCollector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(PAGE_SIZE + 5));
        assertThat(((BytesRef)collectingProjector.rows.get(0)[0]).utf8ToString(), is("Austria") );
        assertThat(((BytesRef)collectingProjector.rows.get(1)[0]).utf8ToString(), is("Germany") );
        assertThat(((BytesRef)collectingProjector.rows.get(2)[0]).utf8ToString(), is("USA") );
        assertThat(((BytesRef)collectingProjector.rows.get(3)[0]).utf8ToString(), is("USA") );
    }

    @Test
    public void testOrderedWithoutLimit() throws Exception {
        collectingProjector.rows.clear();
        LuceneDocCollector docCollector = createDocCollector(orderBy, null, orderBy.orderBySymbols(), WhereClause.MATCH_ALL, 1);
        docCollector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(NUMBER_OF_DOCS));
        assertThat(((BytesRef)collectingProjector.rows.get(0)[0]).utf8ToString(), is("Austria") );
        assertThat(((BytesRef)collectingProjector.rows.get(1)[0]).utf8ToString(), is("Germany") );
        assertThat(((BytesRef)collectingProjector.rows.get(2)[0]).utf8ToString(), is("USA") );
        assertThat(collectingProjector.rows.get(NUMBER_OF_DOCS -1)[0], is(nullValue()));
    }

    @Test
    public void testOrderedNullsFirstWithoutLimit() throws Exception {
        collectingProjector.rows.clear();
        ReferenceIdent ident = new ReferenceIdent(new TableIdent("doc", "countries"), "countryName");
        Reference ref = new Reference(new ReferenceInfo(ident, RowGranularity.DOC, DataTypes.STRING));
        OrderBy orderBy = new OrderBy(ImmutableList.of((Symbol)ref), new boolean[]{false}, new Boolean[]{true});
        LuceneDocCollector docCollector = createDocCollector(orderBy, null, orderBy.orderBySymbols(), WhereClause.MATCH_ALL, 1);
        docCollector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(NUMBER_OF_DOCS));
        assertThat(collectingProjector.rows.get(0)[0], is(nullValue()));
        assertThat(collectingProjector.rows.get(1)[0], is(nullValue()));
        assertThat(collectingProjector.rows.get(2)[0], is(nullValue()));
        assertThat(((BytesRef)collectingProjector.rows.get(3)[0]).utf8ToString(), is("Austria") );
        assertThat(((BytesRef)collectingProjector.rows.get(4)[0]).utf8ToString(), is("Germany") );
        assertThat(((BytesRef)collectingProjector.rows.get(5)[0]).utf8ToString(), is("USA") );
    }

    @Test
    public void testOrderedDescendingWithoutLimit() throws Exception {
        collectingProjector.rows.clear();
        ReferenceIdent ident = new ReferenceIdent(new TableIdent("doc", "countries"), "countryName");
        Reference ref = new Reference(new ReferenceInfo(ident, RowGranularity.DOC, DataTypes.STRING));
        OrderBy orderBy = new OrderBy(ImmutableList.of((Symbol)ref), new boolean[]{true}, new Boolean[]{false});
        LuceneDocCollector docCollector = createDocCollector(orderBy, null, orderBy.orderBySymbols(), WhereClause.MATCH_ALL, 1);
        docCollector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(NUMBER_OF_DOCS));
        assertThat(collectingProjector.rows.get(NUMBER_OF_DOCS - 1)[0], is(nullValue()));
        assertThat(collectingProjector.rows.get(NUMBER_OF_DOCS - 2)[0], is(nullValue()));
        assertThat(collectingProjector.rows.get(NUMBER_OF_DOCS - 3)[0], is(nullValue()));
        assertThat(((BytesRef)collectingProjector.rows.get(NUMBER_OF_DOCS - 4)[0]).utf8ToString(), is("Austria") );
        assertThat(((BytesRef)collectingProjector.rows.get(NUMBER_OF_DOCS - 5)[0]).utf8ToString(), is("Germany") );
        assertThat(((BytesRef)collectingProjector.rows.get(NUMBER_OF_DOCS - 6)[0]).utf8ToString(), is("USA") );
    }

    @Test
    public void testOrderedDescendingNullsFirstWithoutLimit() throws Exception {
        collectingProjector.rows.clear();
        ReferenceIdent ident = new ReferenceIdent(new TableIdent("doc", "countries"), "countryName");
        Reference ref = new Reference(new ReferenceInfo(ident, RowGranularity.DOC, DataTypes.STRING));
        OrderBy orderBy = new OrderBy(ImmutableList.of((Symbol)ref), new boolean[]{true}, new Boolean[]{true});
        LuceneDocCollector docCollector = createDocCollector(orderBy, null, orderBy.orderBySymbols(), WhereClause.MATCH_ALL, 1);
        docCollector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(NUMBER_OF_DOCS));
        assertThat(collectingProjector.rows.get(0)[0], is(nullValue()));
        assertThat(collectingProjector.rows.get(1)[0], is(nullValue()));
        assertThat(collectingProjector.rows.get(2)[0], is(nullValue()));
        assertThat(((BytesRef)collectingProjector.rows.get(NUMBER_OF_DOCS - 1)[0]).utf8ToString(), is("Austria") );
        assertThat(((BytesRef)collectingProjector.rows.get(NUMBER_OF_DOCS - 2)[0]).utf8ToString(), is("Germany") );
        assertThat(((BytesRef)collectingProjector.rows.get(NUMBER_OF_DOCS - 3)[0]).utf8ToString(), is("USA") );
    }

    @Test
    public void testOrderForNonSelected() throws Exception {
        collectingProjector.rows.clear();
        ReferenceIdent countriesIdent = new ReferenceIdent(new TableIdent("doc", "countries"), "countryName");
        Reference countries = new Reference(new ReferenceInfo(countriesIdent, RowGranularity.DOC, DataTypes.STRING));

        ReferenceIdent populationIdent = new ReferenceIdent(new TableIdent("doc", "countries"), "population");
        Reference population = new Reference(new ReferenceInfo(populationIdent, RowGranularity.DOC, DataTypes.INTEGER));

        OrderBy orderBy = new OrderBy(ImmutableList.of((Symbol)population), new boolean[]{true}, new Boolean[]{true});

        LuceneDocCollector docCollector = createDocCollector(orderBy, null, ImmutableList.of((Symbol)countries));
        docCollector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(NUMBER_OF_DOCS));
        assertThat(collectingProjector.rows.get(0).length, is(1));
        assertThat(((BytesRef)collectingProjector.rows.get(NUMBER_OF_DOCS - 6)[0]).utf8ToString(), is("USA") );
        assertThat(collectingProjector.rows.get(NUMBER_OF_DOCS - 5)[0], is(nullValue()));
        assertThat(collectingProjector.rows.get(NUMBER_OF_DOCS - 4)[0], is(nullValue()));
        assertThat(collectingProjector.rows.get(NUMBER_OF_DOCS - 3)[0], is(nullValue()));
        assertThat(((BytesRef)collectingProjector.rows.get(NUMBER_OF_DOCS - 2)[0]).utf8ToString(), is("Austria") );
        assertThat(((BytesRef)collectingProjector.rows.get(NUMBER_OF_DOCS - 1)[0]).utf8ToString(), is("Germany") );
    }

    @Test
    public void testOrderByScalar() throws Exception {
        collectingProjector.rows.clear();
        Reference population = createReference("population", DataTypes.INTEGER);
        Function scalarFunction = new Function(
                new FunctionInfo(
                        new FunctionIdent(MultiplyFunction.NAME, Arrays.<DataType>asList(DataTypes.INTEGER, DataTypes.INTEGER)),
                        DataTypes.LONG),
                Arrays.asList(population, Literal.newLiteral(-1))
        );

        OrderBy orderBy = new OrderBy(ImmutableList.of((Symbol)scalarFunction), new boolean[]{false}, new Boolean[]{false});
        LuceneDocCollector docCollector = createDocCollector(orderBy, null, ImmutableList.of((Symbol)population));
        docCollector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(NUMBER_OF_DOCS));
        assertThat(((Integer)collectingProjector.rows.get(NUMBER_OF_DOCS - 2)[0]), is(1) );
        assertThat(((Integer)collectingProjector.rows.get(NUMBER_OF_DOCS - 1)[0]), is(0) );
    }

    @Test
    public void testMultiOrdering() throws Exception {
        execute("create table test (x integer, y integer) clustered into 1 shards with (number_of_replicas=0)");
        waitNoPendingTasksOnAll();
        SQLBulkRequest request = new SQLBulkRequest("insert into test values (?, ?)",
                new Object[][]{
                    new Object[]{2, 3},
                    new Object[]{2, 1},
                    new Object[]{2, null},
                    new Object[]{1, null},
                    new Object[]{1, 2},
                    new Object[]{1, 1},
                    new Object[]{1, 0},
                    new Object[]{1, null}
                }
        );
        sqlExecutor.exec(request);
        execute("refresh table test");
        collectingProjector.rows.clear();

        IndicesService instanceFromNode = internalCluster().getDataNodeInstance(IndicesService.class);
        IndexService indexService = instanceFromNode.indexServiceSafe("test");

        ShardCollectService shardCollectService = indexService.shardInjectorSafe(0).getInstance(ShardCollectService.class);
        JobContextService jobContextService = indexService.shardInjectorSafe(0).getInstance(JobContextService.class);

        ReferenceIdent xIdent = new ReferenceIdent(new TableIdent("doc", "test"), "x");
        Reference x = new Reference(new ReferenceInfo(xIdent, RowGranularity.DOC, DataTypes.INTEGER));

        ReferenceIdent yIdent = new ReferenceIdent(new TableIdent("doc", "test"), "y");
        Reference y = new Reference(new ReferenceInfo(yIdent, RowGranularity.DOC, DataTypes.INTEGER));

        OrderBy orderBy = new OrderBy(ImmutableList.<Symbol>of(x, y), new boolean[]{false, false}, new Boolean[]{false, false});

        CollectNode node = new CollectNode(0, "collect");
        node.whereClause(WhereClause.MATCH_ALL);
        node.orderBy(orderBy);
        node.jobId(UUID.randomUUID());
        node.toCollect(orderBy.orderBySymbols());
        node.maxRowGranularity(RowGranularity.DOC);

        JobExecutionContext.Builder builder = jobContextService.newBuilder(node.jobId());
        builder.addSubContext(node.executionNodeId(),
                new JobCollectContext(node.jobId(), node, mock(CollectOperation.class), RAM_ACCOUNTING_CONTEXT, collectingProjector));
        jobContextService.createContext(builder);

        ShardProjectorChain projectorChain = mock(ShardProjectorChain.class);
        when(projectorChain.newShardDownstreamProjector(any(ProjectionToProjectorVisitor.class))).thenReturn(collectingProjector);

        JobCollectContext jobCollectContext = jobContextService.getContext(node.jobId()).getSubContext(node.executionNodeId());
        LuceneDocCollector collector = (LuceneDocCollector)shardCollectService.getCollector(node, projectorChain, jobCollectContext, 0);
        collector.pageSize(1);
        collector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(8));

        String expected = "1| 0\n" +
                "1| 1\n" +
                "1| 2\n" +
                "1| NULL\n" +
                "1| NULL\n" +
                "2| 1\n" +
                "2| 3\n" +
                "2| NULL\n";
        assertEquals(expected, TestingHelpers.printedTable(collectingProjector.doFinish()));


        node.jobId(UUID.randomUUID());
        builder = jobContextService.newBuilder(node.jobId());
        builder.addSubContext(node.executionNodeId(),
                new JobCollectContext(node.jobId(), node, mock(CollectOperation.class), RAM_ACCOUNTING_CONTEXT, collectingProjector));
        jobContextService.createContext(builder);
        jobCollectContext = jobContextService.getContext(node.jobId()).getSubContext(node.executionNodeId());

        collectingProjector.rows.clear();
        orderBy = new OrderBy(ImmutableList.<Symbol>of(x, y), new boolean[]{false, false}, new Boolean[]{false, true});
        node.orderBy(orderBy);
        collector = (LuceneDocCollector)shardCollectService.getCollector(node, projectorChain, jobCollectContext, 0);
        collector.pageSize(1);
        collector.doCollect(jobCollectContext);

        expected = "1| NULL\n" +
                   "1| NULL\n" +
                   "1| 0\n" +
                   "1| 1\n" +
                   "1| 2\n" +
                   "2| NULL\n" +
                   "2| 1\n" +
                   "2| 3\n";
        assertEquals(expected, TestingHelpers.printedTable(collectingProjector.doFinish()));
    }

    @Test
    public void testMinScoreQuery() throws Exception {
        collectingProjector.rows.clear();

        Reference minScore_ref = new Reference(
                new ReferenceInfo(new ReferenceIdent(null, "_score"), RowGranularity.DOC, DataTypes.DOUBLE));

        Function function = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.DOUBLE, DataTypes.DOUBLE)),
                DataTypes.BOOLEAN),
                Arrays.asList(minScore_ref, Literal.newLiteral(1.1))
        );
        WhereClause whereClause = new WhereClause(function);
        LuceneDocCollector docCollector = createDocCollector(null, null, orderBy.orderBySymbols(), whereClause, PAGE_SIZE);
        docCollector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(0));


        collectingProjector.rows.clear();
        function = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.DOUBLE, DataTypes.DOUBLE)),
                DataTypes.BOOLEAN),
                Arrays.asList(minScore_ref, Literal.newLiteral(1.0))
        );
        whereClause = new WhereClause(function);
        docCollector = createDocCollector(null, null, orderBy.orderBySymbols(), whereClause, PAGE_SIZE);
        docCollector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(NUMBER_OF_DOCS));
    }
}

<code block>


package io.crate.operation.collect;

import com.google.common.collect.ImmutableList;
import io.crate.analyze.WhereClause;
import io.crate.blob.BlobEnvironment;
import io.crate.blob.v2.BlobIndices;
import io.crate.breaker.CircuitBreakerModule;
import io.crate.breaker.RamAccountingContext;
import io.crate.core.collections.Bucket;
import io.crate.core.collections.TreeMapBuilder;
import io.crate.exceptions.UnhandledServerException;
import io.crate.executor.transport.TransportActionProvider;
import io.crate.jobs.JobContextService;
import io.crate.jobs.JobExecutionContext;
import io.crate.metadata.*;
import io.crate.metadata.shard.ShardReferenceImplementation;
import io.crate.metadata.shard.ShardReferenceResolver;
import io.crate.metadata.shard.blob.BlobShardReferenceImplementation;
import io.crate.metadata.sys.SysNodesTableInfo;
import io.crate.metadata.sys.SysShardsTableInfo;
import io.crate.operation.Input;
import io.crate.operation.operator.AndOperator;
import io.crate.operation.operator.EqOperator;
import io.crate.operation.operator.OperatorModule;
import io.crate.operation.reference.sys.node.NodeSysExpression;
import io.crate.operation.reference.sys.node.SysNodeExpressionModule;
import io.crate.testing.CollectingProjector;
import io.crate.operation.projectors.ResultProvider;
import io.crate.operation.projectors.ResultProviderFactory;
import io.crate.operation.reference.sys.shard.SysShardExpression;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.symbol.Function;
import io.crate.planner.symbol.Literal;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.test.integration.CrateUnitTest;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.action.admin.indices.create.TransportBulkCreateIndicesAction;
import org.elasticsearch.action.admin.indices.create.TransportCreateIndexAction;
import org.elasticsearch.action.admin.indices.delete.TransportDeleteIndexAction;
import org.elasticsearch.action.admin.indices.settings.put.TransportUpdateSettingsAction;
import org.elasticsearch.action.admin.indices.template.put.TransportPutIndexTemplateAction;
import org.elasticsearch.action.bulk.BulkRetryCoordinator;
import org.elasticsearch.action.bulk.BulkRetryCoordinatorPool;
import org.elasticsearch.action.bulk.TransportShardBulkAction;
import org.elasticsearch.action.support.ActionFilters;
import org.elasticsearch.client.Client;
import org.elasticsearch.cluster.ClusterInfoService;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.cluster.ClusterState;
import org.elasticsearch.cluster.metadata.MetaDataCreateIndexService;
import org.elasticsearch.cluster.metadata.MetaDataDeleteIndexService;
import org.elasticsearch.cluster.metadata.MetaDataUpdateSettingsService;
import org.elasticsearch.cluster.node.DiscoveryNode;
import org.elasticsearch.cluster.node.DiscoveryNodes;
import org.elasticsearch.cluster.routing.allocation.AllocationService;
import org.elasticsearch.cluster.routing.allocation.decider.AllocationDecider;
import org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider;
import org.elasticsearch.cluster.settings.ClusterDynamicSettings;
import org.elasticsearch.cluster.settings.DynamicSettings;
import org.elasticsearch.common.breaker.CircuitBreaker;
import org.elasticsearch.common.breaker.NoopCircuitBreaker;
import org.elasticsearch.common.inject.*;
import org.elasticsearch.common.inject.multibindings.MapBinder;
import org.elasticsearch.common.settings.ImmutableSettings;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.discovery.Discovery;
import org.elasticsearch.discovery.DiscoveryService;
import org.elasticsearch.index.Index;
import org.elasticsearch.index.IndexService;
import org.elasticsearch.index.mapper.MapperService;
import org.elasticsearch.index.settings.IndexSettings;
import org.elasticsearch.index.shard.IndexShard;
import org.elasticsearch.index.shard.ShardId;
import org.elasticsearch.indices.IndicesLifecycle;
import org.elasticsearch.indices.IndicesService;
import org.elasticsearch.indices.breaker.CircuitBreakerService;
import org.elasticsearch.indices.breaker.NoneCircuitBreakerService;
import org.elasticsearch.monitor.network.NetworkService;
import org.elasticsearch.monitor.os.OsService;
import org.elasticsearch.monitor.os.OsStats;
import org.elasticsearch.node.service.NodeService;
import org.elasticsearch.node.settings.NodeSettingsService;
import org.elasticsearch.script.ScriptService;
import org.elasticsearch.search.InternalSearchService;
import org.elasticsearch.search.SearchService;
import org.elasticsearch.threadpool.ThreadPool;
import org.elasticsearch.transport.TransportService;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;
import org.mockito.Answers;

import java.util.*;
import java.util.concurrent.ExecutionException;

import static io.crate.testing.TestingHelpers.isRow;
import static org.hamcrest.Matchers.*;
import static org.mockito.Matchers.any;
import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;

public class LocalDataCollectTest extends CrateUnitTest {

    static class TestFunction extends Scalar<Integer,Object> {
        public static final FunctionIdent ident = new FunctionIdent("twoTimes", Arrays.<DataType>asList(DataTypes.INTEGER));
        public static final FunctionInfo info = new FunctionInfo(ident, DataTypes.INTEGER);

        @Override
        public Integer evaluate(Input<Object>... args) {
            if (args.length == 0) {
                return 0;
            }
            Short value = (Short) args[0].value();
            return value * 2;
        }

        @Override
        public FunctionInfo info() {
            return info;
        }

        @Override
        public Symbol normalizeSymbol(Function symbol) {
            return symbol;
        }
    }

    static class ShardIdExpression extends SysShardExpression<Integer> implements ShardReferenceImplementation<Integer> {

        private final ShardId shardId;

        @Inject
        public ShardIdExpression(ShardId shardId) {
            this.shardId = shardId;
        }

        @Override
        public Integer value() {
            return shardId.id();
        }

        @Override
        public ReferenceImplementation getChildImplementation(String name) {
            return null;
        }
    }

    private DiscoveryService discoveryService;
    private Functions functions;
    private IndexService indexService = mock(IndexService.class);
    private MapSideDataCollectOperation operation;
    private Routing testRouting = new Routing(TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder()
        .put(TEST_NODE_ID, new TreeMap<String, List<Integer>>()).map()
    );

    private JobContextService jobContextService;

    private final ThreadPool testThreadPool = new ThreadPool(getClass().getSimpleName());
    private final static String TEST_NODE_ID = "test_node";
    private final static String TEST_TABLE_NAME = "test_table";

    private static Reference testNodeReference = new Reference(
            SysNodesTableInfo.INFOS.get(new ColumnIdent("os", ImmutableList.of("cpu", "stolen")))
    );
    private static Reference testShardIdReference = new Reference(SysShardsTableInfo.INFOS.get(new ColumnIdent("id")));

    private static final RamAccountingContext RAM_ACCOUNTING_CONTEXT =
            new RamAccountingContext("dummy", new NoopCircuitBreaker(CircuitBreaker.Name.FIELDDATA));

    class TestModule extends AbstractModule {
        protected MapBinder<FunctionIdent, FunctionImplementation> functionBinder;

        @Override
        protected void configure() {
            functionBinder = MapBinder.newMapBinder(binder(), FunctionIdent.class, FunctionImplementation.class);
            functionBinder.addBinding(TestFunction.ident).toInstance(new TestFunction());
            bind(Functions.class).asEagerSingleton();
            bind(ReferenceInfos.class).toInstance(mock(ReferenceInfos.class));
            bind(ThreadPool.class).toInstance(testThreadPool);

            BulkRetryCoordinator bulkRetryCoordinator = mock(BulkRetryCoordinator.class);
            BulkRetryCoordinatorPool bulkRetryCoordinatorPool = mock(BulkRetryCoordinatorPool.class);
            when(bulkRetryCoordinatorPool.coordinator(any(ShardId.class))).thenReturn(bulkRetryCoordinator);
            bind(BulkRetryCoordinatorPool.class).toInstance(bulkRetryCoordinatorPool);

            bind(TransportBulkCreateIndicesAction.class).toInstance(mock(TransportBulkCreateIndicesAction.class));
            bind(CircuitBreakerService.class).toInstance(new NoneCircuitBreakerService());
            bind(ActionFilters.class).toInstance(mock(ActionFilters.class));
            bind(ScriptService.class).toInstance(mock(ScriptService.class));
            bind(SearchService.class).toInstance(mock(InternalSearchService.class));
            bind(AllocationService.class).toInstance(mock(AllocationService.class));
            bind(MetaDataCreateIndexService.class).toInstance(mock(MetaDataCreateIndexService.class));
            bind(DynamicSettings.class).annotatedWith(ClusterDynamicSettings.class).toInstance(mock(DynamicSettings.class));
            bind(MetaDataDeleteIndexService.class).toInstance(mock(MetaDataDeleteIndexService.class));
            bind(ClusterInfoService.class).toInstance(mock(ClusterInfoService.class));
            bind(TransportService.class).toInstance(mock(TransportService.class));
            bind(MapperService.class).toInstance(mock(MapperService.class));

            OsService osService = mock(OsService.class);
            OsStats osStats = mock(OsStats.class);
            when(osService.stats()).thenReturn(osStats);
            OsStats.Cpu osCpu = mock(OsStats.Cpu.class);
            when(osCpu.stolen()).thenReturn((short) 1);
            when(osStats.cpu()).thenReturn(osCpu);

            bind(OsService.class).toInstance(osService);
            bind(NodeService.class).toInstance(mock(NodeService.class));
            bind(Discovery.class).toInstance(mock(Discovery.class));
            bind(NetworkService.class).toInstance(mock(NetworkService.class));

            bind(TransportShardBulkAction.class).toInstance(mock(TransportShardBulkAction.class));
            bind(TransportCreateIndexAction.class).toInstance(mock(TransportCreateIndexAction.class));

            discoveryService = mock(DiscoveryService.class);
            DiscoveryNode discoveryNode = mock(DiscoveryNode.class);
            when(discoveryNode.id()).thenReturn(TEST_NODE_ID);
            when(discoveryService.localNode()).thenReturn(discoveryNode);

            ClusterService clusterService = mock(ClusterService.class);
            ClusterState state = mock(ClusterState.class);
            DiscoveryNodes discoveryNodes = mock(DiscoveryNodes.class);
            when(discoveryNodes.localNodeId()).thenReturn(TEST_NODE_ID);
            when(state.nodes()).thenReturn(discoveryNodes);
            when(clusterService.state()).thenReturn(state);
            when(clusterService.localNode()).thenReturn(discoveryNode);
            bind(ClusterService.class).toInstance(clusterService);

            IndicesService indicesService = mock(IndicesService.class);
            bind(IndicesService.class).toInstance(indicesService);
            bind(Settings.class).toInstance(ImmutableSettings.EMPTY);

            bind(MetaDataUpdateSettingsService.class).toInstance(mock(MetaDataUpdateSettingsService.class));
            bind(Client.class).toInstance(mock(Client.class));

            Provider<TransportCreateIndexAction> transportCreateIndexActionProvider = mock(Provider.class);
            when(transportCreateIndexActionProvider.get()).thenReturn(mock(TransportCreateIndexAction.class));
            Provider<TransportDeleteIndexAction> transportDeleteActionProvider = mock(Provider.class);
            when(transportDeleteActionProvider.get()).thenReturn(mock(TransportDeleteIndexAction.class));
            Provider<TransportUpdateSettingsAction> transportUpdateSettingsActionProvider = mock(Provider.class);
            when(transportUpdateSettingsActionProvider.get()).thenReturn(mock(TransportUpdateSettingsAction.class));

            BlobIndices blobIndices = new BlobIndices(
                    ImmutableSettings.EMPTY,
                    transportCreateIndexActionProvider,
                    transportDeleteActionProvider,
                    transportUpdateSettingsActionProvider,
                    indicesService,
                    mock(IndicesLifecycle.class),
                    mock(BlobEnvironment.class),
                    clusterService
            );
            bind(BlobIndices.class).toInstance(blobIndices);

            bind(ReferenceResolver.class).to(GlobalReferenceResolver.class);

            TransportPutIndexTemplateAction transportPutIndexTemplateAction = mock(TransportPutIndexTemplateAction.class);
            bind(TransportPutIndexTemplateAction.class).toInstance(transportPutIndexTemplateAction);

            bind(IndexService.class).toInstance(indexService);
        }
    }

    class TestShardModule extends AbstractModule {

        private final ShardId shardId;
        private final ShardIdExpression shardIdExpression;

        public TestShardModule(int shardId) {
            super();
            this.shardId = new ShardId(TEST_TABLE_NAME, shardId);
            this.shardIdExpression = new ShardIdExpression(this.shardId);
        }

        @Override
        protected void configure() {
            IndexShard shard = mock(IndexShard.class);
            bind(IndexShard.class).toInstance(shard);
            when(shard.shardId()).thenReturn(shardId);
            Index index = new Index(TEST_TABLE_NAME);
            bind(Index.class).toInstance(index);
            bind(ShardId.class).toInstance(shardId);
            MapBinder<ReferenceIdent, ShardReferenceImplementation> binder = MapBinder
                    .newMapBinder(binder(), ReferenceIdent.class, ShardReferenceImplementation.class);
            binder.addBinding(SysShardsTableInfo.INFOS.get(new ColumnIdent("id")).ident()).toInstance(shardIdExpression);
            bind(ShardReferenceResolver.class).asEagerSingleton();
            bind(AllocationDecider.class).to(DiskThresholdDecider.class);
            bind(ShardCollectService.class).asEagerSingleton();

            bind(DiscoveryService.class).toInstance(discoveryService);



            MapBinder<ReferenceIdent, BlobShardReferenceImplementation> blobBinder = MapBinder
                    .newMapBinder(binder(), ReferenceIdent.class, BlobShardReferenceImplementation.class);
            bind(Settings.class).annotatedWith(IndexSettings.class).toInstance(ImmutableSettings.EMPTY);
        }
    }

    @Before
    public void configure() {
        Injector injector = new ModulesBuilder().add(
                new CircuitBreakerModule(),
                new OperatorModule(),
                new TestModule(),
                new SysNodeExpressionModule()
        ).createInjector();
        Injector shard0Injector = injector.createChildInjector(
                new TestShardModule(0)
        );
        Injector shard1Injector = injector.createChildInjector(
                new TestShardModule(1)
        );
        functions = injector.getInstance(Functions.class);

        IndicesService indicesService = injector.getInstance(IndicesService.class);
        indexService = injector.getInstance(IndexService.class);

        when(indexService.shardInjectorSafe(0)).thenReturn(shard0Injector);
        when(indexService.shardInjectorSafe(1)).thenReturn(shard1Injector);
        when(indexService.shardSafe(0)).thenReturn(shard0Injector.getInstance(IndexShard.class));
        when(indexService.shardSafe(1)).thenReturn(shard1Injector.getInstance(IndexShard.class));
        when(indicesService.indexServiceSafe(TEST_TABLE_NAME)).thenReturn(indexService);

        NodeSettingsService nodeSettingsService = mock(NodeSettingsService.class);
        jobContextService = new JobContextService(ImmutableSettings.EMPTY, testThreadPool, mock(StatsTables.class));

        ClusterService clusterService = injector.getInstance(ClusterService.class);
        operation = new MapSideDataCollectOperation(
                clusterService,
                ImmutableSettings.EMPTY,
                mock(TransportActionProvider.class, Answers.RETURNS_DEEP_STUBS.get()),
                injector.getInstance(BulkRetryCoordinatorPool.class),
                functions,
                injector.getInstance(ReferenceResolver.class),
                injector.getInstance(NodeSysExpression.class),
                indicesService,
                testThreadPool,
                new CollectServiceResolver(discoveryService,
                        new SystemCollectService(
                                discoveryService,
                                functions,
                                new StatsTables(ImmutableSettings.EMPTY, nodeSettingsService))
                ),
                new ResultProviderFactory() {
                    @Override
                    public ResultProvider createDownstream(ExecutionNode node, UUID jobId) {
                        return new CollectingProjector();
                    }
                },
                mock(InformationSchemaCollectService.class),
                mock(UnassignedShardsCollectService.class)
        );
    }

    @After
    public void cleanUp() throws Exception {
        testThreadPool.shutdownNow();
    }

    private Routing shardRouting(final Integer... shardIds) {
        return new Routing(TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder()
            .put(TEST_NODE_ID, TreeMapBuilder.<String, List<Integer>>newMapBuilder()
                            .put(TEST_TABLE_NAME, Arrays.asList(shardIds))
                            .map()
            )
            .map()
        );
    }

    @Test
    public void testCollectExpressions() throws Exception {
        CollectNode collectNode = new CollectNode(0, "collect", testRouting);
        collectNode.jobId(UUID.randomUUID());
        collectNode.toCollect(Arrays.<Symbol>asList(testNodeReference));
        collectNode.maxRowGranularity(RowGranularity.NODE);

        Bucket result = getBucket(collectNode);

        assertThat(result.size(), equalTo(1));
        assertThat(result, contains(isRow((short) 1)));
    }

    @Test
    public void testWrongRouting() throws Exception {

        expectedException.expect(UnhandledServerException.class);
        expectedException.expectMessage("unsupported routing");

        CollectNode collectNode = new CollectNode(0, "wrong", new Routing(TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder()
            .put("bla", TreeMapBuilder.<String, List<Integer>>newMapBuilder()
                .put("my_index", Arrays.asList(1))
                .put("my_index", Arrays.asList(1))
                .map()
            ).map()
        ));
        collectNode.maxRowGranularity(RowGranularity.DOC);
        collectNode.jobId(UUID.randomUUID());
        operation.collect(collectNode, new CollectingProjector(), null);
    }

    @Test
    public void testCollectUnknownReference() throws Throwable {
        expectedException.expect(UnhandledServerException.class);
        expectedException.expectMessage("Unknown Reference some.table.some_column");

        CollectNode collectNode = new CollectNode(0, "unknown", testRouting);
        collectNode.jobId(UUID.randomUUID());
        Reference unknownReference = new Reference(
                new ReferenceInfo(
                        new ReferenceIdent(
                                new TableIdent("some", "table"),
                                "some_column"
                        ),
                        RowGranularity.NODE,
                        DataTypes.BOOLEAN
                )
        );
        collectNode.toCollect(Arrays.<Symbol>asList(unknownReference));
        collectNode.maxRowGranularity(RowGranularity.NODE);
        try {
            getBucket(collectNode);
        } catch (ExecutionException e) {
            throw e.getCause();
        }
    }

    @Test
    public void testCollectFunction() throws Exception {
        CollectNode collectNode = new CollectNode(0, "function", testRouting);
        collectNode.jobId(UUID.randomUUID());
        Function twoTimesTruthFunction = new Function(
                TestFunction.info,
                Arrays.<Symbol>asList(testNodeReference)
        );
        collectNode.toCollect(Arrays.<Symbol>asList(twoTimesTruthFunction, testNodeReference));
        collectNode.maxRowGranularity(RowGranularity.NODE);
        Bucket result = getBucket(collectNode);
        assertThat(result.size(), equalTo(1));
        assertThat(result, contains(isRow(2, (short) 1)));
    }


    @Test
    public void testUnknownFunction() throws Throwable {

        expectedException.expect(IllegalArgumentException.class);
        expectedException.expectMessage("Cannot find implementation for function unknown()");

        CollectNode collectNode = new CollectNode(0, "unknownFunction", testRouting);
        Function unknownFunction = new Function(
                new FunctionInfo(
                        new FunctionIdent("unknown", ImmutableList.<DataType>of()),
                        DataTypes.BOOLEAN
                ),
                ImmutableList.<Symbol>of()
        );
        collectNode.toCollect(Arrays.<Symbol>asList(unknownFunction));
        collectNode.jobId(UUID.randomUUID());
        try {
            getBucket(collectNode);
        } catch (ExecutionException e) {
            throw e.getCause();
        }
    }

    @Test
    public void testCollectLiterals() throws Exception {
        CollectNode collectNode = new CollectNode(0, "literals", testRouting);
        collectNode.jobId(UUID.randomUUID());
        collectNode.toCollect(Arrays.<Symbol>asList(
                Literal.newLiteral("foobar"),
                Literal.newLiteral(true),
                Literal.newLiteral(1),
                Literal.newLiteral(4.2)
        ));
        Bucket result = getBucket(collectNode);
        assertThat(result, contains(isRow(new BytesRef("foobar"), true, 1, 4.2)));
    }

    @Test
    public void testCollectWithFalseWhereClause() throws Exception {
        CollectNode collectNode = new CollectNode(0, "whereClause", testRouting);
        collectNode.jobId(UUID.randomUUID());
        collectNode.toCollect(Arrays.<Symbol>asList(testNodeReference));
        collectNode.whereClause(new WhereClause(new Function(
                AndOperator.INFO,
                Arrays.<Symbol>asList(Literal.newLiteral(false), Literal.newLiteral(false))
        )));
        Bucket result = getBucket(collectNode);
        assertThat(result.size(), is(0));
    }

    @Test
    public void testCollectWithTrueWhereClause() throws Exception {
        CollectNode collectNode = new CollectNode(0, "whereClause", testRouting);
        collectNode.toCollect(Arrays.<Symbol>asList(testNodeReference));
        collectNode.whereClause(new WhereClause(new Function(
                AndOperator.INFO,
                Arrays.<Symbol>asList(Literal.newLiteral(true), Literal.newLiteral(true))
        )));
        collectNode.jobId(UUID.randomUUID());
        collectNode.maxRowGranularity(RowGranularity.NODE);
        Bucket result = getBucket(collectNode);
        assertThat(result, contains(isRow((short) 1)));

    }

    @Test
    public void testCollectWithNullWhereClause() throws Exception {
        EqOperator op = (EqOperator) functions.get(new FunctionIdent(
                EqOperator.NAME, ImmutableList.<DataType>of(DataTypes.INTEGER, DataTypes.INTEGER)));
        CollectNode collectNode = new CollectNode(0, "whereClause", testRouting);
        collectNode.jobId(UUID.randomUUID());
        collectNode.toCollect(Arrays.<Symbol>asList(testNodeReference));
        collectNode.whereClause(new WhereClause(new Function(
                op.info(),
                Arrays.<Symbol>asList(Literal.NULL, Literal.NULL)
        )));
        Bucket result = getBucket(collectNode);
        assertThat(result.size(), is(0));
    }

    private Bucket getBucket(CollectNode collectNode) throws InterruptedException, ExecutionException {
        CollectingProjector cd = new CollectingProjector();
        JobExecutionContext.Builder builder = jobContextService.newBuilder(collectNode.jobId());
        JobCollectContext jobCollectContext =
                new JobCollectContext(collectNode.jobId(), collectNode, operation, RAM_ACCOUNTING_CONTEXT, cd);
        builder.addSubContext(collectNode.executionNodeId(), jobCollectContext);
        JobExecutionContext context = jobContextService.createContext(builder);
        cd.startProjection(jobCollectContext);
        operation.collect(collectNode, cd, jobCollectContext);
        return cd.result().get();
    }

    @Test
    public void testCollectShardExpressions() throws Exception {
        CollectNode collectNode = new CollectNode(0, "shardCollect", shardRouting(0, 1));
        collectNode.jobId(UUID.randomUUID());
        collectNode.toCollect(Arrays.<Symbol>asList(testShardIdReference));
        collectNode.maxRowGranularity(RowGranularity.SHARD);

        Bucket result = getBucket(collectNode);
        assertThat(result.size(), is(2));
        assertThat(result, containsInAnyOrder(isRow(0), isRow(1)));
    }

    @Test
    public void testCollectShardExpressionsWhereShardIdIs0() throws Exception {
        EqOperator op = (EqOperator) functions.get(new FunctionIdent(
                EqOperator.NAME, ImmutableList.<DataType>of(DataTypes.INTEGER, DataTypes.INTEGER)));

        CollectNode collectNode = new CollectNode(0, "shardCollect", shardRouting(0, 1));
        collectNode.jobId(UUID.randomUUID());
        collectNode.toCollect(Arrays.<Symbol>asList(testShardIdReference));
        collectNode.whereClause(new WhereClause(
                new Function(op.info(), Arrays.asList(testShardIdReference, Literal.newLiteral(0)))));
        collectNode.maxRowGranularity(RowGranularity.SHARD);
        Bucket result = getBucket(collectNode);
        assertThat(result, contains(isRow(0)));
    }

    @Test
    public void testCollectShardExpressionsLiteralsAndNodeExpressions() throws Exception {
        CollectNode collectNode = new CollectNode(0, "shardCollect", shardRouting(0, 1));
        collectNode.jobId(UUID.randomUUID());
        collectNode.toCollect(Arrays.asList(testShardIdReference, Literal.newLiteral(true), testNodeReference));
        collectNode.maxRowGranularity(RowGranularity.SHARD);
        Bucket result = getBucket(collectNode);
        assertThat(result.size(), is(2));
        assertThat(result, containsInAnyOrder(isRow(0, true, (short) 1), isRow(1, true, (short) 1)));
    }
}

<code block>


package io.crate.operation.collect;

import com.google.common.collect.ImmutableList;
import com.google.common.util.concurrent.ListenableFuture;
import io.crate.action.job.ContextPreparer;
import io.crate.analyze.WhereClause;
import io.crate.core.collections.Bucket;
import io.crate.core.collections.Row;
import io.crate.integrationtests.SQLTransportIntegrationTest;
import io.crate.jobs.JobContextService;
import io.crate.jobs.JobExecutionContext;
import io.crate.metadata.*;
import io.crate.metadata.doc.DocSchemaInfo;
import io.crate.operation.operator.EqOperator;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.symbol.Function;
import io.crate.planner.symbol.Literal;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.elasticsearch.cluster.routing.ShardRouting;
import org.elasticsearch.test.ElasticsearchIntegrationTest;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;

import java.util.*;
import java.util.concurrent.TimeUnit;

import static io.crate.testing.TestingHelpers.isRow;
import static org.hamcrest.Matchers.contains;
import static org.hamcrest.Matchers.containsInAnyOrder;


@ElasticsearchIntegrationTest.ClusterScope(randomDynamicTemplates = false, numDataNodes = 1)
public class DocLevelCollectTest extends SQLTransportIntegrationTest {

    private static final String TEST_TABLE_NAME = "test_table";
    private static final Reference testDocLevelReference = new Reference(
            new ReferenceInfo(
                    new ReferenceIdent(new TableIdent(null, TEST_TABLE_NAME), "doc"),
                    RowGranularity.DOC,
                    DataTypes.INTEGER
            )
    );
    private static final Reference underscoreIdReference = new Reference(
            new ReferenceInfo(
                    new ReferenceIdent(new TableIdent(null, TEST_TABLE_NAME), "_id"),
                    RowGranularity.DOC,
                    DataTypes.STRING
            )
    );
    private static final Reference underscoreRawReference = new Reference(
            new ReferenceInfo(
                    new ReferenceIdent(new TableIdent(null, TEST_TABLE_NAME), "_raw"),
                    RowGranularity.DOC,
                    DataTypes.STRING
            )
    );

    private static final String PARTITIONED_TABLE_NAME = "parted_table";

    private MapSideDataCollectOperation operation;
    private Functions functions;
    private DocSchemaInfo docSchemaInfo;

    @Before
    public void prepare() {
        operation = internalCluster().getDataNodeInstance(MapSideDataCollectOperation.class);
        functions = internalCluster().getDataNodeInstance(Functions.class);
        docSchemaInfo = internalCluster().getDataNodeInstance(DocSchemaInfo.class);

        execute(String.format(Locale.ENGLISH, "create table %s (" +
                "  id integer," +
                "  name string," +
                "  date timestamp" +
                ") clustered into 2 shards partitioned by (date) with(number_of_replicas=0)", PARTITIONED_TABLE_NAME));
        ensureGreen();
        execute(String.format("insert into %s (id, name, date) values (?, ?, ?)",
                PARTITIONED_TABLE_NAME),
                new Object[]{1, "Ford", 0L});
        execute(String.format("insert into %s (id, name, date) values (?, ?, ?)",
                PARTITIONED_TABLE_NAME),
                new Object[]{2, "Trillian", 1L});
        ensureGreen();
        refresh();

        execute(String.format(Locale.ENGLISH, "create table %s (" +
                " id integer primary key," +
                " doc integer" +
                ") clustered into 2 shards with(number_of_replicas=0)", TEST_TABLE_NAME));
        ensureGreen();
        execute(String.format("insert into %s (id, doc) values (?, ?)", TEST_TABLE_NAME), new Object[]{1, 2});
        execute(String.format("insert into %s (id, doc) values (?, ?)", TEST_TABLE_NAME), new Object[]{3, 4});
        refresh();
    }

    @After
    public void cleanUp() {
        operation = null;
        functions = null;
        docSchemaInfo = null;
    }

    private Routing routing(String table) {
        Map<String, Map<String, List<Integer>>> locations = new TreeMap<>();

        for (final ShardRouting shardRouting : clusterService().state().routingTable().allShards(table)) {
            Map<String, List<Integer>> shardIds = locations.get(shardRouting.currentNodeId());
            if (shardIds == null) {
                shardIds = new TreeMap<>();
                locations.put(shardRouting.currentNodeId(), shardIds);
            }

            List<Integer> shardIdSet = shardIds.get(shardRouting.index());
            if (shardIdSet == null) {
                shardIdSet = new ArrayList<>();
                shardIds.put(shardRouting.index(), shardIdSet);
            }
            shardIdSet.add(shardRouting.id());
        }
        return new Routing(locations);
    }

    @Test
    public void testCollectDocLevel() throws Exception {
        CollectNode collectNode = new CollectNode(0, "docCollect", routing(TEST_TABLE_NAME));
        collectNode.toCollect(Arrays.<Symbol>asList(testDocLevelReference, underscoreRawReference, underscoreIdReference));
        collectNode.maxRowGranularity(RowGranularity.DOC);
        collectNode.jobId(UUID.randomUUID());
        PlanNodeBuilder.setOutputTypes(collectNode);
        Bucket result = collect(collectNode);
        assertThat(result, containsInAnyOrder(
                isRow(2, "{\"id\":1,\"doc\":2}", "1"),
                isRow(4, "{\"id\":3,\"doc\":4}", "3")
        ));
    }

    @Test
    public void testCollectDocLevelWhereClause() throws Exception {
        EqOperator op = (EqOperator) functions.get(new FunctionIdent(EqOperator.NAME,
                ImmutableList.<DataType>of(DataTypes.INTEGER, DataTypes.INTEGER)));
        CollectNode collectNode = new CollectNode(0, "docCollect", routing(TEST_TABLE_NAME));
        collectNode.jobId(UUID.randomUUID());
        collectNode.toCollect(Arrays.<Symbol>asList(testDocLevelReference));
        collectNode.maxRowGranularity(RowGranularity.DOC);
        collectNode.whereClause(new WhereClause(new Function(
                op.info(),
                Arrays.<Symbol>asList(testDocLevelReference, Literal.newLiteral(2)))
        ));
        PlanNodeBuilder.setOutputTypes(collectNode);

        Bucket result = collect(collectNode);
        assertThat(result, contains(isRow(2)));
    }


    @Test
    public void testCollectWithPartitionedColumns() throws Exception {
        Routing routing = docSchemaInfo.getTableInfo(PARTITIONED_TABLE_NAME).getRouting(WhereClause.MATCH_ALL);
        TableIdent tableIdent = new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, PARTITIONED_TABLE_NAME);
        CollectNode collectNode = new CollectNode(0, "docCollect", routing);
        collectNode.toCollect(Arrays.<Symbol>asList(
                new Reference(new ReferenceInfo(
                        new ReferenceIdent(tableIdent, "id"),
                        RowGranularity.DOC, DataTypes.INTEGER)),
                new Reference(new ReferenceInfo(
                        new ReferenceIdent(tableIdent, "date"),
                        RowGranularity.SHARD, DataTypes.TIMESTAMP))
        ));
        collectNode.maxRowGranularity(RowGranularity.DOC);
        collectNode.isPartitioned(true);
        collectNode.jobId(UUID.randomUUID());
        PlanNodeBuilder.setOutputTypes(collectNode);

        Bucket result = collect(collectNode);
        for (Row row : result) {
            System.out.println("Row:" + Arrays.toString(row.materialize()));
        }

        assertThat(result, containsInAnyOrder(
                isRow(1, 0L),
                isRow(2, 1L)
        ));
    }

    private Bucket collect(CollectNode collectNode) throws Exception {
        ContextPreparer contextPreparer = internalCluster().getDataNodeInstance(ContextPreparer.class);
        JobContextService contextService = internalCluster().getDataNodeInstance(JobContextService.class);
        JobExecutionContext.Builder builder = contextService.newBuilder(collectNode.jobId());
        ListenableFuture<Bucket> future = contextPreparer.prepare(collectNode.jobId(), collectNode, builder);
        assert future != null;
        JobExecutionContext context = contextService.createContext(builder);
        context.start();
        return future.get(500, TimeUnit.MILLISECONDS);
    }
}

<code block>


package io.crate.operation.collect;

import com.google.common.collect.ImmutableMap;
import io.crate.core.collections.TreeMapBuilder;
import io.crate.executor.transport.TransportActionProvider;
import io.crate.jobs.ExecutionState;
import io.crate.metadata.*;
import io.crate.operation.reference.sys.node.NodeSysExpression;
import io.crate.testing.CollectingProjector;
import io.crate.operation.projectors.ResultProvider;
import io.crate.operation.projectors.ResultProviderFactory;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.node.dql.FileUriCollectNode;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.Literal;
import io.crate.planner.symbol.Symbol;
import io.crate.types.DataTypes;
import org.elasticsearch.action.bulk.BulkRetryCoordinatorPool;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.cluster.ClusterState;
import org.elasticsearch.cluster.node.DiscoveryNode;
import org.elasticsearch.cluster.node.DiscoveryNodes;
import org.elasticsearch.common.settings.ImmutableSettings;
import org.elasticsearch.discovery.DiscoveryService;
import org.elasticsearch.indices.IndicesService;
import org.elasticsearch.node.settings.NodeSettingsService;
import org.elasticsearch.threadpool.ThreadPool;
import org.junit.Rule;
import org.junit.Test;
import org.junit.rules.TemporaryFolder;
import org.mockito.Answers;

import java.io.File;
import java.io.FileWriter;
import java.nio.file.Paths;
import java.util.*;

import static io.crate.testing.TestingHelpers.createReference;
import static io.crate.testing.TestingHelpers.isRow;
import static org.hamcrest.Matchers.contains;
import static org.junit.Assert.assertThat;
import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;

public class MapSideDataCollectOperationTest {

    @Rule
    public TemporaryFolder temporaryFolder = new TemporaryFolder();

    @Test
    public void testFileUriCollect() throws Exception {
        ClusterService clusterService = mock(ClusterService.class);
        DiscoveryNode discoveryNode = mock(DiscoveryNode.class);
        when(discoveryNode.id()).thenReturn("dummyNodeId");
        DiscoveryNodes discoveryNodes = mock(DiscoveryNodes.class);
        when(discoveryNodes.localNodeId()).thenReturn("dummyNodeId");
        ClusterState clusterState = mock(ClusterState.class);
        when(clusterState.nodes()).thenReturn(discoveryNodes);
        when(clusterService.state()).thenReturn(clusterState);
        DiscoveryService discoveryService = mock(DiscoveryService.class);
        when(discoveryService.localNode()).thenReturn(discoveryNode);
        IndicesService indicesService = mock(IndicesService.class);
        Functions functions = new Functions(
                ImmutableMap.<FunctionIdent, FunctionImplementation>of(),
                ImmutableMap.<String, DynamicFunctionResolver>of());
        ReferenceResolver referenceResolver = new ReferenceResolver() {
            @Override
            public ReferenceImplementation getImplementation(ReferenceIdent ident) {
                return null;
            }
        };

        NodeSettingsService nodeSettingsService = mock(NodeSettingsService.class);

        MapSideDataCollectOperation collectOperation = new MapSideDataCollectOperation(
                clusterService,
                ImmutableSettings.EMPTY,
                mock(TransportActionProvider.class, Answers.RETURNS_DEEP_STUBS.get()),
                mock(BulkRetryCoordinatorPool.class),
                functions,
                referenceResolver,
                mock(NodeSysExpression.class),
                indicesService,
                new ThreadPool(ImmutableSettings.builder().put("name", getClass().getName()).build(), null),
                new CollectServiceResolver(discoveryService,
                        new SystemCollectService(
                                discoveryService,
                                functions,
                                new StatsTables(ImmutableSettings.EMPTY, nodeSettingsService)
                        )
                ),
                new ResultProviderFactory() {
                    @Override
                    public ResultProvider createDownstream(ExecutionNode node, UUID jobId) {
                        return new CollectingProjector();
                    }
                },
                mock(InformationSchemaCollectService.class),
                mock(UnassignedShardsCollectService.class)
        );

        File tmpFile = temporaryFolder.newFile("fileUriCollectOperation.json");
        try (FileWriter writer = new FileWriter(tmpFile)) {
            writer.write("{\"name\": \"Arthur\", \"id\": 4, \"details\": {\"age\": 38}}\n");
            writer.write("{\"id\": 5, \"name\": \"Trillian\", \"details\": {\"age\": 33}}\n");
        }

        Routing routing = new Routing(
                TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder()
                .put("dummyNodeId", new TreeMap<String, List<Integer>>())
                .map()
        );
        FileUriCollectNode collectNode = new FileUriCollectNode(
                0,
                "test",
                routing,
                Literal.newLiteral(Paths.get(tmpFile.toURI()).toUri().toString()),
                Arrays.<Symbol>asList(
                        createReference("name", DataTypes.STRING),
                        createReference(new ColumnIdent("details", "age"), DataTypes.INTEGER)
                ),
                Arrays.<Projection>asList(),
                null,
                false
        );
        collectNode.jobId(UUID.randomUUID());
        PlanNodeBuilder.setOutputTypes(collectNode);
        CollectingProjector cd = new CollectingProjector();
        cd.startProjection(mock(ExecutionState.class));
        collectOperation.collect(collectNode, cd, mock(JobCollectContext.class));
        assertThat(cd.result().get(), contains(
                isRow("Arthur", 38),
                isRow("Trillian", 33)
        ));
    }
}

<code block>


package io.crate.operation.collect;

import com.google.common.collect.ImmutableList;
import io.crate.analyze.WhereClause;
import io.crate.core.collections.Bucket;
import io.crate.core.collections.TreeMapBuilder;
import io.crate.integrationtests.SQLTransportIntegrationTest;
import io.crate.jobs.ExecutionState;
import io.crate.metadata.*;
import io.crate.metadata.information.InformationSchemaInfo;
import io.crate.metadata.sys.SysClusterTableInfo;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.operator.EqOperator;
import io.crate.testing.CollectingProjector;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.symbol.Function;
import io.crate.planner.symbol.Literal;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.testing.TestingHelpers;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.test.ElasticsearchIntegrationTest;
import org.hamcrest.Matchers;
import org.junit.Before;
import org.junit.Test;

import java.util.*;

import static org.hamcrest.core.Is.is;
import static org.mockito.Mockito.mock;

@ElasticsearchIntegrationTest.ClusterScope(numDataNodes = 1)
public class HandlerSideLevelCollectTest extends SQLTransportIntegrationTest {

    private MapSideDataCollectOperation operation;
    private Functions functions;
    private String localNodeId;


    @Before
    public void prepare() {
        operation = internalCluster().getDataNodeInstance(MapSideDataCollectOperation.class);
        functions = internalCluster().getInstance(Functions.class);
        localNodeId = internalCluster().getDataNodeInstance(ClusterService.class).state().nodes().localNodeId();
    }

    @Test
    public void testClusterLevel() throws Exception {
        Routing routing = SysClusterTableInfo.ROUTING;
        CollectNode collectNode = new CollectNode(0, "clusterCollect", routing);
        collectNode.jobId(UUID.randomUUID());
        Reference clusterNameRef = new Reference(SysClusterTableInfo.INFOS.get(new ColumnIdent("name")));
        collectNode.toCollect(Arrays.<Symbol>asList(clusterNameRef));
        collectNode.maxRowGranularity(RowGranularity.CLUSTER);
        collectNode.handlerSideCollect(localNodeId);
        Bucket result = collect(collectNode);
        assertThat(result.size(), is(1));
        assertThat(((BytesRef) result.iterator().next().get(0)).utf8ToString(), Matchers.startsWith("SUITE-"));
    }

    private Bucket collect(CollectNode collectNode) throws InterruptedException, java.util.concurrent.ExecutionException {
        CollectingProjector collectingProjector = new CollectingProjector();
        collectingProjector.startProjection(mock(ExecutionState.class));
        operation.collect(collectNode, collectingProjector, mock(JobCollectContext.class));
        return collectingProjector.result().get();
    }

    @Test
    public void testInformationSchemaTables() throws Exception {
        Routing routing = new Routing(TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder().put(
                TableInfo.NULL_NODE_ID, TreeMapBuilder.<String, List<Integer>>newMapBuilder().put("information_schema.tables", null).map()
        ).map());
        CollectNode collectNode = new CollectNode(0, "tablesCollect", routing);
        collectNode.jobId(UUID.randomUUID());
        InformationSchemaInfo schemaInfo =  internalCluster().getInstance(InformationSchemaInfo.class);
        TableInfo tablesTableInfo = schemaInfo.getTableInfo("tables");
        List<Symbol> toCollect = new ArrayList<>();
        for (ReferenceInfo info : tablesTableInfo.columns()) {
            toCollect.add(new Reference(info));
        }
        Symbol tableNameRef = toCollect.get(1);

        FunctionImplementation eqImpl = functions.get(new FunctionIdent(EqOperator.NAME,
                ImmutableList.<DataType>of(DataTypes.STRING, DataTypes.STRING)));
        Function whereClause = new Function(eqImpl.info(),
                Arrays.asList(tableNameRef, Literal.newLiteral("shards")));

        collectNode.whereClause(new WhereClause(whereClause));
        collectNode.toCollect(toCollect);
        collectNode.maxRowGranularity(RowGranularity.DOC);
        collectNode.handlerSideCollect(localNodeId);
        Bucket result = collect(collectNode);
        System.out.println(TestingHelpers.printedTable(result));
        assertEquals("sys| shards| 1| 0| NULL| NULL| NULL| NULL\n", TestingHelpers.printedTable(result));
    }


    @Test
    public void testInformationSchemaColumns() throws Exception {
        Routing routing = new Routing(TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder().put(
                TableInfo.NULL_NODE_ID, TreeMapBuilder.<String, List<Integer>>newMapBuilder().put("information_schema.columns", null).map()
        ).map());
        CollectNode collectNode = new CollectNode(0, "columnsCollect", routing);
        collectNode.jobId(UUID.randomUUID());
        InformationSchemaInfo schemaInfo =  internalCluster().getInstance(InformationSchemaInfo.class);
        TableInfo tableInfo = schemaInfo.getTableInfo("columns");
        List<Symbol> toCollect = new ArrayList<>();
        for (ReferenceInfo info : tableInfo.columns()) {
            toCollect.add(new Reference(info));
        }
        collectNode.toCollect(toCollect);
        collectNode.maxRowGranularity(RowGranularity.DOC);
        collectNode.handlerSideCollect(localNodeId);
        Bucket result = collect(collectNode);


        String expected = "sys| cluster| id| 1| string\n" +
                "sys| cluster| name| 2| string\n" +
                "sys| cluster| master_node| 3| string\n" +
                "sys| cluster| settings| 4| object";


        assertTrue(TestingHelpers.printedTable(result).contains(expected));


        System.out.println(TestingHelpers.printedTable(result));
        result = collect(collectNode);
        assertTrue(TestingHelpers.printedTable(result).contains(expected));
    }

}

<code block>


package io.crate.executor.transport;

import com.google.common.collect.ImmutableList;
import io.crate.analyze.QuerySpec;
import io.crate.analyze.WhereClause;
import io.crate.analyze.where.DocKeys;
import io.crate.integrationtests.SQLTransportIntegrationTest;
import io.crate.integrationtests.Setup;
import io.crate.metadata.ColumnIdent;
import io.crate.metadata.ReferenceIdent;
import io.crate.metadata.ReferenceInfo;
import io.crate.metadata.TableIdent;
import io.crate.metadata.doc.DocSchemaInfo;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.dql.ESGetNode;
import io.crate.planner.symbol.Literal;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.testing.TestingHelpers;
import io.crate.types.DataTypes;
import org.junit.After;
import org.junit.Before;

import java.util.ArrayList;
import java.util.List;

public class BaseTransportExecutorTest extends SQLTransportIntegrationTest {

    Setup setup = new Setup(sqlExecutor);

    static {
        ClassLoader.getSystemClassLoader().setDefaultAssertionStatus(true);
    }

    TransportExecutor executor;
    DocSchemaInfo docSchemaInfo;

    TableIdent charactersIdent = new TableIdent(null, "characters");
    TableIdent booksIdent = new TableIdent(null, "books");

    Reference idRef = new Reference(new ReferenceInfo(
            new ReferenceIdent(charactersIdent, "id"), RowGranularity.DOC, DataTypes.INTEGER));
    Reference nameRef = new Reference(new ReferenceInfo(
            new ReferenceIdent(charactersIdent, "name"), RowGranularity.DOC, DataTypes.STRING));
    Reference femaleRef = TestingHelpers.createReference(charactersIdent.name(), new ColumnIdent("female"), DataTypes.BOOLEAN);

    TableIdent partedTable = new TableIdent("doc", "parted");
    Reference partedIdRef = new Reference(new ReferenceInfo(
            new ReferenceIdent(partedTable, "id"), RowGranularity.DOC, DataTypes.INTEGER));
    Reference partedNameRef = new Reference(new ReferenceInfo(
            new ReferenceIdent(partedTable, "name"), RowGranularity.DOC, DataTypes.STRING));
    Reference partedDateRef = new Reference(new ReferenceInfo(
            new ReferenceIdent(partedTable, "date"), RowGranularity.PARTITION, DataTypes.TIMESTAMP));

    public static ESGetNode newGetNode(TableInfo tableInfo, List<Symbol> outputs, List<String> singleStringKeys, int executionNodeId) {
        QuerySpec querySpec = new QuerySpec();
        querySpec.outputs(outputs);
        List<List<Symbol>> keys = new ArrayList<>(singleStringKeys.size());
        for (String v : singleStringKeys) {
            keys.add(ImmutableList.<Symbol>of(Literal.newLiteral(v)));
        }
        WhereClause whereClause = new WhereClause(null, new DocKeys(keys, false, -1, null), null);
        querySpec.where(whereClause);
        return new ESGetNode(executionNodeId, tableInfo, querySpec);
    }

    @Before
    public void transportSetUp() {
        executor = internalCluster().getInstance(TransportExecutor.class);
        docSchemaInfo = internalCluster().getInstance(DocSchemaInfo.class);
    }

    @After
    public void transportTearDown() {
        executor = null;
        docSchemaInfo = null;
    }
}

<code block>


package io.crate.executor.transport;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.Lists;
import com.google.common.util.concurrent.ListenableFuture;
import io.crate.Constants;
import io.crate.analyze.OrderBy;
import io.crate.analyze.WhereClause;
import io.crate.core.collections.Bucket;
import io.crate.executor.Job;
import io.crate.executor.RowCountResult;
import io.crate.executor.Task;
import io.crate.executor.TaskResult;
import io.crate.executor.transport.task.KillTask;
import io.crate.executor.transport.task.SymbolBasedUpsertByIdTask;
import io.crate.executor.transport.task.elasticsearch.ESDeleteByQueryTask;
import io.crate.executor.transport.task.elasticsearch.ESGetTask;
import io.crate.metadata.*;
import io.crate.metadata.doc.DocSysColumns;
import io.crate.metadata.doc.DocTableInfo;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.aggregation.impl.CountAggregation;
import io.crate.operation.operator.EqOperator;
import io.crate.operation.projectors.TopN;
import io.crate.operation.scalar.DateTruncFunction;
import io.crate.planner.*;
import io.crate.planner.node.dml.ESDeleteByQueryNode;
import io.crate.planner.node.dml.SymbolBasedUpsertByIdNode;
import io.crate.planner.node.dml.Upsert;
import io.crate.planner.node.dql.*;
import io.crate.planner.node.management.KillPlan;
import io.crate.planner.projection.*;
import io.crate.planner.symbol.*;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.cluster.routing.operation.plain.Preference;
import org.elasticsearch.common.collect.MapBuilder;
import org.elasticsearch.search.SearchHits;
import org.junit.Before;
import org.junit.Rule;
import org.junit.Test;
import org.junit.rules.ExpectedException;

import java.util.*;

import static io.crate.testing.TestingHelpers.isRow;
import static java.util.Arrays.asList;
import static org.hamcrest.Matchers.*;
import static org.hamcrest.core.Is.is;

public class TransportExecutorTest extends BaseTransportExecutorTest {

    @Rule
    public ExpectedException expectedException = ExpectedException.none();

    @Before
    public void setup() {
    }

    protected ESGetNode newGetNode(String tableName, List<Symbol> outputs, String singleStringKey, int executionNodeId) {
        return newGetNode(tableName, outputs, Collections.singletonList(singleStringKey), executionNodeId);
    }

    protected ESGetNode newGetNode(String tableName, List<Symbol> outputs, List<String> singleStringKeys, int executionNodeId) {
        return newGetNode(docSchemaInfo.getTableInfo(tableName), outputs, singleStringKeys, executionNodeId);
    }

    @Test
    public void testESGetTask() throws Exception {
        setup.setUpCharacters();


        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef);
        Planner.Context ctx = new Planner.Context(clusterService());
        ESGetNode node = newGetNode("characters", outputs, "2", ctx.nextExecutionNodeId());
        Plan plan = new IterablePlan(node);
        Job job = executor.newJob(plan);


        assertThat(job.tasks().size(), is(1));
        Task task = job.tasks().get(0);
        assertThat(task, instanceOf(ESGetTask.class));


        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, contains(isRow(2, "Ford")));
    }

    @Test
    public void testESGetTaskWithDynamicReference() throws Exception {
        setup.setUpCharacters();

        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, new DynamicReference(
                new ReferenceIdent(new TableIdent(null, "characters"), "foo"), RowGranularity.DOC));
        Planner.Context ctx = new Planner.Context(clusterService());
        ESGetNode node = newGetNode("characters", outputs, "2", ctx.nextExecutionNodeId());
        Plan plan = new IterablePlan(node);
        Job job = executor.newJob(plan);
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, contains(isRow(2, null)));
    }

    @Test
    public void testESMultiGet() throws Exception {
        setup.setUpCharacters();
        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef);
        Planner.Context ctx = new Planner.Context(clusterService());
        ESGetNode node = newGetNode("characters", outputs, asList("1", "2"), ctx.nextExecutionNodeId());
        Plan plan = new IterablePlan(node);
        Job job = executor.newJob(plan);
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();

        assertThat(objects.size(), is(2));
    }

    @Test
    public void testQTFTask() throws Exception {

        setup.setUpCharacters();
        DocTableInfo characters = docSchemaInfo.getTableInfo("characters");
        ReferenceInfo docIdRefInfo = characters.getReferenceInfo(new ColumnIdent(DocSysColumns.DOCID.name()));
        List<Symbol> collectSymbols = Lists.<Symbol>newArrayList(new Reference(docIdRefInfo));
        List<Symbol> outputSymbols = Lists.<Symbol>newArrayList(idRef, nameRef);

        Planner.Context ctx = new Planner.Context(clusterService());

        CollectNode collectNode = PlanNodeBuilder.collect(
                characters,
                ctx,
                WhereClause.MATCH_ALL,
                collectSymbols,
                ImmutableList.<Projection>of(),
                null,
                Constants.DEFAULT_SELECT_LIMIT
        );
        collectNode.keepContextForFetcher(true);

        FetchProjection fetchProjection = getFetchProjection((DocTableInfo) characters, (List<Symbol>) collectSymbols, (List<Symbol>) outputSymbols, (CollectNode) collectNode, ctx);

        MergeNode localMergeNode = PlanNodeBuilder.localMerge(
                ImmutableList.<Projection>of(fetchProjection),
                collectNode,
                ctx);

        Plan plan = new QueryThenFetch(collectNode, localMergeNode);

        Job job = executor.newJob(plan);
        assertThat(job.tasks().size(), is(1));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, containsInAnyOrder(
                isRow(1, "Arthur"),
                isRow(4, "Arthur"),
                isRow(2, "Ford"),
                isRow(3, "Trillian")
        ));
    }

    @Test
    public void testQTFTaskWithFilter() throws Exception {

        setup.setUpCharacters();
        DocTableInfo characters = docSchemaInfo.getTableInfo("characters");
        ReferenceInfo docIdRefInfo = characters.getReferenceInfo(new ColumnIdent(DocSysColumns.DOCID.name()));
        List<Symbol> collectSymbols = Lists.<Symbol>newArrayList(new Reference(docIdRefInfo));
        List<Symbol> outputSymbols = Lists.<Symbol>newArrayList(idRef, nameRef);

        Function whereClause = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.STRING, DataTypes.STRING)),
                DataTypes.BOOLEAN),
                Arrays.<Symbol>asList(nameRef, Literal.newLiteral("Ford")));

        Planner.Context ctx = new Planner.Context(clusterService());

        CollectNode collectNode = PlanNodeBuilder.collect(
                characters,
                ctx,
                new WhereClause(whereClause),
                collectSymbols,
                ImmutableList.<Projection>of(),
                null,
                Constants.DEFAULT_SELECT_LIMIT
        );
        collectNode.keepContextForFetcher(true);

        FetchProjection fetchProjection = getFetchProjection(characters, collectSymbols, outputSymbols, collectNode, ctx);

        MergeNode localMergeNode = PlanNodeBuilder.localMerge(
                ImmutableList.<Projection>of(fetchProjection),
                collectNode,
                ctx);

        Plan plan = new QueryThenFetch(collectNode, localMergeNode);

        Job job = executor.newJob(plan);
        assertThat(job.tasks().size(), is(1));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, contains(isRow(2, "Ford")));
    }

    private FetchProjection getFetchProjection(DocTableInfo characters, List<Symbol> collectSymbols, List<Symbol> outputSymbols, CollectNode collectNode, Planner.Context ctx) {
        return new FetchProjection(
                collectNode.executionNodeId(),
                new InputColumn(0, DataTypes.STRING), collectSymbols, outputSymbols,
                characters.partitionedByColumns(),
                collectNode.executionNodes(),
                5,
                false,
                ctx.jobSearchContextIdToNode(),
                ctx.jobSearchContextIdToShard()
        );
    }

    @Test
    public void testQTFTaskOrdered() throws Exception {

        setup.setUpCharacters();
        DocTableInfo characters = docSchemaInfo.getTableInfo("characters");

        OrderBy orderBy = new OrderBy(Arrays.<Symbol>asList(nameRef, femaleRef),
                new boolean[]{false, false},
                new Boolean[]{false, false});

        ReferenceInfo docIdRefInfo = characters.getReferenceInfo(new ColumnIdent(DocSysColumns.DOCID.name()));

        List<Symbol> collectSymbols = Lists.<Symbol>newArrayList(new Reference(docIdRefInfo), nameRef, femaleRef);
        List<Symbol> outputSymbols = Lists.<Symbol>newArrayList(idRef, nameRef);

        MergeProjection mergeProjection = new MergeProjection(
                collectSymbols,
                orderBy.orderBySymbols(),
                orderBy.reverseFlags(),
                orderBy.nullsFirst()
        );
        Planner.Context ctx = new Planner.Context(clusterService());

        CollectNode collectNode = PlanNodeBuilder.collect(
                characters,
                ctx,
                WhereClause.MATCH_ALL,
                collectSymbols,
                ImmutableList.<Projection>of(),
                orderBy,
                Constants.DEFAULT_SELECT_LIMIT
        );
        collectNode.projections(ImmutableList.<Projection>of(mergeProjection));
        collectNode.keepContextForFetcher(true);

        FetchProjection fetchProjection = getFetchProjection(characters, collectSymbols, outputSymbols, collectNode, ctx);

        MergeNode localMergeNode = PlanNodeBuilder.sortedLocalMerge(
                ImmutableList.<Projection>of(fetchProjection),
                orderBy,
                collectSymbols,
                null,
                collectNode,
                ctx);

        Plan plan = new QueryThenFetch(collectNode, localMergeNode);

        Job job = executor.newJob(plan);
        assertThat(job.tasks().size(), is(1));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, contains(
                isRow(1, "Arthur"),
                isRow(4, "Arthur"),
                isRow(2, "Ford"),
                isRow(3, "Trillian")
        ));
    }

    @Test
    public void testQTFTaskWithFunction() throws Exception {

        execute("create table searchf (id int primary key, date timestamp) with (number_of_replicas=0)");
        ensureGreen();
        execute("insert into searchf (id, date) values (1, '1980-01-01'), (2, '1980-01-02')");
        refresh();

        Reference id_ref = new Reference(new ReferenceInfo(
                new ReferenceIdent(
                        new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "searchf"),
                        "id"),
                RowGranularity.DOC,
                DataTypes.INTEGER
        ));
        Reference date_ref = new Reference(new ReferenceInfo(
                new ReferenceIdent(
                        new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "searchf"),
                        "date"),
                RowGranularity.DOC,
                DataTypes.TIMESTAMP
        ));
        Function function = new Function(new FunctionInfo(
                new FunctionIdent(DateTruncFunction.NAME, Arrays.<DataType>asList(DataTypes.STRING, DataTypes.TIMESTAMP)),
                DataTypes.TIMESTAMP
        ), Arrays.asList(Literal.newLiteral("month"), date_ref));
        Function whereClause = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.INTEGER, DataTypes.INTEGER)),
                DataTypes.BOOLEAN),
                Arrays.asList(id_ref, Literal.newLiteral(2))
        );

        DocTableInfo searchf = docSchemaInfo.getTableInfo("searchf");
        ReferenceInfo docIdRefInfo = searchf.getReferenceInfo(new ColumnIdent(DocSysColumns.DOCID.name()));

        Planner.Context ctx = new Planner.Context(clusterService());
        List<Symbol> collectSymbols = ImmutableList.<Symbol>of(new Reference(docIdRefInfo));
        CollectNode collectNode = PlanNodeBuilder.collect(
                searchf,
                ctx,
                new WhereClause(whereClause),
                collectSymbols,
                ImmutableList.<Projection>of(),
                null,
                Constants.DEFAULT_SELECT_LIMIT
        );
        collectNode.keepContextForFetcher(true);

        TopNProjection topN = new TopNProjection(2, TopN.NO_OFFSET);
        topN.outputs(Collections.<Symbol>singletonList(new InputColumn(0)));

        FetchProjection fetchProjection = getFetchProjection(searchf, collectSymbols, Arrays.asList(id_ref, function), collectNode, ctx);

        MergeNode mergeNode = PlanNodeBuilder.localMerge(
                ImmutableList.of(topN, fetchProjection),
                collectNode,
                ctx);
        Plan plan = new QueryThenFetch(collectNode, mergeNode);

        Job job = executor.newJob(plan);
        assertThat(job.tasks().size(), is(1));

        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, contains(isRow(2, 315532800000L)));
    }

    @Test
    public void testQTFTaskPartitioned() throws Exception {
        setup.setUpPartitionedTableWithName();
        DocTableInfo parted = docSchemaInfo.getTableInfo("parted");
        Planner.Context ctx = new Planner.Context(clusterService());

        ReferenceInfo docIdRefInfo = parted.getReferenceInfo(new ColumnIdent(DocSysColumns.DOCID.name()));
        List<Symbol> collectSymbols = Lists.<Symbol>newArrayList(new Reference(docIdRefInfo));
        List<Symbol> outputSymbols =  Arrays.<Symbol>asList(partedIdRef, partedNameRef, partedDateRef);

        CollectNode collectNode = PlanNodeBuilder.collect(
                parted,
                ctx,
                WhereClause.MATCH_ALL,
                collectSymbols,
                ImmutableList.<Projection>of(),
                null,
                Constants.DEFAULT_SELECT_LIMIT
        );
        collectNode.keepContextForFetcher(true);

        FetchProjection fetchProjection = getFetchProjection(parted, collectSymbols, outputSymbols, collectNode, ctx);

        MergeNode localMergeNode = PlanNodeBuilder.localMerge(
                ImmutableList.<Projection>of(fetchProjection),
                collectNode,
                ctx);

        Plan plan = new QueryThenFetch(collectNode, localMergeNode);
        Job job = executor.newJob(plan);

        assertThat(job.tasks().size(), is(1));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, containsInAnyOrder(
                isRow(3, "Ford", 1396388720242L),
                isRow(1, "Trillian", null),
                isRow(2, null, 0L)
        ));
    }

    @Test
    public void testESDeleteByQueryTask() throws Exception {
        setup.setUpCharacters();

        Function whereClause = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.STRING, DataTypes.STRING)),
                DataTypes.BOOLEAN),
                Arrays.<Symbol>asList(idRef, Literal.newLiteral(2)));

        ESDeleteByQueryNode node = new ESDeleteByQueryNode(
                1,
                ImmutableList.of(new String[]{"characters"}),
                ImmutableList.of(new WhereClause(whereClause)));
        Plan plan = new IterablePlan(node);
        Job job = executor.newJob(plan);
        ESDeleteByQueryTask task = (ESDeleteByQueryTask) job.tasks().get(0);

        task.start();
        TaskResult taskResult = task.result().get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(-1L)));


        execute("select * from characters where id = 2");
        assertThat(response.rowCount(), is(0L));
    }

    @Test
    public void testInsertWithSymbolBasedUpsertByIdTask() throws Exception {
        execute("create table characters (id int primary key, name string)");
        ensureGreen();


        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(),
                false,
                false,
                null,
                new Reference[]{idRef, nameRef});
        updateNode.add("characters", "99", "99", null, null, new Object[]{99, new BytesRef("Marvin")});

        Plan plan = new IterablePlan(updateNode);
        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));

        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(1L)));


        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef);
        ESGetNode getNode = newGetNode("characters", outputs, "99", ctx.nextExecutionNodeId());
        plan = new IterablePlan(getNode);
        job = executor.newJob(plan);
        result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();

        assertThat(objects, contains(isRow(99, "Marvin")));
    }

    @Test
    public void testInsertIntoPartitionedTableWithSymbolBasedUpsertByIdTask() throws Exception {
        execute("create table parted (" +
                "  id int, " +
                "  name string, " +
                "  date timestamp" +
                ") partitioned by (date)");
        ensureGreen();


        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(),
                true,
                false,
                null,
                new Reference[]{idRef, nameRef});

        PartitionName partitionName = new PartitionName("parted", Arrays.asList(new BytesRef("13959981214861")));
        updateNode.add(partitionName.stringValue(), "123", "123", null, null, new Object[]{0L, new BytesRef("Trillian")});

        Plan plan = new IterablePlan(updateNode);
        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));

        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket indexResult = taskResult.rows();
        assertThat(indexResult, contains(isRow(1L)));

        refresh();

        assertTrue(
                client().admin().indices().prepareExists(partitionName.stringValue())
                        .execute().actionGet().isExists()
        );
        assertTrue(
                client().admin().indices().prepareAliasesExist("parted")
                        .execute().actionGet().exists()
        );
        SearchHits hits = client().prepareSearch(partitionName.stringValue())
                .setTypes(Constants.DEFAULT_MAPPING_TYPE)
                .addFields("id", "name")
                .setQuery(new MapBuilder<String, Object>()
                                .put("match_all", new HashMap<String, Object>())
                                .map()
                ).execute().actionGet().getHits();
        assertThat(hits.getTotalHits(), is(1L));
        assertThat((Integer) hits.getHits()[0].field("id").getValues().get(0), is(0));
        assertThat((String) hits.getHits()[0].field("name").getValues().get(0), is("Trillian"));
    }

    @Test
    public void testInsertMultiValuesWithSymbolBasedUpsertByIdTask() throws Exception {
        execute("create table characters (id int primary key, name string)");
        ensureGreen();


        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(),
                false,
                false,
                null,
                new Reference[]{idRef, nameRef});

        updateNode.add("characters", "99", "99", null, null, new Object[]{99, new BytesRef("Marvin")});
        updateNode.add("characters", "42", "42", null, null, new Object[]{42, new BytesRef("Deep Thought")});

        Plan plan = new IterablePlan(updateNode);
        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));

        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(2L)));


        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef);
        ESGetNode getNode = newGetNode("characters", outputs, Arrays.asList("99", "42"), ctx.nextExecutionNodeId());
        plan = new IterablePlan(getNode);
        job = executor.newJob(plan);
        result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();

        assertThat(objects, contains(
                isRow(99, "Marvin"),
                isRow(42, "Deep Thought")
        ));
    }

    @Test
    public void testUpdateWithSymbolBasedUpsertByIdTask() throws Exception {
        setup.setUpCharacters();


        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(), false, false, new String[]{nameRef.ident().columnIdent().fqn()}, null);
        updateNode.add("characters", "1", "1", new Symbol[]{Literal.newLiteral("Vogon lyric fan")}, null);
        Plan plan = new IterablePlan(updateNode);

        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(1L)));


        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef);
        ESGetNode getNode = newGetNode("characters", outputs, "1", ctx.nextExecutionNodeId());
        plan = new IterablePlan(getNode);
        job = executor.newJob(plan);
        result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();

        assertThat(objects, contains(isRow(1, "Vogon lyric fan")));
    }

    @Test
    public void testInsertOnDuplicateWithSymbolBasedUpsertByIdTask() throws Exception {
        setup.setUpCharacters();

        Object[] missingAssignments = new Object[]{5, new BytesRef("Zaphod Beeblebrox"), false};
        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(),
                false,
                false,
                new String[]{nameRef.ident().columnIdent().fqn()},
                new Reference[]{idRef, nameRef, femaleRef});

        updateNode.add("characters", "5", "5", new Symbol[]{Literal.newLiteral("Zaphod Beeblebrox")}, null, missingAssignments);
        Plan plan = new IterablePlan(updateNode);
        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(1L)));


        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef, femaleRef);
        ESGetNode getNode = newGetNode("characters", outputs, "5", ctx.nextExecutionNodeId());
        plan = new IterablePlan(getNode);
        job = executor.newJob(plan);
        result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();
        assertThat(objects, contains(isRow(5, "Zaphod Beeblebrox", false)));

    }

    @Test
    public void testUpdateOnDuplicateWithSymbolBasedUpsertByIdTask() throws Exception {
        setup.setUpCharacters();

        Object[] missingAssignments = new Object[]{1, new BytesRef("Zaphod Beeblebrox"), true};
        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(),
                false,
                false,
                new String[]{femaleRef.ident().columnIdent().fqn()},
                new Reference[]{idRef, nameRef, femaleRef});
        updateNode.add("characters", "1", "1", new Symbol[]{Literal.newLiteral(true)}, null, missingAssignments);
        Plan plan = new IterablePlan(updateNode);
        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(1L)));


        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef, femaleRef);
        ESGetNode getNode = newGetNode("characters", outputs, "1", ctx.nextExecutionNodeId());
        plan = new IterablePlan(getNode);
        job = executor.newJob(plan);
        result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();

        assertThat(objects, contains(isRow(1, "Arthur", true)));
    }

    @Test
    public void testBulkUpdateByQueryTask() throws Exception {
        setup.setUpCharacters();


        List<Plan> childNodes = new ArrayList<>();
        Planner.Context plannerContext = new Planner.Context(clusterService());

        TableInfo tableInfo = docSchemaInfo.getTableInfo("characters");
        Reference uidReference = new Reference(
                new ReferenceInfo(
                        new ReferenceIdent(tableInfo.ident(), "_uid"),
                        RowGranularity.DOC, DataTypes.STRING));


        Function whereClause1 = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.BOOLEAN, DataTypes.BOOLEAN)),
                DataTypes.BOOLEAN),
                Arrays.<Symbol>asList(femaleRef, Literal.newLiteral(true)));

        UpdateProjection updateProjection = new UpdateProjection(
                new InputColumn(0, DataTypes.STRING),
                new String[]{"name"},
                new Symbol[]{Literal.newLiteral("Zaphod Beeblebrox")},
                null);

        CollectNode collectNode1 = PlanNodeBuilder.collect(
                tableInfo,
                plannerContext,
                new WhereClause(whereClause1),
                ImmutableList.<Symbol>of(uidReference),
                ImmutableList.<Projection>of(updateProjection),
                null,
                Preference.PRIMARY.type()
        );
        MergeNode mergeNode1 = PlanNodeBuilder.localMerge(
                ImmutableList.<Projection>of(CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION), collectNode1,
                plannerContext);
        childNodes.add(new CollectAndMerge(collectNode1, mergeNode1));


        Function whereClause2 = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.BOOLEAN, DataTypes.BOOLEAN)),
                DataTypes.BOOLEAN),
                Arrays.<Symbol>asList(femaleRef, Literal.newLiteral(true)));

        CollectNode collectNode2 = PlanNodeBuilder.collect(
                tableInfo,
                plannerContext,
                new WhereClause(whereClause2),
                ImmutableList.<Symbol>of(uidReference),
                ImmutableList.<Projection>of(updateProjection),
                null,
                Preference.PRIMARY.type()
        );
        MergeNode mergeNode2 = PlanNodeBuilder.localMerge(
                ImmutableList.<Projection>of(CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION), collectNode2,
                plannerContext);
        childNodes.add(new CollectAndMerge(collectNode2, mergeNode2));

        Upsert plan = new Upsert(childNodes);
        Job job = executor.newJob(plan);

        assertThat(job.tasks().size(), is(1));
        assertThat(job.tasks().get(0), instanceOf(ExecutionNodesTask.class));
        List<? extends ListenableFuture<TaskResult>> results = executor.execute(job);
        assertThat(results.size(), is(2));

        for (int i = 0; i < results.size(); i++) {
            TaskResult result = results.get(i).get();
            assertThat(result, instanceOf(RowCountResult.class));

            assertThat(((RowCountResult)result).rowCount(), is(2L));
        }
    }

    @Test
    public void testKillTask() throws Exception {
        Job job = executor.newJob(KillPlan.INSTANCE);
        assertThat(job.tasks(), hasSize(1));
        assertThat(job.tasks().get(0), instanceOf(KillTask.class));

        List<? extends ListenableFuture<TaskResult>> results = executor.execute(job);
        assertThat(results, hasSize(1));
        results.get(0).get();
    }
}

<code block>


package io.crate.executor.transport;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.Sets;
import io.crate.core.collections.TreeMapBuilder;
import io.crate.metadata.Routing;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.node.ExecutionNodeGrouper;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.MergeNode;
import org.hamcrest.Matchers;
import org.junit.Test;

import java.util.*;

import static org.hamcrest.Matchers.is;
import static org.junit.Assert.assertThat;

public class ExecutionNodesTaskTest {


    @Test
    public void testGroupByServer() throws Exception {

        Routing twoNodeRouting = new Routing(TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder()
                .put("node1", TreeMapBuilder.<String, List<Integer>>newMapBuilder().put("t1", Arrays.asList(1, 2)).map())
                .put("node2", TreeMapBuilder.<String, List<Integer>>newMapBuilder().put("t1", Arrays.asList(3, 4)).map())
                .map());

        CollectNode c1 = new CollectNode(1, "c1", twoNodeRouting);

        MergeNode m1 = new MergeNode(2, "merge1", 2);
        m1.executionNodes(Sets.newHashSet("node3", "node4"));

        MergeNode m2 = new MergeNode(3, "merge2", 2);
        m2.executionNodes(Sets.newHashSet("node1", "node3"));

        Map<String, Collection<ExecutionNode>> groupByServer = ExecutionNodeGrouper.groupByServer("node1", ImmutableList.<List<ExecutionNode>>of(ImmutableList.<ExecutionNode>of(c1, m1, m2)));

        assertThat(groupByServer.containsKey("node1"), is(true));
        assertThat(groupByServer.get("node1"), Matchers.<ExecutionNode>containsInAnyOrder(c1, m2));

        assertThat(groupByServer.containsKey("node2"), is(true));
        assertThat(groupByServer.get("node2"), Matchers.<ExecutionNode>containsInAnyOrder(c1));

        assertThat(groupByServer.containsKey("node3"), is(true));
        assertThat(groupByServer.get("node3"), Matchers.<ExecutionNode>containsInAnyOrder(m1, m2));

        assertThat(groupByServer.containsKey("node4"), is(true));
        assertThat(groupByServer.get("node4"), Matchers.<ExecutionNode>containsInAnyOrder(m1));
    }


    @Test
    public void testDetectsHasDirectResponse() throws Exception {
        CollectNode c1 = new CollectNode(1, "c1");
        c1.downstreamNodes(Collections.singletonList("foo"));

        assertThat(ExecutionNodesTask.hasDirectResponse(ImmutableList.<List<ExecutionNode>>of(ImmutableList.<ExecutionNode>of(c1))), is(false));

        CollectNode c2 = new CollectNode(1, "c1");
        c2.downstreamNodes(Collections.singletonList(ExecutionNode.DIRECT_RETURN_DOWNSTREAM_NODE));
        assertThat(ExecutionNodesTask.hasDirectResponse(ImmutableList.<List<ExecutionNode>>of(ImmutableList.<ExecutionNode>of(c2))), is(true));
    }
}
<code block>


package io.crate.executor.transport;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableMap;
import com.google.common.collect.ImmutableSet;
import com.google.common.util.concurrent.Futures;
import com.google.common.util.concurrent.ListenableFuture;
import io.crate.Constants;
import io.crate.core.collections.Bucket;
import io.crate.executor.Job;
import io.crate.executor.TaskResult;
import io.crate.integrationtests.SQLTransportIntegrationTest;
import io.crate.metadata.PartitionName;
import io.crate.metadata.TableIdent;
import io.crate.planner.IterablePlan;
import io.crate.planner.Plan;
import io.crate.planner.node.PlanNode;
import io.crate.planner.node.ddl.CreateTableNode;
import io.crate.planner.node.ddl.ESClusterUpdateSettingsNode;
import io.crate.planner.node.ddl.ESCreateTemplateNode;
import io.crate.planner.node.ddl.ESDeletePartitionNode;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.action.admin.indices.alias.Alias;
import org.elasticsearch.action.admin.indices.exists.indices.IndicesExistsRequest;
import org.elasticsearch.action.admin.indices.template.get.GetIndexTemplatesResponse;
import org.elasticsearch.cluster.metadata.IndexTemplateMetaData;
import org.elasticsearch.cluster.settings.ClusterDynamicSettings;
import org.elasticsearch.cluster.settings.DynamicSettings;
import org.elasticsearch.common.inject.Key;
import org.elasticsearch.common.settings.ImmutableSettings;
import org.elasticsearch.common.settings.Settings;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;

import java.util.Arrays;
import java.util.List;
import java.util.Map;

import static io.crate.testing.TestingHelpers.isRow;
import static org.elasticsearch.common.settings.ImmutableSettings.Builder.EMPTY_SETTINGS;
import static org.hamcrest.Matchers.contains;
import static org.hamcrest.Matchers.is;

public class TransportExecutorDDLTest extends SQLTransportIntegrationTest {

    static {
        ClassLoader.getSystemClassLoader().setDefaultAssertionStatus(true);
    }

    private TransportExecutor executor;

    private final static Map<String, Object> TEST_MAPPING = ImmutableMap.<String, Object>of(
            "properties", ImmutableMap.of(
                    "id", ImmutableMap.builder()
                            .put("type", "integer")
                            .put("store", false)
                            .put("index", "not_analyzed")
                            .put("doc_values", true).build(),
                    "name", ImmutableMap.builder()
                            .put("type", "string")
                            .put("store", false)
                            .put("index", "not_analyzed")
                            .put("doc_values", true).build(),
                    "names", ImmutableMap.builder()
                            .put("type", "string")
                            .put("store", false)
                            .put("index", "not_analyzed")
                            .put("doc_values", false).build()
            ));
    private final static Map<String, Object> TEST_PARTITIONED_MAPPING = ImmutableMap.<String, Object>of(
            "_meta", ImmutableMap.of(
                    "partitioned_by", ImmutableList.of(Arrays.asList("name", "string"))
            ),
            "properties", ImmutableMap.of(
                "id", ImmutableMap.builder()
                    .put("type", "integer")
                    .put("store", false)
                    .put("index", "not_analyzed")
                    .put("doc_values", true).build(),
                "names", ImmutableMap.builder()
                    .put("type", "string")
                    .put("store", false)
                    .put("index", "not_analyzed")
                    .put("doc_values", false).build()
            ));
    private final static Settings TEST_SETTINGS = ImmutableSettings.settingsBuilder()
            .put("number_of_replicas", 0)
            .put("number_of_shards", 2).build();

    @Before
    public void transportSetup() {
        executor = internalCluster().getInstance(TransportExecutor.class);
    }

    @Override
    @After
    public void tearDown() throws Exception {
        super.tearDown();
        client().admin().cluster().prepareUpdateSettings()
                .setPersistentSettingsToRemove(ImmutableSet.of("persistent.level"))
                .setTransientSettingsToRemove(ImmutableSet.of("persistent.level", "transient.uptime"))
                .execute().actionGet();
    }

    @Test
    public void testCreateTableTask() throws Exception {
        CreateTableNode createTableNode = CreateTableNode.createTableNode(
                new TableIdent(null, "test"),
                false,
                TEST_SETTINGS,
                TEST_MAPPING
        );
        Plan plan = new IterablePlan(createTableNode);

        Job job = executor.newJob(plan);
        List<? extends ListenableFuture<TaskResult>> futures = executor.execute(job);
        ListenableFuture<List<TaskResult>> listenableFuture = Futures.allAsList(futures);
        Bucket rows = listenableFuture.get().get(0).rows();
        assertThat(rows, contains(isRow(1L)));
        execute("select * from information_schema.tables where table_name = 'test' and number_of_replicas = 0 and number_of_shards = 2");
        assertThat(response.rowCount(), is(1L));

        execute("select count(*) from information_schema.columns where table_name = 'test'");
        assertThat((Long)response.rows()[0][0], is(3L));
    }

    @Test
    public void testCreateTableWithOrphanedPartitions() throws Exception {
        String partitionName = new PartitionName("test", Arrays.asList(new BytesRef("foo"))).stringValue();
        client().admin().indices().prepareCreate(partitionName)
                .addMapping(Constants.DEFAULT_MAPPING_TYPE, TEST_PARTITIONED_MAPPING)
                .setSettings(TEST_SETTINGS)
                .execute().actionGet();
        ensureGreen();
        CreateTableNode createTableNode = CreateTableNode.createTableNode(
                new TableIdent(null, "test"),
                false,
                TEST_SETTINGS,
                TEST_MAPPING
        );
        Plan plan = new IterablePlan(createTableNode);

        Job job = executor.newJob(plan);
        List<? extends ListenableFuture<TaskResult>> futures = executor.execute(job);
        ListenableFuture<List<TaskResult>> listenableFuture = Futures.allAsList(futures);
        Bucket objects = listenableFuture.get().get(0).rows();
        assertThat(objects, contains(isRow(1L)));

        execute("select * from information_schema.tables where table_name = 'test' and number_of_replicas = 0 and number_of_shards = 2");
        assertThat(response.rowCount(), is(1L));

        execute("select count(*) from information_schema.columns where table_name = 'test'");
        assertThat((Long)response.rows()[0][0], is(3L));


        assertThat(client().admin().indices().exists(new IndicesExistsRequest(partitionName)).actionGet().isExists(), is(false));
    }

    @Test
    public void testCreateTableWithOrphanedAlias() throws Exception {
        String partitionName = new PartitionName("test", Arrays.asList(new BytesRef("foo"))).stringValue();
        client().admin().indices().prepareCreate(partitionName)
                .addMapping(Constants.DEFAULT_MAPPING_TYPE, TEST_PARTITIONED_MAPPING)
                .setSettings(TEST_SETTINGS)
                .addAlias(new Alias("test"))
                .execute().actionGet();
        ensureGreen();
        CreateTableNode createTableNode = CreateTableNode.createTableNode(
                new TableIdent(null, "test"),
                false,
                TEST_SETTINGS,
                TEST_MAPPING
        );
        Plan plan = new IterablePlan(createTableNode);

        Job job = executor.newJob(plan);
        List<? extends ListenableFuture<TaskResult>> futures = executor.execute(job);
        ListenableFuture<List<TaskResult>> listenableFuture = Futures.allAsList(futures);
        Bucket objects = listenableFuture.get().get(0).rows();
        assertThat(objects, contains(isRow(1L)));

        execute("select * from information_schema.tables where table_name = 'test' and number_of_replicas = 0 and number_of_shards = 2");
        assertThat(response.rowCount(), is(1L));

        execute("select count(*) from information_schema.columns where table_name = 'test'");
        assertThat((Long) response.rows()[0][0], is(3L));


        assertThat(client().admin().cluster().prepareState().execute().actionGet()
                .getState().metaData().aliases().containsKey("test"), is(false));

        assertThat(client().admin().indices().exists(new IndicesExistsRequest(partitionName)).actionGet().isExists(), is(false));
    }

    @Test
    public void testDeletePartitionTask() throws Exception {
        execute("create table t (id integer primary key, name string) partitioned by (id)");
        ensureYellow();

        execute("insert into t (id, name) values (1, 'Ford')");
        assertThat(response.rowCount(), is(1L));
        ensureYellow();

        execute("select * from information_schema.table_partitions where table_name = 't'");
        assertThat(response.rowCount(), is(1L));

        String partitionName = new PartitionName("t", ImmutableList.of(new BytesRef("1"))).stringValue();
        ESDeletePartitionNode deleteIndexNode = new ESDeletePartitionNode(partitionName);
        Plan plan = new IterablePlan(deleteIndexNode);

        Job job = executor.newJob(plan);
        List<? extends ListenableFuture<TaskResult>> futures = executor.execute(job);
        ListenableFuture<List<TaskResult>> listenableFuture = Futures.allAsList(futures);
        Bucket objects = listenableFuture.get().get(0).rows();
        assertThat(objects, contains(isRow(-1L)));

        execute("select * from information_schema.table_partitions where table_name = 't'");
        assertThat(response.rowCount(), is(0L));
    }


    @Test
    public void testDeletePartitionTaskClosed() throws Exception {
        execute("create table t (id integer primary key, name string) partitioned by (id)");
        ensureYellow();

        execute("insert into t (id, name) values (1, 'Ford')");
        assertThat(response.rowCount(), is(1L));
        ensureYellow();

        String partitionName = new PartitionName("t", ImmutableList.of(new BytesRef("1"))).stringValue();
        assertTrue(client().admin().indices().prepareClose(partitionName).execute().actionGet().isAcknowledged());

        ESDeletePartitionNode deleteIndexNode = new ESDeletePartitionNode(partitionName);
        Plan plan = new IterablePlan(deleteIndexNode);

        Job job = executor.newJob(plan);
        List<? extends ListenableFuture<TaskResult>> futures = executor.execute(job);
        ListenableFuture<List<TaskResult>> listenableFuture = Futures.allAsList(futures);
        Bucket objects = listenableFuture.get().get(0).rows();
        assertThat(objects, contains(isRow(-1L)));

        execute("select * from information_schema.table_partitions where table_name = 't'");
        assertThat(response.rowCount(), is(0L));
    }

    @Test
    public void testClusterUpdateSettingsTask() throws Exception {
        final String persistentSetting = "persistent.level";
        final String transientSetting = "transient.uptime";


        Key<DynamicSettings> dynamicSettingsKey = Key.get(DynamicSettings.class, ClusterDynamicSettings.class);
        for (DynamicSettings settings : internalCluster().getInstances(dynamicSettingsKey)) {
            settings.addDynamicSetting(persistentSetting);
            settings.addDynamicSetting(transientSetting);
        }


        Settings persistentSettings = ImmutableSettings.builder()
                .put(persistentSetting, "panic")
                .build();

        ESClusterUpdateSettingsNode node = new ESClusterUpdateSettingsNode(persistentSettings);

        Bucket objects = executePlanNode(node);

        assertThat(objects, contains(isRow(1L)));
        assertEquals("panic", client().admin().cluster().prepareState().execute().actionGet().getState().metaData().persistentSettings().get(persistentSetting));


        Settings transientSettings = ImmutableSettings.builder()
                .put(transientSetting, "123")
                .build();

        node = new ESClusterUpdateSettingsNode(EMPTY_SETTINGS, transientSettings);
        objects = executePlanNode(node);

        assertThat(objects, contains(isRow(1L)));
        assertEquals("123", client().admin().cluster().prepareState().execute().actionGet().getState().metaData().transientSettings().get(transientSetting));


        persistentSettings = ImmutableSettings.builder()
                .put(persistentSetting, "normal")
                .build();
        transientSettings = ImmutableSettings.builder()
                .put(transientSetting, "243")
                .build();

        node = new ESClusterUpdateSettingsNode(persistentSettings, transientSettings);
        objects = executePlanNode(node);

        assertThat(objects, contains(isRow(1L)));
        assertEquals("normal", client().admin().cluster().prepareState().execute().actionGet().getState().metaData().persistentSettings().get(persistentSetting));
        assertEquals("243", client().admin().cluster().prepareState().execute().actionGet().getState().metaData().transientSettings().get(transientSetting));
    }

    private Bucket executePlanNode(PlanNode node) throws InterruptedException, java.util.concurrent.ExecutionException {
        Plan plan = new IterablePlan(node);
        Job job = executor.newJob(plan);
        List<? extends ListenableFuture<TaskResult>> futures = executor.execute(job);
        ListenableFuture<List<TaskResult>> listenableFuture = Futures.allAsList(futures);
        return listenableFuture.get().get(0).rows();
    }

    @Test
    public void testCreateIndexTemplateTask() throws Exception {
        Settings indexSettings = ImmutableSettings.builder()
                .put("number_of_replicas", 0)
                .put("number_of_shards", 2)
                .build();
        Map<String, Object> mapping = ImmutableMap.<String, Object>of(
                "properties", ImmutableMap.of(
                        "id", ImmutableMap.builder()
                                .put("type", "integer")
                                .put("store", false)
                                .put("index", "not_analyzed")
                                .put("doc_values", true).build(),
                        "name", ImmutableMap.builder()
                                .put("type", "string")
                                .put("store", false)
                                .put("index", "not_analyzed")
                                .put("doc_values", true).build(),
                        "names", ImmutableMap.builder()
                                .put("type", "string")
                                .put("store", false)
                                .put("index", "not_analyzed")
                                .put("doc_values", false).build()
                ),
                "_meta", ImmutableMap.of(
                        "partitioned_by", ImmutableList.<List<String>>of(
                                ImmutableList.of("name", "string")
                        )
                )
        );
        String templateName = PartitionName.templateName(null, "partitioned");
        String templatePrefix = PartitionName.templateName(null, "partitioned") + "*";
        final String alias = "aliasName";

        ESCreateTemplateNode planNode = new ESCreateTemplateNode(
                templateName,
                templatePrefix,
                indexSettings,
                mapping,
                alias);

        Bucket objects = executePlanNode(planNode);
        assertThat(objects, contains(isRow(1L)));

        refresh();

        GetIndexTemplatesResponse response = client().admin().indices()
                .prepareGetTemplates(".partitioned.partitioned.").execute().actionGet();

        assertThat(response.getIndexTemplates().size(), is(1));
        IndexTemplateMetaData templateMeta = response.getIndexTemplates().get(0);
        assertThat(templateMeta.getName(), is(".partitioned.partitioned."));
        assertThat(templateMeta.mappings().get(Constants.DEFAULT_MAPPING_TYPE).string(),
                is("{\"default\":" +
                        "{\"properties\":{" +
                        "\"id\":{\"type\":\"integer\",\"store\":false,\"index\":\"not_analyzed\",\"doc_values\":true}," +
                        "\"name\":{\"type\":\"string\",\"store\":false,\"index\":\"not_analyzed\",\"doc_values\":true}," +
                        "\"names\":{\"type\":\"string\",\"store\":false,\"index\":\"not_analyzed\",\"doc_values\":false}" +
                        "}," +
                        "\"_meta\":{" +
                        "\"partitioned_by\":[[\"name\",\"string\"]]" +
                        "}}}"));
        assertThat(templateMeta.template(), is(".partitioned.partitioned.*"));
        assertThat(templateMeta.settings().toDelimitedString(','),
                is("index.number_of_replicas=0,index.number_of_shards=2,"));
        assertThat(templateMeta.aliases().get(alias).alias(), is(alias));
    }
}

<code block>
package io.crate.planner;

import com.google.common.collect.Iterables;
import io.crate.Constants;
import io.crate.analyze.Analyzer;
import io.crate.analyze.BaseAnalyzerTest;
import io.crate.analyze.ParameterContext;
import io.crate.analyze.WhereClause;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.core.collections.TreeMapBuilder;
import io.crate.exceptions.UnsupportedFeatureException;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.*;
import io.crate.metadata.blob.BlobSchemaInfo;
import io.crate.metadata.blob.BlobTableInfo;
import io.crate.metadata.doc.DocSysColumns;
import io.crate.metadata.sys.SysClusterTableInfo;
import io.crate.metadata.sys.SysNodesTableInfo;
import io.crate.metadata.sys.SysSchemaInfo;
import io.crate.metadata.sys.SysShardsTableInfo;
import io.crate.metadata.table.ColumnPolicy;
import io.crate.metadata.table.SchemaInfo;
import io.crate.metadata.table.TableInfo;
import io.crate.metadata.table.TestingTableInfo;
import io.crate.operation.aggregation.impl.AggregationImplModule;
import io.crate.operation.operator.OperatorModule;
import io.crate.operation.predicate.PredicateModule;
import io.crate.operation.projectors.FetchProjector;
import io.crate.operation.scalar.ScalarFunctionModule;
import io.crate.planner.node.PlanNode;
import io.crate.planner.node.ddl.DropTableNode;
import io.crate.planner.node.ddl.ESClusterUpdateSettingsNode;
import io.crate.planner.node.dml.*;
import io.crate.planner.node.dql.*;
import io.crate.planner.node.management.KillPlan;
import io.crate.planner.projection.*;
import io.crate.planner.symbol.*;
import io.crate.sql.parser.SqlParser;
import io.crate.test.integration.CrateUnitTest;
import io.crate.testing.TestingHelpers;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.action.admin.indices.template.put.TransportPutIndexTemplateAction;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.cluster.ClusterState;
import org.elasticsearch.cluster.metadata.IndexTemplateMetaData;
import org.elasticsearch.cluster.metadata.MetaData;
import org.elasticsearch.cluster.node.DiscoveryNode;
import org.elasticsearch.cluster.node.DiscoveryNodes;
import org.elasticsearch.common.collect.ImmutableOpenMap;
import org.elasticsearch.common.inject.Injector;
import org.elasticsearch.common.inject.ModulesBuilder;
import org.elasticsearch.index.shard.ShardId;
import org.elasticsearch.threadpool.ThreadPool;
import org.hamcrest.Matchers;
import org.hamcrest.core.Is;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;

import java.util.*;
import java.util.concurrent.TimeUnit;

import static io.crate.testing.TestingHelpers.*;
import static org.hamcrest.Matchers.*;
import static org.hamcrest.core.Is.is;
import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;

public class PlannerTest extends CrateUnitTest {

    private Analyzer analyzer;
    private Planner planner;
    Routing shardRouting = new Routing(TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder()
            .put("nodeOne", TreeMapBuilder.<String, List<Integer>>newMapBuilder().put("t1", Arrays.asList(1, 2)).map())
            .put("nodeTow", TreeMapBuilder.<String, List<Integer>>newMapBuilder().put("t1", Arrays.asList(3, 4)).map())
            .map());

    Routing nodesRouting = new Routing(TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder()
            .put("nodeOne", TreeMapBuilder.<String, List<Integer>>newMapBuilder().map())
            .put("nodeTow", TreeMapBuilder.<String, List<Integer>>newMapBuilder().map())
            .map());

    final Routing partedRouting = new Routing(TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder()
            .put("nodeOne", TreeMapBuilder.<String, List<Integer>>newMapBuilder().put(".partitioned.parted.04232chj", Arrays.asList(1, 2)).map())
            .put("nodeTow", TreeMapBuilder.<String, List<Integer>>newMapBuilder().map())
            .map());

    final Routing clusteredPartedRouting = new Routing(TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder()
            .put("nodeOne", TreeMapBuilder.<String, List<Integer>>newMapBuilder().put(".partitioned.clustered_parted.04732cpp6ks3ed1o60o30c1g",  Arrays.asList(1, 2)).map())
            .put("nodeTwo", TreeMapBuilder.<String, List<Integer>>newMapBuilder().put(".partitioned.clustered_parted.04732cpp6ksjcc9i60o30c1g",  Arrays.asList(3)).map())
            .map());

    private ClusterService clusterService;

    private final static String LOCAL_NODE_ID = "foo";
    private ThreadPool threadPool;


    @Before
    public void prepare() throws Exception {
        threadPool = TestingHelpers.newMockedThreadPool();
        Injector injector = new ModulesBuilder()
                .add(new TestModule())
                .add(new AggregationImplModule())
                .add(new ScalarFunctionModule())
                .add(new PredicateModule())
                .add(new OperatorModule())
                .createInjector();
        analyzer = injector.getInstance(Analyzer.class);
        planner = injector.getInstance(Planner.class);
    }

    @After
    public void after() throws Exception {
        threadPool.shutdown();
        threadPool.awaitTermination(1, TimeUnit.SECONDS);
    }


    class TestModule extends MetaDataModule {

        @Override
        protected void configure() {
            bind(ThreadPool.class).toInstance(threadPool);
            clusterService = mock(ClusterService.class);
            DiscoveryNode localNode = mock(DiscoveryNode.class);
            when(localNode.id()).thenReturn(LOCAL_NODE_ID);
            when(clusterService.localNode()).thenReturn(localNode);
            ClusterState clusterState = mock(ClusterState.class);
            MetaData metaData = mock(MetaData.class);
            when(metaData.concreteAllOpenIndices()).thenReturn(new String[0]);
            when(metaData.getTemplates()).thenReturn(ImmutableOpenMap.<String, IndexTemplateMetaData>of());
            when(metaData.templates()).thenReturn(ImmutableOpenMap.<String, IndexTemplateMetaData>of());
            when(clusterState.metaData()).thenReturn(metaData);
            DiscoveryNodes nodes = mock(DiscoveryNodes.class);
            DiscoveryNode node = mock(DiscoveryNode.class);
            when(clusterService.state()).thenReturn(clusterState);
            when(clusterState.nodes()).thenReturn(nodes);
            ImmutableOpenMap<String, DiscoveryNode> dataNodes =
                    ImmutableOpenMap.<String, DiscoveryNode>builder().fPut("foo", node).build();
            when(nodes.dataNodes()).thenReturn(dataNodes);
            when(nodes.localNodeId()).thenReturn(LOCAL_NODE_ID);
            FulltextAnalyzerResolver fulltextAnalyzerResolver = mock(FulltextAnalyzerResolver.class);
            bind(FulltextAnalyzerResolver.class).toInstance(fulltextAnalyzerResolver);
            bind(ClusterService.class).toInstance(clusterService);
            bind(TransportPutIndexTemplateAction.class).toInstance(mock(TransportPutIndexTemplateAction.class));
            super.configure();
        }

        @Override
        protected void bindSchemas() {
            super.bindSchemas();
            SchemaInfo schemaInfo = mock(SchemaInfo.class);
            TableIdent userTableIdent = new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "users");
            TableInfo userTableInfo = TestingTableInfo.builder(userTableIdent, RowGranularity.DOC, shardRouting)
                    .add("name", DataTypes.STRING, null)
                    .add("id", DataTypes.LONG, null)
                    .add("date", DataTypes.TIMESTAMP, null)
                    .add("text", DataTypes.STRING, null, ReferenceInfo.IndexType.ANALYZED)
                    .add("no_index", DataTypes.STRING, null, ReferenceInfo.IndexType.NO)
                    .addPrimaryKey("id")
                    .clusteredBy("id")
                    .build();
            when(userTableInfo.schemaInfo().name()).thenReturn(ReferenceInfos.DEFAULT_SCHEMA_NAME);
            TableIdent charactersTableIdent = new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "characters");
            TableInfo charactersTableInfo = TestingTableInfo.builder(charactersTableIdent, RowGranularity.DOC, shardRouting)
                    .add("name", DataTypes.STRING, null)
                    .add("id", DataTypes.STRING, null)
                    .addPrimaryKey("id")
                    .clusteredBy("id")
                    .build();
            when(charactersTableInfo.schemaInfo().name()).thenReturn(ReferenceInfos.DEFAULT_SCHEMA_NAME);
            TableIdent partedTableIdent = new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "parted");
            TableInfo partedTableInfo = TestingTableInfo.builder(partedTableIdent, RowGranularity.DOC, partedRouting)
                    .add("name", DataTypes.STRING, null)
                    .add("id", DataTypes.STRING, null)
                    .add("date", DataTypes.TIMESTAMP, null, true)
                    .addPartitions(
                            new PartitionName("parted", new ArrayList<BytesRef>(){{add(null);}}).stringValue(), 
                            new PartitionName("parted", Arrays.asList(new BytesRef("0"))).stringValue(),
                            new PartitionName("parted", Arrays.asList(new BytesRef("123"))).stringValue()
                    )
                    .addPrimaryKey("id")
                    .addPrimaryKey("date")
                    .clusteredBy("id")
                    .build();
            when(partedTableInfo.schemaInfo().name()).thenReturn(ReferenceInfos.DEFAULT_SCHEMA_NAME);
            TableIdent emptyPartedTableIdent = new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "empty_parted");
            TableInfo emptyPartedTableInfo = TestingTableInfo.builder(partedTableIdent, RowGranularity.DOC, shardRouting)
                    .add("name", DataTypes.STRING, null)
                    .add("id", DataTypes.STRING, null)
                    .add("date", DataTypes.TIMESTAMP, null, true)
                    .addPrimaryKey("id")
                    .addPrimaryKey("date")
                    .clusteredBy("id")
                    .build();
            TableIdent multiplePartitionedTableIdent= new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "multi_parted");
            TableInfo multiplePartitionedTableInfo = new TestingTableInfo.Builder(
                    multiplePartitionedTableIdent, RowGranularity.DOC, new Routing())
                    .add("id", DataTypes.INTEGER, null)
                    .add("date", DataTypes.TIMESTAMP, null, true)
                    .add("num", DataTypes.LONG, null)
                    .add("obj", DataTypes.OBJECT, null, ColumnPolicy.DYNAMIC)
                    .add("obj", DataTypes.STRING, Arrays.asList("name"), true)

                    .addPartitions(
                            new PartitionName("multi_parted", Arrays.asList(new BytesRef("1395874800000"), new BytesRef("0"))).stringValue(),
                            new PartitionName("multi_parted", Arrays.asList(new BytesRef("1395961200000"), new BytesRef("-100"))).stringValue(),
                            new PartitionName("multi_parted", Arrays.asList(null, new BytesRef("-100"))).stringValue())
                    .build();
            TableIdent clusteredByParitionedIdent = new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "clustered_parted");
            TableInfo clusteredByPartitionedTableInfo = new TestingTableInfo.Builder(
                    multiplePartitionedTableIdent, RowGranularity.DOC, clusteredPartedRouting)
                    .add("id", DataTypes.INTEGER, null)
                    .add("date", DataTypes.TIMESTAMP, null, true)
                    .add("city", DataTypes.STRING, null)
                    .clusteredBy("city")
                    .addPartitions(
                            new PartitionName("clustered_parted", Arrays.asList(new BytesRef("1395874800000"))).stringValue(),
                            new PartitionName("clustered_parted", Arrays.asList(new BytesRef("1395961200000"))).stringValue())
                    .build();
            when(emptyPartedTableInfo.schemaInfo().name()).thenReturn(ReferenceInfos.DEFAULT_SCHEMA_NAME);
            when(schemaInfo.getTableInfo(charactersTableIdent.name())).thenReturn(charactersTableInfo);
            when(schemaInfo.getTableInfo(userTableIdent.name())).thenReturn(userTableInfo);
            when(schemaInfo.getTableInfo(partedTableIdent.name())).thenReturn(partedTableInfo);
            when(schemaInfo.getTableInfo(emptyPartedTableIdent.name())).thenReturn(emptyPartedTableInfo);
            when(schemaInfo.getTableInfo(multiplePartitionedTableIdent.name())).thenReturn(multiplePartitionedTableInfo);
            when(schemaInfo.getTableInfo(clusteredByParitionedIdent.name())).thenReturn(clusteredByPartitionedTableInfo);
            when(schemaInfo.getTableInfo(BaseAnalyzerTest.IGNORED_NESTED_TABLE_IDENT.name())).thenReturn(BaseAnalyzerTest.IGNORED_NESTED_TABLE_INFO);
            schemaBinder.addBinding(ReferenceInfos.DEFAULT_SCHEMA_NAME).toInstance(schemaInfo);
            schemaBinder.addBinding(SysSchemaInfo.NAME).toInstance(mockSysSchemaInfo());
            schemaBinder.addBinding(BlobSchemaInfo.NAME).toInstance(mockBlobSchemaInfo());
        }

        private SchemaInfo mockBlobSchemaInfo(){
            BlobSchemaInfo blobSchemaInfo = mock(BlobSchemaInfo.class);
            BlobTableInfo tableInfo = mock(BlobTableInfo.class);
            when(blobSchemaInfo.getTableInfo("screenshots")).thenReturn(tableInfo);
            when(tableInfo.schemaInfo()).thenReturn(blobSchemaInfo);
            return blobSchemaInfo;
        }

        private SchemaInfo mockSysSchemaInfo() {
            SchemaInfo schemaInfo = mock(SchemaInfo.class);
            when(schemaInfo.name()).thenReturn(SysSchemaInfo.NAME);
            when(schemaInfo.systemSchema()).thenReturn(true);

            TableInfo sysClusterTableInfo = TestingTableInfo.builder(
                    SysClusterTableInfo.IDENT,


                    RowGranularity.DOC,
                    SysClusterTableInfo.ROUTING
            ).schemaInfo(schemaInfo).add("name", DataTypes.STRING, null).schemaInfo(schemaInfo).build();
            when(schemaInfo.getTableInfo(sysClusterTableInfo.ident().name())).thenReturn(sysClusterTableInfo);

            TableInfo sysNodesTableInfo = TestingTableInfo.builder(
                    SysNodesTableInfo.IDENT,
                    RowGranularity.NODE,
                    nodesRouting)
                    .schemaInfo(schemaInfo)
                    .add("name", DataTypes.STRING, null).schemaInfo(schemaInfo).build();

            when(schemaInfo.getTableInfo(sysNodesTableInfo.ident().name())).thenReturn(sysNodesTableInfo);

            TableInfo sysShardsTableInfo = TestingTableInfo.builder(
                    SysShardsTableInfo.IDENT,
                    RowGranularity.SHARD,
                    nodesRouting
            ).add("id", DataTypes.INTEGER, null)
             .add("table_name", DataTypes.STRING, null)
             .schemaInfo(schemaInfo).build();
            when(schemaInfo.getTableInfo(sysShardsTableInfo.ident().name())).thenReturn(sysShardsTableInfo);
            when(schemaInfo.systemSchema()).thenReturn(true);
            return schemaInfo;
        }
    }

    private Plan plan(String statement) {
        return planner.plan(analyzer.analyze(SqlParser.createStatement(statement),
                new ParameterContext(new Object[0], new Object[0][], ReferenceInfos.DEFAULT_SCHEMA_NAME)));
    }

    @Test
    public void testGroupByWithAggregationStringLiteralArguments() {
        CollectNode collectNode = ((DistributedGroupBy) plan("select count('foo'), name from users group by name")).collectNode();


        GroupProjection groupProjection = (GroupProjection) collectNode.projections().get(0);
        Aggregation aggregation = groupProjection.values().get(0);
    }

    @Test
    public void testGroupByWithAggregationPlan() throws Exception {
        DistributedGroupBy distributedGroupBy = (DistributedGroupBy) plan(
                "select count(*), name from users group by name");


        CollectNode collectNode = distributedGroupBy.collectNode();
        assertThat(collectNode.hasDistributingDownstreams(), is(true));
        assertThat(collectNode.downstreamNodes().size(), is(2));
        assertThat(collectNode.maxRowGranularity(), is(RowGranularity.DOC));
        assertThat(collectNode.executionNodes().size(), is(2));
        assertThat(collectNode.toCollect().size(), is(1));
        assertThat(collectNode.projections().size(), is(1));
        assertThat(collectNode.projections().get(0), instanceOf(GroupProjection.class));
        assertThat(collectNode.outputTypes().size(), is(2));
        assertEquals(DataTypes.STRING, collectNode.outputTypes().get(0));
        assertEquals(DataTypes.UNDEFINED, collectNode.outputTypes().get(1));

        MergeNode mergeNode = distributedGroupBy.reducerMergeNode();

        assertThat(mergeNode.numUpstreams(), is(2));
        assertThat(mergeNode.executionNodes().size(), is(2));
        assertEquals(mergeNode.inputTypes(), collectNode.outputTypes());
        assertThat(mergeNode.projections().size(), is(2)); 
        assertThat(mergeNode.projections().get(1), instanceOf(TopNProjection.class));

        assertThat(mergeNode.projections().get(0), instanceOf(GroupProjection.class));
        GroupProjection groupProjection = (GroupProjection) mergeNode.projections().get(0);
        InputColumn inputColumn = (InputColumn) groupProjection.values().get(0).inputs().get(0);
        assertThat(inputColumn.index(), is(1));

        assertThat(mergeNode.outputTypes().size(), is(2));
        assertEquals(DataTypes.LONG, mergeNode.outputTypes().get(0));
        assertEquals(DataTypes.STRING, mergeNode.outputTypes().get(1));

        MergeNode localMerge = distributedGroupBy.localMergeNode();

        assertThat(localMerge.numUpstreams(), is(2));
        assertThat(localMerge.executionNodes().size(), is(1));
        assertThat(Iterables.getOnlyElement(localMerge.executionNodes()), is(LOCAL_NODE_ID));
        assertEquals(mergeNode.outputTypes(), localMerge.inputTypes());

        assertThat(localMerge.projections().get(0), instanceOf(TopNProjection.class));
        TopNProjection topN = (TopNProjection) localMerge.projections().get(0);
        assertThat(topN.outputs().size(), is(2));

        assertEquals(DataTypes.LONG, localMerge.outputTypes().get(0));
        assertEquals(DataTypes.STRING, localMerge.outputTypes().get(1));

    }

    @Test
    public void testGetPlan() throws Exception {
        IterablePlan plan = (IterablePlan)  plan("select name from users where id = 1");
        Iterator<PlanNode> iterator = plan.iterator();
        ESGetNode node = (ESGetNode) iterator.next();
        assertThat(node.tableInfo().ident().name(), is("users"));
        assertThat(node.docKeys().getOnlyKey(), isDocKey(1L));
        assertThat(node.outputs().size(), is(1));
    }

    @Test
    public void testGetWithVersion() throws Exception{
        expectedException.expect(VersionInvalidException.class);
        expectedException.expectMessage("\"_version\" column is not valid in the WHERE clause of a SELECT statement");
        plan("select name from users where id = 1 and _version = 1");
    }

    @Test
    public void testGetPlanStringLiteral() throws Exception {
        IterablePlan plan = (IterablePlan) plan("select name from characters where id = 'one'");
        Iterator<PlanNode> iterator = plan.iterator();
        ESGetNode node = (ESGetNode) iterator.next();
        assertThat(node.tableInfo().ident().name(), is("characters"));
        assertThat(node.docKeys().getOnlyKey(), isDocKey("one"));
        assertFalse(iterator.hasNext());
        assertThat(node.outputs().size(), is(1));
    }

    @Test
    public void testGetPlanPartitioned() throws Exception {
        IterablePlan plan = (IterablePlan) plan("select name, date from parted where id = 'one' and date = 0");
        Iterator<PlanNode> iterator = plan.iterator();
        PlanNode node = iterator.next();
        assertThat(node, instanceOf(ESGetNode.class));
        ESGetNode getNode = (ESGetNode) node;
        assertThat(getNode.tableInfo().ident().name(), is("parted"));
        assertThat(getNode.docKeys().getOnlyKey(), isDocKey("one", 0L));


        assertEquals(DataTypes.STRING, getNode.outputTypes().get(0));
        assertEquals(DataTypes.TIMESTAMP, getNode.outputTypes().get(1));
    }

    @Test
    public void testMultiGetPlan() throws Exception {
        IterablePlan plan = (IterablePlan) plan("select name from users where id in (1, 2)");
        Iterator<PlanNode> iterator = plan.iterator();
        ESGetNode node = (ESGetNode) iterator.next();
        assertThat(node.docKeys().size(), is(2));
        assertThat(node.docKeys(), containsInAnyOrder(isDocKey(1L), isDocKey(2L)));
    }

    @Test
    public void testDeletePlan() throws Exception {
        IterablePlan plan = (IterablePlan) plan("delete from users where id = 1");
        Iterator<PlanNode> iterator = plan.iterator();
        ESDeleteNode node = (ESDeleteNode) iterator.next();
        assertThat(node.tableInfo().ident().name(), is("users"));
        assertThat(node.docKeys().size(), is(1));
        assertThat(node.docKeys().get(0), isDocKey(1L));
        assertFalse(iterator.hasNext());
    }

    @Test
    public void testMultiDeletePlan() throws Exception {
        IterablePlan plan = (IterablePlan) plan("delete from users where id in (1, 2)");
        Iterator<PlanNode> iterator = plan.iterator();
        assertThat(iterator.next(), instanceOf(ESDeleteByQueryNode.class));
    }

    @Test
    public void testGroupByWithAggregationAndLimit() throws Exception {
        DistributedGroupBy distributedGroupBy = (DistributedGroupBy) plan(
                "select count(*), name from users group by name limit 1 offset 1");


        MergeNode mergeNode = distributedGroupBy.reducerMergeNode();
        assertThat(mergeNode.projections().get(0), instanceOf(GroupProjection.class));
        assertThat(mergeNode.projections().get(1), instanceOf(TopNProjection.class));



        TopNProjection topN = (TopNProjection) mergeNode.projections().get(1);
        assertThat(topN.limit(), is(2));
        assertThat(topN.offset(), is(0));


        DQLPlanNode dqlPlanNode = distributedGroupBy.localMergeNode();
        assertThat(dqlPlanNode.projections().get(0), instanceOf(TopNProjection.class));
        topN = (TopNProjection) dqlPlanNode.projections().get(0);
        assertThat(topN.limit(), is(1));
        assertThat(topN.offset(), is(1));
        assertThat(topN.outputs().get(0), instanceOf(InputColumn.class));
        assertThat(((InputColumn) topN.outputs().get(0)).index(), is(0));
        assertThat(topN.outputs().get(1), instanceOf(InputColumn.class));
        assertThat(((InputColumn) topN.outputs().get(1)).index(), is(1));
    }

    @Test
    public void testGlobalAggregationPlan() throws Exception {
        GlobalAggregate globalAggregate = (GlobalAggregate) plan("select count(name) from users");
        CollectNode collectNode = globalAggregate.collectNode();

        assertEquals(DataTypes.UNDEFINED, collectNode.outputTypes().get(0));
        assertThat(collectNode.maxRowGranularity(), is(RowGranularity.DOC));
        assertThat(collectNode.projections().size(), is(1));
        assertThat(collectNode.projections().get(0), instanceOf(AggregationProjection.class));

        MergeNode mergeNode = globalAggregate.mergeNode();

        assertEquals(DataTypes.UNDEFINED, mergeNode.inputTypes().get(0));
        assertEquals(DataTypes.LONG, mergeNode.outputTypes().get(0));
    }

    @Test
    public void testGroupByOnNodeLevel() throws Exception {
        NonDistributedGroupBy planNode = (NonDistributedGroupBy) plan(
                "select count(*), name from sys.nodes group by name");
        CollectNode collectNode = planNode.collectNode();
        assertFalse(collectNode.hasDistributingDownstreams());
        assertEquals(DataTypes.STRING, collectNode.outputTypes().get(0));
        assertEquals(DataTypes.UNDEFINED, collectNode.outputTypes().get(1));

        MergeNode mergeNode = planNode.localMergeNode();
        assertThat(mergeNode.numUpstreams(), is(2));
        assertThat(mergeNode.projections().size(), is(2));

        assertEquals(DataTypes.LONG, mergeNode.outputTypes().get(0));
        assertEquals(DataTypes.STRING, mergeNode.outputTypes().get(1));

        GroupProjection groupProjection = (GroupProjection) mergeNode.projections().get(0);
        assertThat(groupProjection.keys().size(), is(1));
        assertThat(((InputColumn) groupProjection.outputs().get(0)).index(), is(0));
        assertThat(groupProjection.outputs().get(1), is(instanceOf(Aggregation.class)));
        assertThat(((Aggregation) groupProjection.outputs().get(1)).functionIdent().name(), is("count"));
        assertThat(((Aggregation) groupProjection.outputs().get(1)).fromStep(), is(Aggregation.Step.PARTIAL));
        assertThat(((Aggregation)groupProjection.outputs().get(1)).toStep(), is(Aggregation.Step.FINAL));

        TopNProjection projection = (TopNProjection) mergeNode.projections().get(1);
        assertThat(((InputColumn) projection.outputs().get(0)).index(), is(1));
        assertThat(((InputColumn) projection.outputs().get(1)).index(), is(0));

    }

    @Test
    public void testShardPlan() throws Exception {
        QueryAndFetch planNode = (QueryAndFetch) plan("select id from sys.shards order by id limit 10");
        CollectNode collectNode = planNode.collectNode();

        assertEquals(DataTypes.INTEGER, collectNode.outputTypes().get(0));
        assertThat(collectNode.maxRowGranularity(), is(RowGranularity.SHARD));

        MergeNode mergeNode = planNode.localMergeNode();

        assertThat(mergeNode.inputTypes().size(), is(1));
        assertEquals(DataTypes.INTEGER, mergeNode.inputTypes().get(0));
        assertThat(mergeNode.outputTypes().size(), is(1));
        assertEquals(DataTypes.INTEGER, mergeNode.outputTypes().get(0));

        assertThat(mergeNode.numUpstreams(), is(2));
    }

    @Test
    public void testQueryThenFetchPlan() throws Exception {
        Plan plan = plan("select name from users where name = 'x' order by id limit 10");
        assertThat(plan, instanceOf(QueryThenFetch.class));
        CollectNode collectNode = ((QueryThenFetch) plan).collectNode();
        assertTrue(collectNode.whereClause().hasQuery());
        assertFalse(collectNode.isPartitioned());

        DQLPlanNode resultNode = ((QueryThenFetch) plan).resultNode();
        assertThat(resultNode.outputTypes().size(), is(1));
        assertEquals(DataTypes.STRING, resultNode.outputTypes().get(0));

        assertThat(resultNode, instanceOf(MergeNode.class));
        MergeNode mergeNode = (MergeNode) resultNode;
        assertTrue(mergeNode.finalProjection().isPresent());

        Projection lastProjection = mergeNode.finalProjection().get();
        assertThat(lastProjection, instanceOf(FetchProjection.class));
        FetchProjection fetchProjection = (FetchProjection) lastProjection;
        assertThat(fetchProjection.outputs().size(), is(1));
        assertThat(fetchProjection.outputs().get(0), isReference("_doc['name']"));
    }

    @Test
    public void testQueryThenFetchPlanNoFetch() throws Exception {


        Plan plan = plan("select name from users where name = 'x' order by name limit 10");
        assertThat(plan, instanceOf(QueryThenFetch.class));
        CollectNode collectNode = ((QueryThenFetch) plan).collectNode();
        assertTrue(collectNode.whereClause().hasQuery());
        assertFalse(collectNode.isPartitioned());

        DQLPlanNode resultNode = ((QueryThenFetch) plan).resultNode();
        assertThat(resultNode.outputTypes().size(), is(1));
        assertEquals(DataTypes.STRING, resultNode.outputTypes().get(0));

        assertThat(resultNode, instanceOf(MergeNode.class));
        MergeNode mergeNode = (MergeNode) resultNode;
        assertTrue(mergeNode.finalProjection().isPresent());

        Projection lastProjection = mergeNode.finalProjection().get();
        assertThat(lastProjection, instanceOf(TopNProjection.class));
        TopNProjection topNProjection = (TopNProjection) lastProjection;
        assertThat(topNProjection.outputs().size(), is(1));
    }

    @Test
    public void testQueryThenFetchPlanDefaultLimit() throws Exception {
        QueryThenFetch plan = (QueryThenFetch)plan("select name from users");
        CollectNode collectNode = plan.collectNode();
        assertThat(collectNode.limit(), is(Constants.DEFAULT_SELECT_LIMIT));

        MergeNode mergeNode = plan.mergeNode();
        assertThat(mergeNode.projections().size(), is(2));
        assertThat(mergeNode.finalProjection().get(), instanceOf(FetchProjection.class));
        TopNProjection topN = (TopNProjection)mergeNode.projections().get(0);
        assertThat(topN.limit(), is(Constants.DEFAULT_SELECT_LIMIT));
        assertThat(topN.offset(), is(0));
        assertNull(topN.orderBy());

        FetchProjection fetchProjection = (FetchProjection)mergeNode.projections().get(1);
        assertThat(fetchProjection.bulkSize(), is(FetchProjector.NO_BULK_REQUESTS));


        plan = (QueryThenFetch)plan("select name from users offset 20");
        collectNode = plan.collectNode();
        assertThat(collectNode.limit(), is(Constants.DEFAULT_SELECT_LIMIT + 20));

        mergeNode = plan.mergeNode();
        assertThat(mergeNode.projections().size(), is(2));
        assertThat(mergeNode.finalProjection().get(), instanceOf(FetchProjection.class));
        topN = (TopNProjection)mergeNode.projections().get(0);
        assertThat(topN.limit(), is(Constants.DEFAULT_SELECT_LIMIT));
        assertThat(topN.offset(), is(20));
        assertNull(topN.orderBy());

        fetchProjection = (FetchProjection)mergeNode.projections().get(1);
        assertThat(fetchProjection.bulkSize(), is(FetchProjector.NO_BULK_REQUESTS));
    }

    @Test
    public void testQueryThenFetchPlanHighLimit() throws Exception {
        QueryThenFetch plan = (QueryThenFetch)plan("select name from users limit 100000");
        CollectNode collectNode = plan.collectNode();
        assertThat(collectNode.limit(), is(100_000));

        MergeNode mergeNode = plan.mergeNode();
        assertThat(mergeNode.projections().size(), is(2));
        assertThat(mergeNode.finalProjection().get(), instanceOf(FetchProjection.class));
        TopNProjection topN = (TopNProjection)mergeNode.projections().get(0);
        assertThat(topN.limit(), is(100_000));
        assertThat(topN.offset(), is(0));
        assertNull(topN.orderBy());

        FetchProjection fetchProjection = (FetchProjection)mergeNode.projections().get(1);
        assertThat(fetchProjection.bulkSize(), is(Constants.DEFAULT_SELECT_LIMIT));


        plan = (QueryThenFetch)plan("select name from users limit 100000 offset 20");
        collectNode = plan.collectNode();
        assertThat(collectNode.limit(), is(100_000 + 20));

        mergeNode = plan.mergeNode();
        assertThat(mergeNode.projections().size(), is(2));
        assertThat(mergeNode.finalProjection().get(), instanceOf(FetchProjection.class));
        topN = (TopNProjection)mergeNode.projections().get(0);
        assertThat(topN.limit(), is(100_000));
        assertThat(topN.offset(), is(20));
        assertNull(topN.orderBy());

        fetchProjection = (FetchProjection)mergeNode.projections().get(1);
        assertThat(fetchProjection.bulkSize(), is(Constants.DEFAULT_SELECT_LIMIT));
    }

    @Test
    public void testQueryThenFetchPlanPartitioned() throws Exception {
        Plan plan = plan("select id, name, date from parted where date > 0 and name = 'x' order by id limit 10");
        assertThat(plan, instanceOf(QueryThenFetch.class));
        CollectNode collectNode = ((QueryThenFetch) plan).collectNode();

        List<String> indices = new ArrayList<>();
        Map<String, Map<String, List<Integer>>> locations = collectNode.routing().locations();
        for (Map.Entry<String, Map<String, List<Integer>>> entry : locations.entrySet()) {
            indices.addAll(entry.getValue().keySet());
        }
        assertThat(indices, Matchers.contains(
                new PartitionName("parted", Arrays.asList(new BytesRef("123"))).stringValue()));

        assertTrue(collectNode.whereClause().hasQuery());
        assertTrue(collectNode.isPartitioned());

        DQLPlanNode resultNode = ((QueryThenFetch) plan).resultNode();
        assertThat(resultNode.outputTypes().size(), is(3));
    }

    @Test
    public void testQueryThenFetchPlanFunction() throws Exception {
        Plan plan = plan("select format('Hi, my name is %s', name), name from users where name = 'x' order by id limit 10");
        assertThat(plan, instanceOf(QueryThenFetch.class));
        CollectNode collectNode = ((QueryThenFetch) plan).collectNode();

        assertTrue(collectNode.whereClause().hasQuery());
        assertFalse(collectNode.isPartitioned());

        DQLPlanNode resultNode = ((QueryThenFetch) plan).resultNode();
        assertThat(resultNode.outputTypes().size(), is(2));
        assertEquals(DataTypes.STRING, resultNode.outputTypes().get(0));
        assertEquals(DataTypes.STRING, resultNode.outputTypes().get(1));

        assertThat(resultNode, instanceOf(MergeNode.class));
        MergeNode mergeNode = (MergeNode) resultNode;
        assertTrue(mergeNode.finalProjection().isPresent());

        Projection lastProjection = mergeNode.finalProjection().get();
        assertThat(lastProjection, instanceOf(FetchProjection.class));
        FetchProjection fetchProjection = (FetchProjection) lastProjection;
        assertThat(fetchProjection.outputs().size(), is(2));
        assertThat(fetchProjection.outputs().get(0), isFunction("format"));
        assertThat(fetchProjection.outputs().get(1), isReference("_doc['name']"));

    }

    @Test
    public void testInsertPlan() throws Exception {
        Upsert plan = (Upsert) plan("insert into users (id, name) values (42, 'Deep Thought')");

        assertThat(plan.nodes().size(), is(1));

        PlanNode next = ((IterablePlan) plan.nodes().get(0)).iterator().next();
        assertThat(next, instanceOf(SymbolBasedUpsertByIdNode.class));

        SymbolBasedUpsertByIdNode updateNode = (SymbolBasedUpsertByIdNode)next;

        assertThat(updateNode.insertColumns().length, is(2));
        Reference idRef = updateNode.insertColumns()[0];
        assertThat(idRef.ident().columnIdent().fqn(), is("id"));
        Reference nameRef = updateNode.insertColumns()[1];
        assertThat(nameRef.ident().columnIdent().fqn(), is("name"));

        assertThat(updateNode.items().size(), is(1));
        SymbolBasedUpsertByIdNode.Item item = updateNode.items().get(0);
        assertThat(item.index(), is("users"));
        assertThat(item.id(), is("42"));
        assertThat(item.routing(), is("42"));

        assertThat(item.insertValues().length, is(2));
        assertThat((Long)item.insertValues()[0], is(42L));
        assertThat((BytesRef) item.insertValues()[1], is(new BytesRef("Deep Thought")));
    }

    @Test
    public void testInsertPlanMultipleValues() throws Exception {
        Upsert plan = (Upsert) plan("insert into users (id, name) values (42, 'Deep Thought'), (99, 'Marvin')");

        assertThat(plan.nodes().size(), is(1));

        PlanNode next = ((IterablePlan) plan.nodes().get(0)).iterator().next();
        assertThat(next, instanceOf(SymbolBasedUpsertByIdNode.class));

        SymbolBasedUpsertByIdNode updateNode = (SymbolBasedUpsertByIdNode)next;

        assertThat(updateNode.insertColumns().length, is(2));
        Reference idRef = updateNode.insertColumns()[0];
        assertThat(idRef.ident().columnIdent().fqn(), is("id"));
        Reference nameRef = updateNode.insertColumns()[1];
        assertThat(nameRef.ident().columnIdent().fqn(), is("name"));

        assertThat(updateNode.items().size(), is(2));

        SymbolBasedUpsertByIdNode.Item item1 = updateNode.items().get(0);
        assertThat(item1.index(), is("users"));
        assertThat(item1.id(), is("42"));
        assertThat(item1.routing(), is("42"));
        assertThat(item1.insertValues().length, is(2));
        assertThat((Long)item1.insertValues()[0], is(42L));
        assertThat((BytesRef)item1.insertValues()[1], is(new BytesRef("Deep Thought")));

        SymbolBasedUpsertByIdNode.Item item2 = updateNode.items().get(1);
        assertThat(item2.index(), is("users"));
        assertThat(item2.id(), is("99"));
        assertThat(item2.routing(), is("99"));
        assertThat(item2.insertValues().length, is(2));
        assertThat((Long)item2.insertValues()[0], is(99L));
        assertThat((BytesRef) item2.insertValues()[1], is(new BytesRef("Marvin")));
    }

    @Test
    public void testCountDistinctPlan() throws Exception {
        GlobalAggregate globalAggregate = (GlobalAggregate) plan("select count(distinct name) from users");

        CollectNode collectNode = globalAggregate.collectNode();
        Projection projection = collectNode.projections().get(0);
        assertThat(projection, instanceOf(AggregationProjection.class));
        AggregationProjection aggregationProjection = (AggregationProjection)projection;
        assertThat(aggregationProjection.aggregations().size(), is(1));

        Aggregation aggregation = aggregationProjection.aggregations().get(0);
        assertThat(aggregation.toStep(), is(Aggregation.Step.PARTIAL));
        Symbol aggregationInput = aggregation.inputs().get(0);
        assertThat(aggregationInput.symbolType(), is(SymbolType.INPUT_COLUMN));

        assertThat(collectNode.toCollect().get(0), instanceOf(Reference.class));
        assertThat(((Reference) collectNode.toCollect().get(0)).info().ident().columnIdent().name(), is("name"));

        MergeNode mergeNode = globalAggregate.mergeNode();
        assertThat(mergeNode.projections().size(), is(2));
        Projection projection1 = mergeNode.projections().get(1);
        assertThat(projection1, instanceOf(TopNProjection.class));
        Symbol collection_count = projection1.outputs().get(0);
        assertThat(collection_count, instanceOf(Function.class));
    }

    @Test
    public void testNonDistributedGroupByOnClusteredColumn() throws Exception {
        NonDistributedGroupBy planNode = (NonDistributedGroupBy) plan(
                "select count(*), id from users group by id limit 20");
        CollectNode collectNode = planNode.collectNode();
        assertFalse(collectNode.hasDistributingDownstreams());
        assertThat(collectNode.projections().size(), is(2));
        assertThat(collectNode.projections().get(1), instanceOf(TopNProjection.class));
        assertThat(collectNode.projections().get(0).requiredGranularity(), is(RowGranularity.SHARD));
        MergeNode mergeNode = planNode.localMergeNode();
        assertThat(mergeNode.projections().size(), is(1));
    }

    @Test
    public void testNonDistributedGroupByOnClusteredColumnSorted() throws Exception {
        NonDistributedGroupBy planNode = (NonDistributedGroupBy) plan(
                "select count(*), id from users group by id order by 1 desc nulls last limit 20");
        CollectNode collectNode = planNode.collectNode();
        assertFalse(collectNode.hasDistributingDownstreams());
        assertThat(collectNode.projections().size(), is(2));
        assertThat(collectNode.projections().get(1), instanceOf(TopNProjection.class));
        assertThat(((TopNProjection)collectNode.projections().get(1)).orderBy().size(), is(1));

        assertThat(collectNode.projections().get(0).requiredGranularity(), is(RowGranularity.SHARD));
        MergeNode mergeNode = planNode.localMergeNode();
        assertThat(mergeNode.projections().size(), is(1));
        TopNProjection projection = (TopNProjection)mergeNode.projections().get(0);
        assertThat(projection.orderBy(), is(nullValue()));
        assertThat(mergeNode.sortedInputOutput(), is(true));
        assertThat(mergeNode.orderByIndices().length, is(1));
        assertThat(mergeNode.orderByIndices()[0], is(0));
        assertThat(mergeNode.reverseFlags()[0], is(true));
        assertThat(mergeNode.nullsFirst()[0], is(false));
    }

    @Test
    public void testNonDistributedGroupByOnClusteredColumnSortedScalar() throws Exception {
        NonDistributedGroupBy planNode = (NonDistributedGroupBy) plan(
                "select count(*) + 1, id from users group by id order by count(*) + 1 limit 20");
        CollectNode collectNode = planNode.collectNode();
        assertFalse(collectNode.hasDistributingDownstreams());
        assertThat(collectNode.projections().size(), is(2));
        assertThat(collectNode.projections().get(1), instanceOf(TopNProjection.class));
        assertThat(((TopNProjection)collectNode.projections().get(1)).orderBy().size(), is(1));

        assertThat(collectNode.projections().get(0).requiredGranularity(), is(RowGranularity.SHARD));
        MergeNode mergeNode = planNode.localMergeNode();
        assertThat(mergeNode.projections().size(), is(1));
        TopNProjection projection = (TopNProjection)mergeNode.projections().get(0);
        assertThat(projection.orderBy(), is(nullValue()));
        assertThat(mergeNode.sortedInputOutput(), is(true));
        assertThat(mergeNode.orderByIndices().length, is(1));
        assertThat(mergeNode.orderByIndices()[0], is(0));
        assertThat(mergeNode.reverseFlags()[0], is(false));
        assertThat(mergeNode.nullsFirst()[0], is(nullValue()));
    }

    @Test
    public void testNoDistributedGroupByOnAllPrimaryKeys() throws Exception {
        NonDistributedGroupBy planNode = (NonDistributedGroupBy) plan(
                "select count(*), id, date from empty_parted group by id, date limit 20");
        CollectNode collectNode = planNode.collectNode();
        assertFalse(collectNode.hasDistributingDownstreams());
        assertThat(collectNode.projections().size(), is(2));
        assertThat(collectNode.projections().get(0), instanceOf(GroupProjection.class));
        assertThat(collectNode.projections().get(0).requiredGranularity(), is(RowGranularity.SHARD));
        assertThat(collectNode.projections().get(1), instanceOf(TopNProjection.class));
        MergeNode mergeNode = planNode.localMergeNode();
        assertThat(mergeNode.projections().size(), is(1));
        assertThat(mergeNode.projections().get(0), instanceOf(TopNProjection.class));
    }

    @Test
    public void testNonDistributedGroupByAggregationsWrappedInScalar() throws Exception {
        DistributedGroupBy planNode = (DistributedGroupBy) plan(
                "select (count(*) + 1), id from empty_parted group by id");
        CollectNode collectNode = planNode.collectNode();
        assertThat(collectNode.projections().size(), is(1));
        assertThat(collectNode.projections().get(0), instanceOf(GroupProjection.class));

        TopNProjection topNProjection = (TopNProjection) planNode.reducerMergeNode().projections().get(1);
        assertThat(topNProjection.limit(), is(Constants.DEFAULT_SELECT_LIMIT));
        assertThat(topNProjection.offset(), is(0));

        MergeNode mergeNode = planNode.localMergeNode();
        assertThat(mergeNode.projections().size(), is(1));
        assertThat(mergeNode.projections().get(0), instanceOf(TopNProjection.class));
    }

    @Test
    public void testGroupByWithOrderOnAggregate() throws Exception {
        DistributedGroupBy distributedGroupBy = (DistributedGroupBy) plan(
                "select count(*), name from users group by name order by count(*)");



        MergeNode mergeNode = distributedGroupBy.localMergeNode();
        assertThat(mergeNode.projections().size(), is(1));

        TopNProjection topNProjection = (TopNProjection)mergeNode.projections().get(0);
        Symbol orderBy = topNProjection.orderBy().get(0);
        assertThat(orderBy, instanceOf(InputColumn.class));

        assertThat(orderBy.valueType(), Is.<DataType>is(DataTypes.LONG));
    }

    @Test
    public void testHandlerSideRouting() throws Exception {

        QueryAndFetch plan = (QueryAndFetch) plan("select * from sys.cluster");
    }

    @Test
    public void testHandlerSideRoutingGroupBy() throws Exception {
        NonDistributedGroupBy planNode = (NonDistributedGroupBy) plan(
                "select count(*) from sys.cluster group by name");

        CollectNode collectNode = planNode.collectNode();
        assertThat(collectNode.toCollect().get(0), instanceOf(Reference.class));
        assertThat(collectNode.toCollect().size(), is(1));

        MergeNode mergeNode = planNode.localMergeNode();
        assertThat(mergeNode.projections().size(), is(2));
        assertThat(mergeNode.projections().get(0), instanceOf(GroupProjection.class));
        assertThat(mergeNode.projections().get(1), instanceOf(TopNProjection.class));
    }

    @Test
    public void testCountDistinctWithGroupBy() throws Exception {
        DistributedGroupBy distributedGroupBy = (DistributedGroupBy) plan(
                "select count(distinct id), name from users group by name order by count(distinct id)");
        CollectNode collectNode = distributedGroupBy.collectNode();


        assertThat(collectNode.toCollect().get(0), instanceOf(Reference.class));
        assertThat(collectNode.toCollect().size(), is(2));
        assertThat(((Reference)collectNode.toCollect().get(0)).info().ident().columnIdent().name(), is("id"));
        assertThat(((Reference)collectNode.toCollect().get(1)).info().ident().columnIdent().name(), is("name"));
        Projection projection = collectNode.projections().get(0);
        assertThat(projection, instanceOf(GroupProjection.class));
        GroupProjection groupProjection = (GroupProjection)projection;
        Symbol groupKey = groupProjection.keys().get(0);
        assertThat(groupKey, instanceOf(InputColumn.class));
        assertThat(((InputColumn)groupKey).index(), is(1));
        assertThat(groupProjection.values().size(), is(1));

        Aggregation aggregation = groupProjection.values().get(0);
        assertThat(aggregation.toStep(), is(Aggregation.Step.PARTIAL));
        Symbol aggregationInput = aggregation.inputs().get(0);
        assertThat(aggregationInput.symbolType(), is(SymbolType.INPUT_COLUMN));




        MergeNode mergeNode = distributedGroupBy.reducerMergeNode();
        assertThat(mergeNode.projections().size(), is(2));
        Projection groupProjection1 = mergeNode.projections().get(0);
        assertThat(groupProjection1, instanceOf(GroupProjection.class));
        groupProjection = (GroupProjection)groupProjection1;
        assertThat(groupProjection.keys().get(0), instanceOf(InputColumn.class));
        assertThat(((InputColumn)groupProjection.keys().get(0)).index(), is(0));

        assertThat(groupProjection.values().get(0), instanceOf(Aggregation.class));
        Aggregation aggregationStep2 = groupProjection.values().get(0);
        assertThat(aggregationStep2.toStep(), is(Aggregation.Step.FINAL));

        TopNProjection topNProjection = (TopNProjection)mergeNode.projections().get(1);
        Symbol collection_count = topNProjection.outputs().get(0);
        assertThat(collection_count, instanceOf(Function.class));



        MergeNode localMergeNode = distributedGroupBy.localMergeNode();
        assertThat(localMergeNode.projections().size(), is(1));
        Projection localTopN = localMergeNode.projections().get(0);
        assertThat(localTopN, instanceOf(TopNProjection.class));
    }

    @Test
    public void testUpdateByQueryPlan() throws Exception {
        Upsert plan = (Upsert) plan("update users set name='Vogon lyric fan'");
        assertThat(plan.nodes().size(), is(1));

        CollectAndMerge planNode = (CollectAndMerge) plan.nodes().get(0);

        CollectNode collectNode = planNode.collectNode();
        assertThat(collectNode.routing(), is(shardRouting));
        assertFalse(collectNode.whereClause().noMatch());
        assertFalse(collectNode.whereClause().hasQuery());
        assertThat(collectNode.projections().size(), is(1));
        assertThat(collectNode.projections().get(0), instanceOf(UpdateProjection.class));
        assertThat(collectNode.toCollect().size(), is(1));
        assertThat(collectNode.toCollect().get(0), instanceOf(Reference.class));
        assertThat(((Reference)collectNode.toCollect().get(0)).info().ident().columnIdent().fqn(), is("_uid"));

        UpdateProjection updateProjection = (UpdateProjection)collectNode.projections().get(0);
        assertThat(updateProjection.uidSymbol(), instanceOf(InputColumn.class));

        assertThat(updateProjection.assignmentsColumns()[0], is("name"));
        Symbol symbol = updateProjection.assignments()[0];
        assertThat(symbol, isLiteral("Vogon lyric fan", DataTypes.STRING));

        MergeNode mergeNode = planNode.localMergeNode();
        assertThat(mergeNode.projections().size(), is(1));
        assertThat(mergeNode.projections().get(0), instanceOf(AggregationProjection.class));

        assertThat(mergeNode.outputTypes().size(), is(1));
    }

    @Test
    public void testUpdateByIdPlan() throws Exception {
        Upsert planNode = (Upsert) plan("update users set name='Vogon lyric fan' where id=1");
        assertThat(planNode.nodes().size(), is(1));

        PlanNode next = ((IterablePlan) planNode.nodes().get(0)).iterator().next();
        assertThat(next, instanceOf(SymbolBasedUpsertByIdNode.class));

        SymbolBasedUpsertByIdNode updateNode = (SymbolBasedUpsertByIdNode) next;
        assertThat(updateNode.items().size(), is(1));

        assertThat(updateNode.updateColumns()[0], is("name"));

        SymbolBasedUpsertByIdNode.Item item = updateNode.items().get(0);
        assertThat(item.index(), is("users"));
        assertThat(item.id(), is("1"));

        Symbol symbol = item.updateAssignments()[0];
        assertThat(symbol, isLiteral("Vogon lyric fan", DataTypes.STRING));
    }

    @Test
    public void testUpdatePlanWithMultiplePrimaryKeyValues() throws Exception {
        Upsert planNode =  (Upsert) plan("update users set name='Vogon lyric fan' where id in (1,2,3)");
        assertThat(planNode.nodes().size(), is(1));

        PlanNode next = ((IterablePlan) planNode.nodes().get(0)).iterator().next();

        assertThat(next, instanceOf(SymbolBasedUpsertByIdNode.class));
        SymbolBasedUpsertByIdNode updateNode = (SymbolBasedUpsertByIdNode) next;

        List<String> ids = new ArrayList<>(3);
        for (SymbolBasedUpsertByIdNode.Item item : updateNode.items()) {
            ids.add(item.id());
            assertThat(item.updateAssignments().length, is(1));
            assertThat(item.updateAssignments()[0], isLiteral("Vogon lyric fan", DataTypes.STRING));
        }

        assertThat(ids, containsInAnyOrder("1", "2", "3"));
    }

    @Test
    public void testUpdatePlanWithMultiplePrimaryKeyValuesPartitioned() throws Exception {
        Upsert planNode =  (Upsert) plan("update parted set name='Vogon lyric fan' where " +
                "(id=2 and date = 0) OR" +
                "(id=3 and date=123)");
        assertThat(planNode.nodes().size(), is(1));

        PlanNode next = ((IterablePlan) planNode.nodes().get(0)).iterator().next();

        assertThat(next, instanceOf(SymbolBasedUpsertByIdNode.class));
        SymbolBasedUpsertByIdNode updateNode = (SymbolBasedUpsertByIdNode) next;

        List<String> partitions = new ArrayList<>(2);
        List<String> ids = new ArrayList<>(2);
        for (SymbolBasedUpsertByIdNode.Item item : updateNode.items()) {
            partitions.add(item.index());
            ids.add(item.id());
            assertThat(item.updateAssignments().length, is(1));
            assertThat(item.updateAssignments()[0], isLiteral("Vogon lyric fan", DataTypes.STRING));
        }
        assertThat(ids, containsInAnyOrder("AgEyATA=", "AgEzAzEyMw==")); 
        assertThat(partitions, containsInAnyOrder(".partitioned.parted.04130", ".partitioned.parted.04232chj"));
    }

    @Test
    public void testCopyFromPlan() throws Exception {
        CollectAndMerge plan = (CollectAndMerge) plan("copy users from '/path/to/file.extension'");
        assertThat(plan.collectNode(), instanceOf(FileUriCollectNode.class));

        FileUriCollectNode collectNode = (FileUriCollectNode)plan.collectNode();
        assertThat((BytesRef) ((Literal) collectNode.targetUri()).value(),
                is(new BytesRef("/path/to/file.extension")));
    }

    @Test
    public void testCopyFromNumReadersSetting() throws Exception {
        CollectAndMerge plan = (CollectAndMerge) plan("copy users from '/path/to/file.extension' with (num_readers=1)");
        assertThat(plan.collectNode(), instanceOf(FileUriCollectNode.class));
        FileUriCollectNode collectNode = (FileUriCollectNode) plan.collectNode();
        assertThat(collectNode.executionNodes().size(), is(1));
    }

    @Test
    public void testCopyFromPlanWithParameters() throws Exception {
        CollectAndMerge plan = (CollectAndMerge) plan("copy users from '/path/to/file.ext' with (bulk_size=30, compression='gzip', shared=true)");
        assertThat(plan.collectNode(), instanceOf(FileUriCollectNode.class));
        FileUriCollectNode collectNode = (FileUriCollectNode)plan.collectNode();
        SourceIndexWriterProjection indexWriterProjection = (SourceIndexWriterProjection) collectNode.projections().get(0);
        assertThat(indexWriterProjection.bulkActions(), is(30));
        assertThat(collectNode.compression(), is("gzip"));
        assertThat(collectNode.sharedStorage(), is(true));


        plan = (CollectAndMerge) plan("copy users from '/path/to/file.ext'");
        collectNode = (FileUriCollectNode)plan.collectNode();
        assertNull(collectNode.compression());
        assertNull(collectNode.sharedStorage());
    }

    @Test
    public void testCopyToWithColumnsReferenceRewrite() throws Exception {
        CollectAndMerge plan = (CollectAndMerge) plan("copy users (name) to '/file.ext'");
        CollectNode node = plan.collectNode();
        Reference nameRef = (Reference)node.toCollect().get(0);

        assertThat(nameRef.info().ident().columnIdent().name(), is(DocSysColumns.DOC.name()));
        assertThat(nameRef.info().ident().columnIdent().path().get(0), is("name"));
    }

    @Test
    public void testCopyToWithNonExistentPartitionClause() throws Exception {
        CollectAndMerge plan = (CollectAndMerge) plan("copy parted partition (date=0) to '/foo.txt' ");
        assertFalse(plan.collectNode().routing().hasLocations());
    }

    @Test (expected = IllegalArgumentException.class)
    public void testCopyFromPlanWithInvalidParameters() throws Exception {
        plan("copy users from '/path/to/file.ext' with (bulk_size=-28)");
    }

    @Test
    public void testShardSelect() throws Exception {
        QueryAndFetch planNode = (QueryAndFetch) plan("select id from sys.shards");
        CollectNode collectNode = planNode.collectNode();
        assertTrue(collectNode.isRouted());
        assertThat(collectNode.maxRowGranularity(), is(RowGranularity.SHARD));
    }

    @Test
    public void testDropTable() throws Exception {
        IterablePlan plan = (IterablePlan) plan("drop table users");
        Iterator<PlanNode> iterator = plan.iterator();
        PlanNode planNode = iterator.next();
        assertThat(planNode, instanceOf(DropTableNode.class));

        DropTableNode node = (DropTableNode) planNode;
        assertThat(node.tableInfo().ident().name(), is("users"));
    }

    @Test
    public void testDropTableIfExistsWithUnknownSchema() throws Exception {
        Plan plan = plan("drop table if exists unknown_schema.unknwon_table");
        assertThat(plan, instanceOf(NoopPlan.class));
    }

    @Test
    public void testDropTableIfExists() throws Exception {
        IterablePlan plan = (IterablePlan) plan("drop table if exists users");
        Iterator<PlanNode> iterator = plan.iterator();
        PlanNode planNode = iterator.next();
        assertThat(planNode, instanceOf(DropTableNode.class));

        DropTableNode node = (DropTableNode) planNode;
        assertThat(node.tableInfo().ident().name(), is("users"));
    }

    @Test
    public void testDropTableIfExistsNonExistentTableCreatesNoop() throws Exception {
        Plan plan = plan("drop table if exists groups");
        assertThat(plan, instanceOf(NoopPlan.class));
    }


    @Test
    public void testDropPartitionedTable() throws Exception {
        IterablePlan plan = (IterablePlan) plan("drop table parted");
        Iterator<PlanNode> iterator = plan.iterator();
        PlanNode planNode = iterator.next();

        assertThat(planNode, instanceOf(DropTableNode.class));
        DropTableNode node = (DropTableNode) planNode;
        assertThat(node.tableInfo().ident().name(), is("parted"));

        assertFalse(iterator.hasNext());
    }

    @Test
    public void testDropBlobTableIfExistsCreatesIterablePlan() throws Exception {
        Plan plan = plan("drop blob table if exists screenshots");
        assertThat(plan, instanceOf(IterablePlan.class));
    }

    @Test
    public void testDropNonExistentBlobTableCreatesNoop() throws Exception {
        Plan plan = plan("drop blob table if exists unknown");
        assertThat(plan, instanceOf(NoopPlan.class));
    }

    @Test
    public void testGlobalCountPlan() throws Exception {
        CountPlan plan = (CountPlan) plan("select count(*) from users");
        assertThat(plan, instanceOf(PlannedAnalyzedRelation.class));

        assertThat(plan.countNode().whereClause(), equalTo(WhereClause.MATCH_ALL));

        assertThat(plan.mergeNode().projections().size(), is(1));
        assertThat(plan.mergeNode().projections().get(0), instanceOf(AggregationProjection.class));
    }

    @Test
    public void testSetPlan() throws Exception {
        IterablePlan plan = (IterablePlan) plan("set GLOBAL PERSISTENT stats.jobs_log_size=1024");
        Iterator<PlanNode> iterator = plan.iterator();
        PlanNode planNode = iterator.next();
        assertThat(planNode, instanceOf(ESClusterUpdateSettingsNode.class));

        ESClusterUpdateSettingsNode node = (ESClusterUpdateSettingsNode) planNode;

        assertThat(node.transientSettings().toDelimitedString(','), is("stats.jobs_log_size=1024,"));
        assertThat(node.persistentSettings().toDelimitedString(','), is("stats.jobs_log_size=1024,"));

        plan = (IterablePlan)  plan("set GLOBAL TRANSIENT stats.enabled=false,stats.jobs_log_size=0");
        iterator = plan.iterator();
        planNode = iterator.next();
        assertThat(planNode, instanceOf(ESClusterUpdateSettingsNode.class));

        node = (ESClusterUpdateSettingsNode) planNode;
        assertThat(node.persistentSettings().getAsMap().size(), is(0));
        assertThat(node.transientSettings().toDelimitedString(','), is("stats.enabled=false,stats.jobs_log_size=0,"));
    }

    @Test
    public void testInsertFromSubQueryNonDistributedGroupBy() throws Exception {
        InsertFromSubQuery planNode = (InsertFromSubQuery) plan(
                "insert into users (id, name) (select name, count(*) from sys.nodes group by name)");
        NonDistributedGroupBy nonDistributedGroupBy = (NonDistributedGroupBy)planNode.innerPlan();
        MergeNode mergeNode = nonDistributedGroupBy.localMergeNode();
        assertThat(mergeNode.projections().size(), is(3));
        assertThat(mergeNode.projections().get(0), instanceOf(GroupProjection.class));
        assertThat(mergeNode.projections().get(1), instanceOf(TopNProjection.class));
        assertThat(mergeNode.projections().get(2), instanceOf(ColumnIndexWriterProjection.class));
        assertThat(planNode.handlerMergeNode().isPresent(), is(false));
    }

    @Test (expected = UnsupportedFeatureException.class)
    public void testInsertFromSubQueryDistributedGroupByWithLimit() throws Exception {
        IterablePlan plan = (IterablePlan) plan("insert into users (id, name) (select name, count(*) from users group by name order by name limit 10)");
        Iterator<PlanNode> iterator = plan.iterator();
        PlanNode planNode = iterator.next();
        assertThat(planNode, instanceOf(CollectNode.class));

        planNode = iterator.next();
        assertThat(planNode, instanceOf(MergeNode.class));
        MergeNode mergeNode = (MergeNode)planNode;
        assertThat(mergeNode.projections().size(), is(2));
        assertThat(mergeNode.projections().get(1), instanceOf(TopNProjection.class));

        planNode = iterator.next();
        assertThat(planNode, instanceOf(MergeNode.class));
        mergeNode = (MergeNode)planNode;
        assertThat(mergeNode.projections().size(), is(2));
        assertThat(mergeNode.projections().get(0), instanceOf(TopNProjection.class));
        assertThat(((TopNProjection)mergeNode.projections().get(0)).limit(), is(10));

        assertThat(mergeNode.projections().get(1), instanceOf(ColumnIndexWriterProjection.class));

        assertThat(iterator.hasNext(), is(false));
    }

    @Test
    public void testInsertFromSubQueryDistributedGroupByWithoutLimit() throws Exception {
        InsertFromSubQuery planNode = (InsertFromSubQuery) plan(
                "insert into users (id, name) (select name, count(*) from users group by name)");
        DistributedGroupBy groupBy = (DistributedGroupBy)planNode.innerPlan();
        MergeNode mergeNode = groupBy.reducerMergeNode();
        assertThat(mergeNode.projections().size(), is(2));
        assertThat(mergeNode.projections().get(1), instanceOf(ColumnIndexWriterProjection.class));
        ColumnIndexWriterProjection projection = (ColumnIndexWriterProjection)mergeNode.projections().get(1);
        assertThat(projection.primaryKeys().size(), is(1));
        assertThat(projection.primaryKeys().get(0).fqn(), is("id"));
        assertThat(projection.columnReferences().size(), is(2));
        assertThat(projection.columnReferences().get(0).ident().columnIdent().fqn(), is("id"));
        assertThat(projection.columnReferences().get(1).ident().columnIdent().fqn(), is("name"));

        assertNotNull(projection.clusteredByIdent());
        assertThat(projection.clusteredByIdent().fqn(), is("id"));
        assertThat(projection.tableIdent().fqn(), is("users"));
        assertThat(projection.partitionedBySymbols().isEmpty(), is(true));

        MergeNode localMergeNode = planNode.handlerMergeNode().get();
        assertThat(localMergeNode.projections().size(), is(1));
        assertThat(localMergeNode.projections().get(0), instanceOf(AggregationProjection.class));
        assertThat(localMergeNode.finalProjection().get().outputs().size(), is(1));

    }

    @Test
    public void testInsertFromSubQueryDistributedGroupByPartitioned() throws Exception {
        InsertFromSubQuery planNode = (InsertFromSubQuery) plan(
                "insert into parted (id, date) (select id, date from users group by id, date)");
        DistributedGroupBy groupBy = (DistributedGroupBy)planNode.innerPlan();
        MergeNode mergeNode = groupBy.reducerMergeNode();
        assertThat(mergeNode.projections().size(), is(2));
        assertThat(mergeNode.projections().get(1), instanceOf(ColumnIndexWriterProjection.class));
        ColumnIndexWriterProjection projection = (ColumnIndexWriterProjection)mergeNode.projections().get(1);
        assertThat(projection.primaryKeys().size(), is(2));
        assertThat(projection.primaryKeys().get(0).fqn(), is("id"));
        assertThat(projection.primaryKeys().get(1).fqn(), is("date"));

        assertThat(projection.columnReferences().size(), is(1));
        assertThat(projection.columnReferences().get(0).ident().columnIdent().fqn(), is("id"));

        assertThat(projection.partitionedBySymbols().size(), is(1));
        assertThat(((InputColumn) projection.partitionedBySymbols().get(0)).index(), is(1));

        assertNotNull(projection.clusteredByIdent());
        assertThat(projection.clusteredByIdent().fqn(), is("id"));
        assertThat(projection.tableIdent().fqn(), is("parted"));

        MergeNode localMergeNode = planNode.handlerMergeNode().get();

        assertThat(localMergeNode.projections().size(), is(1));
        assertThat(localMergeNode.projections().get(0), instanceOf(AggregationProjection.class));
        assertThat(localMergeNode.finalProjection().get().outputs().size(), is(1));

    }

    @Test
    public void testInsertFromSubQueryGlobalAggregate() throws Exception {
        InsertFromSubQuery planNode = (InsertFromSubQuery) plan(
                "insert into users (name, id) (select arbitrary(name), count(*) from users)");
        GlobalAggregate globalAggregate = (GlobalAggregate)planNode.innerPlan();
        MergeNode mergeNode = globalAggregate.mergeNode();
        assertThat(mergeNode.projections().size(), is(3));
        assertThat(mergeNode.projections().get(1), instanceOf(TopNProjection.class));
        assertThat(mergeNode.projections().get(2), instanceOf(ColumnIndexWriterProjection.class));
        ColumnIndexWriterProjection projection = (ColumnIndexWriterProjection)mergeNode.projections().get(2);

        assertThat(projection.columnReferences().size(), is(2));
        assertThat(projection.columnReferences().get(0).ident().columnIdent().fqn(), is("name"));
        assertThat(projection.columnReferences().get(1).ident().columnIdent().fqn(), is("id"));

        assertThat(projection.columnSymbols().size(), is(2));
        assertThat(((InputColumn)projection.columnSymbols().get(0)).index(), is(0));
        assertThat(((InputColumn)projection.columnSymbols().get(1)).index(), is(1));

        assertNotNull(projection.clusteredByIdent());
        assertThat(projection.clusteredByIdent().fqn(), is("id"));
        assertThat(projection.tableIdent().fqn(), is("users"));
        assertThat(projection.partitionedBySymbols().isEmpty(), is(true));

        assertThat(planNode.handlerMergeNode().isPresent(), is(false));
    }

    @Test
    public void testInsertFromSubQueryESGet() throws Exception {


        InsertFromSubQuery planNode = (InsertFromSubQuery) plan(
                "insert into users (date, id, name) (select date, id, name from users where id=1)");
        QueryAndFetch queryAndFetch = (QueryAndFetch)planNode.innerPlan();
        CollectNode collectNode = queryAndFetch.collectNode();

        assertThat(collectNode.projections().size(), is(1));
        assertThat(collectNode.projections().get(0), instanceOf(ColumnIndexWriterProjection.class));
        ColumnIndexWriterProjection projection = (ColumnIndexWriterProjection)collectNode.projections().get(0);

        assertThat(projection.columnReferences().size(), is(3));
        assertThat(projection.columnReferences().get(0).ident().columnIdent().fqn(), is("date"));
        assertThat(projection.columnReferences().get(1).ident().columnIdent().fqn(), is("id"));
        assertThat(projection.columnReferences().get(2).ident().columnIdent().fqn(), is("name"));
        assertThat(((InputColumn) projection.ids().get(0)).index(), is(1));
        assertThat(((InputColumn)projection.clusteredBy()).index(), is(1));
        assertThat(projection.partitionedBySymbols().isEmpty(), is(true));

        assertThat(planNode.handlerMergeNode().isPresent(), is(true));
    }

    @Test (expected = UnsupportedFeatureException.class)
    public void testInsertFromSubQueryWithLimit() throws Exception {
        Plan plan = plan("insert into users (date, id, name) (select date, id, name from users limit 10)");
        assertThat(plan, instanceOf(QueryThenFetch.class));
        CollectNode collectNode = ((QueryThenFetch) plan).collectNode();
        assertTrue(collectNode.whereClause().hasQuery());
        assertFalse(collectNode.isPartitioned());

        DQLPlanNode resultNode = ((QueryThenFetch) plan).resultNode();
        assertThat(resultNode.outputTypes().size(), is(1));
        assertEquals(DataTypes.STRING, resultNode.outputTypes().get(0));

        assertThat(resultNode, instanceOf(MergeNode.class));
        MergeNode mergeNode = (MergeNode) resultNode;
        assertTrue(mergeNode.finalProjection().isPresent());

        assertThat(mergeNode.projections().size(), is(2));
        assertThat(mergeNode.projections().get(1), instanceOf(ColumnIndexWriterProjection.class));
    }

    @Test (expected = UnsupportedFeatureException.class)
    public void testInsertFromSubQueryWithOffset() throws Exception {
        plan("insert into users (date, id, name) (select date, id, name from users offset 10)");
    }

    @Test (expected = UnsupportedFeatureException.class)
    public void testInsertFromSubQueryWithOrderBy() throws Exception {
        plan("insert into users (date, id, name) (select date, id, name from users order by id)");
    }

    @Test
    public void testInsertFromSubQueryWithoutLimit() throws Exception {
        InsertFromSubQuery planNode = (InsertFromSubQuery) plan(
                "insert into users (date, id, name) (select date, id, name from users)");
        QueryAndFetch queryAndFetch = (QueryAndFetch)planNode.innerPlan();
        CollectNode collectNode = queryAndFetch.collectNode();
        assertThat(collectNode.projections().size(), is(1));
        assertThat(collectNode.projections().get(0), instanceOf(ColumnIndexWriterProjection.class));
        assertNull(queryAndFetch.localMergeNode());

        MergeNode localMergeNode = planNode.handlerMergeNode().get();

        assertThat(localMergeNode.projections().size(), is(1));
        assertThat(localMergeNode.projections().get(0), instanceOf(AggregationProjection.class));
    }

    @Test
    public void testGroupByHaving() throws Exception {
        DistributedGroupBy distributedGroupBy = (DistributedGroupBy) plan(
                "select avg(date), name from users group by name having min(date) > '1970-01-01'");
        CollectNode collectNode = distributedGroupBy.collectNode();
        assertThat(collectNode.projections().size(), is(1));
        assertThat(collectNode.projections().get(0), instanceOf(GroupProjection.class));

        MergeNode mergeNode = distributedGroupBy.reducerMergeNode();

        assertThat(mergeNode.projections().size(), is(3));


        assertThat(mergeNode.projections().get(0), instanceOf(GroupProjection.class));
        GroupProjection groupProjection = (GroupProjection)mergeNode.projections().get(0);
        assertThat(groupProjection.values().size(), is(2));


        assertThat(mergeNode.projections().get(1), instanceOf(FilterProjection.class));
        FilterProjection filterProjection = (FilterProjection)mergeNode.projections().get(1);


        assertThat(mergeNode.projections().get(2), instanceOf(TopNProjection.class));
        TopNProjection topN = (TopNProjection)mergeNode.projections().get(2);
        assertThat(topN.outputs().get(0).valueType(), Is.<DataType>is(DataTypes.DOUBLE));
        assertThat(topN.outputs().get(1).valueType(), Is.<DataType>is(DataTypes.STRING));
        assertThat(topN.limit(), is(Constants.DEFAULT_SELECT_LIMIT));
    }

    @Test
    public void testInsertFromQueryWithPartitionedColumn() throws Exception {
        InsertFromSubQuery planNode = (InsertFromSubQuery) plan(
                "insert into users (id, date) (select id, date from parted)");
        QueryAndFetch queryAndFetch = (QueryAndFetch)planNode.innerPlan();
        CollectNode collectNode = queryAndFetch.collectNode();
        List<Symbol> toCollect = collectNode.toCollect();
        assertThat(toCollect.size(), is(2));
        assertThat(toCollect.get(0), isFunction("toLong"));
        assertThat(((Function) toCollect.get(0)).arguments().get(0), isReference("_doc['id']"));
        assertThat((Reference) toCollect.get(1), equalTo(new Reference(new ReferenceInfo(
                new ReferenceIdent(new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "parted"), "date"), RowGranularity.PARTITION, DataTypes.TIMESTAMP))));
    }

    @Test
    public void testGroupByHavingInsertInto() throws Exception {
        InsertFromSubQuery planNode = (InsertFromSubQuery) plan(
                "insert into users (id, name) (select name, count(*) from users group by name having count(*) > 3)");
        DistributedGroupBy groupByNode = (DistributedGroupBy)planNode.innerPlan();
        MergeNode mergeNode = groupByNode.reducerMergeNode();
        assertThat(mergeNode.projections().size(), is(3));
        assertThat(mergeNode.projections().get(0), instanceOf(GroupProjection.class));
        assertThat(mergeNode.projections().get(1), instanceOf(FilterProjection.class));
        assertThat(mergeNode.projections().get(2), instanceOf(ColumnIndexWriterProjection.class));

        FilterProjection filterProjection = (FilterProjection)mergeNode.projections().get(1);
        assertThat(filterProjection.outputs().size(), is(2));
        assertThat(filterProjection.outputs().get(0), instanceOf(InputColumn.class));
        assertThat(filterProjection.outputs().get(1), instanceOf(InputColumn.class));

        InputColumn inputColumn = (InputColumn)filterProjection.outputs().get(0);
        assertThat(inputColumn.index(), is(0));
        inputColumn = (InputColumn)filterProjection.outputs().get(1);
        assertThat(inputColumn.index(), is(1));
        MergeNode localMergeNode = planNode.handlerMergeNode().get();

        assertThat(localMergeNode.projections().size(), is(1));
        assertThat(localMergeNode.projections().get(0), instanceOf(AggregationProjection.class));
        assertThat(localMergeNode.finalProjection().get().outputs().size(), is(1));

    }

    @Test
    public void testGroupByHavingNonDistributed() throws Exception {
        NonDistributedGroupBy planNode = (NonDistributedGroupBy) plan(
                "select id from users group by id having id > 0");
        CollectNode collectNode = planNode.collectNode();
        assertThat(collectNode.projections().size(), is(2));
        assertThat(collectNode.projections().get(0), instanceOf(GroupProjection.class));
        assertThat(collectNode.projections().get(1), instanceOf(FilterProjection.class));

        FilterProjection filterProjection = (FilterProjection)collectNode.projections().get(1);
        assertThat(filterProjection.requiredGranularity(), is(RowGranularity.SHARD));
        assertThat(filterProjection.outputs().size(), is(1));
        assertThat(filterProjection.outputs().get(0), instanceOf(InputColumn.class));
        InputColumn inputColumn = (InputColumn)filterProjection.outputs().get(0);
        assertThat(inputColumn.index(), is(0));

        MergeNode localMergeNode = planNode.localMergeNode();

        assertThat(localMergeNode.projections().size(), is(1));
        assertThat(localMergeNode.projections().get(0), instanceOf(TopNProjection.class));
    }

    @Test
    public void testGlobalAggregationHaving() throws Exception {
        GlobalAggregate globalAggregate = (GlobalAggregate) plan(
                "select avg(date) from users having min(date) > '1970-01-01'");
        CollectNode collectNode = globalAggregate.collectNode();
        assertThat(collectNode.projections().size(), is(1));
        assertThat(collectNode.projections().get(0), instanceOf(AggregationProjection.class));

        MergeNode localMergeNode = globalAggregate.mergeNode();

        assertThat(localMergeNode.projections().size(), is(3));
        assertThat(localMergeNode.projections().get(0), instanceOf(AggregationProjection.class));
        assertThat(localMergeNode.projections().get(1), instanceOf(FilterProjection.class));
        assertThat(localMergeNode.projections().get(2), instanceOf(TopNProjection.class));

        AggregationProjection aggregationProjection = (AggregationProjection)localMergeNode.projections().get(0);
        assertThat(aggregationProjection.aggregations().size(), is(2));

        FilterProjection filterProjection = (FilterProjection)localMergeNode.projections().get(1);
        assertThat(filterProjection.outputs().size(), is(2));
        assertThat(filterProjection.outputs().get(0), instanceOf(InputColumn.class));
        InputColumn inputColumn = (InputColumn)filterProjection.outputs().get(0);
        assertThat(inputColumn.index(), is(0));

        TopNProjection topNProjection = (TopNProjection)localMergeNode.projections().get(2);
        assertThat(topNProjection.outputs().size(), is(1));
    }

    @Test
    public void testCountOnPartitionedTable() throws Exception {
        CountPlan plan = (CountPlan) plan("select count(*) from parted where date = 123");
        assertThat(plan, instanceOf(PlannedAnalyzedRelation.class));
        assertThat(plan.countNode().whereClause().partitions(), containsInAnyOrder(".partitioned.parted.04232chj"));
    }

    @Test(expected = UnsupportedOperationException.class)
    public void testSelectPartitionedTableOrderByPartitionedColumn() throws Exception {
        plan("select name from parted order by date");
    }

    @Test(expected = UnsupportedOperationException.class)
    public void testSelectPartitionedTableOrderByPartitionedColumnInFunction() throws Exception {
        plan("select name from parted order by year(date)");
    }

    @Test(expected = UnsupportedOperationException.class)
    public void testSelectOrderByPartitionedNestedColumn() throws Exception {
        plan("select id from multi_parted order by obj['name']");
    }

    @Test(expected = UnsupportedOperationException.class)
    public void testSelectOrderByPartitionedNestedColumnInFunction() throws Exception {
        plan("select id from multi_parted order by format('abc %s', obj['name'])");
    }

    @Test(expected = UnsupportedFeatureException.class)
    public void testQueryRequiresScalar() throws Exception {

        plan("select * from sys.shards where match(table_name, 'characters')");
    }

    @Test
    public void testGroupByWithHavingAndLimit() throws Exception {
        DistributedGroupBy planNode = (DistributedGroupBy) plan(
                "select count(*), name from users group by name having count(*) > 1 limit 100");;

        MergeNode mergeNode = planNode.reducerMergeNode(); 

        Projection projection = mergeNode.projections().get(1);
        assertThat(projection, instanceOf(FilterProjection.class));
        FilterProjection filterProjection = (FilterProjection) projection;

        Symbol countArgument = ((Function) filterProjection.query()).arguments().get(0);
        assertThat(countArgument, instanceOf(InputColumn.class));
        assertThat(((InputColumn) countArgument).index(), is(1));  


        TopNProjection topN = (TopNProjection) mergeNode.projections().get(2);
        assertThat(topN.outputs().get(0).valueType(), Is.<DataType>is(DataTypes.LONG));
        assertThat(topN.outputs().get(1).valueType(), Is.<DataType>is(DataTypes.STRING));


        MergeNode localMerge = planNode.localMergeNode();


        topN = (TopNProjection) localMerge.projections().get(0);
        assertThat(topN.outputs().get(0).valueType(), Is.<DataType>is(DataTypes.LONG));
        assertThat(topN.outputs().get(1).valueType(), Is.<DataType>is(DataTypes.STRING));
    }

    @Test
    public void testGroupByWithHavingAndNoLimit() throws Exception {
        DistributedGroupBy planNode = (DistributedGroupBy) plan(
                "select count(*), name from users group by name having count(*) > 1");

        MergeNode mergeNode = planNode.reducerMergeNode(); 




        Projection projection = mergeNode.projections().get(1);
        assertThat(projection, instanceOf(FilterProjection.class));
        FilterProjection filterProjection = (FilterProjection) projection;

        Symbol countArgument = ((Function) filterProjection.query()).arguments().get(0);
        assertThat(countArgument, instanceOf(InputColumn.class));
        assertThat(((InputColumn) countArgument).index(), is(1));  

        assertThat(mergeNode.outputTypes().get(0), equalTo((DataType) DataTypes.LONG));
        assertThat(mergeNode.outputTypes().get(1), equalTo((DataType) DataTypes.STRING));

        mergeNode = planNode.localMergeNode();

        assertThat(mergeNode.outputTypes().get(0), equalTo((DataType) DataTypes.LONG));
        assertThat(mergeNode.outputTypes().get(1), equalTo((DataType) DataTypes.STRING));
    }

    @Test
    public void testGroupByWithHavingAndNoSelectListReordering() throws Exception {
        DistributedGroupBy planNode = (DistributedGroupBy) plan(
                "select name, count(*) from users group by name having count(*) > 1");

        MergeNode mergeNode = planNode.reducerMergeNode(); 






        Projection projection = mergeNode.projections().get(1);
        assertThat(projection, instanceOf(FilterProjection.class));
        FilterProjection filterProjection = (FilterProjection) projection;

        Symbol countArgument = ((Function) filterProjection.query()).arguments().get(0);
        assertThat(countArgument, instanceOf(InputColumn.class));
        assertThat(((InputColumn) countArgument).index(), is(1));  


        assertThat(((InputColumn) filterProjection.outputs().get(0)).index(), is(0));
        assertThat(((InputColumn) filterProjection.outputs().get(1)).index(), is(1));

        MergeNode localMerge = planNode.localMergeNode();


        TopNProjection topN = (TopNProjection) localMerge.projections().get(0);
        assertThat(((InputColumn) topN.outputs().get(0)).index(), is(0));
        assertThat(((InputColumn) topN.outputs().get(1)).index(), is(1));
    }

    @Test
    public void testGroupByHavingAndNoSelectListReOrderingWithLimit() throws Exception {
        DistributedGroupBy planNode = (DistributedGroupBy) plan(
                "select name, count(*) from users group by name having count(*) > 1 limit 100");

        MergeNode mergeNode = planNode.reducerMergeNode(); 








        Projection projection = mergeNode.projections().get(1);
        assertThat(projection, instanceOf(FilterProjection.class));
        FilterProjection filterProjection = (FilterProjection) projection;

        Symbol countArgument = ((Function) filterProjection.query()).arguments().get(0);
        assertThat(countArgument, instanceOf(InputColumn.class));
        assertThat(((InputColumn) countArgument).index(), is(1));  


        assertThat(((InputColumn) filterProjection.outputs().get(0)).index(), is(0));
        assertThat(((InputColumn) filterProjection.outputs().get(1)).index(), is(1));


        TopNProjection topN = (TopNProjection) mergeNode.projections().get(2);
        assertThat(((InputColumn) topN.outputs().get(0)).index(), is(0));
        assertThat(((InputColumn) topN.outputs().get(1)).index(), is(1));


        MergeNode localMerge = planNode.localMergeNode();



        topN = (TopNProjection) localMerge.projections().get(0);
        assertThat(((InputColumn) topN.outputs().get(0)).index(), is(0));
        assertThat(((InputColumn) topN.outputs().get(1)).index(), is(1));
    }

    @Test
    public void testOrderByOnAnalyzed() throws Exception {
        expectedException.expect(UnsupportedOperationException.class);
        expectedException.expectMessage("Cannot ORDER BY 'users.text': sorting on analyzed/fulltext columns is not possible");
        plan("select text from users u order by 1");
    }

    @Test
    public void testSortOnUnknownColumn() throws Exception {
        expectedException.expect(UnsupportedOperationException.class);
        expectedException.expectMessage("Cannot ORDER BY 'details['unknown_column']': invalid data type 'null'.");
        plan("select details from ignored_nested order by details['unknown_column']");
    }

    @Test
    public void testOrderByOnIndexOff() throws Exception {
        expectedException.expect(UnsupportedOperationException.class);
        expectedException.expectMessage("Cannot ORDER BY 'users.no_index': sorting on non-indexed columns is not possible");
        plan("select no_index from users u order by 1");
    }

    @Test
    public void testGroupByOnAnalyzed() throws Exception {
        expectedException.expect(IllegalArgumentException.class);
        expectedException.expectMessage("Cannot GROUP BY 'users.text': grouping on analyzed/fulltext columns is not possible");
        plan("select text from users u group by 1");
    }

    @Test
    public void testGroupByOnIndexOff() throws Exception {
        expectedException.expect(IllegalArgumentException.class);
        expectedException.expectMessage("Cannot GROUP BY 'users.no_index': grouping on non-indexed columns is not possible");
        plan("select no_index from users u group by 1");
    }

    @Test
    public void testSelectAnalyzedReferenceInFunctionGroupBy() throws Exception {
        expectedException.expect(IllegalArgumentException.class);
        expectedException.expectMessage("Cannot GROUP BY 'users.text': grouping on analyzed/fulltext columns is not possible");
        plan("select substr(text, 0, 2) from users u group by 1");
    }

    @Test
    public void testSelectAnalyzedReferenceInFunctionAggregation() throws Exception {
        expectedException.expect(IllegalArgumentException.class);
        expectedException.expectMessage("Cannot select analyzed column 'users.text' within grouping or aggregations");
        plan("select min(substr(text, 0, 2)) from users");
    }

    @Test
    public void testSelectNonIndexedReferenceInFunctionGroupBy() throws Exception {
        expectedException.expect(IllegalArgumentException.class);
        expectedException.expectMessage("Cannot GROUP BY 'users.no_index': grouping on non-indexed columns is not possible");
        plan("select substr(no_index, 0, 2) from users u group by 1");
    }

    @Test
    public void testSelectNonIndexedReferenceInFunctionAggregation() throws Exception {
        expectedException.expect(IllegalArgumentException.class);
        expectedException.expectMessage("Cannot select non-indexed column 'users.no_index' within grouping or aggregations");
        plan("select min(substr(no_index, 0, 2)) from users");
    }

    @Test
    public void testGlobalAggregateWithWhereOnPartitionColumn() throws Exception {
        GlobalAggregate globalAggregate = (GlobalAggregate) plan(
                "select min(name) from parted where date > 0");

        WhereClause whereClause = globalAggregate.collectNode().whereClause();
        assertThat(whereClause.partitions().size(), is(1));
        assertThat(whereClause.noMatch(), is(false));
    }

    private void assertNoop(Plan plan){
        assertThat(plan, instanceOf(NoopPlan.class));
    }

    @Test
    public void testHasNoResultFromHaving() throws Exception {
        assertNoop(plan("select min(name) from users having 1 = 2"));
    }

    @Test
    public void testHasNoResultFromLimit() {
        assertNoop(plan("select count(*) from users limit 1 offset 1"));
        assertNoop(plan("select count(*) from users limit 5 offset 1"));
        assertNoop(plan("select count(*) from users limit 0"));
    }

    @Test
    public void testHasNoResultFromQuery() {
        assertNoop(plan("select name from users where false"));
    }

    @Test
    public void testInsertFromValuesWithOnDuplicateKey() throws Exception {
        Upsert plan = (Upsert) plan("insert into users (id, name) values (1, null) on duplicate key update name = values(name)");
        PlanNode planNode = ((IterablePlan) plan.nodes().get(0)).iterator().next();
        assertThat(planNode, instanceOf(SymbolBasedUpsertByIdNode.class));
        SymbolBasedUpsertByIdNode node = (SymbolBasedUpsertByIdNode) planNode;

        assertThat(node.updateColumns(), is(new String[]{ "name" }));

        assertThat(node.insertColumns().length, is(2));
        Reference idRef = node.insertColumns()[0];
        assertThat(idRef.ident().columnIdent().fqn(), is("id"));
        Reference nameRef = node.insertColumns()[1];
        assertThat(nameRef.ident().columnIdent().fqn(), is("name"));

        assertThat(node.items().size(), is(1));
        SymbolBasedUpsertByIdNode.Item item = node.items().get(0);
        assertThat(item.index(), is("users"));
        assertThat(item.id(), is("1"));
        assertThat(item.routing(), is("1"));

        assertThat(item.insertValues().length, is(2));
        assertThat((Long)item.insertValues()[0], is(1L));
        assertNull(item.insertValues()[1]);

        assertThat(item.updateAssignments().length, is(1));
        assertThat(item.updateAssignments()[0], isLiteral(null, DataTypes.STRING));
    }

    @Test
    public void testGroupByOnClusteredByColumnPartitionedOnePartition() throws Exception {

        Plan optimizedPlan = plan("select count(*), city from clustered_parted where date=1395874800000 group by city");
        assertThat(optimizedPlan, instanceOf(NonDistributedGroupBy.class));
        NonDistributedGroupBy optimizedGroupBy = (NonDistributedGroupBy) optimizedPlan;

        assertThat(optimizedGroupBy.collectNode().isPartitioned(), is(true));
        assertThat(optimizedGroupBy.collectNode().projections().size(), is(1));
        assertThat(optimizedGroupBy.collectNode().projections().get(0), instanceOf(GroupProjection.class));

        assertThat(optimizedGroupBy.localMergeNode().projections().size(), is(1));
        assertThat(optimizedGroupBy.localMergeNode().projections().get(0), instanceOf(TopNProjection.class));


        Plan plan = plan("select count(*), city from clustered_parted where date=1395874800000 or date=1395961200000 group by city");
        assertThat(plan, instanceOf(DistributedGroupBy.class));
    }

    @Test
    public void testIndices() throws Exception {
        TableIdent custom = new TableIdent("custom", "table");
        String[] indices = Planner.indices(TestingTableInfo.builder(custom, RowGranularity.DOC, shardRouting).add("id", DataTypes.INTEGER, null).build(), WhereClause.MATCH_ALL);
        assertThat(indices, arrayContainingInAnyOrder("custom.table"));

        indices = Planner.indices(TestingTableInfo.builder(new TableIdent(null, "table"), RowGranularity.DOC, shardRouting).add("id", DataTypes.INTEGER, null).build(), WhereClause.MATCH_ALL);
        assertThat(indices, arrayContainingInAnyOrder("table"));

        indices = Planner.indices(TestingTableInfo.builder(custom, RowGranularity.DOC, shardRouting)
                .add("id", DataTypes.INTEGER, null)
                .add("date", DataTypes.TIMESTAMP, null, true)
                .addPartitions(new PartitionName(custom, Arrays.asList(new BytesRef("0"))).stringValue())
                .addPartitions(new PartitionName(custom, Arrays.asList(new BytesRef("12345"))).stringValue())
                .build(), WhereClause.MATCH_ALL);
        assertThat(indices, arrayContainingInAnyOrder("custom..partitioned.table.04130", "custom..partitioned.table.04332chj6gqg"));
    }

    @Test
    public void testAllocatedJobSearchContextIds() throws Exception {
        Planner.Context plannerContext = new Planner.Context(clusterService);
        CollectNode collectNode = new CollectNode(
                plannerContext.nextExecutionNodeId(), "collect", shardRouting);
        int shardNum = collectNode.routing().numShards();

        plannerContext.allocateJobSearchContextIds(collectNode.routing());

        java.lang.reflect.Field f = plannerContext.getClass().getDeclaredField("jobSearchContextIdBaseSeq");
        f.setAccessible(true);
        int jobSearchContextIdBaseSeq = (Integer)f.get(plannerContext);

        assertThat(jobSearchContextIdBaseSeq, is(shardNum));
        assertThat(collectNode.routing().jobSearchContextIdBase(), is(jobSearchContextIdBaseSeq-shardNum));

        int idx = 0;
        for (Map.Entry<String, Map<String, List<Integer>>> locations : collectNode.routing().locations().entrySet()) {
            String nodeId = locations.getKey();
            for (Map.Entry<String, List<Integer>> entry : locations.getValue().entrySet()) {
                for (Integer shardId : entry.getValue()) {
                    assertThat(plannerContext.shardId(idx), is(new ShardId(entry.getKey(), shardId)));
                    assertThat(plannerContext.nodeId(idx), is(nodeId));
                    idx++;
                }
            }
        }


        int jobSearchContextIdBase = collectNode.routing().jobSearchContextIdBase();
        plannerContext.allocateJobSearchContextIds(collectNode.routing());
        assertThat(collectNode.routing().jobSearchContextIdBase(), is(jobSearchContextIdBase));
    }

    @Test
    public void testExecutionNodeIdSequence() throws Exception {
        Planner.Context plannerContext = new Planner.Context(clusterService);
        CollectNode collectNode1 = new CollectNode(
                plannerContext.nextExecutionNodeId(), "collect1", shardRouting);
        CollectNode collectNode2 = new CollectNode(
                plannerContext.nextExecutionNodeId(), "collect2", shardRouting);

        assertThat(collectNode1.executionNodeId(), is(0));
        assertThat(collectNode2.executionNodeId(), is(1));
    }

    @SuppressWarnings("ConstantConditions")
    @Test
    public void testLimitThatIsBiggerThanPageSizeCausesQTFPUshPlan() throws Exception {
        QueryThenFetch plan = (QueryThenFetch) plan("select * from users limit 2147483647 ");
        assertThat(plan.collectNode().downstreamNodes().size(), is(1));
        assertThat(plan.collectNode().downstreamNodes().get(0), is(LOCAL_NODE_ID));
        assertThat(plan.collectNode().hasDistributingDownstreams(), is(true));
    }

    @Test
    public void testKillPlan() throws Exception {
        Plan killPlan = plan("kill all");
        assertThat(killPlan, Matchers.<Plan>is(KillPlan.INSTANCE));
    }
}

<code block>


package io.crate.planner.node;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.Sets;
import io.crate.metadata.FunctionIdent;
import io.crate.metadata.FunctionInfo;
import io.crate.operation.aggregation.impl.CountAggregation;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.GroupProjection;
import io.crate.planner.projection.TopNProjection;
import io.crate.planner.symbol.Aggregation;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.test.integration.CrateUnitTest;
import io.crate.testing.TestingHelpers;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.elasticsearch.common.io.stream.BytesStreamInput;
import org.elasticsearch.common.io.stream.BytesStreamOutput;
import org.junit.Test;

import java.util.Arrays;
import java.util.UUID;

import static org.hamcrest.core.Is.is;

public class MergeNodeTest extends CrateUnitTest {


    @Test
    public void testSerialization() throws Exception {
        MergeNode node = new MergeNode(0, "merge", 2);
        node.jobId(UUID.randomUUID());
        node.executionNodes(Sets.newHashSet("node1", "node2"));
        node.inputTypes(Arrays.<DataType>asList(DataTypes.UNDEFINED, DataTypes.STRING));
        node.downstreamNodes(Sets.newHashSet("node3", "node4"));

        Reference nameRef = TestingHelpers.createReference("name", DataTypes.STRING);
        GroupProjection groupProjection = new GroupProjection();
        groupProjection.keys(Arrays.<Symbol>asList(nameRef));
        groupProjection.values(Arrays.asList(
                new Aggregation(
                        new FunctionInfo(new FunctionIdent(CountAggregation.NAME, ImmutableList.<DataType>of()), DataTypes.LONG),
                        ImmutableList.<Symbol>of(),
                        Aggregation.Step.PARTIAL,
                        Aggregation.Step.FINAL
                )
        ));
        TopNProjection topNProjection = new TopNProjection(10, 0);

        node.projections(Arrays.asList(groupProjection, topNProjection));

        BytesStreamOutput output = new BytesStreamOutput();
        node.writeTo(output);


        BytesStreamInput input = new BytesStreamInput(output.bytes());
        MergeNode node2 = new MergeNode();
        node2.readFrom(input);

        assertThat(node.downstreamNodes(), is(node2.downstreamNodes()));
        assertThat(node.numUpstreams(), is(node2.numUpstreams()));
        assertThat(node.executionNodes(), is(node2.executionNodes()));
        assertThat(node.jobId(), is(node2.jobId()));
        assertThat(node.inputTypes(), is(node2.inputTypes()));
        assertThat(node.executionNodeId(), is(node2.executionNodeId()));
    }
}

<code block>


package io.crate.planner.node;

import com.google.common.collect.ImmutableList;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.symbol.Symbol;
import io.crate.planner.symbol.Value;
import io.crate.test.integration.CrateUnitTest;
import io.crate.types.DataTypes;
import org.elasticsearch.common.io.stream.BytesStreamInput;
import org.elasticsearch.common.io.stream.BytesStreamOutput;
import org.junit.Test;

import java.util.Arrays;
import java.util.UUID;

import static org.hamcrest.Matchers.equalTo;
import static org.hamcrest.Matchers.is;

public class CollectNodeTest extends CrateUnitTest {

    @Test
    public void testStreaming() throws Exception {
        CollectNode cn = new CollectNode(0, "cn");
        cn.maxRowGranularity(RowGranularity.DOC);
        cn.downstreamNodes(Arrays.asList("n1", "n2"));
        cn.toCollect(ImmutableList.<Symbol>of(new Value(DataTypes.STRING)));
        cn.jobId(UUID.randomUUID());

        BytesStreamOutput out = new BytesStreamOutput();
        cn.writeTo(out);

        BytesStreamInput in = new BytesStreamInput(out.bytes());
        CollectNode cn2 = new CollectNode(1, "collect");
        cn2.readFrom(in);
        assertThat(cn, equalTo(cn2));

        assertThat(cn.toCollect(), is(cn2.toCollect()));
        assertThat(cn.downstreamNodes(), is(cn2.downstreamNodes()));
        assertThat(cn.executionNodes(), is(cn2.executionNodes()));
        assertThat(cn.jobId(), is(cn2.jobId()));
        assertThat(cn.executionNodeId(), is(cn2.executionNodeId()));
        assertThat(cn.maxRowGranularity(), is(cn2.maxRowGranularity()));
    }
}

<code block>


package io.crate.planner.node;

import com.google.common.collect.ImmutableList;
import io.crate.Streamer;
import io.crate.metadata.FunctionIdent;
import io.crate.metadata.FunctionInfo;
import io.crate.metadata.Functions;
import io.crate.metadata.Routing;
import io.crate.operation.aggregation.impl.AggregationImplModule;
import io.crate.operation.aggregation.impl.CountAggregation;
import io.crate.operation.aggregation.impl.MaximumAggregation;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.AggregationProjection;
import io.crate.planner.projection.GroupProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.TopNProjection;
import io.crate.planner.symbol.Aggregation;
import io.crate.planner.symbol.InputColumn;
import io.crate.planner.symbol.Literal;
import io.crate.planner.symbol.Symbol;
import io.crate.test.integration.CrateUnitTest;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.elasticsearch.common.inject.Injector;
import org.elasticsearch.common.inject.ModulesBuilder;
import org.junit.Before;
import org.junit.Test;

import java.util.Arrays;
import java.util.List;
import java.util.Map;
import java.util.TreeMap;

import static org.hamcrest.Matchers.instanceOf;
import static org.hamcrest.Matchers.is;

public class StreamerVisitorTest extends CrateUnitTest {

    private StreamerVisitor visitor;
    private FunctionInfo maxInfo;
    private FunctionInfo countInfo;

    final static Routing EMPTY_ROUTING = new Routing(new TreeMap<String, Map<String, List<Integer>>>());

    @Before
    public void prepare() {
        Injector injector = new ModulesBuilder()
                .add(new AggregationImplModule())
                .createInjector();
        Functions functions = injector.getInstance(Functions.class);
        visitor = new StreamerVisitor(functions);
        maxInfo = new FunctionInfo(new FunctionIdent(MaximumAggregation.NAME, Arrays.<DataType>asList(DataTypes.INTEGER)), DataTypes.INTEGER);
        countInfo = new FunctionInfo(new FunctionIdent(CountAggregation.NAME, ImmutableList.<DataType>of()), DataTypes.LONG);
    }

    @Test
    public void testGetOutputStreamersFromCollectNode() throws Exception {
        CollectNode collectNode = new CollectNode(0, "bla", EMPTY_ROUTING);
        collectNode.outputTypes(Arrays.<DataType>asList(DataTypes.BOOLEAN, DataTypes.FLOAT, DataTypes.OBJECT));
        StreamerVisitor.Context ctx = visitor.processPlanNode(collectNode);
        Streamer<?>[] streamers = ctx.outputStreamers();
        assertThat(streamers.length, is(3));
        assertThat(streamers[0], instanceOf(DataTypes.BOOLEAN.streamer().getClass()));
        assertThat(streamers[1], instanceOf(DataTypes.FLOAT.streamer().getClass()));
        assertThat(streamers[2], instanceOf(DataTypes.OBJECT.streamer().getClass()));


        ctx = visitor.processExecutionNode(collectNode);
        streamers = ctx.outputStreamers();
        assertThat(streamers.length, is(3));
        assertThat(streamers[0], instanceOf(DataTypes.BOOLEAN.streamer().getClass()));
        assertThat(streamers[1], instanceOf(DataTypes.FLOAT.streamer().getClass()));
        assertThat(streamers[2], instanceOf(DataTypes.OBJECT.streamer().getClass()));
    }

    @Test
    public void testGetOutputStreamersFromCollectNodeWithWrongNull() throws Exception {

        CollectNode collectNode = new CollectNode(0, "bla", EMPTY_ROUTING);
        collectNode.outputTypes(Arrays.<DataType>asList(DataTypes.BOOLEAN, null, DataTypes.OBJECT));
        StreamerVisitor.Context ctx = visitor.processPlanNode(collectNode);

        assertEquals(DataTypes.UNDEFINED.streamer(), ctx.outputStreamers()[1]);


        ctx = visitor.processExecutionNode(collectNode);

        assertEquals(DataTypes.UNDEFINED.streamer(), ctx.outputStreamers()[1]);
    }

    @Test
    public void testGetOutputStreamersFromCollectNodeWithAggregations() throws Exception {
        CollectNode collectNode = new CollectNode(0, "bla", EMPTY_ROUTING);
        collectNode.outputTypes(Arrays.<DataType>asList(DataTypes.BOOLEAN, null, null, DataTypes.DOUBLE));
        AggregationProjection aggregationProjection = new AggregationProjection();
        aggregationProjection.aggregations(Arrays.asList( 
                new Aggregation(maxInfo, Arrays.<Symbol>asList(new InputColumn(0)), Aggregation.Step.ITER, Aggregation.Step.FINAL),
                new Aggregation(maxInfo, Arrays.<Symbol>asList(new InputColumn(1)), Aggregation.Step.ITER, Aggregation.Step.PARTIAL)
        ));
        collectNode.projections(Arrays.<Projection>asList(aggregationProjection));
        StreamerVisitor.Context ctx = visitor.processPlanNode(collectNode);
        Streamer<?>[] streamers = ctx.outputStreamers();
        assertThat(streamers.length, is(4));
        assertThat(streamers[0], instanceOf(DataTypes.BOOLEAN.streamer().getClass()));
        assertThat(streamers[1], instanceOf(DataTypes.INTEGER.streamer().getClass()));
        assertThat(streamers[2], instanceOf(DataTypes.INTEGER.streamer().getClass()));
        assertThat(streamers[3], instanceOf(DataTypes.DOUBLE.streamer().getClass()));
    }

    @Test
    public void testGetOutputStreamersFromCollectNodeWithGroupAndTopNProjection() throws Exception {
        CollectNode collectNode = new CollectNode(0, "mynode", EMPTY_ROUTING);
        collectNode.outputTypes(Arrays.<DataType>asList(DataTypes.UNDEFINED));
        GroupProjection groupProjection = new GroupProjection(
                Arrays.<Symbol>asList(Literal.newLiteral("key")),
                Arrays.asList(new Aggregation(
                        countInfo,
                        ImmutableList.<Symbol>of(),
                        Aggregation.Step.PARTIAL, Aggregation.Step.FINAL))
        );
        collectNode.projections(Arrays.<Projection>asList(groupProjection, new TopNProjection(10,0)));
        StreamerVisitor.Context ctx = visitor.processPlanNode(collectNode);
        Streamer<?>[] streamers = ctx.outputStreamers();
        assertThat(streamers.length, is(1));
        assertThat(streamers[0], instanceOf(DataTypes.LONG.streamer().getClass()));
    }

    @Test
    public void testGetInputStreamersForMergeNode() throws Exception {
        MergeNode mergeNode = new MergeNode(0, "mÃ¶rtsch", 2);
        mergeNode.inputTypes(Arrays.<DataType>asList(DataTypes.BOOLEAN, DataTypes.SHORT, DataTypes.TIMESTAMP));
        StreamerVisitor.Context ctx = visitor.processPlanNode(mergeNode);
        Streamer<?>[] streamers = ctx.inputStreamers();
        assertThat(streamers.length, is(3));
        assertThat(streamers[0], instanceOf(DataTypes.BOOLEAN.streamer().getClass()));
        assertThat(streamers[1], instanceOf(DataTypes.SHORT.streamer().getClass()));
        assertThat(streamers[2], instanceOf(DataTypes.TIMESTAMP.streamer().getClass()));
    }

    @Test(expected= IllegalStateException.class)
    public void testGetInputStreamersForMergeNodeWithWrongNull() throws Exception {
        MergeNode mergeNode = new MergeNode(0, "mÃ¶rtsch", 2);
        mergeNode.inputTypes(Arrays.<DataType>asList(DataTypes.BOOLEAN, null, DataTypes.TIMESTAMP));
        visitor.processPlanNode(mergeNode);
    }

    @Test
    public void testGetInputStreamersForMergeNodeWithAggregations() throws Exception {
        MergeNode mergeNode = new MergeNode(0, "mÃ¶rtsch", 2);
        mergeNode.inputTypes(Arrays.<DataType>asList(DataTypes.UNDEFINED, DataTypes.TIMESTAMP));
        AggregationProjection aggregationProjection = new AggregationProjection();
        aggregationProjection.aggregations(Arrays.asList(
                new Aggregation(maxInfo, Arrays.<Symbol>asList(new InputColumn(0)), Aggregation.Step.PARTIAL, Aggregation.Step.FINAL)
        ));
        mergeNode.projections(Arrays.<Projection>asList(aggregationProjection));
        StreamerVisitor.Context ctx = visitor.processPlanNode(mergeNode);
        Streamer<?>[] streamers = ctx.inputStreamers();
        assertThat(streamers.length, is(2));
        assertThat(streamers[0], instanceOf(DataTypes.INTEGER.streamer().getClass()));
        assertThat(streamers[1], instanceOf(DataTypes.TIMESTAMP.streamer().getClass()));
    }

    @Test
    public void testOutputStreamerFromGroupByMergeNode() throws Exception {


        MergeNode mergeNode = new MergeNode(0, "mÃ¶rtsch", 2);
        mergeNode.inputTypes(Arrays.<DataType>asList(DataTypes.STRING, DataTypes.UNDEFINED));
        GroupProjection groupProjection = new GroupProjection(
                Arrays.<Symbol>asList(Literal.newLiteral("key")),
                Arrays.asList(new Aggregation(
                        countInfo,
                        ImmutableList.<Symbol>of(),
                        Aggregation.Step.PARTIAL, Aggregation.Step.FINAL))
        );

        TopNProjection topNProjection = new TopNProjection(2, 0);
        topNProjection.outputs(Arrays.<Symbol>asList(
                new InputColumn(1),
                new InputColumn(0)
        ));

        mergeNode.projections(Arrays.asList(groupProjection, topNProjection));
        mergeNode.outputTypes(Arrays.<DataType>asList(DataTypes.LONG, DataTypes.STRING));
        StreamerVisitor.Context context = visitor.processPlanNode(mergeNode);
        assertSame(DataTypes.STRING.streamer(), context.outputStreamers()[1]);
        assertSame(DataTypes.LONG.streamer(), context.outputStreamers()[0]);
    }
}

<code block>


package io.crate.planner.node.dql;

import io.crate.analyze.WhereClause;
import io.crate.core.collections.TreeMapBuilder;
import io.crate.metadata.Routing;
import io.crate.planner.node.ExecutionNode;
import io.crate.test.integration.CrateUnitTest;
import org.elasticsearch.common.io.stream.BytesStreamInput;
import org.elasticsearch.common.io.stream.BytesStreamOutput;
import org.junit.Test;

import java.util.*;

import static org.hamcrest.Matchers.*;

public class CountNodeTest extends CrateUnitTest {

    @Test
    public void testStreaming() throws Exception {
        Routing routing = new Routing(
                TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder()
                        .put("n1", TreeMapBuilder.<String, List<Integer>>newMapBuilder()
                                .put("i1", Arrays.asList(1, 2))
                                .put("i2", Arrays.asList(1, 2)).map())
                        .put("n2", TreeMapBuilder.<String, List<Integer>>newMapBuilder()
                                .put("i1", Collections.singletonList(3)).map()).map());
        CountNode countNode = new CountNode(1, routing, WhereClause.MATCH_ALL);
        UUID jobId = UUID.randomUUID();
        countNode.jobId(jobId);

        BytesStreamOutput out = new BytesStreamOutput(10);
        countNode.writeTo(out);

        BytesStreamInput in = new BytesStreamInput(out.bytes());

        CountNode streamedNode = CountNode.FACTORY.create();
        streamedNode.readFrom(in);

        assertThat(streamedNode.jobId(), is(jobId));
        assertThat(streamedNode.executionNodeId(), is(1));
        assertThat(streamedNode.downstreamNodes(), contains(ExecutionNode.DIRECT_RETURN_DOWNSTREAM_NODE));
        assertThat(streamedNode.executionNodes(), containsInAnyOrder("n1", "n2"));
        assertThat(streamedNode.routing(), equalTo(routing));
    }
}
<code block>


package io.crate.action.sql;

import com.google.common.cache.CacheBuilder;
import com.google.common.cache.CacheLoader;
import com.google.common.cache.LoadingCache;
import com.google.common.util.concurrent.FutureCallback;
import com.google.common.util.concurrent.Futures;
import com.google.common.util.concurrent.ListenableFuture;
import io.crate.Constants;
import io.crate.analyze.Analysis;
import io.crate.analyze.Analyzer;
import io.crate.exceptions.*;
import io.crate.executor.Executor;
import io.crate.executor.Job;
import io.crate.executor.TaskResult;
import io.crate.metadata.PartitionName;
import io.crate.metadata.TableIdent;
import io.crate.operation.collect.StatsTables;
import io.crate.planner.Plan;
import io.crate.planner.PlanPrinter;
import io.crate.planner.Planner;
import io.crate.planner.symbol.Field;
import io.crate.sql.parser.ParsingException;
import io.crate.sql.parser.SqlParser;
import io.crate.sql.tree.Statement;
import io.crate.types.DataType;
import org.elasticsearch.action.ActionListener;
import org.elasticsearch.action.support.ActionFilters;
import org.elasticsearch.action.support.TransportAction;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.common.collect.Tuple;
import org.elasticsearch.common.inject.Provider;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.index.engine.DocumentAlreadyExistsException;
import org.elasticsearch.index.mapper.MapperParsingException;
import org.elasticsearch.indices.IndexAlreadyExistsException;
import org.elasticsearch.indices.IndexMissingException;
import org.elasticsearch.indices.InvalidIndexNameException;
import org.elasticsearch.indices.InvalidIndexTemplateException;
import org.elasticsearch.rest.RestStatus;
import org.elasticsearch.threadpool.ThreadPool;
import org.elasticsearch.transport.NodeDisconnectedException;

import javax.annotation.Nonnull;
import javax.annotation.Nullable;
import java.io.PrintWriter;
import java.io.StringWriter;
import java.util.List;
import java.util.concurrent.CancellationException;

import static com.google.common.base.MoreObjects.firstNonNull;

public abstract class TransportBaseSQLAction<TRequest extends SQLBaseRequest, TResponse extends SQLBaseResponse>
        extends TransportAction<TRequest, TResponse> {

    private static final DataType[] EMPTY_TYPES = new DataType[0];
    private static final String[] EMPTY_NAMES = new String[0];


    private final LoadingCache<String, Statement> statementCache = CacheBuilder.newBuilder()
            .maximumSize(100)
            .build(
                    new CacheLoader<String, Statement>() {
                        @Override
                        public Statement load(@Nonnull String statement) throws Exception {
                            return SqlParser.createStatement(statement);
                        }
                    }
            );

    private final ClusterService clusterService;
    protected final Analyzer analyzer;
    protected final Planner planner;
    private final Provider<Executor> executorProvider;
    private final StatsTables statsTables;
    private volatile boolean disabled;

    public TransportBaseSQLAction(ClusterService clusterService,
                                  Settings settings,
                                  String actionName,
                                  ThreadPool threadPool,
                                  Analyzer analyzer,
                                  Planner planner,
                                  Provider<Executor> executorProvider,
                                  StatsTables statsTables,
                                  ActionFilters actionFilters) {
        super(settings, actionName, threadPool, actionFilters);
        this.clusterService = clusterService;
        this.analyzer = analyzer;
        this.planner = planner;
        this.executorProvider = executorProvider;
        this.statsTables = statsTables;
    }

    public abstract Analysis getAnalysis(Statement statement, TRequest request);



    protected abstract TResponse emptyResponse(TRequest request, String[] outputNames, @Nullable DataType[] types);


    protected abstract TResponse createResponseFromResult(String[] outputNames,
                                                          DataType[] outputTypes,
                                                          List<TaskResult> result,
                                                          boolean expectsAffectedRows,
                                                          TRequest request);


    private TResponse createResponseFromResult(@Nullable List<TaskResult> result, Analysis analysis, TRequest request) {
        String[] outputNames;
        DataType[] outputTypes;
        if (analysis.expectsAffectedRows()) {
            outputNames = EMPTY_NAMES;
            outputTypes = EMPTY_TYPES;
        } else {
            assert analysis.rootRelation() != null;
            outputNames = new String[analysis.rootRelation().fields().size()];
            outputTypes = new DataType[analysis.rootRelation().fields().size()];
            for (int i = 0; i < analysis.rootRelation().fields().size(); i++) {
                Field field = analysis.rootRelation().fields().get(i);
                outputNames[i] = field.path().outputName();
                outputTypes[i] = field.valueType();
            }
        }
        if (result == null) {
            return emptyResponse(request, outputNames, outputTypes);
        } else {
            return createResponseFromResult(outputNames, outputTypes, result, analysis.expectsAffectedRows(), request);
        }

    }

    @Override
    protected void doExecute(TRequest request, ActionListener<TResponse> listener) {
        logger.debug("{}", request);
        statsTables.activeRequestsInc();
        if (disabled) {
            sendResponse(listener, new NodeDisconnectedException(clusterService.localNode(), actionName));
            return;
        }
        try {
            Statement statement = statementCache.get(request.stmt());
            Analysis analysis = getAnalysis(statement, request);
            processAnalysis(analysis, request, listener);
        } catch (Throwable e) {
            logger.debug("Error executing SQLRequest", e);
            sendResponse(listener, buildSQLActionException(e));
        }
    }

    private void sendResponse(ActionListener<TResponse> listener, Throwable throwable) {
        listener.onFailure(throwable);
        statsTables.activeRequestsDec();
    }

    private void sendResponse(ActionListener<TResponse> listener, TResponse response) {
        listener.onResponse(response);
        statsTables.activeRequestsDec();
    }

    private void processAnalysis(Analysis analysis, TRequest request, ActionListener<TResponse> listener) {
        final Plan plan = planner.plan(analysis);
        tracePlan(plan);
        executePlan(analysis, plan, listener, request);
    }

    private void executePlan(final Analysis analysis,
                             final Plan plan,
                             final ActionListener<TResponse> listener,
                             final TRequest request) {
        Executor executor = executorProvider.get();
        Job job = executor.newJob(plan);

        statsTables.jobStarted(plan.jobId(), request.stmt());
        List<? extends ListenableFuture<TaskResult>> resultFutureList = executor.execute(job);
        Futures.addCallback(Futures.allAsList(resultFutureList), new FutureCallback<List<TaskResult>>() {
                    @Override
                    public void onSuccess(@Nullable List<TaskResult> result) {
                        TResponse response;
                        try {
                            response = createResponseFromResult(result, analysis, request);
                        } catch (Throwable e) {
                            sendResponse(listener, buildSQLActionException(e));
                            return;
                        }
                        statsTables.jobFinished(plan.jobId(), null);
                        sendResponse(listener, response);
                    }

                    @Override
                    public void onFailure(@Nonnull Throwable t) {
                        String message;
                        if (t instanceof CancellationException) {
                            message = Constants.KILLED_MESSAGE;
                            logger.debug("KILLED: [{}]", request.stmt());
                        } else {
                            message = Exceptions.messageOf(t);
                            logger.debug("Error processing SQLRequest", t);
                        }
                        statsTables.jobFinished(plan.jobId(), message);
                        sendResponse(listener, buildSQLActionException(t));
                    }
                }

        );
    }

    private void tracePlan(Plan plan) {
        if (logger.isTraceEnabled()) {
            PlanPrinter printer = new PlanPrinter();
            logger.trace(printer.print(plan));
        }
    }



    public Throwable esToCrateException(Throwable e) {
        e = Exceptions.unwrap(e);

        if (e instanceof IllegalArgumentException || e instanceof ParsingException) {
            return new SQLParseException(e.getMessage(), (Exception) e);
        } else if (e instanceof UnsupportedOperationException) {
            return new UnsupportedFeatureException(e.getMessage(), (Exception) e);
        } else if (e instanceof DocumentAlreadyExistsException) {
            return new DuplicateKeyException(
                    "A document with the same primary key exists already", e);
        } else if (e instanceof IndexAlreadyExistsException) {
            return new TableAlreadyExistsException(((IndexAlreadyExistsException) e).index().name(), e);
        } else if ((e instanceof InvalidIndexNameException)) {
            if (e.getMessage().contains("already exists as alias")) {

                return new TableAlreadyExistsException(((InvalidIndexNameException) e).index().getName(),
                        e);
            }
            return new InvalidTableNameException(((InvalidIndexNameException) e).index().getName(), e);
        } else if (e instanceof InvalidIndexTemplateException) {
            Tuple<String, String> schemaAndTable = PartitionName.schemaAndTableName(((InvalidIndexTemplateException) e).name());
            return new InvalidTableNameException(new TableIdent(schemaAndTable.v1(), schemaAndTable.v2()).fqn(), e);
        } else if (e instanceof IndexMissingException) {
            return new TableUnknownException(((IndexMissingException) e).index().name(), e);
        } else if (e instanceof org.elasticsearch.common.breaker.CircuitBreakingException) {
            return new CircuitBreakingException(e.getMessage());
        } else if (e instanceof CancellationException) {
            return new JobKilledException();
        }
        return e;
    }


    public SQLActionException buildSQLActionException(Throwable e) {
        if (e instanceof SQLActionException) {
            return (SQLActionException) e;
        }
        e = esToCrateException(e);

        int errorCode = 5000;
        RestStatus restStatus = RestStatus.INTERNAL_SERVER_ERROR;
        String message = e.getMessage();
        StringWriter stackTrace = new StringWriter();
        e.printStackTrace(new PrintWriter(stackTrace));

        if (e instanceof CrateException) {
            CrateException crateException = (CrateException) e;
            if (e instanceof ValidationException) {
                errorCode = 4000 + crateException.errorCode();
                restStatus = RestStatus.BAD_REQUEST;
            } else if (e instanceof ResourceUnknownException) {
                errorCode = 4040 + crateException.errorCode();
                restStatus = RestStatus.NOT_FOUND;
            } else if (e instanceof ConflictException) {
                errorCode = 4090 + crateException.errorCode();
                restStatus = RestStatus.CONFLICT;
            } else if (e instanceof UnhandledServerException) {
                errorCode = 5000 + crateException.errorCode();
            }
        } else if (e instanceof ParsingException) {
            errorCode = 4000;
            restStatus = RestStatus.BAD_REQUEST;
        } else if (e instanceof MapperParsingException) {
            errorCode = 4000;
            restStatus = RestStatus.BAD_REQUEST;
        }

        if (e instanceof NullPointerException && message == null) {
            StackTraceElement[] stackTrace1 = e.getStackTrace();
            if (stackTrace1.length > 0) {
                message = String.format("NPE in %s", stackTrace1[0]);
            }
        } else if (e instanceof ArrayIndexOutOfBoundsException) {

            StackTraceElement[] stackTrace1 = e.getStackTrace();
            if (stackTrace1.length > 0) {
                message = String.format("ArrayIndexOutOfBoundsException in %s", stackTrace1[0]);
            }
        }
        if (logger.isTraceEnabled()) {
            message = firstNonNull(message, stackTrace.toString());
        } else if (Constants.DEBUG_MODE) {

            Throwable t;
            if (e instanceof CrateException && e.getCause() != null) {


                t = e.getCause();
            } else {
                t = e;
            }
            StringWriter stringWriter = new StringWriter();
            t.printStackTrace(new PrintWriter(stringWriter));
            stackTrace = stringWriter;
            message = firstNonNull(message, stackTrace.toString());
        }
        return new SQLActionException(message, errorCode, restStatus, stackTrace.toString());
    }

    public void enable() {
        disabled = false;
    }

    public void disable() {
        disabled = true;
    }
}

<code block>


package io.crate.executor;

import java.util.ArrayList;
import java.util.Collection;
import java.util.List;
import java.util.UUID;

public class Job {

    private final UUID id;
    private List<Task> tasks = new ArrayList<>();

    public Job(UUID id) {
        this.id = id;
    }

    public UUID id() {
        return id;
    }

    public void addTasks(Collection<Task> tasks) {
        this.tasks.addAll(tasks);
    }

    public List<Task> tasks() {
        return tasks;
    }
}

<code block>


package io.crate.executor.transport;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.Iterables;
import com.google.common.util.concurrent.ListenableFuture;
import io.crate.action.job.ContextPreparer;
import io.crate.action.sql.DDLStatementDispatcher;
import io.crate.breaker.CrateCircuitBreakerService;
import io.crate.executor.*;
import io.crate.executor.task.DDLTask;
import io.crate.executor.task.NoopTask;
import io.crate.executor.transport.task.CreateTableTask;
import io.crate.executor.transport.task.DropTableTask;
import io.crate.executor.transport.task.KillTask;
import io.crate.executor.transport.task.SymbolBasedUpsertByIdTask;
import io.crate.executor.transport.task.elasticsearch.*;
import io.crate.jobs.JobContextService;
import io.crate.metadata.Functions;
import io.crate.metadata.ReferenceResolver;
import io.crate.operation.ImplementationSymbolVisitor;
import io.crate.operation.PageDownstreamFactory;
import io.crate.operation.projectors.ProjectionToProjectorVisitor;
import io.crate.planner.*;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.node.PlanNode;
import io.crate.planner.node.PlanNodeVisitor;
import io.crate.planner.node.StreamerVisitor;
import io.crate.planner.node.ddl.*;
import io.crate.planner.node.dml.*;
import io.crate.planner.node.dql.*;
import io.crate.planner.node.management.KillPlan;
import org.elasticsearch.action.bulk.BulkRetryCoordinatorPool;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.common.Nullable;
import org.elasticsearch.common.breaker.CircuitBreaker;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.inject.Provider;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.threadpool.ThreadPool;

import java.util.ArrayList;
import java.util.Collection;
import java.util.List;
import java.util.UUID;

public class TransportExecutor implements Executor, TaskExecutor {

    private final Functions functions;
    private final TaskCollectingVisitor planVisitor;
    private Provider<DDLStatementDispatcher> ddlAnalysisDispatcherProvider;
    private final NodeVisitor nodeVisitor;
    private final ThreadPool threadPool;

    private final ClusterService clusterService;
    private final JobContextService jobContextService;
    private final ContextPreparer contextPreparer;
    private final TransportActionProvider transportActionProvider;
    private final BulkRetryCoordinatorPool bulkRetryCoordinatorPool;

    private final ProjectionToProjectorVisitor globalProjectionToProjectionVisitor;


    private final CircuitBreaker circuitBreaker;

    private final PageDownstreamFactory pageDownstreamFactory;

    private final StreamerVisitor streamerVisitor;

    @Inject
    public TransportExecutor(Settings settings,
                             JobContextService jobContextService,
                             ContextPreparer contextPreparer,
                             TransportActionProvider transportActionProvider,
                             ThreadPool threadPool,
                             Functions functions,
                             ReferenceResolver referenceResolver,
                             PageDownstreamFactory pageDownstreamFactory,
                             Provider<DDLStatementDispatcher> ddlAnalysisDispatcherProvider,
                             ClusterService clusterService,
                             CrateCircuitBreakerService breakerService,
                             BulkRetryCoordinatorPool bulkRetryCoordinatorPool,
                             StreamerVisitor streamerVisitor) {
        this.jobContextService = jobContextService;
        this.contextPreparer = contextPreparer;
        this.transportActionProvider = transportActionProvider;
        this.pageDownstreamFactory = pageDownstreamFactory;
        this.threadPool = threadPool;
        this.functions = functions;
        this.ddlAnalysisDispatcherProvider = ddlAnalysisDispatcherProvider;
        this.clusterService = clusterService;
        this.bulkRetryCoordinatorPool = bulkRetryCoordinatorPool;
        this.streamerVisitor = streamerVisitor;
        this.nodeVisitor = new NodeVisitor();
        this.planVisitor = new TaskCollectingVisitor();
        this.circuitBreaker = breakerService.getBreaker(CrateCircuitBreakerService.QUERY_BREAKER);
        ImplementationSymbolVisitor globalImplementationSymbolVisitor = new ImplementationSymbolVisitor(
                referenceResolver, functions, RowGranularity.CLUSTER);
        this.globalProjectionToProjectionVisitor = new ProjectionToProjectorVisitor(
                clusterService,
                threadPool,
                settings,
                transportActionProvider,
                bulkRetryCoordinatorPool,
                globalImplementationSymbolVisitor);
    }

    @Override
    public Job newJob(Plan plan) {
        final Job job = new Job(plan.jobId());
        List<Task> tasks = planVisitor.process(plan, job);
        job.addTasks(tasks);
        return job;
    }

    @Override
    public List<? extends ListenableFuture<TaskResult>> execute(Job job) {
        assert job.tasks().size() > 0;
        return execute(job.tasks());

    }

    @Override
    public List<Task> newTasks(PlanNode planNode, UUID jobId) {
        return planNode.accept(nodeVisitor, jobId);
    }

    @Override
    public List<? extends ListenableFuture<TaskResult>> execute(Collection<Task> tasks) {
        Task lastTask = null;
        assert tasks.size() > 0 : "need at least one task to execute";
        for (Task task : tasks) {

            if (lastTask != null) {
                task.upstreamResult(lastTask.result());
            }
            task.start();
            lastTask = task;
        }
        assert lastTask != null;
        return lastTask.result();
    }

    class TaskCollectingVisitor extends PlanVisitor<Job, List<Task>> {

        @Override
        public List<Task> visitIterablePlan(IterablePlan plan, Job job) {
            List<Task> tasks = new ArrayList<>();
            for (PlanNode planNode : plan) {
                tasks.addAll(planNode.accept(nodeVisitor, job.id()));
            }
            return tasks;
        }

        @Override
        public List<Task> visitNoopPlan(NoopPlan plan, Job job) {
            return ImmutableList.<Task>of(NoopTask.INSTANCE);
        }

        @Override
        public List<Task> visitGlobalAggregate(GlobalAggregate plan, Job job) {
            return ImmutableList.of(createExecutableNodesTask(job, plan.collectNode(), plan.mergeNode()));
        }

        @Override
        public List<Task> visitCollectAndMerge(CollectAndMerge plan, Job job) {
            return ImmutableList.of(createExecutableNodesTask(job, plan.collectNode(), plan.localMergeNode()));
        }

        @Override
        public List<Task> visitQueryAndFetch(QueryAndFetch plan, Job job) {
            return ImmutableList.of(createExecutableNodesTask(job, plan.collectNode(), plan.localMergeNode()));
        }

        @Override
        public List<Task> visitCountPlan(CountPlan countPlan, Job job) {
            return ImmutableList.of(createExecutableNodesTask(job, countPlan.countNode(), countPlan.mergeNode()));
        }

        private Task createExecutableNodesTask(Job job, ExecutionNode executionNode, @Nullable MergeNode localMergeNode) {
            return createExecutableNodesTask(job,
                    ImmutableList.<List<ExecutionNode>>of(ImmutableList.of(executionNode)),
                    localMergeNode == null ? null : ImmutableList.of(localMergeNode));
        }

        private ExecutionNodesTask createExecutableNodesTask(Job job, List<List<ExecutionNode>> groupedExecutionNodes, @Nullable List<MergeNode> localMergeNodes) {
            return new ExecutionNodesTask(
                    job.id(),
                    clusterService,
                    contextPreparer,
                    jobContextService,
                    pageDownstreamFactory,
                    threadPool,
                    transportActionProvider.transportJobInitAction(),
                    transportActionProvider.transportCloseContextNodeAction(),
                    streamerVisitor,
                    circuitBreaker,
                    localMergeNodes,
                    groupedExecutionNodes
            );
        }

        @Override
        public List<Task> visitNonDistributedGroupBy(NonDistributedGroupBy plan, Job job) {
            return ImmutableList.of(createExecutableNodesTask(job, plan.collectNode(), plan.localMergeNode()));
        }

        @Override
        public List<Task> visitUpsert(Upsert plan, Job job) {
            if (plan.nodes().size() == 1 && plan.nodes().get(0) instanceof IterablePlan) {
                return process(plan.nodes().get(0), job);
            }

            List<List<ExecutionNode>> groupedExecutionNodes = new ArrayList<>(plan.nodes().size());
            List<MergeNode> mergeNodes = new ArrayList<>(plan.nodes().size());
            for (Plan subPlan : plan.nodes()) {
                assert subPlan instanceof CollectAndMerge;
                groupedExecutionNodes.add(ImmutableList.<ExecutionNode>of(((CollectAndMerge) subPlan).collectNode()));
                mergeNodes.add(((CollectAndMerge) subPlan).localMergeNode());
            }
            ExecutionNodesTask task = createExecutableNodesTask(job, groupedExecutionNodes, mergeNodes);
            task.rowCountResult(true);
            return ImmutableList.<Task>of(task);
        }

        @Override
        public List<Task> visitDistributedGroupBy(DistributedGroupBy plan, Job job) {
            MergeNode localMergeNode = plan.localMergeNode();
            List<MergeNode> mergeNodes = null;
            if (localMergeNode != null) {
                mergeNodes = ImmutableList.of(localMergeNode);
            }
            return ImmutableList.<Task>of(
                    createExecutableNodesTask(job,
                            ImmutableList.<List<ExecutionNode>>of(
                                    ImmutableList.<ExecutionNode>of(
                                            plan.collectNode(),
                                            plan.reducerMergeNode())),
                            mergeNodes));
        }

        @Override
        public List<Task> visitInsertByQuery(InsertFromSubQuery node, Job job) {
            List<Task> tasks = process(node.innerPlan(), job);
            if(node.handlerMergeNode().isPresent()) {

                Task previousTask = Iterables.getLast(tasks);
                if (previousTask instanceof ExecutionNodesTask) {
                    ((ExecutionNodesTask) previousTask).mergeNodes(ImmutableList.of(node.handlerMergeNode().get()));
                } else {
                    ArrayList<Task> tasks2 = new ArrayList<>(tasks);
                    tasks2.addAll(nodeVisitor.visitMergeNode(node.handlerMergeNode().get(), job.id()));
                    return tasks2;
                }
            }
            return tasks;
        }

        @Override
        public List<Task> visitQueryThenFetch(QueryThenFetch plan, Job job) {
            return ImmutableList.of(createExecutableNodesTask(job, plan.collectNode(), plan.mergeNode()));
        }

        @Override
        public List<Task> visitKillPlan(KillPlan killPlan, Job job) {
            return ImmutableList.<Task>of(new KillTask(
                    clusterService,
                    transportActionProvider.transportKillAllNodeAction(),
                    job.id()));
        }
    }

    class NodeVisitor extends PlanNodeVisitor<UUID, ImmutableList<Task>> {

        private ImmutableList<Task> singleTask(Task task) {
            return ImmutableList.of(task);
        }

        @Override
        public ImmutableList<Task> visitGenericDDLNode(GenericDDLNode node, UUID jobId) {
            return singleTask(new DDLTask(jobId, ddlAnalysisDispatcherProvider.get(), node));
        }

        @Override
        public ImmutableList<Task> visitESGetNode(ESGetNode node, UUID jobId) {
            return singleTask(new ESGetTask(
                    jobId,
                    functions,
                    globalProjectionToProjectionVisitor,
                    transportActionProvider.transportMultiGetAction(),
                    transportActionProvider.transportGetAction(),
                    node,
                    jobContextService));
        }

        @Override
        public ImmutableList<Task> visitESDeleteByQueryNode(ESDeleteByQueryNode node, UUID jobId) {
            return singleTask(new ESDeleteByQueryTask(
                    jobId,
                    node,
                    transportActionProvider.transportDeleteByQueryAction(),
                    jobContextService));
        }

        @Override
        public ImmutableList<Task> visitESDeleteNode(ESDeleteNode node, UUID jobId) {
            return singleTask(new ESDeleteTask(
                    jobId,
                    node,
                    transportActionProvider.transportDeleteAction(),
                    jobContextService));
        }

        @Override
        public ImmutableList<Task> visitCreateTableNode(CreateTableNode node, UUID jobId) {
            return singleTask(new CreateTableTask(
                            jobId,
                            clusterService,
                            transportActionProvider.transportCreateIndexAction(),
                            transportActionProvider.transportDeleteIndexAction(),
                            transportActionProvider.transportPutIndexTemplateAction(),
                            node)
            );
        }

        @Override
        public ImmutableList<Task> visitESCreateTemplateNode(ESCreateTemplateNode node, UUID jobId) {
            return singleTask(new ESCreateTemplateTask(jobId,
                    node,
                    transportActionProvider.transportPutIndexTemplateAction()));
        }

        @Override
        public ImmutableList<Task> visitSymbolBasedUpsertByIdNode(SymbolBasedUpsertByIdNode node, UUID jobId) {
            return singleTask(new SymbolBasedUpsertByIdTask(jobId,
                    clusterService,
                    clusterService.state().metaData().settings(),
                    transportActionProvider.symbolBasedTransportShardUpsertActionDelegate(),
                    transportActionProvider.transportCreateIndexAction(),
                    transportActionProvider.transportBulkCreateIndicesAction(),
                    bulkRetryCoordinatorPool,
                    node,
                    jobContextService));
        }

        @Override
        public ImmutableList<Task> visitDropTableNode(DropTableNode node, UUID jobId) {
            return singleTask(new DropTableTask(jobId,
                    transportActionProvider.transportDeleteIndexTemplateAction(),
                    transportActionProvider.transportDeleteIndexAction(),
                    node));
        }

        @Override
        public ImmutableList<Task> visitESDeletePartitionNode(ESDeletePartitionNode node, UUID jobId) {
            return singleTask(new ESDeletePartitionTask(jobId,
                    transportActionProvider.transportDeleteIndexAction(),
                    node));
        }

        @Override
        public ImmutableList<Task> visitESClusterUpdateSettingsNode(ESClusterUpdateSettingsNode node, UUID jobId) {
            return singleTask(new ESClusterUpdateSettingsTask(
                    jobId,
                    transportActionProvider.transportClusterUpdateSettingsAction(),
                    node));
        }

        @Override
        protected ImmutableList<Task> visitPlanNode(PlanNode node, UUID jobId) {
            throw new UnsupportedOperationException(
                    String.format("Can't generate job/task for planNode %s", node));
        }
    }
}

<code block>


package io.crate.planner;

import com.google.common.collect.Lists;
import io.crate.planner.node.PlanNode;

import java.util.ArrayList;
import java.util.Iterator;
import java.util.UUID;


public class IterablePlan implements Iterable<PlanNode>, Plan {

    private ArrayList<PlanNode> nodes;
    private final UUID id;

    public IterablePlan(UUID id, PlanNode... nodes) {
        this.id = id;
        this.nodes = Lists.newArrayList(nodes);
    }

    public void add(PlanNode node) {
        nodes.add(node);
    }

    @Override
    public Iterator<PlanNode> iterator() {
        return nodes.iterator();
    }

    public boolean isEmpty() {
        return nodes.isEmpty();
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitIterablePlan(this, context);
    }

    @Override
    public UUID jobId() {
        return id;
    }
}

<code block>


package io.crate.planner;

import java.util.UUID;


public class NoopPlan implements Plan {

    private final UUID id;

    public NoopPlan(UUID id) {
        this.id = id;
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitNoopPlan(this, context);
    }

    @Override
    public UUID jobId() {
        return id;
    }
}

<code block>


package io.crate.planner;

import com.google.common.base.MoreObjects;
import com.google.common.base.Predicates;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableSet;
import com.google.common.collect.Iterables;
import io.crate.analyze.OrderBy;
import io.crate.analyze.WhereClause;
import io.crate.metadata.PartitionName;
import io.crate.metadata.Routing;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.consumer.OrderByPositionVisitor;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.DQLPlanNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.InputColumn;
import io.crate.planner.symbol.Symbol;
import io.crate.planner.symbol.Symbols;

import javax.annotation.Nullable;
import java.util.List;
import java.util.Map;
import java.util.TreeMap;
import java.util.UUID;

public class PlanNodeBuilder {

    public static CollectNode distributingCollect(UUID jobId,
                                                  TableInfo tableInfo,
                                                  Planner.Context plannerContext,
                                                  WhereClause whereClause,
                                                  List<Symbol> toCollect,
                                                  List<String> downstreamNodes,
                                                  ImmutableList<Projection> projections) {
        Routing routing = tableInfo.getRouting(whereClause, null);
        plannerContext.allocateJobSearchContextIds(routing);
        CollectNode node = new CollectNode(
                jobId,
                plannerContext.nextExecutionNodeId(),
                "distributing collect",
                routing);
        node.whereClause(whereClause);
        node.maxRowGranularity(tableInfo.rowGranularity());
        node.downstreamNodes(downstreamNodes);
        node.toCollect(toCollect);
        node.projections(projections);

        node.isPartitioned(tableInfo.isPartitioned());
        setOutputTypes(node);
        return node;
    }

    public static MergeNode distributedMerge(UUID jobId,
                                             CollectNode collectNode,
                                             Planner.Context plannerContext,
                                             List<Projection> projections) {
        MergeNode node = new MergeNode(
                jobId,
                plannerContext.nextExecutionNodeId(),
                "distributed merge",
                collectNode.executionNodes().size());
        node.projections(projections);

        assert collectNode.hasDistributingDownstreams();
        node.executionNodes(ImmutableSet.copyOf(collectNode.downstreamNodes()));
        connectTypes(collectNode, node);
        return node;
    }

    public static MergeNode localMerge(UUID jobId,
                                       List<Projection> projections,
                                       DQLPlanNode previousNode,
                                       Planner.Context plannerContext) {
        MergeNode node = new MergeNode(
                jobId,
                plannerContext.nextExecutionNodeId(),
                "localMerge",
                previousNode.executionNodes().size());
        node.projections(projections);
        connectTypes(previousNode, node);
        return node;
    }


    public static MergeNode sortedLocalMerge(UUID jobId,
                                             List<Projection> projections,
                                             OrderBy orderBy,
                                             List<Symbol> sourceSymbols,
                                             @Nullable List<Symbol> orderBySymbols,
                                             DQLPlanNode previousNode,
                                             Planner.Context plannerContext) {
        int[] orderByIndices = OrderByPositionVisitor.orderByPositions(
                MoreObjects.firstNonNull(orderBySymbols, orderBy.orderBySymbols()),
                sourceSymbols
        );
        MergeNode node = MergeNode.sortedMergeNode(
                jobId,
                plannerContext.nextExecutionNodeId(),
                "sortedLocalMerge",
                previousNode.executionNodes().size(),
                orderByIndices,
                orderBy.reverseFlags(),
                orderBy.nullsFirst()
        );
        node.projections(projections);
        connectTypes(previousNode, node);
        return node;
    }


    public static void setOutputTypes(CollectNode node) {
        if (node.projections().isEmpty()) {
            node.outputTypes(Symbols.extractTypes(node.toCollect()));
        } else {
            node.outputTypes(Planner.extractDataTypes(node.projections(), Symbols.extractTypes(node.toCollect())));
        }
    }


    public static void connectTypes(DQLPlanNode previousNode, DQLPlanNode nextNode) {
        nextNode.inputTypes(previousNode.outputTypes());
        nextNode.outputTypes(Planner.extractDataTypes(nextNode.projections(), nextNode.inputTypes()));
    }

    public static CollectNode collect(UUID jobId,
                                      TableInfo tableInfo,
                                      Planner.Context plannerContext,
                                      WhereClause whereClause,
                                      List<Symbol> toCollect,
                                      ImmutableList<Projection> projections,
                                      @Nullable String partitionIdent,
                                      @Nullable String routingPreference,
                                      @Nullable OrderBy orderBy,
                                      @Nullable Integer limit) {
        assert !Iterables.any(toCollect, Predicates.instanceOf(InputColumn.class)) : "cannot collect inputcolumns";
        Routing routing = tableInfo.getRouting(whereClause, routingPreference);
        if (partitionIdent != null && routing.hasLocations()) {
            routing = filterRouting(routing, PartitionName.fromPartitionIdent(
                    tableInfo.ident().schema(), tableInfo.ident().name(), partitionIdent).stringValue());
        }
        plannerContext.allocateJobSearchContextIds(routing);
        CollectNode node = new CollectNode(
                jobId,
                plannerContext.nextExecutionNodeId(),
                "collect",
                routing,
                toCollect,
                projections);
        node.whereClause(whereClause);
        node.maxRowGranularity(tableInfo.rowGranularity());
        node.isPartitioned(tableInfo.isPartitioned());
        setOutputTypes(node);
        node.orderBy(orderBy);
        node.limit(limit);
        return node;
    }

    private static Routing filterRouting(Routing routing, String includeTableName) {
        assert routing.hasLocations();
        assert includeTableName != null;
        Map<String, Map<String, List<Integer>>> newLocations = new TreeMap<>();

        for (Map.Entry<String, Map<String, List<Integer>>> entry : routing.locations().entrySet()) {
            Map<String, List<Integer>> tableMap = new TreeMap<>();
            for (Map.Entry<String, List<Integer>> tableEntry : entry.getValue().entrySet()) {
                if (includeTableName.equals(tableEntry.getKey())) {
                    tableMap.put(tableEntry.getKey(), tableEntry.getValue());
                }
            }
            if (tableMap.size()>0){
                newLocations.put(entry.getKey(), tableMap);
            }

        }
        if (newLocations.size()>0) {
            return new Routing(newLocations);
        } else {
            return new Routing();
        }

    }

    public static CollectNode collect(UUID jobId,
                                      TableInfo tableInfo,
                                      Planner.Context plannerContext,
                                      WhereClause whereClause,
                                      List<Symbol> toCollect,
                                      ImmutableList<Projection> projections) {
        return collect(jobId, tableInfo, plannerContext, whereClause, toCollect, projections, null, null, null, null);
    }

    public static CollectNode collect(UUID jobId,
                                      TableInfo tableInfo,
                                      Planner.Context plannerContext,
                                      WhereClause whereClause,
                                      List<Symbol> toCollect,
                                      ImmutableList<Projection> projections,
                                      @Nullable String partitionIdent,
                                      @Nullable String routingPreference) {
        return collect(jobId, tableInfo, plannerContext, whereClause, toCollect, projections, partitionIdent, routingPreference, null, null);
    }

    public static CollectNode collect(UUID jobId,
                                      TableInfo tableInfo,
                                      Planner.Context plannerContext,
                                      WhereClause whereClause,
                                      List<Symbol> toCollect,
                                      ImmutableList<Projection> projections,
                                      @Nullable String partitionIdent) {
        return collect(jobId, tableInfo, plannerContext, whereClause, toCollect, projections, partitionIdent, null);
    }

    public static CollectNode collect(UUID jobId,
                                      TableInfo tableInfo,
                                      Planner.Context plannerContext,
                                      WhereClause whereClause,
                                      List<Symbol> toCollect,
                                      ImmutableList<Projection> projections,
                                      @Nullable OrderBy orderBy,
                                      @Nullable Integer limit) {
        return collect(jobId, tableInfo, plannerContext, whereClause, toCollect, projections, null, null, orderBy, limit);
    }
}

<code block>


package io.crate.planner;

import com.carrotsearch.hppc.IntObjectOpenHashMap;
import com.carrotsearch.hppc.procedures.ObjectProcedure;
import com.google.common.base.Preconditions;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.Lists;
import io.crate.analyze.*;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.analyze.relations.TableRelation;
import io.crate.analyze.where.DocKeys;
import io.crate.core.collections.TreeMapBuilder;
import io.crate.exceptions.UnhandledServerException;
import io.crate.metadata.*;
import io.crate.metadata.doc.DocSysColumns;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.aggregation.impl.CountAggregation;
import io.crate.planner.consumer.ConsumerContext;
import io.crate.planner.consumer.ConsumingPlanner;
import io.crate.planner.consumer.UpdateConsumer;
import io.crate.planner.node.ddl.*;
import io.crate.planner.node.dml.ESDeleteByQueryNode;
import io.crate.planner.node.dml.ESDeleteNode;
import io.crate.planner.node.dml.SymbolBasedUpsertByIdNode;
import io.crate.planner.node.dml.Upsert;
import io.crate.planner.node.dql.CollectAndMerge;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.FileUriCollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.management.KillPlan;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.SourceIndexWriterProjection;
import io.crate.planner.projection.WriterProjection;
import io.crate.planner.symbol.InputColumn;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.cluster.node.DiscoveryNodes;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.inject.Singleton;
import org.elasticsearch.common.settings.ImmutableSettings;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.index.shard.ShardId;

import javax.annotation.Nullable;
import java.io.IOException;
import java.util.*;
import java.util.concurrent.atomic.AtomicInteger;

import static com.google.common.base.MoreObjects.firstNonNull;

@Singleton
public class Planner extends AnalyzedStatementVisitor<Planner.Context, Plan> {

    private final ConsumingPlanner consumingPlanner;
    private final ClusterService clusterService;
    private UpdateConsumer updateConsumer;

    public static class Context {

        private final IntObjectOpenHashMap<ShardId> jobSearchContextIdToShard = new IntObjectOpenHashMap<>();
        private final IntObjectOpenHashMap<String> jobSearchContextIdToNode = new IntObjectOpenHashMap<>();
        private final ClusterService clusterService;
        private final UUID jobId = UUID.randomUUID();
        private int jobSearchContextIdBaseSeq = 0;
        private int executionNodeId = 0;

        public Context(ClusterService clusterService) {
            this.clusterService = clusterService;
        }

        public ClusterService clusterService() {
            return clusterService;
        }

        public UUID jobId() {
            return jobId;
        }


        public void allocateJobSearchContextIds(Routing routing) {
            if (routing.jobSearchContextIdBase() > -1 || routing.hasLocations() == false
                    || routing.numShards() == 0) {
                return;
            }
            int jobSearchContextId = jobSearchContextIdBaseSeq;
            jobSearchContextIdBaseSeq += routing.numShards();
            routing.jobSearchContextIdBase(jobSearchContextId);
            for (Map.Entry<String, Map<String, List<Integer>>> nodeEntry : routing.locations().entrySet()) {
                String nodeId = nodeEntry.getKey();
                Map<String, List<Integer>> nodeRouting = nodeEntry.getValue();
                if (nodeRouting != null) {
                    for (Map.Entry<String, List<Integer>> entry : nodeRouting.entrySet()) {
                        for (Integer shardId : entry.getValue()) {
                            jobSearchContextIdToShard.put(jobSearchContextId, new ShardId(entry.getKey(), shardId));
                            jobSearchContextIdToNode.put(jobSearchContextId, nodeId);
                            jobSearchContextId++;
                        }
                    }
                }
            }
        }

        @Nullable
        public ShardId shardId(int jobSearchContextId) {
            return jobSearchContextIdToShard.get(jobSearchContextId);
        }

        public IntObjectOpenHashMap<ShardId> jobSearchContextIdToShard() {
            return jobSearchContextIdToShard;
        }

        @Nullable
        public String nodeId(int jobSearchContextId) {
            return jobSearchContextIdToNode.get(jobSearchContextId);
        }

        public IntObjectOpenHashMap<String> jobSearchContextIdToNode() {
            return jobSearchContextIdToNode;
        }

        public int nextExecutionNodeId() {
            return executionNodeId++;
        }
    }

    @Inject
    public Planner(ClusterService clusterService, ConsumingPlanner consumingPlanner, UpdateConsumer updateConsumer) {
        this.clusterService = clusterService;
        this.updateConsumer = updateConsumer;
        this.consumingPlanner = consumingPlanner;
    }


    public Plan plan(Analysis analysis) {
        AnalyzedStatement analyzedStatement = analysis.analyzedStatement();
        return process(analyzedStatement, new Context(clusterService));
    }

    @Override
    protected Plan visitAnalyzedStatement(AnalyzedStatement analyzedStatement, Context context) {
        throw new UnsupportedOperationException(String.format("AnalyzedStatement \"%s\" not supported.", analyzedStatement));
    }

    @Override
    protected Plan visitSelectStatement(SelectAnalyzedStatement statement, Context context) {
        return consumingPlanner.plan(statement.relation(), context);
    }

    @Override
    protected Plan visitInsertFromValuesStatement(InsertFromValuesAnalyzedStatement statement, Context context) {
        Preconditions.checkState(!statement.sourceMaps().isEmpty(), "no values given");
        return processInsertStatement(statement, context);
    }

    @Override
    protected Plan visitInsertFromSubQueryStatement(InsertFromSubQueryAnalyzedStatement statement, Context context) {
        return consumingPlanner.plan(statement, context);
    }

    @Override
    protected Plan visitUpdateStatement(UpdateAnalyzedStatement statement, Context context) {
        ConsumerContext consumerContext = new ConsumerContext(statement, context);
        if (updateConsumer.consume(statement, consumerContext)) {
            return ((PlannedAnalyzedRelation) consumerContext.rootRelation()).plan();
        }
        throw new IllegalArgumentException("Couldn't plan Update statement");
    }

    @Override
    protected Plan visitDeleteStatement(DeleteAnalyzedStatement analyzedStatement, Context context) {
        IterablePlan plan = new IterablePlan(context.jobId());
        TableRelation tableRelation = analyzedStatement.analyzedRelation();
        List<WhereClause> whereClauses = new ArrayList<>(analyzedStatement.whereClauses().size());
        List<DocKeys.DocKey> docKeys = new ArrayList<>(analyzedStatement.whereClauses().size());
        for (WhereClause whereClause : analyzedStatement.whereClauses()) {
            if (whereClause.noMatch()) {
                continue;
            }
            if (whereClause.docKeys().isPresent() && whereClause.docKeys().get().size() == 1) {
                docKeys.add(whereClause.docKeys().get().getOnlyKey());
            } else if (!whereClause.noMatch()) {
                whereClauses.add(whereClause);
            }
        }
        if (!docKeys.isEmpty()) {
            plan.add(new ESDeleteNode(context.nextExecutionNodeId(), tableRelation.tableInfo(), docKeys));
        } else if (!whereClauses.isEmpty()) {
            createESDeleteByQueryNode(tableRelation.tableInfo(), whereClauses, plan, context);
        }

        if (plan.isEmpty()) {
            return new NoopPlan(context.jobId());
        }
        return plan;
    }

    @Override
    protected Plan visitCopyStatement(final CopyAnalyzedStatement analysis, Context context) {
        switch (analysis.mode()) {
            case FROM:
                return copyFromPlan(analysis, context);
            case TO:
                return copyToPlan(analysis, context);
            default:
                throw new UnsupportedOperationException("mode not supported: " + analysis.mode());
        }
    }

    private Plan copyToPlan(CopyAnalyzedStatement analysis, Context context) {
        TableInfo tableInfo = analysis.table();
        WriterProjection projection = new WriterProjection();
        projection.uri(analysis.uri());
        projection.isDirectoryUri(analysis.directoryUri());
        projection.settings(analysis.settings());

        List<Symbol> outputs;
        if (analysis.selectedColumns() != null && !analysis.selectedColumns().isEmpty()) {
            outputs = new ArrayList<>(analysis.selectedColumns().size());
            List<Symbol> columnSymbols = new ArrayList<>(analysis.selectedColumns().size());
            for (int i = 0; i < analysis.selectedColumns().size(); i++) {
                outputs.add(DocReferenceConverter.convertIfPossible(analysis.selectedColumns().get(i), analysis.table()));
                columnSymbols.add(new InputColumn(i, null));
            }
            projection.inputs(columnSymbols);
        } else {
            Reference sourceRef;
            if (analysis.table().isPartitioned() && analysis.partitionIdent() == null) {

                sourceRef = new Reference(analysis.table().getReferenceInfo(DocSysColumns.DOC));
                Map<ColumnIdent, Symbol> overwrites = new HashMap<>();
                for (ReferenceInfo referenceInfo : analysis.table().partitionedByColumns()) {
                    overwrites.put(referenceInfo.ident().columnIdent(), new Reference(referenceInfo));
                }
                projection.overwrites(overwrites);
            } else {
                sourceRef = new Reference(analysis.table().getReferenceInfo(DocSysColumns.RAW));
            }
            outputs = ImmutableList.<Symbol>of(sourceRef);
        }
        CollectNode collectNode = PlanNodeBuilder.collect(
                context.jobId(),
                tableInfo,
                context,
                WhereClause.MATCH_ALL,
                outputs,
                ImmutableList.<Projection>of(projection),
                analysis.partitionIdent()
        );

        MergeNode mergeNode = PlanNodeBuilder.localMerge(context.jobId(),
                ImmutableList.<Projection>of(CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION), collectNode, context);
        return new CollectAndMerge(collectNode, mergeNode, context.jobId());
    }

    private Plan copyFromPlan(CopyAnalyzedStatement analysis, Context context) {


        TableInfo table = analysis.table();
        int clusteredByPrimaryKeyIdx = table.primaryKey().indexOf(analysis.table().clusteredBy());
        List<String> partitionedByNames;
        String partitionIdent = null;

        List<BytesRef> partitionValues;
        if (analysis.partitionIdent() == null) {

            if (table.isPartitioned()) {
                partitionedByNames = Lists.newArrayList(
                        Lists.transform(table.partitionedBy(), ColumnIdent.GET_FQN_NAME_FUNCTION));
            } else {
                partitionedByNames = Collections.emptyList();
            }
            partitionValues = ImmutableList.of();
        } else {
            assert table.isPartitioned() : "table must be partitioned if partitionIdent is set";

            PartitionName partitionName = PartitionName.fromPartitionIdent(table.ident().schema(), table.ident().name(), analysis.partitionIdent());
            partitionValues = partitionName.values();

            partitionIdent = partitionName.ident();
            partitionedByNames = Collections.emptyList();
        }

        SourceIndexWriterProjection sourceIndexWriterProjection = new SourceIndexWriterProjection(
                table.ident(),
                partitionIdent,
                new Reference(table.getReferenceInfo(DocSysColumns.RAW)),
                table.primaryKey(),
                table.partitionedBy(),
                partitionValues,
                table.clusteredBy(),
                clusteredByPrimaryKeyIdx,
                analysis.settings(),
                null,
                partitionedByNames.size() > 0 ? partitionedByNames.toArray(new String[partitionedByNames.size()]) : null,
                table.isPartitioned() 
        );
        List<Projection> projections = Arrays.<Projection>asList(sourceIndexWriterProjection);
        partitionedByNames.removeAll(Lists.transform(table.primaryKey(), ColumnIdent.GET_FQN_NAME_FUNCTION));
        int referencesSize = table.primaryKey().size() + partitionedByNames.size() + 1;
        referencesSize = clusteredByPrimaryKeyIdx == -1 ? referencesSize + 1 : referencesSize;

        List<Symbol> toCollect = new ArrayList<>(referencesSize);

        for (ColumnIdent primaryKey : table.primaryKey()) {
            toCollect.add(new Reference(table.getReferenceInfo(primaryKey)));
        }


        for (String partitionedColumn : partitionedByNames) {
            toCollect.add(
                    new Reference(table.getReferenceInfo(ColumnIdent.fromPath(partitionedColumn)))
            );
        }

        if (clusteredByPrimaryKeyIdx == -1) {
            toCollect.add(
                    new Reference(table.getReferenceInfo(table.clusteredBy())));
        }

        if (table.isPartitioned() && analysis.partitionIdent() == null) {
            toCollect.add(new Reference(table.getReferenceInfo(DocSysColumns.DOC)));
        } else {
            toCollect.add(new Reference(table.getReferenceInfo(DocSysColumns.RAW)));
        }

        DiscoveryNodes allNodes = clusterService.state().nodes();
        FileUriCollectNode collectNode = new FileUriCollectNode(
                context.jobId(),
                context.nextExecutionNodeId(),
                "copyFrom",
                generateRouting(allNodes, analysis.settings().getAsInt("num_readers", allNodes.getSize())),
                analysis.uri(),
                toCollect,
                projections,
                analysis.settings().get("compression", null),
                analysis.settings().getAsBoolean("shared", null)
        );
        PlanNodeBuilder.setOutputTypes(collectNode);

        return new CollectAndMerge(collectNode, PlanNodeBuilder.localMerge(context.jobId(),
                ImmutableList.<Projection>of(CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION), collectNode, context), context.jobId());
    }

    private Routing generateRouting(DiscoveryNodes allNodes, int maxNodes) {
        final AtomicInteger counter = new AtomicInteger(maxNodes);
        final Map<String, Map<String, List<Integer>>> locations = new TreeMap<>();
        allNodes.dataNodes().keys().forEach(new ObjectProcedure<String>() {
            @Override
            public void apply(String value) {
                if (counter.getAndDecrement() > 0) {
                    locations.put(value, TreeMapBuilder.<String, List<Integer>>newMapBuilder().map());
                }
            }
        });
        return new Routing(locations);
    }

    @Override
    protected Plan visitDDLAnalyzedStatement(AbstractDDLAnalyzedStatement statement, Context context) {
        return new IterablePlan(context.jobId(), new GenericDDLNode(statement));
    }

    @Override
    public Plan visitDropBlobTableStatement(DropBlobTableAnalyzedStatement analysis, Context context) {
        if (analysis.noop()) {
            return new NoopPlan(context.jobId());
        }
        return visitDDLAnalyzedStatement(analysis, context);
    }

    @Override
    protected Plan visitDropTableStatement(DropTableAnalyzedStatement analysis, Context context) {
        if (analysis.noop()) {
            return new NoopPlan(context.jobId());
        }
        return new IterablePlan(context.jobId(), new DropTableNode(analysis.table(), analysis.dropIfExists()));
    }

    @Override
    protected Plan visitCreateTableStatement(CreateTableAnalyzedStatement analysis, Context context) {
        if (analysis.noOp()) {
            return new NoopPlan(context.jobId());
        }
        TableIdent tableIdent = analysis.tableIdent();

        CreateTableNode createTableNode;
        if (analysis.isPartitioned()) {
            createTableNode = CreateTableNode.createPartitionedTableNode(
                    tableIdent,
                    analysis.ifNotExists(),
                    analysis.tableParameter().settings().getByPrefix("index."),
                    analysis.mapping(),
                    analysis.templateName(),
                    analysis.templatePrefix()
            );
        } else {
            createTableNode = CreateTableNode.createTableNode(
                    tableIdent,
                    analysis.ifNotExists(),
                    analysis.tableParameter().settings(),
                    analysis.mapping()
            );
        }
        return new IterablePlan(context.jobId(), createTableNode);
    }

    @Override
    protected Plan visitCreateAnalyzerStatement(CreateAnalyzerAnalyzedStatement analysis, Context context) {
        Settings analyzerSettings;
        try {
            analyzerSettings = analysis.buildSettings();
        } catch (IOException ioe) {
            throw new UnhandledServerException("Could not build analyzer Settings", ioe);
        }

        ESClusterUpdateSettingsNode node = new ESClusterUpdateSettingsNode(analyzerSettings);
        return new IterablePlan(context.jobId(), node);
    }

    @Override
    public Plan visitSetStatement(SetAnalyzedStatement analysis, Context context) {
        ESClusterUpdateSettingsNode node = null;
        if (analysis.isReset()) {

            if (analysis.settingsToRemove() != null) {
                node = new ESClusterUpdateSettingsNode(analysis.settingsToRemove(), analysis.settingsToRemove());
            }
        } else {
            if (analysis.settings() != null) {
                if (analysis.isPersistent()) {
                    node = new ESClusterUpdateSettingsNode(analysis.settings());
                } else {
                    node = new ESClusterUpdateSettingsNode(ImmutableSettings.EMPTY, analysis.settings());
                }
            }
        }
        return node != null ? new IterablePlan(context.jobId(), node) : new NoopPlan(context.jobId());
    }

    @Override
    public Plan visitKillAnalyzedStatement(KillAnalyzedStatement analysis, Context context) {
        return new KillPlan(context.jobId());
    }

    private void createESDeleteByQueryNode(TableInfo tableInfo,
                                           List<WhereClause> whereClauses,
                                           IterablePlan plan,
                                           Context context) {

        List<String[]> indicesList = new ArrayList<>(whereClauses.size());
        for (WhereClause whereClause : whereClauses) {
            String[] indices = indices(tableInfo, whereClauses.get(0));
            if (indices.length > 0) {
                if (!whereClause.hasQuery() && tableInfo.isPartitioned()) {
                    plan.add(new ESDeletePartitionNode(indices));
                } else {
                    indicesList.add(indices);
                }
            }
        }



        if (!indicesList.isEmpty()) {
            plan.add(new ESDeleteByQueryNode(context.nextExecutionNodeId(), indicesList, whereClauses));
        }
    }

    private Upsert processInsertStatement(InsertFromValuesAnalyzedStatement analysis, Context context) {
        String[] onDuplicateKeyAssignmentsColumns = null;
        if (analysis.onDuplicateKeyAssignmentsColumns().size() > 0) {
            onDuplicateKeyAssignmentsColumns = analysis.onDuplicateKeyAssignmentsColumns().get(0);
        }
        SymbolBasedUpsertByIdNode upsertByIdNode = new SymbolBasedUpsertByIdNode(
                context.nextExecutionNodeId(),
                analysis.tableInfo().isPartitioned(),
                analysis.isBulkRequest(),
                onDuplicateKeyAssignmentsColumns,
                analysis.columns().toArray(new Reference[analysis.columns().size()])
        );
        if (analysis.tableInfo().isPartitioned()) {
            List<String> partitions = analysis.generatePartitions();
            String[] indices = partitions.toArray(new String[partitions.size()]);
            for (int i = 0; i < indices.length; i++) {
                Symbol[] onDuplicateKeyAssignments = null;
                if (analysis.onDuplicateKeyAssignmentsColumns().size() > i) {
                    onDuplicateKeyAssignments = analysis.onDuplicateKeyAssignments().get(i);
                }
                upsertByIdNode.add(
                        indices[i],
                        analysis.ids().get(i),
                        analysis.routingValues().get(i),
                        onDuplicateKeyAssignments,
                        null,
                        analysis.sourceMaps().get(i));
            }
        } else {
            for (int i = 0; i < analysis.ids().size(); i++) {
                Symbol[] onDuplicateKeyAssignments = null;
                if (analysis.onDuplicateKeyAssignments().size() > i) {
                    onDuplicateKeyAssignments = analysis.onDuplicateKeyAssignments().get(i);
                }
                upsertByIdNode.add(
                        analysis.tableInfo().ident().esName(),
                        analysis.ids().get(i),
                        analysis.routingValues().get(i),
                        onDuplicateKeyAssignments,
                        null,
                        analysis.sourceMaps().get(i));
            }
        }

        return new Upsert(ImmutableList.<Plan>of(new IterablePlan(context.jobId(), upsertByIdNode)), context.jobId());
    }

    static List<DataType> extractDataTypes(List<Projection> projections, @Nullable List<DataType> inputTypes) {
        if (projections.size() == 0) {
            return inputTypes;
        }
        int projectionIdx = projections.size() - 1;
        Projection lastProjection = projections.get(projectionIdx);
        List<DataType> types = new ArrayList<>(lastProjection.outputs().size());
        List<DataType> dataTypes = firstNonNull(inputTypes, ImmutableList.<DataType>of());

        for (int c = 0; c < lastProjection.outputs().size(); c++) {
            types.add(resolveType(projections, projectionIdx, c, dataTypes));
        }
        return types;
    }

    private static DataType resolveType(List<Projection> projections, int projectionIdx, int columnIdx, List<DataType> inputTypes) {
        Projection projection = projections.get(projectionIdx);
        Symbol symbol = projection.outputs().get(columnIdx);
        DataType type = symbol.valueType();
        if (type == null || (type.equals(DataTypes.UNDEFINED) && symbol instanceof InputColumn)) {
            if (projectionIdx > 0) {
                if (symbol instanceof InputColumn) {
                    columnIdx = ((InputColumn) symbol).index();
                }
                return resolveType(projections, projectionIdx - 1, columnIdx, inputTypes);
            } else {
                assert symbol instanceof InputColumn; 
                return inputTypes.get(((InputColumn) symbol).index());
            }
        }

        return type;
    }



    public static String[] indices(TableInfo tableInfo, WhereClause whereClause) {
        String[] indices;

        if (whereClause.noMatch()) {
            indices = org.elasticsearch.common.Strings.EMPTY_ARRAY;
        } else if (!tableInfo.isPartitioned()) {

            indices = new String[]{tableInfo.ident().esName()};
        } else if (whereClause.partitions().isEmpty()) {
            if (whereClause.noMatch()) {
                return new String[0];
            }


            indices = new String[tableInfo.partitions().size()];
            int i = 0;
            for (PartitionName partitionName: tableInfo.partitions()) {
                indices[i] = partitionName.stringValue();
                i++;
            }
        } else {
            indices = whereClause.partitions().toArray(new String[whereClause.partitions().size()]);
        }
        return indices;
    }
}


<code block>


package io.crate.planner;

import java.util.UUID;

public interface Plan {

    <C, R> R accept(PlanVisitor<C, R> visitor, C context);

    public UUID jobId();
}

<code block>


package io.crate.planner.consumer;

import com.google.common.collect.ImmutableList;
import io.crate.analyze.UpdateAnalyzedStatement;
import io.crate.analyze.VersionRewriter;
import io.crate.analyze.WhereClause;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.analyze.relations.TableRelation;
import io.crate.analyze.where.DocKeys;
import io.crate.metadata.PartitionName;
import io.crate.metadata.ReferenceIdent;
import io.crate.metadata.ReferenceInfo;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.aggregation.impl.CountAggregation;
import io.crate.planner.*;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dml.SymbolBasedUpsertByIdNode;
import io.crate.planner.node.dml.Upsert;
import io.crate.planner.node.dql.CollectAndMerge;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.UpdateProjection;
import io.crate.planner.symbol.InputColumn;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.planner.symbol.ValueSymbolVisitor;
import io.crate.types.DataTypes;
import org.elasticsearch.cluster.routing.operation.plain.Preference;
import org.elasticsearch.common.collect.Tuple;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.inject.Singleton;

import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;
import java.util.Map;

@Singleton
public class UpdateConsumer implements Consumer {

    private final Visitor visitor;

    @Inject
    public UpdateConsumer() {
        visitor = new Visitor();
    }

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        PlannedAnalyzedRelation plannedAnalyzedRelation = visitor.process(rootRelation, context);
        if (plannedAnalyzedRelation == null) {
            return false;
        }
        context.rootRelation(plannedAnalyzedRelation);
        return true;
    }

    class Visitor extends AnalyzedRelationVisitor<ConsumerContext, PlannedAnalyzedRelation> {

        @Override
        public PlannedAnalyzedRelation visitUpdateAnalyzedStatement(UpdateAnalyzedStatement statement, ConsumerContext context) {
            assert statement.sourceRelation() instanceof TableRelation : "sourceRelation of update statement must be a TableRelation";
            TableRelation tableRelation = (TableRelation) statement.sourceRelation();
            TableInfo tableInfo = tableRelation.tableInfo();

            if (tableInfo.schemaInfo().systemSchema() || tableInfo.rowGranularity() != RowGranularity.DOC) {
                return null;
            }

            List<Plan> childNodes = new ArrayList<>(statement.nestedStatements().size());
            SymbolBasedUpsertByIdNode upsertByIdNode = null;
            for (UpdateAnalyzedStatement.NestedAnalyzedStatement nestedAnalysis : statement.nestedStatements()) {
                WhereClause whereClause = nestedAnalysis.whereClause();
                if (whereClause.noMatch()){
                    continue;
                }
                if (whereClause.docKeys().isPresent()) {
                    if (upsertByIdNode == null) {
                        Tuple<String[], Symbol[]> assignments = convertAssignments(nestedAnalysis.assignments());
                        upsertByIdNode = new SymbolBasedUpsertByIdNode(context.plannerContext().nextExecutionNodeId(), false, statement.nestedStatements().size() > 1, assignments.v1(), null);
                        childNodes.add(new IterablePlan(context.plannerContext().jobId(), upsertByIdNode));
                    }
                    upsertById(nestedAnalysis, tableInfo, whereClause, upsertByIdNode);
                } else {
                    Plan plan = upsertByQuery(nestedAnalysis, context, tableInfo, whereClause);
                    if (plan != null) {
                        childNodes.add(plan);
                    }
                }
            }
            if (childNodes.size() > 0){
                return new Upsert(childNodes, context.plannerContext().jobId());
            } else {
                return new NoopPlannedAnalyzedRelation(statement, context.plannerContext().jobId());
            }
        }

        private Plan upsertByQuery(UpdateAnalyzedStatement.NestedAnalyzedStatement nestedAnalysis,
                                   ConsumerContext consumerContext,
                                   TableInfo tableInfo,
                                   WhereClause whereClause) {

            Symbol versionSymbol = null;
            if(whereClause.hasVersions()){
                versionSymbol = VersionRewriter.get(whereClause.query());
                whereClause = new WhereClause(whereClause.query(), whereClause.docKeys().orNull(), whereClause.partitions());
            }


            if (!whereClause.noMatch() || !(tableInfo.isPartitioned() && whereClause.partitions().isEmpty())) {

                Reference uidReference = new Reference(
                        new ReferenceInfo(
                                new ReferenceIdent(tableInfo.ident(), "_uid"),
                                RowGranularity.DOC, DataTypes.STRING));

                Tuple<String[], Symbol[]> assignments = convertAssignments(nestedAnalysis.assignments());

                Long version = null;
                if (versionSymbol != null){
                    version = ValueSymbolVisitor.LONG.process(versionSymbol);
                }

                UpdateProjection updateProjection = new UpdateProjection(
                        new InputColumn(0, DataTypes.STRING),
                        assignments.v1(),
                        assignments.v2(),
                        version);

                CollectNode collectNode = PlanNodeBuilder.collect(
                        consumerContext.plannerContext().jobId(),
                        tableInfo,
                        consumerContext.plannerContext(),
                        whereClause,
                        ImmutableList.<Symbol>of(uidReference),
                        ImmutableList.<Projection>of(updateProjection),
                        null,
                        Preference.PRIMARY.type()
                );
                MergeNode mergeNode = PlanNodeBuilder.localMerge(
                        consumerContext.plannerContext().jobId(),
                        ImmutableList.<Projection>of(CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION), collectNode,
                        consumerContext.plannerContext());
                return new CollectAndMerge(collectNode, mergeNode, consumerContext.plannerContext().jobId());
            } else {
                return null;
            }
        }

        private void upsertById(UpdateAnalyzedStatement.NestedAnalyzedStatement nestedAnalysis,
                                             TableInfo tableInfo,
                                             WhereClause whereClause,
                                             SymbolBasedUpsertByIdNode upsertByIdNode) {
            String[] indices = Planner.indices(tableInfo, whereClause);
            assert tableInfo.isPartitioned() || indices.length == 1;

            Tuple<String[], Symbol[]> assignments = convertAssignments(nestedAnalysis.assignments());


            for (DocKeys.DocKey key : whereClause.docKeys().get()) {
                String index;
                if (key.partitionValues().isPresent()) {
                    index = new PartitionName(tableInfo.ident(), key.partitionValues().get()).stringValue();
                } else {
                    index = indices[0];
                }
                upsertByIdNode.add(
                        index,
                        key.id(),
                        key.routing(),
                        assignments.v2(),
                        key.version().orNull());
            }
        }


        private Tuple<String[], Symbol[]> convertAssignments(Map<Reference, Symbol> assignments) {
            String[] assignmentColumns = new String[assignments.size()];
            Symbol[] assignmentSymbols = new Symbol[assignments.size()];
            Iterator<Reference> it = assignments.keySet().iterator();
            int i = 0;
            while(it.hasNext()) {
                Reference key = it.next();
                assignmentColumns[i] = key.ident().columnIdent().fqn();
                assignmentSymbols[i] = assignments.get(key);
                i++;
            }
            return new Tuple<>(assignmentColumns, assignmentSymbols);
        }

        @Override
        protected PlannedAnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, ConsumerContext context) {
            return null;
        }
    }
}

<code block>

package io.crate.planner.consumer;

import com.google.common.collect.ImmutableList;
import io.crate.analyze.*;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.Routing;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.GroupByConsumer;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.dql.NonDistributedGroupBy;
import io.crate.planner.projection.GroupProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.builder.ProjectionBuilder;
import io.crate.planner.projection.builder.SplitPoints;
import io.crate.planner.symbol.Aggregation;
import io.crate.planner.symbol.Symbol;

import java.util.ArrayList;
import java.util.List;

public class NonDistributedGroupByConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        Context ctx = new Context(context);
        context.rootRelation(VISITOR.process(context.rootRelation(), ctx));
        return ctx.result;
    }

    private static class Context {
        ConsumerContext consumerContext;
        boolean result = false;

        public Context(ConsumerContext context) {
            this.consumerContext = context;
        }
    }

    private static class Visitor extends AnalyzedRelationVisitor<Context, AnalyzedRelation> {

        @Override
        public AnalyzedRelation visitQueriedTable(QueriedTable table, Context context) {
            if (table.querySpec().groupBy() == null) {
                return table;
            }
            TableInfo tableInfo = table.tableRelation().tableInfo();

            if (table.querySpec().where().hasVersions()) {
                context.consumerContext.validationException(new VersionInvalidException());
                return table;
            }

            Routing routing = tableInfo.getRouting(table.querySpec().where(), null);

            if (GroupByConsumer.requiresDistribution(tableInfo, routing) && !(tableInfo.schemaInfo().systemSchema())) {
                return table;
            }

            context.result = true;
            return nonDistributedGroupBy(table, context);
        }

        @Override
        public AnalyzedRelation visitInsertFromQuery(InsertFromSubQueryAnalyzedStatement insertFromSubQueryAnalyzedStatement, Context context) {
            InsertFromSubQueryConsumer.planInnerRelation(insertFromSubQueryAnalyzedStatement, context, this);
            return insertFromSubQueryAnalyzedStatement;

        }

        @Override
        protected AnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, Context context) {
            return relation;
        }


        private AnalyzedRelation nonDistributedGroupBy(QueriedTable table, Context context) {
            TableInfo tableInfo = table.tableRelation().tableInfo();

            GroupByConsumer.validateGroupBySymbols(table.tableRelation(), table.querySpec().groupBy());
            List<Symbol> groupBy = table.querySpec().groupBy();

            ProjectionBuilder projectionBuilder = new ProjectionBuilder(table.querySpec());
            SplitPoints splitPoints = projectionBuilder.getSplitPoints();


            GroupProjection groupProjection = projectionBuilder.groupProjection(
                    splitPoints.leaves(),
                    table.querySpec().groupBy(),
                    splitPoints.aggregates(),
                    Aggregation.Step.ITER,
                    Aggregation.Step.PARTIAL);

            CollectNode collectNode = PlanNodeBuilder.collect(
                    context.consumerContext.plannerContext().jobId(),
                    tableInfo,
                    context.consumerContext.plannerContext(),
                    table.querySpec().where(),
                    splitPoints.leaves(),
                    ImmutableList.<Projection>of(groupProjection)
            );

            List<Symbol> collectOutputs = new ArrayList<>(
                    groupBy.size() +
                            splitPoints.aggregates().size());
            collectOutputs.addAll(groupBy);
            collectOutputs.addAll(splitPoints.aggregates());


            OrderBy orderBy = table.querySpec().orderBy();
            if (orderBy != null) {
                table.tableRelation().validateOrderBy(orderBy);
            }

            List<Projection> projections = new ArrayList<>();
            projections.add(projectionBuilder.groupProjection(
                    collectOutputs,
                    table.querySpec().groupBy(),
                    splitPoints.aggregates(),
                    Aggregation.Step.PARTIAL,
                    Aggregation.Step.FINAL
            ));

            HavingClause havingClause = table.querySpec().having();
            if (havingClause != null) {
                if (havingClause.noMatch()) {
                    return new NoopPlannedAnalyzedRelation(table, context.consumerContext.plannerContext().jobId());
                } else if (havingClause.hasQuery()){
                    projections.add(projectionBuilder.filterProjection(
                            collectOutputs,
                            havingClause.query()
                    ));
                }
            }


            boolean outputsMatch = table.querySpec().outputs().size() == collectOutputs.size() &&
                                    collectOutputs.containsAll(table.querySpec().outputs());
            if (context.consumerContext.rootRelation() == table || !outputsMatch){
                projections.add(projectionBuilder.topNProjection(
                        collectOutputs,
                        orderBy,
                        table.querySpec().offset(),
                        table.querySpec().limit(),
                        table.querySpec().outputs()
                ));
            }
            MergeNode localMergeNode = PlanNodeBuilder.localMerge(context.consumerContext.plannerContext().jobId(), projections, collectNode,
                    context.consumerContext.plannerContext());
            return new NonDistributedGroupBy(collectNode, localMergeNode, context.consumerContext.plannerContext().jobId());
        }
    }

}

<code block>


package io.crate.planner.consumer;

import io.crate.analyze.QueriedTable;
import io.crate.analyze.QuerySpec;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.Routing;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.aggregation.impl.CountAggregation;
import io.crate.planner.Planner;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.CountNode;
import io.crate.planner.node.dql.CountPlan;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.Function;
import io.crate.planner.symbol.Symbol;
import io.crate.types.DataType;
import io.crate.types.DataTypes;

import java.util.Collections;
import java.util.List;

import static com.google.common.base.MoreObjects.firstNonNull;

public class CountConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        AnalyzedRelation analyzedRelation = VISITOR.process(rootRelation, context);
        if (analyzedRelation != null) {
            context.rootRelation(analyzedRelation);
            return true;
        }
        return false;
    }

    private static class Visitor extends AnalyzedRelationVisitor<ConsumerContext, PlannedAnalyzedRelation> {

        @Override
        public PlannedAnalyzedRelation visitQueriedTable(QueriedTable table, ConsumerContext context) {
            QuerySpec querySpec = table.querySpec();
            if (!querySpec.hasAggregates() || querySpec.groupBy()!=null) {
                return null;
            }
            TableInfo tableInfo = table.tableRelation().tableInfo();
            if (tableInfo.schemaInfo().systemSchema()) {
                return null;
            }
            if (!hasOnlyGlobalCount(querySpec.outputs())) {
                return null;
            }
            if(querySpec.where().hasVersions()){
                context.validationException(new VersionInvalidException());
                return null;
            }

            if (firstNonNull(querySpec.limit(), 1) < 1 ||
                    querySpec.offset() > 0){
                return new NoopPlannedAnalyzedRelation(table, context.plannerContext().jobId());
            }

            Routing routing = tableInfo.getRouting(querySpec.where(), null);
            Planner.Context plannerContext = context.plannerContext();
            CountNode countNode = new CountNode(plannerContext.jobId(), plannerContext.nextExecutionNodeId(), routing, querySpec.where());
            MergeNode mergeNode = new MergeNode(
                    plannerContext.jobId(),
                    plannerContext.nextExecutionNodeId(),
                    "count-merge",
                    countNode.executionNodes().size());
            mergeNode.inputTypes(Collections.<DataType>singletonList(DataTypes.LONG));
            mergeNode.projections(Collections.<Projection>singletonList(
                    CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION));
            return new CountPlan(countNode, mergeNode, plannerContext.jobId());

        }

        @Override
        protected PlannedAnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, ConsumerContext context) {
            return null;
        }

        private boolean hasOnlyGlobalCount(List<Symbol> symbols) {
            if (symbols.size() != 1) {
                return false;
            }
            Symbol symbol = symbols.get(0);
            if (!(symbol instanceof Function)) {
                return false;
            }
            Function function = (Function) symbol;
            return function.info().equals(CountAggregation.COUNT_STAR_FUNCTION);
        }
    }
}

<code block>


package io.crate.planner.consumer;

import com.google.common.collect.ImmutableList;
import io.crate.Constants;
import io.crate.analyze.*;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.TableRelation;
import io.crate.exceptions.UnsupportedFeatureException;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.DocReferenceConverter;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.predicate.MatchPredicate;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.dql.QueryAndFetch;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.TopNProjection;
import io.crate.planner.symbol.Function;
import io.crate.planner.symbol.InputColumn;
import io.crate.planner.symbol.Symbol;
import io.crate.planner.symbol.SymbolVisitor;

import java.util.ArrayList;
import java.util.List;

import static com.google.common.base.MoreObjects.firstNonNull;

public class QueryAndFetchConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        Context ctx = new Context(context);
        context.rootRelation(VISITOR.process(context.rootRelation(), ctx));
        return ctx.result;
    }

    private static class Context {
        ConsumerContext consumerContext;
        boolean result = false;

        public Context(ConsumerContext context) {
            this.consumerContext = context;
        }
    }

    private static class Visitor extends AnalyzedRelationVisitor<Context, AnalyzedRelation> {

        @Override
        public AnalyzedRelation visitQueriedTable(QueriedTable table, Context context) {
            TableRelation tableRelation = table.tableRelation();
            if(table.querySpec().where().hasVersions()){
                context.consumerContext.validationException(new VersionInvalidException());
                return table;
            }
            TableInfo tableInfo = tableRelation.tableInfo();

            if (tableInfo.schemaInfo().systemSchema() && table.querySpec().where().hasQuery()) {
                ensureNoLuceneOnlyPredicates(table.querySpec().where().query());
            }
            if (table.querySpec().hasAggregates()) {
                context.result = true;
                return GlobalAggregateConsumer.globalAggregates(table, tableRelation, table.querySpec().where(), context.consumerContext);
            } else {
               context.result = true;
               return normalSelect(table, table.querySpec().where(), tableRelation, context);
            }
        }

        @Override
        public AnalyzedRelation visitInsertFromQuery(InsertFromSubQueryAnalyzedStatement insertFromSubQueryAnalyzedStatement,
                                                     Context context) {
            InsertFromSubQueryConsumer.planInnerRelation(insertFromSubQueryAnalyzedStatement, context, this);
            return insertFromSubQueryAnalyzedStatement;
        }

        @Override
        protected AnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, Context context) {
            return relation;
        }

        private void ensureNoLuceneOnlyPredicates(Symbol query) {
            NoPredicateVisitor noPredicateVisitor = new NoPredicateVisitor();
            noPredicateVisitor.process(query, null);
        }

        private static class NoPredicateVisitor extends SymbolVisitor<Void, Void> {
            @Override
            public Void visitFunction(Function symbol, Void context) {
                if (symbol.info().ident().name().equals(MatchPredicate.NAME)) {
                    throw new UnsupportedFeatureException("Cannot use match predicate on system tables");
                }
                for (Symbol argument : symbol.arguments()) {
                    process(argument, context);
                }
                return null;
            }
        }

        private AnalyzedRelation normalSelect(QueriedTable table,
                                              WhereClause whereClause,
                                              TableRelation tableRelation,
                                              Context context){
            QuerySpec querySpec = table.querySpec();
            TableInfo tableInfo = tableRelation.tableInfo();

            List<Symbol> outputSymbols;
            if (tableInfo.schemaInfo().systemSchema()) {
                outputSymbols = tableRelation.resolve(querySpec.outputs());
            } else {
                outputSymbols = new ArrayList<>(querySpec.outputs().size());
                for (Symbol symbol : querySpec.outputs()) {
                    outputSymbols.add(DocReferenceConverter.convertIfPossible(tableRelation.resolve(symbol), tableInfo));
                }
            }
            CollectNode collectNode;
            MergeNode mergeNode = null;
            OrderBy orderBy = querySpec.orderBy();
            if (context.consumerContext.rootRelation() != table) {

                assert !querySpec.isLimited() : "insert from sub query with limit or order by is not supported. " +
                        "Analyzer should have thrown an exception already.";

                ImmutableList<Projection> projections = ImmutableList.<Projection>of();
                collectNode = PlanNodeBuilder.collect(
                        context.consumerContext.plannerContext().jobId(),
                        tableInfo,
                        context.consumerContext.plannerContext(),
                        whereClause, outputSymbols, projections);
            } else if (querySpec.isLimited() || orderBy != null) {

                List<Symbol> toCollect;
                List<Symbol> orderByInputColumns = null;
                if (orderBy != null){
                    List<Symbol> orderBySymbols = tableRelation.resolve(orderBy.orderBySymbols());
                    toCollect = new ArrayList<>(outputSymbols.size() + orderBySymbols.size());
                    toCollect.addAll(outputSymbols);

                    for (Symbol orderBySymbol : orderBySymbols) {
                        if (!toCollect.contains(orderBySymbol)) {
                            toCollect.add(orderBySymbol);
                        }
                    }
                    orderByInputColumns = new ArrayList<>();
                    for (Symbol symbol : orderBySymbols) {
                        orderByInputColumns.add(new InputColumn(toCollect.indexOf(symbol), symbol.valueType()));
                    }
                } else {
                    toCollect = new ArrayList<>(outputSymbols.size());
                    toCollect.addAll(outputSymbols);
                }

                List<Symbol> allOutputs = new ArrayList<>(toCollect.size());        
                for (int i = 0; i < toCollect.size(); i++) {
                    allOutputs.add(new InputColumn(i, toCollect.get(i).valueType()));
                }
                List<Symbol> finalOutputs = new ArrayList<>(outputSymbols.size());  
                for (int i = 0; i < outputSymbols.size(); i++) {
                    finalOutputs.add(new InputColumn(i, outputSymbols.get(i).valueType()));
                }



                TopNProjection tnp;
                int limit = firstNonNull(querySpec.limit(), Constants.DEFAULT_SELECT_LIMIT);
                if (orderBy == null){
                    tnp = new TopNProjection(querySpec.offset() + limit, 0);
                } else {
                    tnp = new TopNProjection(querySpec.offset() + limit, 0,
                            orderByInputColumns,
                            orderBy.reverseFlags(),
                            orderBy.nullsFirst()
                    );
                }
                tnp.outputs(allOutputs);
                collectNode = PlanNodeBuilder.collect(context.consumerContext.plannerContext().jobId(),
                        tableInfo,
                        context.consumerContext.plannerContext(),
                        whereClause, toCollect, ImmutableList.<Projection>of(tnp));


                tnp = new TopNProjection(limit, querySpec.offset());
                tnp.outputs(finalOutputs);
                if (orderBy == null) {

                    mergeNode = PlanNodeBuilder.localMerge(
                            context.consumerContext.plannerContext().jobId(),
                            ImmutableList.<Projection>of(tnp), collectNode,
                            context.consumerContext.plannerContext());
                } else {


                    mergeNode = PlanNodeBuilder.sortedLocalMerge(
                            context.consumerContext.plannerContext().jobId(),
                            ImmutableList.<Projection>of(tnp),
                            orderBy,
                            allOutputs,
                            orderByInputColumns,
                            collectNode,
                            context.consumerContext.plannerContext()
                    );
                }
            } else {
                collectNode = PlanNodeBuilder.collect(
                        context.consumerContext.plannerContext().jobId(),
                        tableInfo,
                        context.consumerContext.plannerContext(),
                        whereClause, outputSymbols, ImmutableList.<Projection>of());
                mergeNode = PlanNodeBuilder.localMerge(
                        context.consumerContext.plannerContext().jobId(),
                        ImmutableList.<Projection>of(), collectNode,
                        context.consumerContext.plannerContext());
            }
            return new QueryAndFetch(collectNode, mergeNode, context.consumerContext.plannerContext().jobId());
        }
    }
}

<code block>


package io.crate.planner.consumer;

import com.google.common.collect.ImmutableList;
import io.crate.analyze.*;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.analyze.relations.TableRelation;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.FunctionInfo;
import io.crate.metadata.ReferenceInfo;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.GlobalAggregate;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.AggregationProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.TopNProjection;
import io.crate.planner.projection.builder.ProjectionBuilder;
import io.crate.planner.projection.builder.SplitPoints;
import io.crate.planner.symbol.*;

import java.util.ArrayList;
import java.util.Collection;
import java.util.List;

import static com.google.common.base.MoreObjects.firstNonNull;


public class GlobalAggregateConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();
    private static final AggregationOutputValidator AGGREGATION_OUTPUT_VALIDATOR = new AggregationOutputValidator();

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        AnalyzedRelation analyzedRelation = VISITOR.process(rootRelation, context);
        if (analyzedRelation != null) {
            context.rootRelation(analyzedRelation);
            return true;
        }
        return false;
    }

    private static class Visitor extends AnalyzedRelationVisitor<ConsumerContext, PlannedAnalyzedRelation> {

        @Override
        public PlannedAnalyzedRelation visitQueriedTable(QueriedTable table, ConsumerContext context) {
            if (table.querySpec().groupBy()!=null || !table.querySpec().hasAggregates()) {
                return null;
            }
            if (firstNonNull(table.querySpec().limit(), 1) < 1 || table.querySpec().offset() > 0){
                return new NoopPlannedAnalyzedRelation(table, context.plannerContext().jobId());
            }

            if (table.querySpec().where().hasVersions()){
                context.validationException(new VersionInvalidException());
                return null;
            }
            return globalAggregates(table, table.tableRelation(),  table.querySpec().where(), context);
        }

        @Override
        public PlannedAnalyzedRelation visitInsertFromQuery(InsertFromSubQueryAnalyzedStatement insertFromSubQueryAnalyzedStatement, ConsumerContext context) {
            InsertFromSubQueryConsumer.planInnerRelation(insertFromSubQueryAnalyzedStatement, context, this);
            return null;
        }

        @Override
        protected PlannedAnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, ConsumerContext context) {
            return null;
        }
    }

    private static boolean noGroupBy(List<Symbol> groupBy) {
        return groupBy == null || groupBy.isEmpty();
    }

    public static PlannedAnalyzedRelation globalAggregates(QueriedTable table,
                                                           TableRelation tableRelation,
                                                           WhereClause whereClause,
                                                           ConsumerContext context) {
        assert noGroupBy(table.querySpec().groupBy()) : "must not have group by clause for global aggregate queries";
        validateAggregationOutputs(tableRelation, table.querySpec().outputs());


        ProjectionBuilder projectionBuilder = new ProjectionBuilder(table.querySpec());
        SplitPoints splitPoints = projectionBuilder.getSplitPoints();

        AggregationProjection ap = projectionBuilder.aggregationProjection(
                splitPoints.leaves(),
                splitPoints.aggregates(),
                Aggregation.Step.ITER,
                Aggregation.Step.PARTIAL);

        CollectNode collectNode = PlanNodeBuilder.collect(
                context.plannerContext().jobId(),
                tableRelation.tableInfo(),
                context.plannerContext(),
                whereClause,
                splitPoints.leaves(),
                ImmutableList.<Projection>of(ap)
        );


        List<Projection> projections = new ArrayList<>();
        projections.add(projectionBuilder.aggregationProjection(
                splitPoints.aggregates(),
                splitPoints.aggregates(),
                Aggregation.Step.PARTIAL,
                Aggregation.Step.FINAL));

        HavingClause havingClause = table.querySpec().having();
        if(havingClause != null){
            if (havingClause.noMatch()) {
                return new NoopPlannedAnalyzedRelation(table, context.plannerContext().jobId());
            } else if (havingClause.hasQuery()){
                projections.add(projectionBuilder.filterProjection(
                        splitPoints.aggregates(),
                        havingClause.query()
                ));
            }
        }

        TopNProjection topNProjection = projectionBuilder.topNProjection(
                splitPoints.aggregates(),
                null, 0, 1,
                table.querySpec().outputs()
                );
        projections.add(topNProjection);
        MergeNode localMergeNode = PlanNodeBuilder.localMerge(context.plannerContext().jobId(), projections, collectNode,
                context.plannerContext());
        return new GlobalAggregate(collectNode, localMergeNode, context.plannerContext().jobId());
    }

    private static void validateAggregationOutputs(TableRelation tableRelation, Collection<? extends Symbol> outputSymbols) {
        OutputValidatorContext context = new OutputValidatorContext(tableRelation);
        for (Symbol outputSymbol : outputSymbols) {
            context.insideAggregation = false;
            AGGREGATION_OUTPUT_VALIDATOR.process(outputSymbol, context);
        }
    }

    private static class OutputValidatorContext {
        private final TableRelation tableRelation;
        private boolean insideAggregation = false;

        public OutputValidatorContext(TableRelation tableRelation) {
            this.tableRelation = tableRelation;
        }
    }

    private static class AggregationOutputValidator extends SymbolVisitor<OutputValidatorContext, Void> {

        @Override
        public Void visitFunction(Function symbol, OutputValidatorContext context) {
            context.insideAggregation = context.insideAggregation || symbol.info().type().equals(FunctionInfo.Type.AGGREGATE);
            for (Symbol argument : symbol.arguments()) {
                process(argument, context);
            }
            context.insideAggregation = false;
            return null;
        }

        @Override
        public Void visitReference(Reference symbol, OutputValidatorContext context) {
            if (context.insideAggregation) {
                ReferenceInfo.IndexType indexType = symbol.info().indexType();
                if (indexType == ReferenceInfo.IndexType.ANALYZED) {
                    throw new IllegalArgumentException(String.format(
                            "Cannot select analyzed column '%s' within grouping or aggregations", SymbolFormatter.format(symbol)));
                } else if (indexType == ReferenceInfo.IndexType.NO) {
                    throw new IllegalArgumentException(String.format(
                            "Cannot select non-indexed column '%s' within grouping or aggregations", SymbolFormatter.format(symbol)));
                }
            }
            return null;
        }

        @Override
        public Void visitField(Field field, OutputValidatorContext context) {
            return process(context.tableRelation.resolveField(field), context);
        }

        @Override
        protected Void visitSymbol(Symbol symbol, OutputValidatorContext context) {
            return null;
        }
    }
}

<code block>


package io.crate.planner.consumer;


import io.crate.analyze.OrderBy;
import io.crate.analyze.QueriedTable;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.ESGetNode;

public class ESGetConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        PlannedAnalyzedRelation relation = VISITOR.process(rootRelation, context);
        if (relation == null) {
            return false;
        }
        context.rootRelation(relation);
        return true;
    }

    private static class Visitor extends AnalyzedRelationVisitor<ConsumerContext, PlannedAnalyzedRelation> {

        @Override
        public PlannedAnalyzedRelation visitQueriedTable(QueriedTable table, ConsumerContext context) {
            if (table.querySpec().hasAggregates() || table.querySpec().groupBy()!=null) {
                return null;
            }
            TableInfo tableInfo = table.tableRelation().tableInfo();
            if (tableInfo.isAlias()
                    || tableInfo.schemaInfo().systemSchema()
                    || tableInfo.rowGranularity() != RowGranularity.DOC) {
                return null;
            }

            if (!table.querySpec().where().docKeys().isPresent()) {
                return null;
            }

            if(table.querySpec().where().docKeys().get().withVersions()){
                context.validationException(new VersionInvalidException());
                return null;
            }
            Integer limit = table.querySpec().limit();
            if (limit != null){
                if (limit == 0){
                    return new NoopPlannedAnalyzedRelation(table, context.plannerContext().jobId());
                }
            }

            OrderBy orderBy = table.querySpec().orderBy();
            if (orderBy != null){
                table.tableRelation().validateOrderBy(orderBy);
            }
            return new ESGetNode(context.plannerContext().nextExecutionNodeId(), tableInfo, table.querySpec(), context.plannerContext().jobId());
        }

        @Override
        protected PlannedAnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, ConsumerContext context) {
            return null;
        }
    }
}

<code block>


package io.crate.planner.consumer;

import com.google.common.collect.ImmutableList;
import io.crate.Constants;
import io.crate.analyze.OrderBy;
import io.crate.analyze.QueriedTable;
import io.crate.analyze.QuerySpec;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.ColumnIdent;
import io.crate.metadata.DocReferenceConverter;
import io.crate.metadata.ReferenceInfo;
import io.crate.metadata.ScoreReferenceDetector;
import io.crate.metadata.doc.DocSysColumns;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.projectors.FetchProjector;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.dql.QueryThenFetch;
import io.crate.planner.projection.FetchProjection;
import io.crate.planner.projection.MergeProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.TopNProjection;
import io.crate.planner.projection.builder.ProjectionBuilder;
import io.crate.planner.projection.builder.SplitPoints;
import io.crate.planner.symbol.*;
import io.crate.types.DataTypes;

import java.util.ArrayList;
import java.util.Collections;
import java.util.List;

public class QueryThenFetchConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();
    private static final OutputOrderReferenceCollector OUTPUT_ORDER_REFERENCE_COLLECTOR = new OutputOrderReferenceCollector();
    private static final ReferencesCollector REFERENCES_COLLECTOR = new ReferencesCollector();
    private static final ScoreReferenceDetector SCORE_REFERENCE_DETECTOR = new ScoreReferenceDetector();
    private static final ColumnIdent DOC_ID_COLUMN_IDENT = new ColumnIdent(DocSysColumns.DOCID.name());
    private static final InputColumn DEFAULT_DOC_ID_INPUT_COLUMN = new InputColumn(0, DataTypes.STRING);

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        PlannedAnalyzedRelation plannedAnalyzedRelation = VISITOR.process(rootRelation, context);
        if (plannedAnalyzedRelation == null) {
            return false;
        }
        context.rootRelation(plannedAnalyzedRelation);
        return true;
    }

    private static class Visitor extends AnalyzedRelationVisitor<ConsumerContext, PlannedAnalyzedRelation> {

        @Override
        public PlannedAnalyzedRelation visitQueriedTable(QueriedTable table, ConsumerContext context) {
            QuerySpec querySpec = table.querySpec();
            if (querySpec.hasAggregates() || querySpec.groupBy()!=null) {
                return null;
            }
            TableInfo tableInfo = table.tableRelation().tableInfo();
            if (tableInfo.schemaInfo().systemSchema() || tableInfo.rowGranularity() != RowGranularity.DOC) {
                return null;
            }

            if(querySpec.where().hasVersions()){
                context.validationException(new VersionInvalidException());
                return null;
            }

            if (querySpec.where().noMatch()) {
                return new NoopPlannedAnalyzedRelation(table, context.plannerContext().jobId());
            }

            boolean outputsAreAllOrdered = false;
            boolean needFetchProjection = REFERENCES_COLLECTOR.collect(querySpec.outputs()).containsAnyReference();
            List<Projection> collectProjections = new ArrayList<>();
            List<Projection> mergeProjections = new ArrayList<>();
            List<Symbol> collectSymbols = new ArrayList<>();
            List<Symbol> outputSymbols = new ArrayList<>();
            ReferenceInfo docIdRefInfo = tableInfo.getReferenceInfo(DOC_ID_COLUMN_IDENT);

            ProjectionBuilder projectionBuilder = new ProjectionBuilder(querySpec);
            SplitPoints splitPoints = projectionBuilder.getSplitPoints();


            OrderBy orderBy = querySpec.orderBy();
            if (orderBy != null) {
                table.tableRelation().validateOrderBy(orderBy);




                OutputOrderReferenceContext outputOrderContext =
                        OUTPUT_ORDER_REFERENCE_COLLECTOR.collect(splitPoints.leaves());
                outputOrderContext.collectOrderBy = true;
                OUTPUT_ORDER_REFERENCE_COLLECTOR.collect(orderBy.orderBySymbols(), outputOrderContext);
                outputsAreAllOrdered = outputOrderContext.outputsAreAllOrdered();
                if (outputsAreAllOrdered) {
                    collectSymbols = splitPoints.toCollect();
                } else {
                    collectSymbols.addAll(orderBy.orderBySymbols());
                }
            }

            needFetchProjection = needFetchProjection & !outputsAreAllOrdered;

            if (needFetchProjection) {
                collectSymbols.add(0, new Reference(docIdRefInfo));
                for (Symbol symbol : querySpec.outputs()) {

                    if (SCORE_REFERENCE_DETECTOR.detect(symbol) && !collectSymbols.contains(symbol)) {
                        collectSymbols.add(symbol);
                    }
                    outputSymbols.add(DocReferenceConverter.convertIfPossible(symbol, tableInfo));
                }
            } else {

                collectSymbols = splitPoints.toCollect();
            }
            if (orderBy != null) {
                MergeProjection mergeProjection = projectionBuilder.mergeProjection(
                        collectSymbols,
                        orderBy);
                collectProjections.add(mergeProjection);
            }

            Integer limit = querySpec.limit();

            if ( limit == null && context.rootRelation() == table) {
                limit = Constants.DEFAULT_SELECT_LIMIT;
            }

            CollectNode collectNode = PlanNodeBuilder.collect(
                    context.plannerContext().jobId(),
                    tableInfo,
                    context.plannerContext(),
                    querySpec.where(),
                    collectSymbols,
                    ImmutableList.<Projection>of(),
                    orderBy,
                    limit == null ? null : limit + querySpec.offset()
            );


            collectNode.keepContextForFetcher(needFetchProjection);
            collectNode.projections(collectProjections);



            TopNProjection topNProjection;
            if (needFetchProjection) {
                topNProjection = projectionBuilder.topNProjection(
                        collectSymbols,
                        null,
                        querySpec.offset(),
                        querySpec.limit(),
                        null);
                mergeProjections.add(topNProjection);



                int bulkSize = FetchProjector.NO_BULK_REQUESTS;
                if (topNProjection.limit() > Constants.DEFAULT_SELECT_LIMIT) {
                    bulkSize = Constants.DEFAULT_SELECT_LIMIT;
                }

                FetchProjection fetchProjection = new FetchProjection(
                        collectNode.executionNodeId(),
                        DEFAULT_DOC_ID_INPUT_COLUMN, collectSymbols, outputSymbols,
                        tableInfo.partitionedByColumns(),
                        collectNode.executionNodes(),
                        bulkSize,
                        querySpec.isLimited(),
                        context.plannerContext().jobSearchContextIdToNode(),
                        context.plannerContext().jobSearchContextIdToShard()
                );
                mergeProjections.add(fetchProjection);
            } else {
                topNProjection = projectionBuilder.topNProjection(
                        collectSymbols,
                        null,
                        querySpec.offset(),
                        querySpec.limit(),
                        querySpec.outputs());
                mergeProjections.add(topNProjection);
            }

            MergeNode localMergeNode;
            if (orderBy != null) {
                localMergeNode = PlanNodeBuilder.sortedLocalMerge(
                        context.plannerContext().jobId(),
                        mergeProjections,
                        orderBy,
                        collectSymbols,
                        null,
                        collectNode,
                        context.plannerContext());
            } else {
                localMergeNode = PlanNodeBuilder.localMerge(
                        context.plannerContext().jobId(),
                        mergeProjections,
                        collectNode,
                        context.plannerContext());
            }


            if (limit != null && limit + querySpec.offset() > Constants.PAGE_SIZE) {
                collectNode.downstreamNodes(Collections.singletonList(context.plannerContext().clusterService().localNode().id()));
                collectNode.downstreamExecutionNodeId(localMergeNode.executionNodeId());
            }
            return new QueryThenFetch(collectNode, localMergeNode, context.plannerContext().jobId());
        }

        @Override
        protected PlannedAnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, ConsumerContext context) {
            return null;
        }
    }

    static class OutputOrderReferenceContext {

        private List<Reference> outputReferences = new ArrayList<>();
        private List<Reference> orderByReferences = new ArrayList<>();
        public boolean collectOrderBy = false;

        public void addReference(Reference reference) {
            if (collectOrderBy) {
                orderByReferences.add(reference);
            } else {
                outputReferences.add(reference);
            }
        }

        public boolean outputsAreAllOrdered() {
            return orderByReferences.containsAll(outputReferences);
        }

    }

    static class OutputOrderReferenceCollector extends SymbolVisitor<OutputOrderReferenceContext, Void> {

        public OutputOrderReferenceContext collect(List<Symbol> symbols) {
            OutputOrderReferenceContext context = new OutputOrderReferenceContext();
            collect(symbols, context);
            return context;
        }

        public void collect(List<Symbol> symbols, OutputOrderReferenceContext context) {
            for (Symbol symbol : symbols) {
                process(symbol, context);
            }
        }

        @Override
        public Void visitAggregation(Aggregation aggregation, OutputOrderReferenceContext context) {
            for (Symbol symbol : aggregation.inputs()) {
                process(symbol, context);
            }
            return null;
        }

        @Override
        public Void visitReference(Reference symbol, OutputOrderReferenceContext context) {
            context.addReference(symbol);
            return null;
        }

        @Override
        public Void visitDynamicReference(DynamicReference symbol, OutputOrderReferenceContext context) {
            return visitReference(symbol, context);
        }

        @Override
        public Void visitFunction(Function function, OutputOrderReferenceContext context) {
            for (Symbol symbol : function.arguments()) {
                process(symbol, context);
            }
            return null;
        }
    }

    static class ReferencesCollectorContext {
        private List<Reference> outputReferences = new ArrayList<>();

        public void addReference(Reference reference) {
            outputReferences.add(reference);
        }

        public boolean containsAnyReference() {
            return !outputReferences.isEmpty();
        }
    }

    static class ReferencesCollector extends SymbolVisitor<ReferencesCollectorContext, Void> {

        public ReferencesCollectorContext collect(List<Symbol> symbols) {
            ReferencesCollectorContext context = new ReferencesCollectorContext();
            collect(symbols, context);
            return context;
        }

        public void collect(List<Symbol> symbols, ReferencesCollectorContext context) {
            for (Symbol symbol : symbols) {
                process(symbol, context);
            }
        }

        @Override
        public Void visitAggregation(Aggregation aggregation, ReferencesCollectorContext context) {
            for (Symbol symbol : aggregation.inputs()) {
                process(symbol, context);
            }
            return null;
        }

        @Override
        public Void visitReference(Reference symbol, ReferencesCollectorContext context) {
            context.addReference(symbol);
            return null;
        }

        @Override
        public Void visitDynamicReference(DynamicReference symbol, ReferencesCollectorContext context) {
            return visitReference(symbol, context);
        }

        @Override
        public Void visitFunction(Function function, ReferencesCollectorContext context) {
            for (Symbol symbol : function.arguments()) {
                process(symbol, context);
            }
            return null;
        }

    }
}

<code block>


package io.crate.planner.consumer;

import com.google.common.base.MoreObjects;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.Lists;
import com.google.common.collect.Sets;
import io.crate.Constants;
import io.crate.analyze.HavingClause;
import io.crate.analyze.InsertFromSubQueryAnalyzedStatement;
import io.crate.analyze.OrderBy;
import io.crate.analyze.QueriedTable;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.Routing;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.DistributedGroupBy;
import io.crate.planner.node.dql.GroupByConsumer;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.GroupProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.TopNProjection;
import io.crate.planner.projection.builder.ProjectionBuilder;
import io.crate.planner.projection.builder.SplitPoints;
import io.crate.planner.symbol.Aggregation;
import io.crate.planner.symbol.Symbol;

import java.util.ArrayList;
import java.util.LinkedList;
import java.util.List;

public class DistributedGroupByConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        Context ctx = new Context(context);
        context.rootRelation(VISITOR.process(context.rootRelation(), ctx));
        return ctx.result;
    }

    private static class Context {
        ConsumerContext consumerContext;
        boolean result = false;

        public Context(ConsumerContext context) {
            this.consumerContext = context;
        }
    }

    private static class Visitor extends AnalyzedRelationVisitor<Context, AnalyzedRelation> {

        @Override
        public AnalyzedRelation visitQueriedTable(QueriedTable table, Context context) {
            List<Symbol> groupBy = table.querySpec().groupBy();
            if (groupBy == null) {
                return table;
            }

            TableInfo tableInfo = table.tableRelation().tableInfo();
            if(table.querySpec().where().hasVersions()){
                context.consumerContext.validationException(new VersionInvalidException());
                return table;
            }

            Routing routing = tableInfo.getRouting(table.querySpec().where(), null);

            GroupByConsumer.validateGroupBySymbols(table.tableRelation(), table.querySpec().groupBy());

            ProjectionBuilder projectionBuilder = new ProjectionBuilder(table.querySpec());

            SplitPoints splitPoints = projectionBuilder.getSplitPoints();


            GroupProjection groupProjection = projectionBuilder.groupProjection(
                    splitPoints.leaves(),
                    table.querySpec().groupBy(),
                    splitPoints.aggregates(),
                    Aggregation.Step.ITER,
                    Aggregation.Step.PARTIAL);

            CollectNode collectNode = PlanNodeBuilder.distributingCollect(
                    context.consumerContext.plannerContext().jobId(),
                    tableInfo,
                    context.consumerContext.plannerContext(),
                    table.querySpec().where(),
                    splitPoints.leaves(),
                    Lists.newArrayList(routing.nodes()),
                    ImmutableList.<Projection>of(groupProjection)
            );



            List<Symbol> collectOutputs = new ArrayList<>(
                    groupBy.size() +
                            splitPoints.aggregates().size());
            collectOutputs.addAll(groupBy);
            collectOutputs.addAll(splitPoints.aggregates());

            List<Projection> reducerProjections = new LinkedList<>();
            reducerProjections.add(projectionBuilder.groupProjection(
                    collectOutputs,
                    table.querySpec().groupBy(),
                    splitPoints.aggregates(),
                    Aggregation.Step.PARTIAL,
                    Aggregation.Step.FINAL)
            );

            OrderBy orderBy = table.querySpec().orderBy();
            if (orderBy != null) {
                table.tableRelation().validateOrderBy(orderBy);
            }

            HavingClause havingClause = table.querySpec().having();
            if (havingClause != null) {
                if (havingClause.noMatch()) {
                    return new NoopPlannedAnalyzedRelation(table, context.consumerContext.plannerContext().jobId());
                } else if (havingClause.hasQuery()) {
                    reducerProjections.add(projectionBuilder.filterProjection(
                            collectOutputs,
                            havingClause.query()
                    ));
                }
            }

            boolean isRootRelation = context.consumerContext.rootRelation() == table;
            if (isRootRelation) {
                reducerProjections.add(projectionBuilder.topNProjection(
                        collectOutputs,
                        orderBy,
                        0,
                        MoreObjects.firstNonNull(table.querySpec().limit(),
                                Constants.DEFAULT_SELECT_LIMIT) + table.querySpec().offset(),
                        table.querySpec().outputs()));
            }
            MergeNode mergeNode = PlanNodeBuilder.distributedMerge(
                    context.consumerContext.plannerContext().jobId(),
                    collectNode,
                    context.consumerContext.plannerContext(),
                    reducerProjections
            );


            MergeNode localMergeNode = null;
            String localNodeId = context.consumerContext.plannerContext().clusterService().state().nodes().localNodeId();
            if(isRootRelation) {
                TopNProjection topN = projectionBuilder.topNProjection(
                        table.querySpec().outputs(),
                        orderBy,
                        table.querySpec().offset(),
                        table.querySpec().limit(),
                        null);
                localMergeNode = PlanNodeBuilder.localMerge(context.consumerContext.plannerContext().jobId(),
                        ImmutableList.<Projection>of(topN),
                        mergeNode, context.consumerContext.plannerContext());
                localMergeNode.executionNodes(Sets.newHashSet(localNodeId));

                mergeNode.downstreamNodes(localMergeNode.executionNodes());
                mergeNode.downstreamExecutionNodeId(localMergeNode.executionNodeId());
            } else {
                mergeNode.downstreamNodes(Sets.newHashSet(localNodeId));
                mergeNode.downstreamExecutionNodeId(mergeNode.executionNodeId() + 1);
            }
            context.result = true;

            collectNode.downstreamExecutionNodeId(mergeNode.executionNodeId());
            return new DistributedGroupBy(
                    collectNode,
                    mergeNode,
                    localMergeNode,
                    context.consumerContext.plannerContext().jobId()
            );
        }

        @Override
        public AnalyzedRelation visitInsertFromQuery(InsertFromSubQueryAnalyzedStatement insertFromSubQueryAnalyzedStatement, Context context) {
            InsertFromSubQueryConsumer.planInnerRelation(insertFromSubQueryAnalyzedStatement, context, this);
            return insertFromSubQueryAnalyzedStatement;
        }

        @Override
        protected AnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, Context context) {
            return relation;
        }

    }
}

<code block>


package io.crate.planner.consumer;

import com.google.common.collect.ImmutableList;
import io.crate.Constants;
import io.crate.analyze.HavingClause;
import io.crate.analyze.InsertFromSubQueryAnalyzedStatement;
import io.crate.analyze.OrderBy;
import io.crate.analyze.QueriedTable;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.TableRelation;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.projectors.TopN;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.GroupByConsumer;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.dql.NonDistributedGroupBy;
import io.crate.planner.projection.FilterProjection;
import io.crate.planner.projection.GroupProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.builder.ProjectionBuilder;
import io.crate.planner.projection.builder.SplitPoints;
import io.crate.planner.symbol.Aggregation;
import io.crate.planner.symbol.Symbol;

import java.util.ArrayList;
import java.util.List;

import static com.google.common.base.MoreObjects.firstNonNull;

public class ReduceOnCollectorGroupByConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        Context ctx = new Context(context);
        context.rootRelation(VISITOR.process(context.rootRelation(), ctx));
        return ctx.result;
    }

    private static class Context {
        ConsumerContext consumerContext;
        boolean result = false;

        public Context(ConsumerContext context) {
            this.consumerContext = context;
        }
    }

    private static class Visitor extends AnalyzedRelationVisitor<Context, AnalyzedRelation> {

        @Override
        public AnalyzedRelation visitQueriedTable(QueriedTable table, Context context) {
            if (table.querySpec().groupBy() == null) {
                return table;
            }

            if (!GroupByConsumer.groupedByClusteredColumnOrPrimaryKeys(
                    table.tableRelation(), table.querySpec().where(), table.querySpec().groupBy())) {
                return table;
            }

            if (table.querySpec().where().hasVersions()) {
                context.consumerContext.validationException(new VersionInvalidException());
                return table;
            }
            context.result = true;
            return optimizedReduceOnCollectorGroupBy(table, table.tableRelation(), context.consumerContext);
        }

        @Override
        public AnalyzedRelation visitInsertFromQuery(InsertFromSubQueryAnalyzedStatement insertFromSubQueryAnalyzedStatement, Context context) {
            InsertFromSubQueryConsumer.planInnerRelation(insertFromSubQueryAnalyzedStatement, context, this);
            return insertFromSubQueryAnalyzedStatement;
        }

        @Override
        protected AnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, Context context) {
            return relation;
        }


        private AnalyzedRelation optimizedReduceOnCollectorGroupBy(QueriedTable table, TableRelation tableRelation, ConsumerContext context) {
            assert GroupByConsumer.groupedByClusteredColumnOrPrimaryKeys(
                    tableRelation, table.querySpec().where(), table.querySpec().groupBy()) : "not grouped by clustered column or primary keys";
            TableInfo tableInfo = tableRelation.tableInfo();
            GroupByConsumer.validateGroupBySymbols(tableRelation, table.querySpec().groupBy());
            List<Symbol> groupBy = table.querySpec().groupBy();

            boolean ignoreSorting = context.rootRelation() != table
                    && table.querySpec().limit() == null
                    && table.querySpec().offset() == TopN.NO_OFFSET;

            ProjectionBuilder projectionBuilder = new ProjectionBuilder(table.querySpec());
            SplitPoints splitPoints = projectionBuilder.getSplitPoints();


            List<Symbol> collectOutputs = new ArrayList<>(
                    groupBy.size() +
                            splitPoints.aggregates().size());
            collectOutputs.addAll(groupBy);
            collectOutputs.addAll(splitPoints.aggregates());

            OrderBy orderBy = table.querySpec().orderBy();
            if (orderBy != null) {
                table.tableRelation().validateOrderBy(orderBy);
            }

            List<Projection> projections = new ArrayList<>();
            GroupProjection groupProjection = projectionBuilder.groupProjection(
                    splitPoints.leaves(),
                    table.querySpec().groupBy(),
                    splitPoints.aggregates(),
                    Aggregation.Step.ITER,
                    Aggregation.Step.FINAL
            );
            groupProjection.setRequiredGranularity(RowGranularity.SHARD);
            projections.add(groupProjection);

            HavingClause havingClause = table.querySpec().having();
            if (havingClause != null) {
                if (havingClause.noMatch()) {
                    return new NoopPlannedAnalyzedRelation(table, context.plannerContext().jobId());
                } else if (havingClause.hasQuery()) {
                    FilterProjection fp = projectionBuilder.filterProjection(
                            collectOutputs,
                            havingClause.query()
                    );
                    fp.requiredGranularity(RowGranularity.SHARD);
                    projections.add(fp);
                }
            }


            boolean outputsMatch = table.querySpec().outputs().size() == collectOutputs.size() &&
                    collectOutputs.containsAll(table.querySpec().outputs());
            boolean collectorTopN = table.querySpec().limit() != null || table.querySpec().offset() > 0 || !outputsMatch;

            if (collectorTopN) {
                projections.add(projectionBuilder.topNProjection(
                        collectOutputs,
                        orderBy,
                        0, 
                        firstNonNull(table.querySpec().limit(), Constants.DEFAULT_SELECT_LIMIT) + table.querySpec().offset(),
                        table.querySpec().outputs()
                ));
            }

            CollectNode collectNode = PlanNodeBuilder.collect(
                    context.plannerContext().jobId(),
                    tableInfo,
                    context.plannerContext(),
                    table.querySpec().where(),
                    splitPoints.leaves(),
                    ImmutableList.copyOf(projections)
            );


            List<Projection> handlerProjections = new ArrayList<>();
            MergeNode localMergeNode;
            if (!ignoreSorting && collectorTopN && orderBy != null && orderBy.isSorted()) {


                handlerProjections.add(
                        projectionBuilder.topNProjection(
                                table.querySpec().outputs(),
                                null, 
                                table.querySpec().offset(),
                                firstNonNull(table.querySpec().limit(), Constants.DEFAULT_SELECT_LIMIT),
                                table.querySpec().outputs()
                        )
                );
                localMergeNode = PlanNodeBuilder.sortedLocalMerge(
                        context.plannerContext().jobId(),
                        handlerProjections, orderBy, table.querySpec().outputs(), null,
                        collectNode, context.plannerContext());
            } else {
                handlerProjections.add(
                        projectionBuilder.topNProjection(
                                collectorTopN ? table.querySpec().outputs() : collectOutputs,
                                orderBy,
                                table.querySpec().offset(),
                                firstNonNull(table.querySpec().limit(), Constants.DEFAULT_SELECT_LIMIT),
                                table.querySpec().outputs()
                        )
                );

                localMergeNode = PlanNodeBuilder.localMerge(context.plannerContext().jobId(), handlerProjections, collectNode,
                        context.plannerContext());
            }
            return new NonDistributedGroupBy(collectNode, localMergeNode, context.plannerContext().jobId());
        }


    }
}

<code block>


package io.crate.planner.consumer;


import com.google.common.collect.ImmutableList;
import io.crate.analyze.InsertFromSubQueryAnalyzedStatement;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.operation.aggregation.impl.CountAggregation;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.node.dml.InsertFromSubQuery;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.AggregationProjection;
import io.crate.planner.projection.ColumnIndexWriterProjection;
import io.crate.planner.projection.Projection;
import org.elasticsearch.common.settings.ImmutableSettings;


public class InsertFromSubQueryConsumer implements Consumer {

    private final Visitor visitor;

    public InsertFromSubQueryConsumer(){
        visitor = new Visitor();
    }

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        Context ctx = new Context(context);
        context.rootRelation(visitor.process(context.rootRelation(), ctx));
        return ctx.result;
    }

    private static class Context {
        ConsumerContext consumerContext;
        boolean result = false;

        public Context(ConsumerContext context){
            this.consumerContext = context;
        }
    }

    private static class Visitor extends AnalyzedRelationVisitor<Context, AnalyzedRelation> {

        @Override
        public AnalyzedRelation visitInsertFromQuery(InsertFromSubQueryAnalyzedStatement insertFromSubQueryAnalyzedStatement, Context context) {

            ColumnIndexWriterProjection indexWriterProjection = new ColumnIndexWriterProjection(
                    insertFromSubQueryAnalyzedStatement.tableInfo().ident(),
                    null,
                    insertFromSubQueryAnalyzedStatement.tableInfo().primaryKey(),
                    insertFromSubQueryAnalyzedStatement.columns(),
                    insertFromSubQueryAnalyzedStatement.onDuplicateKeyAssignments(),
                    insertFromSubQueryAnalyzedStatement.primaryKeyColumnIndices(),
                    insertFromSubQueryAnalyzedStatement.partitionedByIndices(),
                    insertFromSubQueryAnalyzedStatement.routingColumn(),
                    insertFromSubQueryAnalyzedStatement.routingColumnIndex(),
                    ImmutableSettings.EMPTY,
                    insertFromSubQueryAnalyzedStatement.tableInfo().isPartitioned()
            );

            AnalyzedRelation innerRelation = insertFromSubQueryAnalyzedStatement.subQueryRelation();
            if (innerRelation instanceof PlannedAnalyzedRelation) {
                PlannedAnalyzedRelation analyzedRelation = (PlannedAnalyzedRelation)innerRelation;
                analyzedRelation.addProjection(indexWriterProjection);

                MergeNode mergeNode = null;
                if (analyzedRelation.resultIsDistributed()) {

                    AggregationProjection aggregationProjection = CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION;
                    mergeNode = PlanNodeBuilder.localMerge(
                            context.consumerContext.plannerContext().jobId(),
                            ImmutableList.<Projection>of(aggregationProjection),
                            analyzedRelation.resultNode(),
                            context.consumerContext.plannerContext());
                }
                context.result = true;
                return new InsertFromSubQuery(((PlannedAnalyzedRelation) innerRelation).plan(), mergeNode, context.consumerContext.plannerContext().jobId());
            } else {
                return insertFromSubQueryAnalyzedStatement;
            }
        }

        @Override
        protected AnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, Context context) {
            return relation;
        }
    }

    public static <C, R> void planInnerRelation(InsertFromSubQueryAnalyzedStatement insertFromSubQueryAnalyzedStatement,
                                                C context, AnalyzedRelationVisitor<C,R> visitor) {
        if (insertFromSubQueryAnalyzedStatement.subQueryRelation() instanceof PlannedAnalyzedRelation) {

            return;
        }
        R innerRelation = visitor.process(insertFromSubQueryAnalyzedStatement.subQueryRelation(), context);
        if (innerRelation != null && innerRelation instanceof PlannedAnalyzedRelation) {
            insertFromSubQueryAnalyzedStatement.subQueryRelation((PlannedAnalyzedRelation)innerRelation);
        }
    }


}

<code block>


package io.crate.planner.node;

import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.exceptions.ColumnUnknownException;
import io.crate.metadata.Path;
import io.crate.planner.NoopPlan;
import io.crate.planner.Plan;
import io.crate.planner.node.dql.DQLPlanNode;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.Field;

import javax.annotation.Nullable;
import java.util.List;
import java.util.UUID;

public class NoopPlannedAnalyzedRelation implements PlannedAnalyzedRelation {

    private final AnalyzedRelation relation;
    private final UUID id;

    public NoopPlannedAnalyzedRelation(AnalyzedRelation relation, UUID id) {
        this.relation = relation;
        this.id = id;
    }

    @Override
    public Plan plan() {
        return new NoopPlan(id);
    }

    @Override
    public <C, R> R accept(AnalyzedRelationVisitor<C, R> visitor, C context) {
        return visitor.visitPlanedAnalyzedRelation(this, context);
    }

    @Nullable
    @Override
    public Field getField(Path path) {
        return relation.getField(path);
    }

    @Override
    public Field getWritableField(Path path) throws UnsupportedOperationException, ColumnUnknownException {
        return relation.getWritableField(path);
    }

    @Override
    public List<Field> fields() {
        return relation.fields();
    }

    @Override
    public void addProjection(Projection projection) {
        throw new UnsupportedOperationException("addingProjection not supported");
    }

    @Override
    public boolean resultIsDistributed() {
        throw new UnsupportedOperationException("resultIsDistributed is not supported");
    }

    @Override
    public DQLPlanNode resultNode() {
        throw new UnsupportedOperationException("resultNode is not supported");
    }

}

<code block>


package io.crate.planner.node;

import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.CountNode;
import io.crate.planner.node.dql.FileUriCollectNode;
import io.crate.planner.node.dql.MergeNode;
import org.elasticsearch.common.io.stream.Streamable;

import java.util.List;
import java.util.Set;
import java.util.UUID;

public interface ExecutionNode extends Streamable {

    String DIRECT_RETURN_DOWNSTREAM_NODE = "_response";

    int NO_EXECUTION_NODE = Integer.MAX_VALUE;

    interface ExecutionNodeFactory<T extends ExecutionNode> {
        T create();
    }

    enum Type {
        COLLECT(CollectNode.FACTORY),
        COUNT(CountNode.FACTORY),
        FILE_URI_COLLECT(FileUriCollectNode.FACTORY),
        MERGE(MergeNode.FACTORY);

        private final ExecutionNodeFactory factory;

        Type(ExecutionNodeFactory factory) {
            this.factory = factory;
        }

        public ExecutionNodeFactory factory() {
            return factory;
        }
    }

    Type type();

    String name();

    int executionNodeId();

    Set<String> executionNodes();

    List<String> downstreamNodes();

    int downstreamExecutionNodeId();

    UUID jobId();

    <C, R> R accept(ExecutionNodeVisitor<C, R> visitor, C context);
}

<code block>


package io.crate.planner.node.dql;

import io.crate.planner.PlanAndPlannedAnalyzedRelation;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.PlanVisitor;
import io.crate.planner.projection.Projection;

import javax.annotation.Nullable;
import java.util.UUID;

public class DistributedGroupBy extends PlanAndPlannedAnalyzedRelation {

    private final CollectNode collectNode;
    private final MergeNode reducerMergeNode;
    private MergeNode localMergeNode;
    private final UUID id;

    public DistributedGroupBy(CollectNode collectNode, MergeNode reducerMergeNode, @Nullable MergeNode localMergeNode, UUID id) {
        this.collectNode = collectNode;
        this.reducerMergeNode = reducerMergeNode;
        this.localMergeNode = localMergeNode;
        this.id = id;
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitDistributedGroupBy(this, context);
    }

    @Override
    public UUID jobId() {
        return id;
    }

    public CollectNode collectNode() {
        return collectNode;
    }

    public MergeNode reducerMergeNode() {
        return reducerMergeNode;
    }

    public MergeNode localMergeNode() {
        return localMergeNode;
    }

    @Override
    public void addProjection(Projection projection) {
        DQLPlanNode node = resultNode();
        node.addProjection(projection);
        if (node instanceof CollectNode) {
            PlanNodeBuilder.setOutputTypes((CollectNode)node);
        } else if (node instanceof MergeNode) {
            PlanNodeBuilder.connectTypes(reducerMergeNode, node);
        }
    }

    @Override
    public boolean resultIsDistributed() {
        return localMergeNode == null;
    }

    @Override
    public DQLPlanNode resultNode() {
        return localMergeNode != null ? localMergeNode : reducerMergeNode;
    }
}

<code block>


package io.crate.planner.node.dql;

import io.crate.planner.PlanAndPlannedAnalyzedRelation;
import io.crate.planner.PlanVisitor;
import io.crate.planner.projection.Projection;

import java.util.UUID;

public class CountPlan extends PlanAndPlannedAnalyzedRelation{

    private final CountNode countNode;
    private final MergeNode mergeNode;
    private final UUID id;

    public CountPlan(CountNode countNode, MergeNode mergeNode, UUID id) {
        this.countNode = countNode;
        this.mergeNode = mergeNode;
        this.id = id;
    }

    public CountNode countNode() {
        return countNode;
    }

    public MergeNode mergeNode() {
        return mergeNode;
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitCountPlan(this, context);
    }

    @Override
    public UUID jobId() {
        return id;
    }

    @Override
    public void addProjection(Projection projection) {
        mergeNode.addProjection(projection);
    }

    @Override
    public boolean resultIsDistributed() {
        return false;
    }

    @Override
    public DQLPlanNode resultNode() {
        return mergeNode;
    }
}

<code block>


package io.crate.planner.node.dql;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.Sets;
import io.crate.analyze.WhereClause;
import io.crate.metadata.Routing;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.node.ExecutionNodeVisitor;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;

import java.io.IOException;
import java.util.List;
import java.util.Set;
import java.util.UUID;

public class CountNode implements ExecutionNode {

    public static final ExecutionNodeFactory<CountNode> FACTORY = new ExecutionNodeFactory<CountNode>() {
        @Override
        public CountNode create() {
            return new CountNode();
        }
    };
    private UUID jobId;
    private int executionNodeId;
    private Routing routing;
    private WhereClause whereClause;

    CountNode() {}

    public CountNode(UUID jobId, int executionNodeId, Routing routing, WhereClause whereClause) {
        this.jobId = jobId;
        this.executionNodeId = executionNodeId;
        this.routing = routing;
        this.whereClause = whereClause;
    }

    @Override
    public Type type() {
        return Type.COUNT;
    }

    @Override
    public String name() {
        return "count";
    }

    @Override
    public UUID jobId() {
        return jobId;
    }

    public Routing routing() {
        return routing;
    }

    public WhereClause whereClause() {
        return whereClause;
    }

    @Override
    public int executionNodeId() {
        return executionNodeId;
    }

    @Override
    public Set<String> executionNodes() {
        if (routing.isNullRouting()) {
            return routing.nodes();
        } else {
            return Sets.filter(routing.nodes(), TableInfo.IS_NOT_NULL_NODE_ID);
        }
    }

    @Override
    public List<String> downstreamNodes() {
        return ImmutableList.of(ExecutionNode.DIRECT_RETURN_DOWNSTREAM_NODE);
    }

    @Override
    public int downstreamExecutionNodeId() {
        return ExecutionNode.NO_EXECUTION_NODE;
    }

    @Override
    public <C, R> R accept(ExecutionNodeVisitor<C, R> visitor, C context) {
        return visitor.visitCountNode(this, context);
    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        jobId = new UUID(in.readLong(), in.readLong());
        executionNodeId = in.readVInt();
        routing = new Routing();
        routing.readFrom(in);
        whereClause = new WhereClause(in);
    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        assert jobId != null : "jobId must not be null";
        out.writeLong(jobId.getMostSignificantBits());
        out.writeLong(jobId.getLeastSignificantBits());
        out.writeVInt(executionNodeId);
        routing.writeTo(out);
        whereClause.writeTo(out);
    }
}

<code block>


package io.crate.planner.node.dql;

import com.google.common.base.MoreObjects;
import com.google.common.base.Optional;
import com.google.common.collect.ImmutableList;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.projection.Projection;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;
import org.elasticsearch.common.io.stream.Streamable;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import java.util.UUID;

public abstract class AbstractDQLPlanNode implements DQLPlanNode, Streamable, ExecutionNode {

    private UUID jobId;
    private int executionNodeId;
    private String name;
    protected List<Projection> projections = ImmutableList.of();
    protected List<DataType> outputTypes = ImmutableList.of();
    private List<DataType> inputTypes;

    public AbstractDQLPlanNode() {

    }

    protected AbstractDQLPlanNode(UUID jobId, int executionNodeId, String name) {
        this.jobId = jobId;
        this.executionNodeId = executionNodeId;
        this.name = name;
    }

    public String name() {
        return name;
    }

    @Override
    public UUID jobId() {
        return jobId;
    }

    @Override
    public int executionNodeId() {
        return executionNodeId;
    }

    public boolean hasProjections() {
        return projections != null && projections.size() > 0;
    }

    @Override
    public List<Projection> projections() {
        return projections;
    }

    public void projections(List<Projection> projections) {
        this.projections = projections;
    }

    @Override
    public void addProjection(Projection projection) {
        List<Projection> projections = new ArrayList<>(this.projections);
        projections.add(projection);
        this.projections = ImmutableList.copyOf(projections);
    }

    public Optional<Projection> finalProjection() {
        if (projections.size() == 0) {
            return Optional.absent();
        } else {
            return Optional.of(projections.get(projections.size()-1));
        }
    }


    public void outputTypes(List<DataType> outputTypes) {
        this.outputTypes = outputTypes;
    }

    public List<DataType> outputTypes() {
        return outputTypes;
    }

    @Override
    public void inputTypes(List<DataType> dataTypes) {
        this.inputTypes = dataTypes;
    }

    @Override
    public List<DataType> inputTypes() {
        return inputTypes;
    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        name = in.readString();
        jobId = new UUID(in.readLong(), in.readLong());
        executionNodeId = in.readVInt();

        int numCols = in.readVInt();
        if (numCols > 0) {
            outputTypes = new ArrayList<>(numCols);
            for (int i = 0; i < numCols; i++) {
                outputTypes.add(DataTypes.fromStream(in));
            }
        }

        int numProjections = in.readVInt();
        if (numProjections > 0) {
            projections = new ArrayList<>(numProjections);
            for (int i = 0; i < numProjections; i++) {
                projections.add(Projection.fromStream(in));
            }
        }

    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        out.writeString(name);
        assert jobId != null : "jobId must not be null";
        out.writeLong(jobId.getMostSignificantBits());
        out.writeLong(jobId.getLeastSignificantBits());
        out.writeVInt(executionNodeId);

        int numCols = outputTypes.size();
        out.writeVInt(numCols);
        for (int i = 0; i < numCols; i++) {
            DataTypes.toStream(outputTypes.get(i), out);
        }

        if (hasProjections()) {
            out.writeVInt(projections.size());
            for (Projection p : projections) {
                Projection.toStream(p, out);
            }
        } else {
            out.writeVInt(0);
        }
    }

    @Override
    public boolean equals(Object o) {
        if (this == o) return true;
        if (o == null || getClass() != o.getClass()) return false;

        AbstractDQLPlanNode node = (AbstractDQLPlanNode) o;

        return !(name != null ? !name.equals(node.name) : node.name != null);

    }

    @Override
    public int hashCode() {
        return name != null ? name.hashCode() : 0;
    }

    @Override
    public String toString() {
        return MoreObjects.toStringHelper(this)
                .add("name", name)
                .add("projections", projections)
                .add("outputTypes", outputTypes)
                .toString();
    }
}

<code block>


package io.crate.planner.node.dql;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableSet;
import com.google.common.collect.Sets;
import io.crate.analyze.EvaluatingNormalizer;
import io.crate.analyze.OrderBy;
import io.crate.analyze.WhereClause;
import io.crate.metadata.Routing;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.node.ExecutionNodeVisitor;
import io.crate.planner.node.PlanNodeVisitor;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.Symbol;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;

import javax.annotation.Nullable;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import java.util.Set;
import java.util.UUID;


public class CollectNode extends AbstractDQLPlanNode {

    public static final ExecutionNodeFactory<CollectNode> FACTORY = new ExecutionNodeFactory<CollectNode>() {
        @Override
        public CollectNode create() {
            return new CollectNode();
        }
    };
    private Routing routing;
    private List<Symbol> toCollect;
    private WhereClause whereClause = WhereClause.MATCH_ALL;
    private RowGranularity maxRowGranularity = RowGranularity.CLUSTER;

    @Nullable
    private List<String> downstreamNodes;

    private int downstreamExecutionNodeId = ExecutionNode.NO_EXECUTION_NODE;

    private boolean isPartitioned = false;
    private boolean keepContextForFetcher = false;
    private @Nullable String handlerSideCollect = null;

    private @Nullable Integer limit = null;
    private @Nullable OrderBy orderBy = null;

    protected CollectNode() {
        super();
    }

    public CollectNode(UUID jobId, int executionNodeId, String name) {
        super(jobId, executionNodeId, name);
    }

    public CollectNode(UUID jobId, int executionNodeId, String name, Routing routing) {
        this(jobId, executionNodeId, name, routing, ImmutableList.<Symbol>of(), ImmutableList.<Projection>of());
    }

    public CollectNode(UUID jobId, int executionNodeId, String name, Routing routing, List<Symbol> toCollect, List<Projection> projections) {
        super(jobId, executionNodeId, name);
        this.routing = routing;
        this.toCollect = toCollect;
        this.projections = projections;
        this.downstreamNodes = ImmutableList.of(ExecutionNode.DIRECT_RETURN_DOWNSTREAM_NODE);
    }

    @Override
    public Type type() {
        return Type.COLLECT;
    }


    @Override
    public Set<String> executionNodes() {
        if (routing != null) {
            if (routing.isNullRouting()) {
                return routing.nodes();
            } else {
                return Sets.filter(routing.nodes(), TableInfo.IS_NOT_NULL_NODE_ID);
            }
        } else {
            return ImmutableSet.of();
        }
    }

    public @Nullable Integer limit() {
        return limit;
    }

    public void limit(Integer limit) {
        this.limit = limit;
    }

    public @Nullable OrderBy orderBy() {
        return orderBy;
    }

    public void orderBy(@Nullable OrderBy orderBy) {
        this.orderBy = orderBy;
    }

    @Nullable
    public List<String> downstreamNodes() {
        return downstreamNodes;
    }


    public boolean hasDistributingDownstreams() {
        if (downstreamNodes != null && downstreamNodes.size() > 0) {
            if (downstreamNodes.size() == 1
                    && downstreamNodes.get(0).equals(ExecutionNode.DIRECT_RETURN_DOWNSTREAM_NODE)) {
                return false;
            }
            return true;
        }
        return false;
    }

    public void downstreamNodes(List<String> downStreamNodes) {
        this.downstreamNodes = downStreamNodes;
    }

    public void downstreamExecutionNodeId(int executionNodeId) {
        this.downstreamExecutionNodeId = executionNodeId;
    }

    public int downstreamExecutionNodeId() {
        return downstreamExecutionNodeId;
    }

    public WhereClause whereClause() {
        return whereClause;
    }

    public void whereClause(WhereClause whereClause) {
        assert whereClause != null;
        this.whereClause = whereClause;
    }

    public Routing routing() {
        return routing;
    }

    public List<Symbol> toCollect() {
        return toCollect;
    }

    public void toCollect(List<Symbol> toCollect) {
        assert toCollect != null;
        this.toCollect = toCollect;
    }

    public boolean isRouted() {
        return routing != null && routing.hasLocations();
    }


    public boolean isPartitioned() {
        return isPartitioned;
    }

    public void isPartitioned(boolean isPartitioned) {
        this.isPartitioned = isPartitioned;
    }

    public RowGranularity maxRowGranularity() {
        return maxRowGranularity;
    }

    public void maxRowGranularity(RowGranularity newRowGranularity) {
        if (maxRowGranularity.compareTo(newRowGranularity) < 0) {
            maxRowGranularity = newRowGranularity;
        }
    }

    @Override
    public <C, R> R accept(PlanNodeVisitor<C, R> visitor, C context) {
        return visitor.visitCollectNode(this, context);
    }

    @Override
    public <C, R> R accept(ExecutionNodeVisitor<C, R> visitor, C context) {
        return visitor.visitCollectNode(this, context);
    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        super.readFrom(in);
        downstreamExecutionNodeId = in.readVInt();

        int numCols = in.readVInt();
        if (numCols > 0) {
            toCollect = new ArrayList<>(numCols);
            for (int i = 0; i < numCols; i++) {
                toCollect.add(Symbol.fromStream(in));
            }
        } else {
            toCollect = ImmutableList.of();
        }

        maxRowGranularity = RowGranularity.fromStream(in);

        if (in.readBoolean()) {
            routing = new Routing();
            routing.readFrom(in);
        }

        whereClause = new WhereClause(in);

        int numDownStreams = in.readVInt();
        downstreamNodes = new ArrayList<>(numDownStreams);
        for (int i = 0; i < numDownStreams; i++) {
            downstreamNodes.add(in.readString());
        }
        keepContextForFetcher = in.readBoolean();

        if( in.readBoolean()) {
            limit = in.readVInt();
        }

        if (in.readBoolean()) {
            orderBy = OrderBy.fromStream(in);
        }
        isPartitioned = in.readBoolean();
        handlerSideCollect = in.readOptionalString();
    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        super.writeTo(out);
        out.writeVInt(downstreamExecutionNodeId);

        int numCols = toCollect.size();
        out.writeVInt(numCols);
        for (int i = 0; i < numCols; i++) {
            Symbol.toStream(toCollect.get(i), out);
        }

        RowGranularity.toStream(maxRowGranularity, out);

        if (routing != null) {
            out.writeBoolean(true);
            routing.writeTo(out);
        } else {
            out.writeBoolean(false);
        }
        whereClause.writeTo(out);

        if (downstreamNodes != null) {
            out.writeVInt(downstreamNodes.size());
            for (String downstreamNode : downstreamNodes) {
                out.writeString(downstreamNode);
            }
        } else {
            out.writeVInt(0);
        }
        out.writeBoolean(keepContextForFetcher);
        if (limit != null ) {
            out.writeBoolean(true);
            out.writeVInt(limit);
        } else {
            out.writeBoolean(false);
        }
        if (orderBy != null) {
            out.writeBoolean(true);
            OrderBy.toStream(orderBy, out);
        } else {
            out.writeBoolean(false);
        }
        out.writeBoolean(isPartitioned);
        out.writeOptionalString(handlerSideCollect);
    }


    public CollectNode normalize(EvaluatingNormalizer normalizer) {
        assert whereClause() != null;
        CollectNode result = this;
        List<Symbol> newToCollect = normalizer.normalize(toCollect());
        boolean changed = newToCollect != toCollect();
        WhereClause newWhereClause = whereClause().normalize(normalizer);
        if (newWhereClause != whereClause()) {
            changed = changed || newWhereClause != whereClause();
        }
        if (changed) {
            result = new CollectNode(jobId(), executionNodeId(), name(), routing, newToCollect, projections);
            result.downstreamNodes = downstreamNodes;
            result.maxRowGranularity = maxRowGranularity;
            result.keepContextForFetcher = keepContextForFetcher;
            result.handlerSideCollect = handlerSideCollect;
            result.isPartitioned(isPartitioned);
            result.whereClause(newWhereClause);
        }
        return result;
    }

    public void keepContextForFetcher(boolean keepContextForFetcher) {
        this.keepContextForFetcher = keepContextForFetcher;
    }

    public boolean keepContextForFetcher() {
        return keepContextForFetcher;
    }

    public void handlerSideCollect(String handlerSideCollect) {
        this.handlerSideCollect = handlerSideCollect;
    }

    @Nullable
    public String handlerSideCollect() {
        return handlerSideCollect;
    }
}
<code block>


package io.crate.planner.node.dql;

import com.google.common.base.MoreObjects;
import com.google.common.base.Preconditions;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableSet;
import io.crate.planner.node.ExecutionNodeVisitor;
import io.crate.planner.node.PlanNodeVisitor;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;

import javax.annotation.Nullable;
import java.io.IOException;
import java.util.*;


public class MergeNode extends AbstractDQLPlanNode {

    public static final ExecutionNodeFactory<MergeNode> FACTORY = new ExecutionNodeFactory<MergeNode>() {
        @Override
        public MergeNode create() {
            return new MergeNode();
        }
    };

    private List<DataType> inputTypes;
    private int numUpstreams;
    private Set<String> executionNodes;


    private boolean sortedInputOutput = false;
    private int[] orderByIndices;
    private boolean[] reverseFlags;
    private Boolean[] nullsFirst;
    private int downstreamExecutionNodeId = NO_EXECUTION_NODE;
    private List<String> downstreamNodes = ImmutableList.of();

    public MergeNode() {
        numUpstreams = 0;
    }

    public MergeNode(UUID jobId, int executionNodeId, String name, int numUpstreams) {
        super(jobId, executionNodeId, name);
        this.numUpstreams = numUpstreams;
    }

    public static MergeNode sortedMergeNode(UUID jobId,
                                            int executionNodeId,
                                            String name,
                                            int numUpstreams,
                                            int[] orderByIndices,
                                            boolean[] reverseFlags,
                                            Boolean[] nullsFirst) {
        Preconditions.checkArgument(
                orderByIndices.length == reverseFlags.length && reverseFlags.length == nullsFirst.length,
                "ordering parameters must be of the same length");
        MergeNode mergeNode = new MergeNode(jobId, executionNodeId, name, numUpstreams);
        mergeNode.sortedInputOutput = true;
        mergeNode.orderByIndices = orderByIndices;
        mergeNode.reverseFlags = reverseFlags;
        mergeNode.nullsFirst = nullsFirst;
        return mergeNode;
    }

    @Override
    public Type type() {
        return Type.MERGE;
    }

    @Override
    public Set<String> executionNodes() {
        if (executionNodes == null) {
            return ImmutableSet.of();
        } else {
            return executionNodes;
        }
    }

    @Override
    public List<String> downstreamNodes() {
        return downstreamNodes;
    }

    public void downstreamNodes(Set<String> nodes) {
        downstreamNodes = ImmutableList.copyOf(nodes);
    }

    @Override
    public int downstreamExecutionNodeId() {
        return downstreamExecutionNodeId;
    }

    public void executionNodes(Set<String> executionNodes) {
        this.executionNodes = executionNodes;
    }

    public int numUpstreams() {
        return numUpstreams;
    }

    public List<DataType> inputTypes() {
        return inputTypes;
    }

    public void inputTypes(List<DataType> inputTypes) {
        this.inputTypes = inputTypes;
    }

    public boolean sortedInputOutput() {
        return sortedInputOutput;
    }

    @Nullable
    public int[] orderByIndices() {
        return orderByIndices;
    }

    @Nullable
    public boolean[] reverseFlags() {
        return reverseFlags;
    }

    @Nullable
    public Boolean[] nullsFirst() {
        return nullsFirst;
    }

    @Override
    public <C, R> R accept(PlanNodeVisitor<C, R> visitor, C context) {
        return visitor.visitMergeNode(this, context);
    }

    @Override
    public <C, R> R accept(ExecutionNodeVisitor<C, R> visitor, C context) {
        return visitor.visitMergeNode(this, context);
    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        super.readFrom(in);
        downstreamExecutionNodeId = in.readVInt();

        int numDownstreamNodes = in.readVInt();
        downstreamNodes = new ArrayList<>(numDownstreamNodes);
        for (int i = 0; i < numDownstreamNodes; i++) {
            downstreamNodes.add(in.readString());
        }

        numUpstreams = in.readVInt();

        int numCols = in.readVInt();
        if (numCols > 0) {
            inputTypes = new ArrayList<>(numCols);
            for (int i = 0; i < numCols; i++) {
                inputTypes.add(DataTypes.fromStream(in));
            }
        }
        int numExecutionNodes = in.readVInt();

        if (numExecutionNodes > 0) {
            executionNodes = new HashSet<>(numExecutionNodes);
            for (int i = 0; i < numExecutionNodes; i++) {
                executionNodes.add(in.readString());
            }
        }

        sortedInputOutput = in.readBoolean();
        if (sortedInputOutput) {
            int orderByIndicesLength = in.readVInt();
            orderByIndices = new int[orderByIndicesLength];
            reverseFlags = new boolean[orderByIndicesLength];
            nullsFirst = new Boolean[orderByIndicesLength];
            for (int i = 0; i < orderByIndicesLength; i++) {
                orderByIndices[i] = in.readVInt();
                reverseFlags[i] = in.readBoolean();
                nullsFirst[i] = in.readOptionalBoolean();
            }
        }
    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        super.writeTo(out);
        out.writeVInt(downstreamExecutionNodeId);

        out.writeVInt(downstreamNodes.size());
        for (String downstreamNode : downstreamNodes) {
            out.writeString(downstreamNode);
        }

        out.writeVInt(numUpstreams);

        int numCols = inputTypes.size();
        out.writeVInt(numCols);
        for (DataType inputType : inputTypes) {
            DataTypes.toStream(inputType, out);
        }

        if (executionNodes == null) {
            out.writeVInt(0);
        } else {
            out.writeVInt(executionNodes.size());
            for (String node : executionNodes) {
                out.writeString(node);
            }
        }

        out.writeBoolean(sortedInputOutput);
        if (sortedInputOutput) {
            out.writeVInt(orderByIndices.length);
            for (int i = 0; i < orderByIndices.length; i++) {
                out.writeVInt(orderByIndices[i]);
                out.writeBoolean(reverseFlags[i]);
                out.writeOptionalBoolean(nullsFirst[i]);
            }
        }
    }

    @Override
    public String toString() {
        MoreObjects.ToStringHelper helper = MoreObjects.toStringHelper(this)
                .add("executionNodeId", executionNodeId())
                .add("name", name())
                .add("projections", projections)
                .add("outputTypes", outputTypes)
                .add("jobId", jobId())
                .add("numUpstreams", numUpstreams)
                .add("executionNodes", executionNodes)
                .add("inputTypes", inputTypes)
                .add("sortedInputOutput", sortedInputOutput);
        if (sortedInputOutput) {
            helper.add("orderByIndices", Arrays.toString(orderByIndices))
                  .add("reverseFlags", Arrays.toString(reverseFlags))
                  .add("nullsFirst", Arrays.toString(nullsFirst));
        }
        return helper.toString();
    }

    public void downstreamExecutionNodeId(int downstreamExecutionNodeId) {
        this.downstreamExecutionNodeId = downstreamExecutionNodeId;
    }
}

<code block>
package io.crate.planner.node.dql;


import io.crate.planner.PlanAndPlannedAnalyzedRelation;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.PlanVisitor;
import io.crate.planner.projection.Projection;

import javax.annotation.Nullable;
import java.util.UUID;

public class QueryAndFetch extends PlanAndPlannedAnalyzedRelation {

    private final CollectNode collectNode;
    private MergeNode localMergeNode;
    private final UUID id;

    public QueryAndFetch(CollectNode collectNode, @Nullable MergeNode localMergeNode, UUID id){
        this.collectNode = collectNode;
        this.localMergeNode = localMergeNode;
        this.id = id;
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitQueryAndFetch(this, context);
    }

    @Override
    public UUID jobId() {
        return id;
    }

    public CollectNode collectNode() {
        return collectNode;
    }

    public MergeNode localMergeNode(){
        return localMergeNode;
    }

    @Override
    public void addProjection(Projection projection) {
        DQLPlanNode node = resultNode();
        node.addProjection(projection);
        if (node instanceof CollectNode) {
            PlanNodeBuilder.setOutputTypes((CollectNode) node);
        } else if (node instanceof MergeNode) {
            PlanNodeBuilder.connectTypes(collectNode, node);
        }
    }

    @Override
    public boolean resultIsDistributed() {
        return localMergeNode == null;
    }

    @Override
    public DQLPlanNode resultNode() {
        return localMergeNode == null ? collectNode : localMergeNode;
    }
}

<code block>


package io.crate.planner.node.dql;

import com.google.common.base.MoreObjects;
import com.google.common.collect.ImmutableList;
import io.crate.analyze.OrderBy;
import io.crate.analyze.QuerySpec;
import io.crate.analyze.where.DocKeys;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.IterablePlan;
import io.crate.planner.Plan;
import io.crate.planner.node.PlanNodeVisitor;
import io.crate.planner.symbol.Symbol;
import io.crate.planner.symbol.Symbols;
import org.elasticsearch.common.Nullable;

import java.util.List;
import java.util.Set;
import java.util.UUID;


public class ESGetNode extends ESDQLPlanNode implements DQLPlanNode {

    private final TableInfo tableInfo;
    private final QuerySpec querySpec;
    private final List<Symbol> sortSymbols;
    private final boolean[] reverseFlags;
    private final Boolean[] nullsFirst;
    private final int executionNodeId;
    private final UUID jobId;

    private final static boolean[] EMPTY_REVERSE_FLAGS = new boolean[0];
    private final static Boolean[] EMPTY_NULLS_FIRST = new Boolean[0];
    private final DocKeys docKeys;

    public ESGetNode(int executionNodeId,
                     TableInfo tableInfo,
                     QuerySpec querySpec,
                     UUID jobId) {

        assert querySpec.where().docKeys().isPresent();
        this.tableInfo = tableInfo;
        this.querySpec = querySpec;
        this.outputs = querySpec.outputs();
        this.docKeys = querySpec.where().docKeys().get();
        this.executionNodeId = executionNodeId;
        this.jobId = jobId;


        outputTypes(Symbols.extractTypes(outputs));

        OrderBy orderBy = querySpec.orderBy();
        if (orderBy != null && orderBy.isSorted()){
            this.sortSymbols = orderBy.orderBySymbols();
            this.reverseFlags = orderBy.reverseFlags();
            this.nullsFirst = orderBy.nullsFirst();
        } else {
            this.sortSymbols = ImmutableList.<Symbol>of();
            this.reverseFlags = EMPTY_REVERSE_FLAGS;
            this.nullsFirst = EMPTY_NULLS_FIRST;
        }
    }

    @Override
    public <C, R> R accept(PlanNodeVisitor<C, R> visitor, C context) {
        return visitor.visitESGetNode(this, context);
    }

    public TableInfo tableInfo() {
        return tableInfo;
    }

    public QuerySpec querySpec() {
        return querySpec;
    }

    public DocKeys docKeys() {
        return docKeys;
    }

    @Nullable
    public Integer limit() {
        return querySpec().limit();
    }

    public int offset() {
        return querySpec().offset();
    }

    public List<Symbol> sortSymbols() {
        return sortSymbols;
    }

    public boolean[] reverseFlags() {
        return reverseFlags;
    }

    public Boolean[] nullsFirst() {
        return nullsFirst;
    }

    public int executionNodeId() {
        return executionNodeId;
    }

    @Override
    public String toString() {
        return MoreObjects.toStringHelper(this)
                .add("docKeys", docKeys)
                .add("outputs", outputs)
                .toString();
    }

    public UUID jobId() {
        return jobId;
    }

    @Override
    public Plan plan() {
        return new IterablePlan(jobId, this);
    }
}

<code block>

package io.crate.planner.node.dql;


import io.crate.planner.PlanAndPlannedAnalyzedRelation;
import io.crate.planner.PlanVisitor;
import io.crate.planner.projection.Projection;

import java.util.UUID;

public class NonDistributedGroupBy extends PlanAndPlannedAnalyzedRelation {

    private final CollectNode collectNode;
    private MergeNode localMergeNode;
    private final UUID id;

    public NonDistributedGroupBy(CollectNode collectNode, MergeNode localMergeNode, UUID id){
        this.collectNode = collectNode;
        this.localMergeNode = localMergeNode;
        this.id = id;
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitNonDistributedGroupBy(this, context);
    }

    @Override
    public UUID jobId() {
        return id;
    }

    public MergeNode localMergeNode() {
        return localMergeNode;
    }

    public CollectNode collectNode() {
        return collectNode;
    }

    @Override
    public void addProjection(Projection projection) {
        this.resultNode().addProjection(projection);
    }

    @Override
    public boolean resultIsDistributed() {
        return localMergeNode == null;
    }

    @Override
    public DQLPlanNode resultNode() {
        return localMergeNode == null ? collectNode : localMergeNode;
    }
}

<code block>


package io.crate.planner.node.dql;

import com.google.common.base.Optional;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableSet;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.exceptions.ColumnUnknownException;
import io.crate.metadata.Path;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.Field;
import io.crate.planner.symbol.Symbol;
import io.crate.planner.symbol.ValueSymbolVisitor;
import io.crate.types.DataType;

import javax.annotation.Nullable;
import java.util.List;
import java.util.Set;

public abstract class ESDQLPlanNode implements DQLPlanNode, PlannedAnalyzedRelation {

    private static final char COMMA = ',';

    protected List<Symbol> outputs;
    private List<DataType> inputTypes;
    private List<DataType> outputTypes;

    public List<Symbol> outputs() {
        return outputs;
    }

    @Nullable
    public static String noCommaStringRouting(Optional<Set<Symbol>> clusteredBy) {
        if (clusteredBy.isPresent()){
            StringBuilder sb = new StringBuilder();
            boolean first = true;
            for (Symbol symbol : clusteredBy.get()) {
                String s = ValueSymbolVisitor.STRING.process(symbol);
                if (s.indexOf(COMMA)>-1){
                    return null;
                }
                if (!first){
                    sb.append(COMMA);
                } else {
                    first = false;
                }
                sb.append(s);
            }
            return sb.toString();
        }
        return null;
    }

    @Override
    public boolean hasProjections() {
        return false;
    }

    @Override
    public List<Projection> projections() {
        return ImmutableList.of();
    }

    @Override
    public void inputTypes(List<DataType> dataTypes) {
        this.inputTypes = dataTypes;
    }

    @Override
    public List<DataType> inputTypes() {
        return inputTypes;
    }

    @Override
    public Set<String> executionNodes() {
        return ImmutableSet.of();
    }

    @Override
    public void outputTypes(List<DataType> outputTypes) {
        this.outputTypes = outputTypes;
    }

    @Override
    public List<DataType> outputTypes() {
        return outputTypes;
    }

    @Override
    public <C, R> R accept(AnalyzedRelationVisitor<C, R> visitor, C context) {
        return visitor.visitPlanedAnalyzedRelation(this, context);
    }

    @javax.annotation.Nullable
    @Override
    public Field getField(Path path) {
        throw new UnsupportedOperationException("getField is not supported on ESDQLPlanNode");
    }

    @Override
    public Field getWritableField(Path path) throws UnsupportedOperationException, ColumnUnknownException {
        throw new UnsupportedOperationException("getWritableField is not supported on ESDQLPlanNode");
    }

    @Override
    public List<Field> fields() {
        throw new UnsupportedOperationException("fields is not supported on ESDQLPlanNode");
    }

    @Override
    public void addProjection(Projection projection) {
        throw new UnsupportedOperationException("addProjection not supported on ESDQLPlanNode");
    }

    @Override
    public boolean resultIsDistributed() {
        throw new UnsupportedOperationException("resultIsDistributed is not supported on ESDQLPlanNode");
    }

    @Override
    public DQLPlanNode resultNode() {
        throw new UnsupportedOperationException("resultNode is not supported on ESDQLPLanNode");
    }

}

<code block>


package io.crate.planner.node.dql;

import io.crate.planner.PlanAndPlannedAnalyzedRelation;
import io.crate.planner.PlanVisitor;
import io.crate.planner.projection.Projection;

import java.util.UUID;

public class GlobalAggregate extends PlanAndPlannedAnalyzedRelation {

    private final CollectNode collectNode;
    private MergeNode mergeNode;
    private final UUID id;

    public GlobalAggregate(CollectNode collectNode, MergeNode mergeNode, UUID id) {
        this.collectNode = collectNode;
        this.mergeNode = mergeNode;
        this.id = id;
    }

    public CollectNode collectNode() {
        return collectNode;
    }

    public MergeNode mergeNode() {
        return mergeNode;
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitGlobalAggregate(this, context);
    }

    @Override
    public UUID jobId() {
        return id;
    }

    @Override
    public void addProjection(Projection projection) {
        mergeNode.projections().add(projection);
    }

    @Override
    public boolean resultIsDistributed() {
        return mergeNode == null;
    }

    @Override
    public DQLPlanNode resultNode() {
        return mergeNode == null ? collectNode : mergeNode;
    }
}

<code block>


package io.crate.planner.node.dql;

import io.crate.planner.Plan;
import io.crate.planner.PlanVisitor;
import io.crate.planner.node.PlanNode;

import java.util.Arrays;
import java.util.Iterator;
import java.util.UUID;

public class CollectAndMerge implements Iterable<PlanNode>, Plan {

    private final CollectNode collectNode;
    private final MergeNode localMergeNode;
    private Iterable<PlanNode> nodes;
    private final UUID id;

    public CollectAndMerge(CollectNode collectNode, MergeNode localMergeNode, UUID id) {
        this.collectNode = collectNode;
        this.localMergeNode = localMergeNode;
        nodes = Arrays.<PlanNode>asList(collectNode, localMergeNode);
        this.id = id;
    }

    public CollectNode collectNode() {
        return collectNode;
    }

    public MergeNode localMergeNode() {
        return localMergeNode;
    }

    @Override
    public Iterator<PlanNode> iterator() {
        return nodes.iterator();
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitCollectAndMerge(this, context);
    }

    @Override
    public UUID jobId() {
        return id;
    }
}

<code block>


package io.crate.planner.node.dql;

import com.google.common.base.MoreObjects;
import io.crate.analyze.EvaluatingNormalizer;
import io.crate.analyze.WhereClause;
import io.crate.metadata.Routing;
import io.crate.operation.collect.files.FileReadingCollector;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.Symbol;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;

import javax.annotation.Nullable;
import java.io.IOException;
import java.util.List;
import java.util.UUID;

public class FileUriCollectNode extends CollectNode {

    public static final ExecutionNodeFactory<FileUriCollectNode> FACTORY = new ExecutionNodeFactory<FileUriCollectNode>() {
        @Override
        public FileUriCollectNode create() {
            return new FileUriCollectNode();
        }
    };
    private Symbol targetUri;
    private String compression;
    private Boolean sharedStorage;

    private FileUriCollectNode() {
        super();
    }

    public FileUriCollectNode(UUID jobId,
                              int executionNodeId,
                              String name,
                              Routing routing,
                              Symbol targetUri,
                              List<Symbol> toCollect,
                              List<Projection> projections,
                              String compression,
                              Boolean sharedStorage) {
        super(jobId, executionNodeId, name, routing, toCollect, projections);
        this.targetUri = targetUri;
        this.compression = compression;
        this.sharedStorage = sharedStorage;
    }

    public Symbol targetUri() {
        return targetUri;
    }

    public FileReadingCollector.FileFormat fileFormat() {
        return FileReadingCollector.FileFormat.JSON;
    }

    @Override
    public Type type() {
        return Type.FILE_URI_COLLECT;
    }

    @Override
    public FileUriCollectNode normalize(EvaluatingNormalizer normalizer) {
        List<Symbol> normalizedToCollect = normalizer.normalize(toCollect());
        Symbol normalizedTargetUri = normalizer.normalize(targetUri);
        WhereClause normalizedWhereClause = whereClause().normalize(normalizer);
        boolean changed =
                (normalizedToCollect != toCollect() )
                        || (normalizedTargetUri != targetUri)
                        || (normalizedWhereClause != whereClause());
        if (!changed) {
            return this;
        }
        FileUriCollectNode result = new FileUriCollectNode(
                jobId(),
                executionNodeId(),
                name(),
                routing(),
                normalizedTargetUri,
                normalizedToCollect,
                projections(),
                compression(),
                sharedStorage());
        result.downstreamNodes(downstreamNodes());
        result.maxRowGranularity(maxRowGranularity());
        result.isPartitioned(isPartitioned());
        result.whereClause(normalizedWhereClause);
        return result;
    }

    @Nullable
    public String compression() {
        return compression;
    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        super.readFrom(in);
        compression = in.readOptionalString();
        sharedStorage = in.readOptionalBoolean();
        targetUri = Symbol.fromStream(in);
    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        super.writeTo(out);
        out.writeOptionalString(compression);
        out.writeOptionalBoolean(sharedStorage);
        Symbol.toStream(targetUri, out);
    }

    @Override
    public String toString() {
        return MoreObjects.toStringHelper(this)
                .add("name", name())
                .add("targetUri", targetUri)
                .add("projections", projections)
                .add("outputTypes", outputTypes)
                .add("compression", compression)
                .add("sharedStorageDefault", sharedStorage)
                .toString();
    }

    @Nullable
    public Boolean sharedStorage() {
        return sharedStorage;
    }
}


<code block>


package io.crate.planner.node.dql;

import io.crate.planner.PlanAndPlannedAnalyzedRelation;
import io.crate.planner.PlanVisitor;
import io.crate.planner.projection.Projection;

import java.util.UUID;

public class QueryThenFetch extends PlanAndPlannedAnalyzedRelation {

    private final CollectNode collectNode;
    private MergeNode mergeNode;
    private final UUID id;

    public QueryThenFetch(CollectNode collectNode, MergeNode mergeNode, UUID id) {
        this.collectNode = collectNode;
        this.mergeNode = mergeNode;
        this.id = id;
    }

    public CollectNode collectNode() {
        return collectNode;
    }

    public MergeNode mergeNode() {
        return mergeNode;
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitQueryThenFetch(this, context);
    }

    @Override
    public UUID jobId() {
        return id;
    }

    @Override
    public void addProjection(Projection projection) {
        mergeNode.projections().add(projection);
    }

    @Override
    public boolean resultIsDistributed() {
        return mergeNode == null;
    }

    @Override
    public DQLPlanNode resultNode() {
        return mergeNode == null ? collectNode : mergeNode;
    }
}

<code block>


package io.crate.planner.node.dml;


import com.google.common.base.Optional;
import io.crate.planner.Plan;
import io.crate.planner.PlanAndPlannedAnalyzedRelation;
import io.crate.planner.PlanVisitor;
import io.crate.planner.node.dql.DQLPlanNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.Projection;

import javax.annotation.Nullable;
import java.util.UUID;

public class InsertFromSubQuery extends PlanAndPlannedAnalyzedRelation {


    private final Optional<MergeNode> handlerMergeNode;

    private final Plan innerPlan;

    private final UUID id;

    public InsertFromSubQuery(Plan innerPlan, @Nullable MergeNode handlerMergeNode, UUID id) {
        this.innerPlan = innerPlan;
        this.handlerMergeNode = Optional.fromNullable(handlerMergeNode);
        this.id = id;
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitInsertByQuery(this, context);
    }

    @Override
    public UUID jobId() {
        return id;
    }

    public Plan innerPlan() {
        return innerPlan;
    }

    public Optional<MergeNode> handlerMergeNode() {
        return handlerMergeNode;
    }

    @Override
    public void addProjection(Projection projection) {
        throw new UnsupportedOperationException("addingProjection not supported");
    }

    @Override
    public boolean resultIsDistributed() {
        throw new UnsupportedOperationException("resultIsDistributed is not supported");
    }

    @Override
    public DQLPlanNode resultNode() {
        throw new UnsupportedOperationException("resultNode is not supported");
    }
}

<code block>


package io.crate.planner.node.dml;

import io.crate.planner.Plan;
import io.crate.planner.PlanAndPlannedAnalyzedRelation;
import io.crate.planner.PlanVisitor;
import io.crate.planner.node.dql.DQLPlanNode;
import io.crate.planner.projection.Projection;

import java.util.List;
import java.util.UUID;

public class Upsert extends PlanAndPlannedAnalyzedRelation {

    private final List<Plan> nodes;
    private final UUID id;

    public Upsert(List<Plan> nodes, UUID id) {
        this.nodes = nodes;
        this.id = id;
    }

    public List<Plan> nodes() {
        return nodes;
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitUpsert(this, context);
    }

    @Override
    public UUID jobId() {
        return id;
    }

    @Override
    public void addProjection(Projection projection) {
        throw new UnsupportedOperationException("adding projection not supported");
    }

    @Override
    public boolean resultIsDistributed() {
        throw new UnsupportedOperationException("resultIsDistributed is not supported");
    }

    @Override
    public DQLPlanNode resultNode() {
        throw new UnsupportedOperationException("resultNode is not supported");
    }

}

<code block>


package io.crate.planner.node.management;

import io.crate.planner.Plan;
import io.crate.planner.PlanVisitor;

import java.util.UUID;

public class KillPlan implements Plan {

    private final UUID id;

    public KillPlan(UUID id) {
        this.id = id;
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitKillPlan(this, context);
    }

    @Override
    public UUID jobId() {
        return id;
    }
}

<code block>


package io.crate.integrationtests;

import com.carrotsearch.hppc.LongArrayList;
import com.google.common.base.Predicates;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.Iterables;
import com.google.common.util.concurrent.FutureCallback;
import com.google.common.util.concurrent.Futures;
import com.google.common.util.concurrent.ListenableFuture;
import io.crate.action.job.ContextPreparer;
import io.crate.analyze.Analysis;
import io.crate.analyze.Analyzer;
import io.crate.analyze.ParameterContext;
import io.crate.analyze.WhereClause;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.core.collections.Bucket;
import io.crate.core.collections.Row;
import io.crate.executor.Job;
import io.crate.executor.TaskResult;
import io.crate.executor.transport.NodeFetchRequest;
import io.crate.executor.transport.NodeFetchResponse;
import io.crate.executor.transport.TransportExecutor;
import io.crate.executor.transport.TransportFetchNodeAction;
import io.crate.jobs.JobContextService;
import io.crate.jobs.JobExecutionContext;
import io.crate.metadata.ColumnIdent;
import io.crate.metadata.Functions;
import io.crate.metadata.ReferenceInfo;
import io.crate.metadata.doc.DocSchemaInfo;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.fetch.RowInputSymbolVisitor;
import io.crate.planner.Plan;
import io.crate.planner.Planner;
import io.crate.planner.RowGranularity;
import io.crate.planner.consumer.ConsumerContext;
import io.crate.planner.consumer.QueryThenFetchConsumer;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.dql.QueryThenFetch;
import io.crate.planner.projection.FetchProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.sql.parser.SqlParser;
import io.crate.types.DataType;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.action.ActionListener;
import org.elasticsearch.test.ElasticsearchIntegrationTest;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;

import java.util.*;
import java.util.concurrent.CountDownLatch;

import static org.hamcrest.Matchers.*;
import static org.hamcrest.core.Is.is;

@ElasticsearchIntegrationTest.ClusterScope(numDataNodes = 2, numClientNodes = 0)
public class FetchOperationIntegrationTest extends SQLTransportIntegrationTest {

    Setup setup = new Setup(sqlExecutor);
    TransportExecutor executor;
    DocSchemaInfo docSchemaInfo;

    @Before
    public void transportSetUp() {
        executor = internalCluster().getInstance(TransportExecutor.class);
        docSchemaInfo = internalCluster().getInstance(DocSchemaInfo.class);
    }

    @After
    public void transportTearDown() {
        executor = null;
        docSchemaInfo = null;
    }

    private void setUpCharacters() {
        sqlExecutor.exec("create table characters (id int primary key, name string) " +
                "clustered into 2 shards with(number_of_replicas=0)");
        sqlExecutor.ensureYellowOrGreen();
        sqlExecutor.exec("insert into characters (id, name) values (?, ?)",
                new Object[][]{
                        new Object[]{1, "Arthur"},
                        new Object[]{2, "Ford"},
                }
        );
        sqlExecutor.refresh("characters");
    }

    private Plan analyzeAndPlan(String stmt) {
        Analysis analysis = analyze(stmt);
        Planner planner = internalCluster().getInstance(Planner.class);
        return planner.plan(analysis);
    }

    private Analysis analyze(String stmt) {
        Analyzer analyzer = internalCluster().getInstance(Analyzer.class);
        return analyzer.analyze(
                SqlParser.createStatement(stmt),
                new ParameterContext(new Object[0], new Object[0][], null)
        );
    }

    private CollectNode createCollectNode(Planner.Context plannerContext, boolean keepContextForFetcher) {
        TableInfo tableInfo = docSchemaInfo.getTableInfo("characters");

        ReferenceInfo docIdRefInfo = tableInfo.getReferenceInfo(new ColumnIdent("_docid"));
        Symbol docIdRef = new Reference(docIdRefInfo);
        List<Symbol> toCollect = ImmutableList.of(docIdRef);

        List<DataType> outputTypes = new ArrayList<>(toCollect.size());
        for (Symbol symbol : toCollect) {
            outputTypes.add(symbol.valueType());
        }
        CollectNode collectNode = new CollectNode(
                UUID.randomUUID(),
                plannerContext.nextExecutionNodeId(),
                "collect",
                tableInfo.getRouting(WhereClause.MATCH_ALL, null));
        collectNode.toCollect(toCollect);
        collectNode.outputTypes(outputTypes);
        collectNode.maxRowGranularity(RowGranularity.DOC);
        collectNode.keepContextForFetcher(keepContextForFetcher);
        plannerContext.allocateJobSearchContextIds(collectNode.routing());

        return collectNode;
    }

    private List<Bucket> getBuckets(CollectNode collectNode) throws InterruptedException, java.util.concurrent.ExecutionException {
        List<Bucket> results = new ArrayList<>();
        for (String nodeName : internalCluster().getNodeNames()) {
            ContextPreparer contextPreparer = internalCluster().getInstance(ContextPreparer.class, nodeName);
            JobContextService contextService = internalCluster().getInstance(JobContextService.class, nodeName);

            JobExecutionContext.Builder builder = contextService.newBuilder(collectNode.jobId());
            ListenableFuture<Bucket> future = contextPreparer.prepare(collectNode.jobId(), collectNode, builder);
            assert future != null;

            JobExecutionContext context = contextService.createContext(builder);
            context.start();
            results.add(future.get());
        }
        return results;
    }

    @Test
    public void testCollectDocId() throws Exception {
        setUpCharacters();
        Planner.Context plannerContext = new Planner.Context(clusterService());
        CollectNode collectNode = createCollectNode(plannerContext, false);

        List<Bucket> results = getBuckets(collectNode);

        assertThat(results.size(), is(2));
        int seenJobSearchContextId = -1;
        for (Bucket rows : results) {
            assertThat(rows.size(), is(1));
            Object docIdCol = rows.iterator().next().get(0);
            assertNotNull(docIdCol);
            assertThat(docIdCol, instanceOf(Long.class));
            long docId = (long)docIdCol;

            int jobSearchContextId = (int)(docId >> 32);
            int doc = (int)docId;
            assertThat(doc, is(0));
            assertThat(jobSearchContextId, greaterThan(-1));
            if (seenJobSearchContextId == -1) {
                assertThat(jobSearchContextId, anyOf(is(0), is(1)));
                seenJobSearchContextId = jobSearchContextId;
            } else {
                assertThat(jobSearchContextId, is(seenJobSearchContextId == 0 ? 1 : 0));
            }
        }
    }

    @Test
    public void testFetchAction() throws Exception {
        setUpCharacters();

        Analysis analysis = analyze("select id, name from characters");
        QueryThenFetchConsumer queryThenFetchConsumer = internalCluster().getInstance(QueryThenFetchConsumer.class);
        Planner.Context plannerContext = new Planner.Context(clusterService());
        ConsumerContext consumerContext = new ConsumerContext(analysis.rootRelation(), plannerContext);
        queryThenFetchConsumer.consume(analysis.rootRelation(), consumerContext);

        QueryThenFetch plan = ((QueryThenFetch) ((PlannedAnalyzedRelation) consumerContext.rootRelation()).plan());
        List<Bucket> results = getBuckets(plan.collectNode());


        TransportFetchNodeAction transportFetchNodeAction = internalCluster().getInstance(TransportFetchNodeAction.class);


        Map<String, LongArrayList> jobSearchContextDocIds = new HashMap<>();
        for (Bucket rows : results) {
            long docId = (long)rows.iterator().next().get(0);

            int jobSearchContextId = (int)(docId >> 32);
            String nodeId = plannerContext.nodeId(jobSearchContextId);
            LongArrayList docIdsPerNode = jobSearchContextDocIds.get(nodeId);
            if (docIdsPerNode == null) {
                docIdsPerNode = new LongArrayList();
                jobSearchContextDocIds.put(nodeId, docIdsPerNode);
            }
            docIdsPerNode.add(docId);
        }

        Iterable<Projection> projections = Iterables.filter(plan.mergeNode().projections(), Predicates.instanceOf(FetchProjection.class));
        FetchProjection fetchProjection = (FetchProjection )Iterables.getOnlyElement(projections);
        RowInputSymbolVisitor rowInputSymbolVisitor = new RowInputSymbolVisitor(internalCluster().getInstance(Functions.class));
        RowInputSymbolVisitor.Context context = rowInputSymbolVisitor.extractImplementations(fetchProjection.outputSymbols());

        final CountDownLatch latch = new CountDownLatch(jobSearchContextDocIds.size());
        final List<Row> rows = new ArrayList<>();
        for (Map.Entry<String, LongArrayList> nodeEntry : jobSearchContextDocIds.entrySet()) {
            NodeFetchRequest nodeFetchRequest = new NodeFetchRequest();
            nodeFetchRequest.jobId(plan.collectNode().jobId());
            nodeFetchRequest.executionNodeId(plan.collectNode().executionNodeId());
            nodeFetchRequest.toFetchReferences(context.references());
            nodeFetchRequest.closeContext(true);
            nodeFetchRequest.jobSearchContextDocIds(nodeEntry.getValue());

            transportFetchNodeAction.execute(nodeEntry.getKey(), nodeFetchRequest, new ActionListener<NodeFetchResponse>() {
                @Override
                public void onResponse(NodeFetchResponse nodeFetchResponse) {
                    for (Row row : nodeFetchResponse.rows()) {
                        rows.add(row);
                    }
                    latch.countDown();
                }

                @Override
                public void onFailure(Throwable e) {
                    latch.countDown();
                    fail(e.getMessage());
                }
            });
        }
        latch.await();

        assertThat(rows.size(), is(2));
        for (Row row : rows) {
            assertThat((Integer) row.get(0), anyOf(is(1), is(2)));
            assertThat((BytesRef) row.get(1), anyOf(is(new BytesRef("Arthur")), is(new BytesRef("Ford"))));
        }
    }

    @Test
    public void testFetchProjection() throws Exception {
        setUpCharacters();

        Plan plan = analyzeAndPlan("select id, name, substr(name, 2) from characters order by id");
        assertThat(plan, instanceOf(QueryThenFetch.class));
        QueryThenFetch qtf = (QueryThenFetch) plan;

        assertThat(qtf.collectNode().keepContextForFetcher(), is(true));
        assertThat(((FetchProjection) qtf.mergeNode().projections().get(1)).jobSearchContextIdToNode(), notNullValue());
        assertThat(((FetchProjection) qtf.mergeNode().projections().get(1)).jobSearchContextIdToShard(), notNullValue());

        Job job = executor.newJob(plan);
        ListenableFuture<List<TaskResult>> results = Futures.allAsList(executor.execute(job));

        final List<Object[]> resultingRows = new ArrayList<>();
        final CountDownLatch latch = new CountDownLatch(1);
        Futures.addCallback(results, new FutureCallback<List<TaskResult>>() {
            @Override
            public void onSuccess(List<TaskResult> resultList) {
                for (Row row : resultList.get(0).rows()) {
                    resultingRows.add(row.materialize());
                }
                latch.countDown();
            }

            @Override
            public void onFailure(Throwable t) {
                latch.countDown();
                fail(t.getMessage());
            }
        });

        latch.await();
        assertThat(resultingRows.size(), is(2));
        assertThat(resultingRows.get(0).length, is(3));
        assertThat((Integer) resultingRows.get(0)[0], is(1));
        assertThat((BytesRef) resultingRows.get(0)[1], is(new BytesRef("Arthur")));
        assertThat((BytesRef) resultingRows.get(0)[2], is(new BytesRef("rthur")));
        assertThat((Integer) resultingRows.get(1)[0], is(2));
        assertThat((BytesRef) resultingRows.get(1)[1], is(new BytesRef("Ford")));
        assertThat((BytesRef) resultingRows.get(1)[2], is(new BytesRef("ord")));
    }

    @Test
    public void testFetchProjectionWithBulkSize() throws Exception {

        setup.setUpLocations();
        sqlExecutor.refresh("locations");
        int bulkSize = 2;

        Plan plan = analyzeAndPlan("select position, name from locations order by position");
        assertThat(plan, instanceOf(QueryThenFetch.class));

        rewriteFetchProjectionToBulkSize(bulkSize, ((QueryThenFetch) plan).mergeNode());

        Job job = executor.newJob(plan);
        ListenableFuture<List<TaskResult>> results = Futures.allAsList(executor.execute(job));

        final List<Object[]> resultingRows = new ArrayList<>();
        final CountDownLatch latch = new CountDownLatch(1);
        Futures.addCallback(results, new FutureCallback<List<TaskResult>>() {
            @Override
            public void onSuccess(List<TaskResult> resultList) {
                for (Row row : resultList.get(0).rows()) {
                    resultingRows.add(row.materialize());
                }
                latch.countDown();
            }

            @Override
            public void onFailure(Throwable t) {
                latch.countDown();
                fail(t.getMessage());
            }
        });
        latch.await();

        assertThat(resultingRows.size(), is(13));
        assertThat(resultingRows.get(0).length, is(2));
        assertThat((Integer) resultingRows.get(0)[0], is(1));
        assertThat((Integer) resultingRows.get(12)[0], is(6));
    }

    private void rewriteFetchProjectionToBulkSize(int bulkSize, MergeNode mergeNode) {
        List<Projection> newProjections = new ArrayList<>(mergeNode.projections().size());
        for (Projection projection : mergeNode.projections()) {
            if (projection instanceof FetchProjection) {
                FetchProjection fetchProjection = (FetchProjection) projection;
                newProjections.add(new FetchProjection(
                        fetchProjection.executionNodeId(),
                        fetchProjection.docIdSymbol(),
                        fetchProjection.inputSymbols(),
                        fetchProjection.outputSymbols(),
                        fetchProjection.partitionedBy(),
                        fetchProjection.executionNodes(),
                        bulkSize,
                        fetchProjection.closeContexts(),
                        fetchProjection.jobSearchContextIdToNode(),
                        fetchProjection.jobSearchContextIdToShard()));
            } else {
                newProjections.add(projection);
            }
        }
        mergeNode.projections(newProjections);
    }
}
<code block>


package io.crate.operation;

import com.google.common.base.Optional;
import com.google.common.collect.Iterators;
import com.google.common.util.concurrent.Futures;
import com.google.common.util.concurrent.SettableFuture;
import io.crate.breaker.RamAccountingContext;
import io.crate.core.collections.ArrayBucket;
import io.crate.core.collections.Bucket;
import io.crate.core.collections.BucketPage;
import io.crate.executor.transport.TransportActionProvider;
import io.crate.jobs.ExecutionState;
import io.crate.metadata.*;
import io.crate.operation.aggregation.impl.AggregationImplModule;
import io.crate.operation.aggregation.impl.MinimumAggregation;
import io.crate.operation.projectors.FlatProjectorChain;
import io.crate.testing.CollectingProjector;
import io.crate.operation.projectors.TopN;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.GroupProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.TopNProjection;
import io.crate.planner.symbol.Aggregation;
import io.crate.planner.symbol.InputColumn;
import io.crate.planner.symbol.Symbol;
import io.crate.test.integration.CrateUnitTest;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.elasticsearch.action.bulk.BulkRetryCoordinatorPool;
import org.elasticsearch.client.Client;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.common.breaker.CircuitBreaker;
import org.elasticsearch.common.breaker.NoopCircuitBreaker;
import org.elasticsearch.common.collect.Tuple;
import org.elasticsearch.common.inject.AbstractModule;
import org.elasticsearch.common.inject.Injector;
import org.elasticsearch.common.inject.ModulesBuilder;
import org.elasticsearch.common.settings.ImmutableSettings;
import org.elasticsearch.threadpool.ThreadPool;
import org.hamcrest.collection.IsIterableContainingInOrder;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;
import org.mockito.Answers;

import java.util.Arrays;
import java.util.Collections;
import java.util.Iterator;
import java.util.UUID;
import java.util.concurrent.Executor;
import java.util.concurrent.TimeUnit;

import static io.crate.testing.TestingHelpers.isRow;
import static org.hamcrest.Matchers.contains;
import static org.mockito.Mockito.mock;

public class PageDownstreamFactoryTest extends CrateUnitTest {

    private static final RamAccountingContext ramAccountingContext =
            new RamAccountingContext("dummy", new NoopCircuitBreaker(CircuitBreaker.Name.FIELDDATA));

    private GroupProjection groupProjection;
    private Functions functions;
    private ReferenceResolver referenceResolver;
    private ThreadPool threadPool;

    @Before
    @SuppressWarnings("unchecked")
    public void prepare() {
        Injector injector = new ModulesBuilder()
                .add(new AggregationImplModule())
                .add(new AbstractModule() {
                    @Override
                    protected void configure() {
                        bind(Client.class).toInstance(mock(Client.class));
                    }
                })
                .createInjector();
        threadPool = new ThreadPool("testing");
        functions = injector.getInstance(Functions.class);
        referenceResolver = new GlobalReferenceResolver(
                Collections.<ReferenceIdent, ReferenceImplementation>emptyMap());

        FunctionIdent minAggIdent = new FunctionIdent(MinimumAggregation.NAME, Arrays.<DataType>asList(DataTypes.DOUBLE));
        FunctionInfo minAggInfo = new FunctionInfo(minAggIdent, DataTypes.DOUBLE);

        groupProjection = new GroupProjection();
        groupProjection.keys(Arrays.<Symbol>asList(new InputColumn(0, DataTypes.INTEGER)));
        groupProjection.values(Arrays.asList(
                new Aggregation(minAggInfo, Arrays.<Symbol>asList(new InputColumn(1)),
                        Aggregation.Step.PARTIAL, Aggregation.Step.FINAL)
        ));
    }

    @Override
    @After
    public void tearDown() throws Exception {
        super.tearDown();
        threadPool.shutdown();
        threadPool.awaitTermination(1, TimeUnit.SECONDS);
    }

    @Test
    public void testMergeSingleResult() throws Exception {
        TopNProjection topNProjection = new TopNProjection(3, TopN.NO_OFFSET,
                Arrays.<Symbol>asList(new InputColumn(0)), new boolean[]{false}, new Boolean[]{null});
        topNProjection.outputs(Arrays.<Symbol>asList(new InputColumn(0), new InputColumn(1)));
        MergeNode mergeNode = new MergeNode(UUID.randomUUID(), 0, "merge", 2); 
        mergeNode.projections(Arrays.asList(
                groupProjection,
                topNProjection
        ));

        Object[][] objs = new Object[20][];
        for (int i = 0; i < objs.length; i++) {
            objs[i] = new Object[]{i % 4, i + 0.5d};
        }
        Bucket rows = new ArrayBucket(objs);
        BucketPage page = new BucketPage(Futures.immediateFuture(rows));
        final PageDownstreamFactory pageDownstreamFactory = new PageDownstreamFactory(
                mock(ClusterService.class),
                threadPool,
                ImmutableSettings.EMPTY,
                mock(TransportActionProvider.class, Answers.RETURNS_DEEP_STUBS.get()),
                mock(BulkRetryCoordinatorPool.class),
                referenceResolver,
                functions
        );
        CollectingProjector collectingProjector = new CollectingProjector();
        final PageDownstream pageDownstream = getPageDownstream(mergeNode, pageDownstreamFactory, collectingProjector);
        final SettableFuture<?> future = SettableFuture.create();
        pageDownstream.nextPage(page, new PageConsumeListener() {
            @Override
            public void needMore() {
                pageDownstream.finish();
                future.set(null);
            }

            @Override
            public void finish() {
                fail("operation should want more");
            }
        });
        future.get();
        Bucket mergeResult = collectingProjector.result().get();
        assertThat(mergeResult, IsIterableContainingInOrder.contains(
                isRow(0, 0.5d),
                isRow(1, 1.5d),
                isRow(2, 2.5d)
        ));
    }

    private PageDownstream getPageDownstream(MergeNode mergeNode, PageDownstreamFactory pageDownstreamFactory, CollectingProjector collectingProjector) {
        Tuple<PageDownstream, FlatProjectorChain> downstreamFlatProjectorChainTuple =
                pageDownstreamFactory.createMergeNodePageDownstream(
                        mergeNode, collectingProjector, ramAccountingContext, Optional.<Executor>absent());
        downstreamFlatProjectorChainTuple.v2().startProjections(mock(ExecutionState.class));
        return downstreamFlatProjectorChainTuple.v1();
    }

    @Test
    public void testMergeMultipleResults() throws Exception {
        MergeNode mergeNode = new MergeNode(UUID.randomUUID(), 0, "merge", 2); 
        mergeNode.projections(Arrays.<Projection>asList(
                groupProjection
        ));
        final PageDownstreamFactory pageDownstreamFactory = new PageDownstreamFactory(
                mock(ClusterService.class),
                threadPool,
                ImmutableSettings.EMPTY,
                mock(TransportActionProvider.class, Answers.RETURNS_DEEP_STUBS.get()),
                mock(BulkRetryCoordinatorPool.class),
                referenceResolver,
                functions
        );
        CollectingProjector collectingProjector = new CollectingProjector();
        final PageDownstream pageDownstream = getPageDownstream(mergeNode, pageDownstreamFactory, collectingProjector);

        Bucket rows = new ArrayBucket(new Object[][]{{0, 100.0d}});
        BucketPage page1 = new BucketPage(Futures.immediateFuture(rows));
        Bucket otherRows = new ArrayBucket(new Object[][]{{0, 2.5d}});
        BucketPage page2 = new BucketPage(Futures.immediateFuture(otherRows));
        final Iterator<BucketPage> iterator = Iterators.forArray(page1, page2);

        final SettableFuture<?> future = SettableFuture.create();
        pageDownstream.nextPage(iterator.next(), new PageConsumeListener() {
            @Override
            public void needMore() {
                if (iterator.hasNext()) {
                    pageDownstream.nextPage(iterator.next(), this);
                } else {
                    pageDownstream.finish();
                    future.set(null);
                }
            }

            @Override
            public void finish() {
                fail("should still want more");
            }
        });
        future.get();
        Bucket mergeResult = collectingProjector.result().get();
        assertThat(mergeResult, contains(isRow(0, 2.5)));
    }

}

<code block>


package io.crate.operation.collect;

import com.google.common.collect.ImmutableList;
import io.crate.action.sql.SQLBulkRequest;
import io.crate.analyze.OrderBy;
import io.crate.analyze.WhereClause;
import io.crate.breaker.RamAccountingContext;
import io.crate.integrationtests.SQLTransportIntegrationTest;
import io.crate.jobs.JobContextService;
import io.crate.jobs.JobExecutionContext;
import io.crate.metadata.*;
import io.crate.operation.operator.EqOperator;
import io.crate.operation.projectors.ProjectionToProjectorVisitor;
import io.crate.operation.scalar.arithmetic.MultiplyFunction;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.symbol.Function;
import io.crate.planner.symbol.Literal;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.testing.CollectingProjector;
import io.crate.testing.TestingHelpers;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.action.bulk.BulkRequest;
import org.elasticsearch.action.bulk.BulkResponse;
import org.elasticsearch.action.index.IndexRequest;
import org.elasticsearch.common.breaker.CircuitBreaker;
import org.elasticsearch.common.breaker.NoopCircuitBreaker;
import org.elasticsearch.common.xcontent.XContentFactory;
import org.elasticsearch.index.IndexService;
import org.elasticsearch.indices.IndicesService;
import org.elasticsearch.test.ElasticsearchIntegrationTest;
import org.junit.Before;
import org.junit.Test;

import java.io.IOException;
import java.util.Arrays;
import java.util.List;
import java.util.UUID;

import static io.crate.testing.TestingHelpers.createReference;
import static org.hamcrest.Matchers.is;
import static org.hamcrest.Matchers.nullValue;
import static org.mockito.Matchers.any;
import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;

@ElasticsearchIntegrationTest.ClusterScope(scope = ElasticsearchIntegrationTest.Scope.SUITE, numDataNodes = 1)
public class LuceneDocCollectorTest extends SQLTransportIntegrationTest {

    private final static Integer PAGE_SIZE = 20;
    private final static String INDEX_NAME = "countries";
    private final static Integer NUMBER_OF_DOCS = 25;
    private OrderBy orderBy;
    private JobContextService jobContextService;
    private ShardCollectService shardCollectService;

    private CollectingProjector collectingProjector = new CollectingProjector();

    private static final RamAccountingContext RAM_ACCOUNTING_CONTEXT =
            new RamAccountingContext("dummy", new NoopCircuitBreaker(CircuitBreaker.Name.FIELDDATA));
    private JobCollectContext jobCollectContext;

    @Before
    public void prepare() throws Exception{
        execute("create table \""+INDEX_NAME+ "\" (" +
                " continent string, " +
                " countryName string," +
                " population integer" +
                ") clustered into 1 shards with (number_of_replicas=0)");
        refresh();
        generateData();
        IndicesService instanceFromNode = internalCluster().getDataNodeInstance(IndicesService.class);
        IndexService indexService = instanceFromNode.indexServiceSafe(INDEX_NAME);

        shardCollectService = indexService.shardInjectorSafe(0).getInstance(ShardCollectService.class);
        jobContextService = indexService.shardInjectorSafe(0).getInstance(JobContextService.class);

        ReferenceIdent ident = new ReferenceIdent(new TableIdent("doc", "countries"), "countryName");
        Reference ref = new Reference(new ReferenceInfo(ident, RowGranularity.DOC, DataTypes.STRING));
        orderBy = new OrderBy(ImmutableList.of((Symbol)ref), new boolean[]{false}, new Boolean[]{false});
    }

    private byte[] generateRowSource(String continent, String countryName, Integer population) throws IOException {
        return XContentFactory.jsonBuilder()
                .startObject()
                .field("continent", continent)
                .field("countryName", countryName)
                .field("population", population)
                .endObject()
                .bytes().toBytes();
    }

    public void generateData() throws Exception {
        BulkRequest bulkRequest = new BulkRequest();
        for (int i=0; i < NUMBER_OF_DOCS; i++) {
            IndexRequest indexRequest = new IndexRequest(INDEX_NAME, "default", String.valueOf(i));
            if (i == 0) {
                indexRequest.source(generateRowSource("Europe", "Germany", i));
            } else if (i == 1) {
                indexRequest.source(generateRowSource("Europe", "Austria", i));
            } else if (i >= 2 && i <=4) {
                indexRequest.source(generateRowSource("Europe", null, i));
            } else {
                indexRequest.source(generateRowSource("America", "USA", i));
            }
            bulkRequest.add(indexRequest);
        }
        BulkResponse response = client().bulk(bulkRequest).actionGet();
        assertFalse(response.hasFailures());
        refresh();
    }

    private LuceneDocCollector createDocCollector(OrderBy orderBy, Integer limit, List<Symbol> toCollect) throws Exception{
        return createDocCollector(orderBy, limit, toCollect, WhereClause.MATCH_ALL, PAGE_SIZE);
    }

    private LuceneDocCollector createDocCollector(OrderBy orderBy, Integer limit, List<Symbol> toCollect, WhereClause whereClause, int pageSize) throws Exception{
        UUID jobId = UUID.randomUUID();
        CollectNode node = new CollectNode(jobId, 0, "collect");
        node.whereClause(whereClause);
        node.orderBy(orderBy);
        node.limit(limit);
        node.toCollect(toCollect);
        node.maxRowGranularity(RowGranularity.DOC);

        ShardProjectorChain projectorChain = mock(ShardProjectorChain.class);
        when(projectorChain.newShardDownstreamProjector(any(ProjectionToProjectorVisitor.class))).thenReturn(collectingProjector);

        JobExecutionContext.Builder builder = jobContextService.newBuilder(jobId);
        jobCollectContext = new JobCollectContext(
                jobId, node, mock(CollectOperation.class), RAM_ACCOUNTING_CONTEXT, collectingProjector);
        builder.addSubContext(node.executionNodeId(), jobCollectContext);
        jobContextService.createContext(builder);
        LuceneDocCollector collector = (LuceneDocCollector)shardCollectService.getCollector(node, projectorChain, jobCollectContext, 0);
        collector.pageSize(pageSize);
        return collector;
    }

    @Test
    public void testLimitWithoutOrder() throws Exception{
        collectingProjector.rows.clear();
        LuceneDocCollector docCollector = createDocCollector(null, 15, orderBy.orderBySymbols());
        docCollector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(15));
    }

    @Test
    public void testOrderedWithLimit() throws Exception{
        collectingProjector.rows.clear();
        LuceneDocCollector docCollector = createDocCollector(orderBy, 15, orderBy.orderBySymbols());
        docCollector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(15));
        assertThat(((BytesRef)collectingProjector.rows.get(0)[0]).utf8ToString(), is("Austria") );
        assertThat(((BytesRef)collectingProjector.rows.get(1)[0]).utf8ToString(), is("Germany") );
        assertThat(((BytesRef)collectingProjector.rows.get(2)[0]).utf8ToString(), is("USA") );
        assertThat(((BytesRef)collectingProjector.rows.get(3)[0]).utf8ToString(), is("USA") );
    }

    @Test
    public void testOrderedWithLimitHigherThanPageSize() throws Exception{
        collectingProjector.rows.clear();
        LuceneDocCollector docCollector = createDocCollector(orderBy, PAGE_SIZE + 5, orderBy.orderBySymbols());
        docCollector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(PAGE_SIZE + 5));
        assertThat(((BytesRef)collectingProjector.rows.get(0)[0]).utf8ToString(), is("Austria") );
        assertThat(((BytesRef)collectingProjector.rows.get(1)[0]).utf8ToString(), is("Germany") );
        assertThat(((BytesRef)collectingProjector.rows.get(2)[0]).utf8ToString(), is("USA") );
        assertThat(((BytesRef)collectingProjector.rows.get(3)[0]).utf8ToString(), is("USA") );
    }

    @Test
    public void testOrderedWithoutLimit() throws Exception {
        collectingProjector.rows.clear();
        LuceneDocCollector docCollector = createDocCollector(orderBy, null, orderBy.orderBySymbols(), WhereClause.MATCH_ALL, 1);
        docCollector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(NUMBER_OF_DOCS));
        assertThat(((BytesRef)collectingProjector.rows.get(0)[0]).utf8ToString(), is("Austria") );
        assertThat(((BytesRef)collectingProjector.rows.get(1)[0]).utf8ToString(), is("Germany") );
        assertThat(((BytesRef)collectingProjector.rows.get(2)[0]).utf8ToString(), is("USA") );
        assertThat(collectingProjector.rows.get(NUMBER_OF_DOCS -1)[0], is(nullValue()));
    }

    @Test
    public void testOrderedNullsFirstWithoutLimit() throws Exception {
        collectingProjector.rows.clear();
        ReferenceIdent ident = new ReferenceIdent(new TableIdent("doc", "countries"), "countryName");
        Reference ref = new Reference(new ReferenceInfo(ident, RowGranularity.DOC, DataTypes.STRING));
        OrderBy orderBy = new OrderBy(ImmutableList.of((Symbol)ref), new boolean[]{false}, new Boolean[]{true});
        LuceneDocCollector docCollector = createDocCollector(orderBy, null, orderBy.orderBySymbols(), WhereClause.MATCH_ALL, 1);
        docCollector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(NUMBER_OF_DOCS));
        assertThat(collectingProjector.rows.get(0)[0], is(nullValue()));
        assertThat(collectingProjector.rows.get(1)[0], is(nullValue()));
        assertThat(collectingProjector.rows.get(2)[0], is(nullValue()));
        assertThat(((BytesRef)collectingProjector.rows.get(3)[0]).utf8ToString(), is("Austria") );
        assertThat(((BytesRef)collectingProjector.rows.get(4)[0]).utf8ToString(), is("Germany") );
        assertThat(((BytesRef)collectingProjector.rows.get(5)[0]).utf8ToString(), is("USA") );
    }

    @Test
    public void testOrderedDescendingWithoutLimit() throws Exception {
        collectingProjector.rows.clear();
        ReferenceIdent ident = new ReferenceIdent(new TableIdent("doc", "countries"), "countryName");
        Reference ref = new Reference(new ReferenceInfo(ident, RowGranularity.DOC, DataTypes.STRING));
        OrderBy orderBy = new OrderBy(ImmutableList.of((Symbol)ref), new boolean[]{true}, new Boolean[]{false});
        LuceneDocCollector docCollector = createDocCollector(orderBy, null, orderBy.orderBySymbols(), WhereClause.MATCH_ALL, 1);
        docCollector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(NUMBER_OF_DOCS));
        assertThat(collectingProjector.rows.get(NUMBER_OF_DOCS - 1)[0], is(nullValue()));
        assertThat(collectingProjector.rows.get(NUMBER_OF_DOCS - 2)[0], is(nullValue()));
        assertThat(collectingProjector.rows.get(NUMBER_OF_DOCS - 3)[0], is(nullValue()));
        assertThat(((BytesRef)collectingProjector.rows.get(NUMBER_OF_DOCS - 4)[0]).utf8ToString(), is("Austria") );
        assertThat(((BytesRef)collectingProjector.rows.get(NUMBER_OF_DOCS - 5)[0]).utf8ToString(), is("Germany") );
        assertThat(((BytesRef)collectingProjector.rows.get(NUMBER_OF_DOCS - 6)[0]).utf8ToString(), is("USA") );
    }

    @Test
    public void testOrderedDescendingNullsFirstWithoutLimit() throws Exception {
        collectingProjector.rows.clear();
        ReferenceIdent ident = new ReferenceIdent(new TableIdent("doc", "countries"), "countryName");
        Reference ref = new Reference(new ReferenceInfo(ident, RowGranularity.DOC, DataTypes.STRING));
        OrderBy orderBy = new OrderBy(ImmutableList.of((Symbol)ref), new boolean[]{true}, new Boolean[]{true});
        LuceneDocCollector docCollector = createDocCollector(orderBy, null, orderBy.orderBySymbols(), WhereClause.MATCH_ALL, 1);
        docCollector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(NUMBER_OF_DOCS));
        assertThat(collectingProjector.rows.get(0)[0], is(nullValue()));
        assertThat(collectingProjector.rows.get(1)[0], is(nullValue()));
        assertThat(collectingProjector.rows.get(2)[0], is(nullValue()));
        assertThat(((BytesRef)collectingProjector.rows.get(NUMBER_OF_DOCS - 1)[0]).utf8ToString(), is("Austria") );
        assertThat(((BytesRef)collectingProjector.rows.get(NUMBER_OF_DOCS - 2)[0]).utf8ToString(), is("Germany") );
        assertThat(((BytesRef)collectingProjector.rows.get(NUMBER_OF_DOCS - 3)[0]).utf8ToString(), is("USA") );
    }

    @Test
    public void testOrderForNonSelected() throws Exception {
        collectingProjector.rows.clear();
        ReferenceIdent countriesIdent = new ReferenceIdent(new TableIdent("doc", "countries"), "countryName");
        Reference countries = new Reference(new ReferenceInfo(countriesIdent, RowGranularity.DOC, DataTypes.STRING));

        ReferenceIdent populationIdent = new ReferenceIdent(new TableIdent("doc", "countries"), "population");
        Reference population = new Reference(new ReferenceInfo(populationIdent, RowGranularity.DOC, DataTypes.INTEGER));

        OrderBy orderBy = new OrderBy(ImmutableList.of((Symbol)population), new boolean[]{true}, new Boolean[]{true});

        LuceneDocCollector docCollector = createDocCollector(orderBy, null, ImmutableList.of((Symbol)countries));
        docCollector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(NUMBER_OF_DOCS));
        assertThat(collectingProjector.rows.get(0).length, is(1));
        assertThat(((BytesRef)collectingProjector.rows.get(NUMBER_OF_DOCS - 6)[0]).utf8ToString(), is("USA") );
        assertThat(collectingProjector.rows.get(NUMBER_OF_DOCS - 5)[0], is(nullValue()));
        assertThat(collectingProjector.rows.get(NUMBER_OF_DOCS - 4)[0], is(nullValue()));
        assertThat(collectingProjector.rows.get(NUMBER_OF_DOCS - 3)[0], is(nullValue()));
        assertThat(((BytesRef)collectingProjector.rows.get(NUMBER_OF_DOCS - 2)[0]).utf8ToString(), is("Austria") );
        assertThat(((BytesRef)collectingProjector.rows.get(NUMBER_OF_DOCS - 1)[0]).utf8ToString(), is("Germany") );
    }

    @Test
    public void testOrderByScalar() throws Exception {
        collectingProjector.rows.clear();
        Reference population = createReference("population", DataTypes.INTEGER);
        Function scalarFunction = new Function(
                new FunctionInfo(
                        new FunctionIdent(MultiplyFunction.NAME, Arrays.<DataType>asList(DataTypes.INTEGER, DataTypes.INTEGER)),
                        DataTypes.LONG),
                Arrays.asList(population, Literal.newLiteral(-1))
        );

        OrderBy orderBy = new OrderBy(ImmutableList.of((Symbol)scalarFunction), new boolean[]{false}, new Boolean[]{false});
        LuceneDocCollector docCollector = createDocCollector(orderBy, null, ImmutableList.of((Symbol)population));
        docCollector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(NUMBER_OF_DOCS));
        assertThat(((Integer)collectingProjector.rows.get(NUMBER_OF_DOCS - 2)[0]), is(1) );
        assertThat(((Integer)collectingProjector.rows.get(NUMBER_OF_DOCS - 1)[0]), is(0) );
    }

    @Test
    public void testMultiOrdering() throws Exception {
        execute("create table test (x integer, y integer) clustered into 1 shards with (number_of_replicas=0)");
        waitNoPendingTasksOnAll();
        SQLBulkRequest request = new SQLBulkRequest("insert into test values (?, ?)",
                new Object[][]{
                    new Object[]{2, 3},
                    new Object[]{2, 1},
                    new Object[]{2, null},
                    new Object[]{1, null},
                    new Object[]{1, 2},
                    new Object[]{1, 1},
                    new Object[]{1, 0},
                    new Object[]{1, null}
                }
        );
        sqlExecutor.exec(request);
        execute("refresh table test");
        collectingProjector.rows.clear();

        IndicesService instanceFromNode = internalCluster().getDataNodeInstance(IndicesService.class);
        IndexService indexService = instanceFromNode.indexServiceSafe("test");

        ShardCollectService shardCollectService = indexService.shardInjectorSafe(0).getInstance(ShardCollectService.class);
        JobContextService jobContextService = indexService.shardInjectorSafe(0).getInstance(JobContextService.class);

        ReferenceIdent xIdent = new ReferenceIdent(new TableIdent("doc", "test"), "x");
        Reference x = new Reference(new ReferenceInfo(xIdent, RowGranularity.DOC, DataTypes.INTEGER));

        ReferenceIdent yIdent = new ReferenceIdent(new TableIdent("doc", "test"), "y");
        Reference y = new Reference(new ReferenceInfo(yIdent, RowGranularity.DOC, DataTypes.INTEGER));

        OrderBy orderBy = new OrderBy(ImmutableList.<Symbol>of(x, y), new boolean[]{false, false}, new Boolean[]{false, false});

        CollectNode node = new CollectNode(UUID.randomUUID(), 0, "collect");
        node.whereClause(WhereClause.MATCH_ALL);
        node.orderBy(orderBy);
        node.toCollect(orderBy.orderBySymbols());
        node.maxRowGranularity(RowGranularity.DOC);

        JobExecutionContext.Builder builder = jobContextService.newBuilder(node.jobId());
        builder.addSubContext(node.executionNodeId(),
                new JobCollectContext(node.jobId(), node, mock(CollectOperation.class), RAM_ACCOUNTING_CONTEXT, collectingProjector));
        jobContextService.createContext(builder);

        ShardProjectorChain projectorChain = mock(ShardProjectorChain.class);
        when(projectorChain.newShardDownstreamProjector(any(ProjectionToProjectorVisitor.class))).thenReturn(collectingProjector);

        JobCollectContext jobCollectContext = jobContextService.getContext(node.jobId()).getSubContext(node.executionNodeId());
        LuceneDocCollector collector = (LuceneDocCollector)shardCollectService.getCollector(node, projectorChain, jobCollectContext, 0);
        collector.pageSize(1);
        collector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(8));

        String expected = "1| 0\n" +
                "1| 1\n" +
                "1| 2\n" +
                "1| NULL\n" +
                "1| NULL\n" +
                "2| 1\n" +
                "2| 3\n" +
                "2| NULL\n";
        assertEquals(expected, TestingHelpers.printedTable(collectingProjector.doFinish()));


        builder = jobContextService.newBuilder(node.jobId());
        builder.addSubContext(node.executionNodeId(),
                new JobCollectContext(node.jobId(), node, mock(CollectOperation.class), RAM_ACCOUNTING_CONTEXT, collectingProjector));
        jobContextService.createContext(builder);
        jobCollectContext = jobContextService.getContext(node.jobId()).getSubContext(node.executionNodeId());

        collectingProjector.rows.clear();
        orderBy = new OrderBy(ImmutableList.<Symbol>of(x, y), new boolean[]{false, false}, new Boolean[]{false, true});
        node.orderBy(orderBy);
        collector = (LuceneDocCollector)shardCollectService.getCollector(node, projectorChain, jobCollectContext, 0);
        collector.pageSize(1);
        collector.doCollect(jobCollectContext);

        expected = "1| NULL\n" +
                   "1| NULL\n" +
                   "1| 0\n" +
                   "1| 1\n" +
                   "1| 2\n" +
                   "2| NULL\n" +
                   "2| 1\n" +
                   "2| 3\n";
        assertEquals(expected, TestingHelpers.printedTable(collectingProjector.doFinish()));
    }

    @Test
    public void testMinScoreQuery() throws Exception {
        collectingProjector.rows.clear();

        Reference minScore_ref = new Reference(
                new ReferenceInfo(new ReferenceIdent(null, "_score"), RowGranularity.DOC, DataTypes.DOUBLE));

        Function function = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.DOUBLE, DataTypes.DOUBLE)),
                DataTypes.BOOLEAN),
                Arrays.asList(minScore_ref, Literal.newLiteral(1.1))
        );
        WhereClause whereClause = new WhereClause(function);
        LuceneDocCollector docCollector = createDocCollector(null, null, orderBy.orderBySymbols(), whereClause, PAGE_SIZE);
        docCollector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(0));


        collectingProjector.rows.clear();
        function = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.DOUBLE, DataTypes.DOUBLE)),
                DataTypes.BOOLEAN),
                Arrays.asList(minScore_ref, Literal.newLiteral(1.0))
        );
        whereClause = new WhereClause(function);
        docCollector = createDocCollector(null, null, orderBy.orderBySymbols(), whereClause, PAGE_SIZE);
        docCollector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(NUMBER_OF_DOCS));
    }
}

<code block>


package io.crate.operation.collect;

import com.google.common.collect.ImmutableList;
import io.crate.analyze.WhereClause;
import io.crate.blob.BlobEnvironment;
import io.crate.blob.v2.BlobIndices;
import io.crate.breaker.CircuitBreakerModule;
import io.crate.breaker.RamAccountingContext;
import io.crate.core.collections.Bucket;
import io.crate.core.collections.TreeMapBuilder;
import io.crate.exceptions.UnhandledServerException;
import io.crate.executor.transport.TransportActionProvider;
import io.crate.jobs.JobContextService;
import io.crate.jobs.JobExecutionContext;
import io.crate.metadata.*;
import io.crate.metadata.shard.ShardReferenceImplementation;
import io.crate.metadata.shard.ShardReferenceResolver;
import io.crate.metadata.shard.blob.BlobShardReferenceImplementation;
import io.crate.metadata.sys.SysNodesTableInfo;
import io.crate.metadata.sys.SysShardsTableInfo;
import io.crate.operation.Input;
import io.crate.operation.operator.AndOperator;
import io.crate.operation.operator.EqOperator;
import io.crate.operation.operator.OperatorModule;
import io.crate.operation.reference.sys.node.NodeSysExpression;
import io.crate.operation.reference.sys.node.SysNodeExpressionModule;
import io.crate.testing.CollectingProjector;
import io.crate.operation.projectors.ResultProvider;
import io.crate.operation.projectors.ResultProviderFactory;
import io.crate.operation.reference.sys.shard.SysShardExpression;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.symbol.Function;
import io.crate.planner.symbol.Literal;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.test.integration.CrateUnitTest;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.action.admin.indices.create.TransportBulkCreateIndicesAction;
import org.elasticsearch.action.admin.indices.create.TransportCreateIndexAction;
import org.elasticsearch.action.admin.indices.delete.TransportDeleteIndexAction;
import org.elasticsearch.action.admin.indices.settings.put.TransportUpdateSettingsAction;
import org.elasticsearch.action.admin.indices.template.put.TransportPutIndexTemplateAction;
import org.elasticsearch.action.bulk.BulkRetryCoordinator;
import org.elasticsearch.action.bulk.BulkRetryCoordinatorPool;
import org.elasticsearch.action.bulk.TransportShardBulkAction;
import org.elasticsearch.action.support.ActionFilters;
import org.elasticsearch.client.Client;
import org.elasticsearch.cluster.ClusterInfoService;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.cluster.ClusterState;
import org.elasticsearch.cluster.metadata.MetaDataCreateIndexService;
import org.elasticsearch.cluster.metadata.MetaDataDeleteIndexService;
import org.elasticsearch.cluster.metadata.MetaDataUpdateSettingsService;
import org.elasticsearch.cluster.node.DiscoveryNode;
import org.elasticsearch.cluster.node.DiscoveryNodes;
import org.elasticsearch.cluster.routing.allocation.AllocationService;
import org.elasticsearch.cluster.routing.allocation.decider.AllocationDecider;
import org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider;
import org.elasticsearch.cluster.settings.ClusterDynamicSettings;
import org.elasticsearch.cluster.settings.DynamicSettings;
import org.elasticsearch.common.breaker.CircuitBreaker;
import org.elasticsearch.common.breaker.NoopCircuitBreaker;
import org.elasticsearch.common.inject.*;
import org.elasticsearch.common.inject.multibindings.MapBinder;
import org.elasticsearch.common.settings.ImmutableSettings;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.discovery.Discovery;
import org.elasticsearch.discovery.DiscoveryService;
import org.elasticsearch.index.Index;
import org.elasticsearch.index.IndexService;
import org.elasticsearch.index.mapper.MapperService;
import org.elasticsearch.index.settings.IndexSettings;
import org.elasticsearch.index.shard.IndexShard;
import org.elasticsearch.index.shard.ShardId;
import org.elasticsearch.indices.IndicesLifecycle;
import org.elasticsearch.indices.IndicesService;
import org.elasticsearch.indices.breaker.CircuitBreakerService;
import org.elasticsearch.indices.breaker.NoneCircuitBreakerService;
import org.elasticsearch.monitor.network.NetworkService;
import org.elasticsearch.monitor.os.OsService;
import org.elasticsearch.monitor.os.OsStats;
import org.elasticsearch.node.service.NodeService;
import org.elasticsearch.node.settings.NodeSettingsService;
import org.elasticsearch.script.ScriptService;
import org.elasticsearch.search.InternalSearchService;
import org.elasticsearch.search.SearchService;
import org.elasticsearch.threadpool.ThreadPool;
import org.elasticsearch.transport.TransportService;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;
import org.mockito.Answers;

import java.util.*;
import java.util.concurrent.ExecutionException;

import static io.crate.testing.TestingHelpers.isRow;
import static org.hamcrest.Matchers.*;
import static org.mockito.Matchers.any;
import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;

public class LocalDataCollectTest extends CrateUnitTest {

    static class TestFunction extends Scalar<Integer,Object> {
        public static final FunctionIdent ident = new FunctionIdent("twoTimes", Arrays.<DataType>asList(DataTypes.INTEGER));
        public static final FunctionInfo info = new FunctionInfo(ident, DataTypes.INTEGER);

        @Override
        public Integer evaluate(Input<Object>... args) {
            if (args.length == 0) {
                return 0;
            }
            Short value = (Short) args[0].value();
            return value * 2;
        }

        @Override
        public FunctionInfo info() {
            return info;
        }

        @Override
        public Symbol normalizeSymbol(Function symbol) {
            return symbol;
        }
    }

    static class ShardIdExpression extends SysShardExpression<Integer> implements ShardReferenceImplementation<Integer> {

        private final ShardId shardId;

        @Inject
        public ShardIdExpression(ShardId shardId) {
            this.shardId = shardId;
        }

        @Override
        public Integer value() {
            return shardId.id();
        }

        @Override
        public ReferenceImplementation getChildImplementation(String name) {
            return null;
        }
    }

    private DiscoveryService discoveryService;
    private Functions functions;
    private IndexService indexService = mock(IndexService.class);
    private MapSideDataCollectOperation operation;
    private Routing testRouting = new Routing(TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder()
        .put(TEST_NODE_ID, new TreeMap<String, List<Integer>>()).map()
    );

    private JobContextService jobContextService;

    private final ThreadPool testThreadPool = new ThreadPool(getClass().getSimpleName());
    private final static String TEST_NODE_ID = "test_node";
    private final static String TEST_TABLE_NAME = "test_table";

    private static Reference testNodeReference = new Reference(
            SysNodesTableInfo.INFOS.get(new ColumnIdent("os", ImmutableList.of("cpu", "stolen")))
    );
    private static Reference testShardIdReference = new Reference(SysShardsTableInfo.INFOS.get(new ColumnIdent("id")));

    private static final RamAccountingContext RAM_ACCOUNTING_CONTEXT =
            new RamAccountingContext("dummy", new NoopCircuitBreaker(CircuitBreaker.Name.FIELDDATA));

    class TestModule extends AbstractModule {
        protected MapBinder<FunctionIdent, FunctionImplementation> functionBinder;

        @Override
        protected void configure() {
            functionBinder = MapBinder.newMapBinder(binder(), FunctionIdent.class, FunctionImplementation.class);
            functionBinder.addBinding(TestFunction.ident).toInstance(new TestFunction());
            bind(Functions.class).asEagerSingleton();
            bind(ReferenceInfos.class).toInstance(mock(ReferenceInfos.class));
            bind(ThreadPool.class).toInstance(testThreadPool);

            BulkRetryCoordinator bulkRetryCoordinator = mock(BulkRetryCoordinator.class);
            BulkRetryCoordinatorPool bulkRetryCoordinatorPool = mock(BulkRetryCoordinatorPool.class);
            when(bulkRetryCoordinatorPool.coordinator(any(ShardId.class))).thenReturn(bulkRetryCoordinator);
            bind(BulkRetryCoordinatorPool.class).toInstance(bulkRetryCoordinatorPool);

            bind(TransportBulkCreateIndicesAction.class).toInstance(mock(TransportBulkCreateIndicesAction.class));
            bind(CircuitBreakerService.class).toInstance(new NoneCircuitBreakerService());
            bind(ActionFilters.class).toInstance(mock(ActionFilters.class));
            bind(ScriptService.class).toInstance(mock(ScriptService.class));
            bind(SearchService.class).toInstance(mock(InternalSearchService.class));
            bind(AllocationService.class).toInstance(mock(AllocationService.class));
            bind(MetaDataCreateIndexService.class).toInstance(mock(MetaDataCreateIndexService.class));
            bind(DynamicSettings.class).annotatedWith(ClusterDynamicSettings.class).toInstance(mock(DynamicSettings.class));
            bind(MetaDataDeleteIndexService.class).toInstance(mock(MetaDataDeleteIndexService.class));
            bind(ClusterInfoService.class).toInstance(mock(ClusterInfoService.class));
            bind(TransportService.class).toInstance(mock(TransportService.class));
            bind(MapperService.class).toInstance(mock(MapperService.class));

            OsService osService = mock(OsService.class);
            OsStats osStats = mock(OsStats.class);
            when(osService.stats()).thenReturn(osStats);
            OsStats.Cpu osCpu = mock(OsStats.Cpu.class);
            when(osCpu.stolen()).thenReturn((short) 1);
            when(osStats.cpu()).thenReturn(osCpu);

            bind(OsService.class).toInstance(osService);
            bind(NodeService.class).toInstance(mock(NodeService.class));
            bind(Discovery.class).toInstance(mock(Discovery.class));
            bind(NetworkService.class).toInstance(mock(NetworkService.class));

            bind(TransportShardBulkAction.class).toInstance(mock(TransportShardBulkAction.class));
            bind(TransportCreateIndexAction.class).toInstance(mock(TransportCreateIndexAction.class));

            discoveryService = mock(DiscoveryService.class);
            DiscoveryNode discoveryNode = mock(DiscoveryNode.class);
            when(discoveryNode.id()).thenReturn(TEST_NODE_ID);
            when(discoveryService.localNode()).thenReturn(discoveryNode);

            ClusterService clusterService = mock(ClusterService.class);
            ClusterState state = mock(ClusterState.class);
            DiscoveryNodes discoveryNodes = mock(DiscoveryNodes.class);
            when(discoveryNodes.localNodeId()).thenReturn(TEST_NODE_ID);
            when(state.nodes()).thenReturn(discoveryNodes);
            when(clusterService.state()).thenReturn(state);
            when(clusterService.localNode()).thenReturn(discoveryNode);
            bind(ClusterService.class).toInstance(clusterService);

            IndicesService indicesService = mock(IndicesService.class);
            bind(IndicesService.class).toInstance(indicesService);
            bind(Settings.class).toInstance(ImmutableSettings.EMPTY);

            bind(MetaDataUpdateSettingsService.class).toInstance(mock(MetaDataUpdateSettingsService.class));
            bind(Client.class).toInstance(mock(Client.class));

            Provider<TransportCreateIndexAction> transportCreateIndexActionProvider = mock(Provider.class);
            when(transportCreateIndexActionProvider.get()).thenReturn(mock(TransportCreateIndexAction.class));
            Provider<TransportDeleteIndexAction> transportDeleteActionProvider = mock(Provider.class);
            when(transportDeleteActionProvider.get()).thenReturn(mock(TransportDeleteIndexAction.class));
            Provider<TransportUpdateSettingsAction> transportUpdateSettingsActionProvider = mock(Provider.class);
            when(transportUpdateSettingsActionProvider.get()).thenReturn(mock(TransportUpdateSettingsAction.class));

            BlobIndices blobIndices = new BlobIndices(
                    ImmutableSettings.EMPTY,
                    transportCreateIndexActionProvider,
                    transportDeleteActionProvider,
                    transportUpdateSettingsActionProvider,
                    indicesService,
                    mock(IndicesLifecycle.class),
                    mock(BlobEnvironment.class),
                    clusterService
            );
            bind(BlobIndices.class).toInstance(blobIndices);

            bind(ReferenceResolver.class).to(GlobalReferenceResolver.class);

            TransportPutIndexTemplateAction transportPutIndexTemplateAction = mock(TransportPutIndexTemplateAction.class);
            bind(TransportPutIndexTemplateAction.class).toInstance(transportPutIndexTemplateAction);

            bind(IndexService.class).toInstance(indexService);
        }
    }

    class TestShardModule extends AbstractModule {

        private final ShardId shardId;
        private final ShardIdExpression shardIdExpression;

        public TestShardModule(int shardId) {
            super();
            this.shardId = new ShardId(TEST_TABLE_NAME, shardId);
            this.shardIdExpression = new ShardIdExpression(this.shardId);
        }

        @Override
        protected void configure() {
            IndexShard shard = mock(IndexShard.class);
            bind(IndexShard.class).toInstance(shard);
            when(shard.shardId()).thenReturn(shardId);
            Index index = new Index(TEST_TABLE_NAME);
            bind(Index.class).toInstance(index);
            bind(ShardId.class).toInstance(shardId);
            MapBinder<ReferenceIdent, ShardReferenceImplementation> binder = MapBinder
                    .newMapBinder(binder(), ReferenceIdent.class, ShardReferenceImplementation.class);
            binder.addBinding(SysShardsTableInfo.INFOS.get(new ColumnIdent("id")).ident()).toInstance(shardIdExpression);
            bind(ShardReferenceResolver.class).asEagerSingleton();
            bind(AllocationDecider.class).to(DiskThresholdDecider.class);
            bind(ShardCollectService.class).asEagerSingleton();

            bind(DiscoveryService.class).toInstance(discoveryService);



            MapBinder<ReferenceIdent, BlobShardReferenceImplementation> blobBinder = MapBinder
                    .newMapBinder(binder(), ReferenceIdent.class, BlobShardReferenceImplementation.class);
            bind(Settings.class).annotatedWith(IndexSettings.class).toInstance(ImmutableSettings.EMPTY);
        }
    }

    @Before
    public void configure() {
        Injector injector = new ModulesBuilder().add(
                new CircuitBreakerModule(),
                new OperatorModule(),
                new TestModule(),
                new SysNodeExpressionModule()
        ).createInjector();
        Injector shard0Injector = injector.createChildInjector(
                new TestShardModule(0)
        );
        Injector shard1Injector = injector.createChildInjector(
                new TestShardModule(1)
        );
        functions = injector.getInstance(Functions.class);

        IndicesService indicesService = injector.getInstance(IndicesService.class);
        indexService = injector.getInstance(IndexService.class);

        when(indexService.shardInjectorSafe(0)).thenReturn(shard0Injector);
        when(indexService.shardInjectorSafe(1)).thenReturn(shard1Injector);
        when(indexService.shardSafe(0)).thenReturn(shard0Injector.getInstance(IndexShard.class));
        when(indexService.shardSafe(1)).thenReturn(shard1Injector.getInstance(IndexShard.class));
        when(indicesService.indexServiceSafe(TEST_TABLE_NAME)).thenReturn(indexService);

        NodeSettingsService nodeSettingsService = mock(NodeSettingsService.class);
        jobContextService = new JobContextService(ImmutableSettings.EMPTY, testThreadPool, mock(StatsTables.class));

        ClusterService clusterService = injector.getInstance(ClusterService.class);
        operation = new MapSideDataCollectOperation(
                clusterService,
                ImmutableSettings.EMPTY,
                mock(TransportActionProvider.class, Answers.RETURNS_DEEP_STUBS.get()),
                injector.getInstance(BulkRetryCoordinatorPool.class),
                functions,
                injector.getInstance(ReferenceResolver.class),
                injector.getInstance(NodeSysExpression.class),
                indicesService,
                testThreadPool,
                new CollectServiceResolver(discoveryService,
                        new SystemCollectService(
                                discoveryService,
                                functions,
                                new StatsTables(ImmutableSettings.EMPTY, nodeSettingsService))
                ),
                new ResultProviderFactory() {
                    @Override
                    public ResultProvider createDownstream(ExecutionNode node, UUID jobId) {
                        return new CollectingProjector();
                    }
                },
                mock(InformationSchemaCollectService.class),
                mock(UnassignedShardsCollectService.class)
        );
    }

    @After
    public void cleanUp() throws Exception {
        testThreadPool.shutdownNow();
    }

    private Routing shardRouting(final Integer... shardIds) {
        return new Routing(TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder()
            .put(TEST_NODE_ID, TreeMapBuilder.<String, List<Integer>>newMapBuilder()
                            .put(TEST_TABLE_NAME, Arrays.asList(shardIds))
                            .map()
            )
            .map()
        );
    }

    @Test
    public void testCollectExpressions() throws Exception {
        CollectNode collectNode = new CollectNode(UUID.randomUUID(), 0, "collect", testRouting);
        collectNode.toCollect(Arrays.<Symbol>asList(testNodeReference));
        collectNode.maxRowGranularity(RowGranularity.NODE);

        Bucket result = getBucket(collectNode);

        assertThat(result.size(), equalTo(1));
        assertThat(result, contains(isRow((short) 1)));
    }

    @Test
    public void testWrongRouting() throws Exception {

        expectedException.expect(UnhandledServerException.class);
        expectedException.expectMessage("unsupported routing");

        CollectNode collectNode = new CollectNode(UUID.randomUUID(), 0, "wrong", new Routing(TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder()
            .put("bla", TreeMapBuilder.<String, List<Integer>>newMapBuilder()
                .put("my_index", Arrays.asList(1))
                .put("my_index", Arrays.asList(1))
                .map()
            ).map()
        ));
        collectNode.maxRowGranularity(RowGranularity.DOC);
        operation.collect(collectNode, new CollectingProjector(), null);
    }

    @Test
    public void testCollectUnknownReference() throws Throwable {
        expectedException.expect(UnhandledServerException.class);
        expectedException.expectMessage("Unknown Reference some.table.some_column");

        CollectNode collectNode = new CollectNode(UUID.randomUUID(), 0, "unknown", testRouting);
        Reference unknownReference = new Reference(
                new ReferenceInfo(
                        new ReferenceIdent(
                                new TableIdent("some", "table"),
                                "some_column"
                        ),
                        RowGranularity.NODE,
                        DataTypes.BOOLEAN
                )
        );
        collectNode.toCollect(Arrays.<Symbol>asList(unknownReference));
        collectNode.maxRowGranularity(RowGranularity.NODE);
        try {
            getBucket(collectNode);
        } catch (ExecutionException e) {
            throw e.getCause();
        }
    }

    @Test
    public void testCollectFunction() throws Exception {
        CollectNode collectNode = new CollectNode(UUID.randomUUID(), 0, "function", testRouting);
        Function twoTimesTruthFunction = new Function(
                TestFunction.info,
                Arrays.<Symbol>asList(testNodeReference)
        );
        collectNode.toCollect(Arrays.<Symbol>asList(twoTimesTruthFunction, testNodeReference));
        collectNode.maxRowGranularity(RowGranularity.NODE);
        Bucket result = getBucket(collectNode);
        assertThat(result.size(), equalTo(1));
        assertThat(result, contains(isRow(2, (short) 1)));
    }


    @Test
    public void testUnknownFunction() throws Throwable {

        expectedException.expect(IllegalArgumentException.class);
        expectedException.expectMessage("Cannot find implementation for function unknown()");

        CollectNode collectNode = new CollectNode(UUID.randomUUID(), 0, "unknownFunction", testRouting);
        Function unknownFunction = new Function(
                new FunctionInfo(
                        new FunctionIdent("unknown", ImmutableList.<DataType>of()),
                        DataTypes.BOOLEAN
                ),
                ImmutableList.<Symbol>of()
        );
        collectNode.toCollect(Arrays.<Symbol>asList(unknownFunction));
        try {
            getBucket(collectNode);
        } catch (ExecutionException e) {
            throw e.getCause();
        }
    }

    @Test
    public void testCollectLiterals() throws Exception {
        CollectNode collectNode = new CollectNode(UUID.randomUUID(), 0, "literals", testRouting);
        collectNode.toCollect(Arrays.<Symbol>asList(
                Literal.newLiteral("foobar"),
                Literal.newLiteral(true),
                Literal.newLiteral(1),
                Literal.newLiteral(4.2)
        ));
        Bucket result = getBucket(collectNode);
        assertThat(result, contains(isRow(new BytesRef("foobar"), true, 1, 4.2)));
    }

    @Test
    public void testCollectWithFalseWhereClause() throws Exception {
        CollectNode collectNode = new CollectNode(UUID.randomUUID(), 0, "whereClause", testRouting);
        collectNode.toCollect(Arrays.<Symbol>asList(testNodeReference));
        collectNode.whereClause(new WhereClause(new Function(
                AndOperator.INFO,
                Arrays.<Symbol>asList(Literal.newLiteral(false), Literal.newLiteral(false))
        )));
        Bucket result = getBucket(collectNode);
        assertThat(result.size(), is(0));
    }

    @Test
    public void testCollectWithTrueWhereClause() throws Exception {
        CollectNode collectNode = new CollectNode(UUID.randomUUID(), 0, "whereClause", testRouting);
        collectNode.toCollect(Arrays.<Symbol>asList(testNodeReference));
        collectNode.whereClause(new WhereClause(new Function(
                AndOperator.INFO,
                Arrays.<Symbol>asList(Literal.newLiteral(true), Literal.newLiteral(true))
        )));
        collectNode.maxRowGranularity(RowGranularity.NODE);
        Bucket result = getBucket(collectNode);
        assertThat(result, contains(isRow((short) 1)));

    }

    @Test
    public void testCollectWithNullWhereClause() throws Exception {
        EqOperator op = (EqOperator) functions.get(new FunctionIdent(
                EqOperator.NAME, ImmutableList.<DataType>of(DataTypes.INTEGER, DataTypes.INTEGER)));
        CollectNode collectNode = new CollectNode(UUID.randomUUID(), 0, "whereClause", testRouting);
        collectNode.toCollect(Arrays.<Symbol>asList(testNodeReference));
        collectNode.whereClause(new WhereClause(new Function(
                op.info(),
                Arrays.<Symbol>asList(Literal.NULL, Literal.NULL)
        )));
        Bucket result = getBucket(collectNode);
        assertThat(result.size(), is(0));
    }

    private Bucket getBucket(CollectNode collectNode) throws InterruptedException, ExecutionException {
        CollectingProjector cd = new CollectingProjector();
        JobExecutionContext.Builder builder = jobContextService.newBuilder(collectNode.jobId());
        JobCollectContext jobCollectContext =
                new JobCollectContext(collectNode.jobId(), collectNode, operation, RAM_ACCOUNTING_CONTEXT, cd);
        builder.addSubContext(collectNode.executionNodeId(), jobCollectContext);
        JobExecutionContext context = jobContextService.createContext(builder);
        cd.startProjection(jobCollectContext);
        operation.collect(collectNode, cd, jobCollectContext);
        return cd.result().get();
    }

    @Test
    public void testCollectShardExpressions() throws Exception {
        CollectNode collectNode = new CollectNode(UUID.randomUUID(), 0, "shardCollect", shardRouting(0, 1));
        collectNode.toCollect(Arrays.<Symbol>asList(testShardIdReference));
        collectNode.maxRowGranularity(RowGranularity.SHARD);

        Bucket result = getBucket(collectNode);
        assertThat(result.size(), is(2));
        assertThat(result, containsInAnyOrder(isRow(0), isRow(1)));
    }

    @Test
    public void testCollectShardExpressionsWhereShardIdIs0() throws Exception {
        EqOperator op = (EqOperator) functions.get(new FunctionIdent(
                EqOperator.NAME, ImmutableList.<DataType>of(DataTypes.INTEGER, DataTypes.INTEGER)));

        CollectNode collectNode = new CollectNode(UUID.randomUUID(), 0, "shardCollect", shardRouting(0, 1));
        collectNode.toCollect(Arrays.<Symbol>asList(testShardIdReference));
        collectNode.whereClause(new WhereClause(
                new Function(op.info(), Arrays.asList(testShardIdReference, Literal.newLiteral(0)))));
        collectNode.maxRowGranularity(RowGranularity.SHARD);
        Bucket result = getBucket(collectNode);
        assertThat(result, contains(isRow(0)));
    }

    @Test
    public void testCollectShardExpressionsLiteralsAndNodeExpressions() throws Exception {
        CollectNode collectNode = new CollectNode(UUID.randomUUID(), 0, "shardCollect", shardRouting(0, 1));
        collectNode.toCollect(Arrays.asList(testShardIdReference, Literal.newLiteral(true), testNodeReference));
        collectNode.maxRowGranularity(RowGranularity.SHARD);
        Bucket result = getBucket(collectNode);
        assertThat(result.size(), is(2));
        assertThat(result, containsInAnyOrder(isRow(0, true, (short) 1), isRow(1, true, (short) 1)));
    }
}

<code block>


package io.crate.operation.collect;

import com.google.common.collect.ImmutableList;
import com.google.common.util.concurrent.ListenableFuture;
import io.crate.action.job.ContextPreparer;
import io.crate.analyze.WhereClause;
import io.crate.core.collections.Bucket;
import io.crate.core.collections.Row;
import io.crate.integrationtests.SQLTransportIntegrationTest;
import io.crate.jobs.JobContextService;
import io.crate.jobs.JobExecutionContext;
import io.crate.metadata.*;
import io.crate.metadata.doc.DocSchemaInfo;
import io.crate.operation.operator.EqOperator;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.symbol.Function;
import io.crate.planner.symbol.Literal;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.elasticsearch.cluster.routing.ShardRouting;
import org.elasticsearch.test.ElasticsearchIntegrationTest;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;

import java.util.*;
import java.util.concurrent.TimeUnit;

import static io.crate.testing.TestingHelpers.isRow;
import static org.hamcrest.Matchers.contains;
import static org.hamcrest.Matchers.containsInAnyOrder;


@ElasticsearchIntegrationTest.ClusterScope(randomDynamicTemplates = false, numDataNodes = 1)
public class DocLevelCollectTest extends SQLTransportIntegrationTest {

    private static final String TEST_TABLE_NAME = "test_table";
    private static final Reference testDocLevelReference = new Reference(
            new ReferenceInfo(
                    new ReferenceIdent(new TableIdent(null, TEST_TABLE_NAME), "doc"),
                    RowGranularity.DOC,
                    DataTypes.INTEGER
            )
    );
    private static final Reference underscoreIdReference = new Reference(
            new ReferenceInfo(
                    new ReferenceIdent(new TableIdent(null, TEST_TABLE_NAME), "_id"),
                    RowGranularity.DOC,
                    DataTypes.STRING
            )
    );
    private static final Reference underscoreRawReference = new Reference(
            new ReferenceInfo(
                    new ReferenceIdent(new TableIdent(null, TEST_TABLE_NAME), "_raw"),
                    RowGranularity.DOC,
                    DataTypes.STRING
            )
    );

    private static final String PARTITIONED_TABLE_NAME = "parted_table";

    private MapSideDataCollectOperation operation;
    private Functions functions;
    private DocSchemaInfo docSchemaInfo;

    @Before
    public void prepare() {
        operation = internalCluster().getDataNodeInstance(MapSideDataCollectOperation.class);
        functions = internalCluster().getDataNodeInstance(Functions.class);
        docSchemaInfo = internalCluster().getDataNodeInstance(DocSchemaInfo.class);

        execute(String.format(Locale.ENGLISH, "create table %s (" +
                "  id integer," +
                "  name string," +
                "  date timestamp" +
                ") clustered into 2 shards partitioned by (date) with(number_of_replicas=0)", PARTITIONED_TABLE_NAME));
        ensureGreen();
        execute(String.format("insert into %s (id, name, date) values (?, ?, ?)",
                PARTITIONED_TABLE_NAME),
                new Object[]{1, "Ford", 0L});
        execute(String.format("insert into %s (id, name, date) values (?, ?, ?)",
                PARTITIONED_TABLE_NAME),
                new Object[]{2, "Trillian", 1L});
        ensureGreen();
        refresh();

        execute(String.format(Locale.ENGLISH, "create table %s (" +
                " id integer primary key," +
                " doc integer" +
                ") clustered into 2 shards with(number_of_replicas=0)", TEST_TABLE_NAME));
        ensureGreen();
        execute(String.format("insert into %s (id, doc) values (?, ?)", TEST_TABLE_NAME), new Object[]{1, 2});
        execute(String.format("insert into %s (id, doc) values (?, ?)", TEST_TABLE_NAME), new Object[]{3, 4});
        refresh();
    }

    @After
    public void cleanUp() {
        operation = null;
        functions = null;
        docSchemaInfo = null;
    }

    private Routing routing(String table) {
        Map<String, Map<String, List<Integer>>> locations = new TreeMap<>();

        for (final ShardRouting shardRouting : clusterService().state().routingTable().allShards(table)) {
            Map<String, List<Integer>> shardIds = locations.get(shardRouting.currentNodeId());
            if (shardIds == null) {
                shardIds = new TreeMap<>();
                locations.put(shardRouting.currentNodeId(), shardIds);
            }

            List<Integer> shardIdSet = shardIds.get(shardRouting.index());
            if (shardIdSet == null) {
                shardIdSet = new ArrayList<>();
                shardIds.put(shardRouting.index(), shardIdSet);
            }
            shardIdSet.add(shardRouting.id());
        }
        return new Routing(locations);
    }

    @Test
    public void testCollectDocLevel() throws Exception {
        CollectNode collectNode = new CollectNode(UUID.randomUUID(), 0, "docCollect", routing(TEST_TABLE_NAME));
        collectNode.toCollect(Arrays.<Symbol>asList(testDocLevelReference, underscoreRawReference, underscoreIdReference));
        collectNode.maxRowGranularity(RowGranularity.DOC);
        PlanNodeBuilder.setOutputTypes(collectNode);
        Bucket result = collect(collectNode);
        assertThat(result, containsInAnyOrder(
                isRow(2, "{\"id\":1,\"doc\":2}", "1"),
                isRow(4, "{\"id\":3,\"doc\":4}", "3")
        ));
    }

    @Test
    public void testCollectDocLevelWhereClause() throws Exception {
        EqOperator op = (EqOperator) functions.get(new FunctionIdent(EqOperator.NAME,
                ImmutableList.<DataType>of(DataTypes.INTEGER, DataTypes.INTEGER)));
        CollectNode collectNode = new CollectNode(UUID.randomUUID(), 0, "docCollect", routing(TEST_TABLE_NAME));
        collectNode.toCollect(Arrays.<Symbol>asList(testDocLevelReference));
        collectNode.maxRowGranularity(RowGranularity.DOC);
        collectNode.whereClause(new WhereClause(new Function(
                op.info(),
                Arrays.<Symbol>asList(testDocLevelReference, Literal.newLiteral(2)))
        ));
        PlanNodeBuilder.setOutputTypes(collectNode);

        Bucket result = collect(collectNode);
        assertThat(result, contains(isRow(2)));
    }

    @Test
    public void testCollectWithPartitionedColumns() throws Exception {
        Routing routing = docSchemaInfo.getTableInfo(PARTITIONED_TABLE_NAME).getRouting(WhereClause.MATCH_ALL);
        TableIdent tableIdent = new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, PARTITIONED_TABLE_NAME);
        CollectNode collectNode = new CollectNode(UUID.randomUUID(), 0, "docCollect", routing);
        collectNode.toCollect(Arrays.<Symbol>asList(
                new Reference(new ReferenceInfo(
                        new ReferenceIdent(tableIdent, "id"),
                        RowGranularity.DOC, DataTypes.INTEGER)),
                new Reference(new ReferenceInfo(
                        new ReferenceIdent(tableIdent, "date"),
                        RowGranularity.SHARD, DataTypes.TIMESTAMP))
        ));
        collectNode.maxRowGranularity(RowGranularity.DOC);
        collectNode.isPartitioned(true);
        PlanNodeBuilder.setOutputTypes(collectNode);

        Bucket result = collect(collectNode);
        for (Row row : result) {
            System.out.println("Row:" + Arrays.toString(row.materialize()));
        }

        assertThat(result, containsInAnyOrder(
                isRow(1, 0L),
                isRow(2, 1L)
        ));
    }

    private Bucket collect(CollectNode collectNode) throws Exception {
        ContextPreparer contextPreparer = internalCluster().getDataNodeInstance(ContextPreparer.class);
        JobContextService contextService = internalCluster().getDataNodeInstance(JobContextService.class);
        JobExecutionContext.Builder builder = contextService.newBuilder(collectNode.jobId());
        ListenableFuture<Bucket> future = contextPreparer.prepare(collectNode.jobId(), collectNode, builder);
        assert future != null;
        JobExecutionContext context = contextService.createContext(builder);
        context.start();
        return future.get(500, TimeUnit.MILLISECONDS);
    }
}

<code block>


package io.crate.operation.collect;

import com.google.common.collect.ImmutableMap;
import io.crate.core.collections.TreeMapBuilder;
import io.crate.executor.transport.TransportActionProvider;
import io.crate.jobs.ExecutionState;
import io.crate.metadata.*;
import io.crate.operation.projectors.ResultProvider;
import io.crate.operation.projectors.ResultProviderFactory;
import io.crate.operation.reference.sys.node.NodeSysExpression;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.node.dql.FileUriCollectNode;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.Literal;
import io.crate.planner.symbol.Symbol;
import io.crate.testing.CollectingProjector;
import io.crate.types.DataTypes;
import org.elasticsearch.action.bulk.BulkRetryCoordinatorPool;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.cluster.ClusterState;
import org.elasticsearch.cluster.node.DiscoveryNode;
import org.elasticsearch.cluster.node.DiscoveryNodes;
import org.elasticsearch.common.settings.ImmutableSettings;
import org.elasticsearch.discovery.DiscoveryService;
import org.elasticsearch.indices.IndicesService;
import org.elasticsearch.node.settings.NodeSettingsService;
import org.elasticsearch.threadpool.ThreadPool;
import org.junit.Rule;
import org.junit.Test;
import org.junit.rules.TemporaryFolder;
import org.mockito.Answers;

import java.io.File;
import java.io.FileWriter;
import java.nio.file.Paths;
import java.util.*;

import static io.crate.testing.TestingHelpers.createReference;
import static io.crate.testing.TestingHelpers.isRow;
import static org.hamcrest.Matchers.contains;
import static org.junit.Assert.assertThat;
import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;

public class MapSideDataCollectOperationTest {

    @Rule
    public TemporaryFolder temporaryFolder = new TemporaryFolder();

    @Test
    public void testFileUriCollect() throws Exception {
        ClusterService clusterService = mock(ClusterService.class);
        DiscoveryNode discoveryNode = mock(DiscoveryNode.class);
        when(discoveryNode.id()).thenReturn("dummyNodeId");
        DiscoveryNodes discoveryNodes = mock(DiscoveryNodes.class);
        when(discoveryNodes.localNodeId()).thenReturn("dummyNodeId");
        ClusterState clusterState = mock(ClusterState.class);
        when(clusterState.nodes()).thenReturn(discoveryNodes);
        when(clusterService.state()).thenReturn(clusterState);
        DiscoveryService discoveryService = mock(DiscoveryService.class);
        when(discoveryService.localNode()).thenReturn(discoveryNode);
        IndicesService indicesService = mock(IndicesService.class);
        Functions functions = new Functions(
                ImmutableMap.<FunctionIdent, FunctionImplementation>of(),
                ImmutableMap.<String, DynamicFunctionResolver>of());
        ReferenceResolver referenceResolver = new ReferenceResolver() {
            @Override
            public ReferenceImplementation getImplementation(ReferenceIdent ident) {
                return null;
            }
        };

        NodeSettingsService nodeSettingsService = mock(NodeSettingsService.class);

        MapSideDataCollectOperation collectOperation = new MapSideDataCollectOperation(
                clusterService,
                ImmutableSettings.EMPTY,
                mock(TransportActionProvider.class, Answers.RETURNS_DEEP_STUBS.get()),
                mock(BulkRetryCoordinatorPool.class),
                functions,
                referenceResolver,
                mock(NodeSysExpression.class),
                indicesService,
                new ThreadPool(ImmutableSettings.builder().put("name", getClass().getName()).build(), null),
                new CollectServiceResolver(discoveryService,
                        new SystemCollectService(
                                discoveryService,
                                functions,
                                new StatsTables(ImmutableSettings.EMPTY, nodeSettingsService)
                        )
                ),
                new ResultProviderFactory() {
                    @Override
                    public ResultProvider createDownstream(ExecutionNode node, UUID jobId) {
                        return new CollectingProjector();
                    }
                },
                mock(InformationSchemaCollectService.class),
                mock(UnassignedShardsCollectService.class)
        );

        File tmpFile = temporaryFolder.newFile("fileUriCollectOperation.json");
        try (FileWriter writer = new FileWriter(tmpFile)) {
            writer.write("{\"name\": \"Arthur\", \"id\": 4, \"details\": {\"age\": 38}}\n");
            writer.write("{\"id\": 5, \"name\": \"Trillian\", \"details\": {\"age\": 33}}\n");
        }

        Routing routing = new Routing(
                TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder()
                .put("dummyNodeId", new TreeMap<String, List<Integer>>())
                .map()
        );
        FileUriCollectNode collectNode = new FileUriCollectNode(
                UUID.randomUUID(),
                0,
                "test",
                routing,
                Literal.newLiteral(Paths.get(tmpFile.toURI()).toUri().toString()),
                Arrays.<Symbol>asList(
                        createReference("name", DataTypes.STRING),
                        createReference(new ColumnIdent("details", "age"), DataTypes.INTEGER)
                ),
                Arrays.<Projection>asList(),
                null,
                false
        );
        CollectingProjector cd = new CollectingProjector();
        cd.startProjection(mock(ExecutionState.class));
        collectOperation.collect(collectNode, cd, mock(JobCollectContext.class));
        assertThat(cd.result().get(), contains(
                isRow("Arthur", 38),
                isRow("Trillian", 33)
        ));
    }
}

<code block>


package io.crate.operation.collect;

import com.google.common.collect.ImmutableList;
import io.crate.analyze.WhereClause;
import io.crate.core.collections.Bucket;
import io.crate.core.collections.TreeMapBuilder;
import io.crate.integrationtests.SQLTransportIntegrationTest;
import io.crate.jobs.ExecutionState;
import io.crate.metadata.*;
import io.crate.metadata.information.InformationSchemaInfo;
import io.crate.metadata.sys.SysClusterTableInfo;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.operator.EqOperator;
import io.crate.testing.CollectingProjector;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.symbol.Function;
import io.crate.planner.symbol.Literal;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.testing.TestingHelpers;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.test.ElasticsearchIntegrationTest;
import org.hamcrest.Matchers;
import org.junit.Before;
import org.junit.Test;

import java.util.*;

import static org.hamcrest.core.Is.is;
import static org.mockito.Mockito.mock;

@ElasticsearchIntegrationTest.ClusterScope(numDataNodes = 1)
public class HandlerSideLevelCollectTest extends SQLTransportIntegrationTest {

    private MapSideDataCollectOperation operation;
    private Functions functions;
    private String localNodeId;


    @Before
    public void prepare() {
        operation = internalCluster().getDataNodeInstance(MapSideDataCollectOperation.class);
        functions = internalCluster().getInstance(Functions.class);
        localNodeId = internalCluster().getDataNodeInstance(ClusterService.class).state().nodes().localNodeId();
    }

    @Test
    public void testClusterLevel() throws Exception {
        Routing routing = SysClusterTableInfo.ROUTING;
        CollectNode collectNode = new CollectNode(UUID.randomUUID(), 0, "clusterCollect", routing);
        Reference clusterNameRef = new Reference(SysClusterTableInfo.INFOS.get(new ColumnIdent("name")));
        collectNode.toCollect(Arrays.<Symbol>asList(clusterNameRef));
        collectNode.maxRowGranularity(RowGranularity.CLUSTER);
        collectNode.handlerSideCollect(localNodeId);
        Bucket result = collect(collectNode);
        assertThat(result.size(), is(1));
        assertThat(((BytesRef) result.iterator().next().get(0)).utf8ToString(), Matchers.startsWith("SUITE-"));
    }

    private Bucket collect(CollectNode collectNode) throws InterruptedException, java.util.concurrent.ExecutionException {
        CollectingProjector collectingProjector = new CollectingProjector();
        collectingProjector.startProjection(mock(ExecutionState.class));
        operation.collect(collectNode, collectingProjector, mock(JobCollectContext.class));
        return collectingProjector.result().get();
    }

    @Test
    public void testInformationSchemaTables() throws Exception {
        Routing routing = new Routing(TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder().put(
                TableInfo.NULL_NODE_ID, TreeMapBuilder.<String, List<Integer>>newMapBuilder().put("information_schema.tables", null).map()
        ).map());
        CollectNode collectNode = new CollectNode(UUID.randomUUID(), 0, "tablesCollect", routing);
        InformationSchemaInfo schemaInfo =  internalCluster().getInstance(InformationSchemaInfo.class);
        TableInfo tablesTableInfo = schemaInfo.getTableInfo("tables");
        List<Symbol> toCollect = new ArrayList<>();
        for (ReferenceInfo info : tablesTableInfo.columns()) {
            toCollect.add(new Reference(info));
        }
        Symbol tableNameRef = toCollect.get(1);

        FunctionImplementation eqImpl = functions.get(new FunctionIdent(EqOperator.NAME,
                ImmutableList.<DataType>of(DataTypes.STRING, DataTypes.STRING)));
        Function whereClause = new Function(eqImpl.info(),
                Arrays.asList(tableNameRef, Literal.newLiteral("shards")));

        collectNode.whereClause(new WhereClause(whereClause));
        collectNode.toCollect(toCollect);
        collectNode.maxRowGranularity(RowGranularity.DOC);
        collectNode.handlerSideCollect(localNodeId);
        Bucket result = collect(collectNode);
        System.out.println(TestingHelpers.printedTable(result));
        assertEquals("sys| shards| 1| 0| NULL| NULL| NULL| NULL\n", TestingHelpers.printedTable(result));
    }


    @Test
    public void testInformationSchemaColumns() throws Exception {
        Routing routing = new Routing(TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder().put(
                TableInfo.NULL_NODE_ID, TreeMapBuilder.<String, List<Integer>>newMapBuilder().put("information_schema.columns", null).map()
        ).map());
        CollectNode collectNode = new CollectNode(UUID.randomUUID(), 0, "columnsCollect", routing);
        InformationSchemaInfo schemaInfo =  internalCluster().getInstance(InformationSchemaInfo.class);
        TableInfo tableInfo = schemaInfo.getTableInfo("columns");
        List<Symbol> toCollect = new ArrayList<>();
        for (ReferenceInfo info : tableInfo.columns()) {
            toCollect.add(new Reference(info));
        }
        collectNode.toCollect(toCollect);
        collectNode.maxRowGranularity(RowGranularity.DOC);
        collectNode.handlerSideCollect(localNodeId);
        Bucket result = collect(collectNode);


        String expected = "sys| cluster| id| 1| string\n" +
                "sys| cluster| name| 2| string\n" +
                "sys| cluster| master_node| 3| string\n" +
                "sys| cluster| settings| 4| object";


        assertTrue(TestingHelpers.printedTable(result).contains(expected));


        System.out.println(TestingHelpers.printedTable(result));
        result = collect(collectNode);
        assertTrue(TestingHelpers.printedTable(result).contains(expected));
    }

}

<code block>


package io.crate.executor.transport;

import com.google.common.collect.ImmutableList;
import io.crate.analyze.QuerySpec;
import io.crate.analyze.WhereClause;
import io.crate.analyze.where.DocKeys;
import io.crate.integrationtests.SQLTransportIntegrationTest;
import io.crate.integrationtests.Setup;
import io.crate.metadata.ColumnIdent;
import io.crate.metadata.ReferenceIdent;
import io.crate.metadata.ReferenceInfo;
import io.crate.metadata.TableIdent;
import io.crate.metadata.doc.DocSchemaInfo;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.dql.ESGetNode;
import io.crate.planner.symbol.Literal;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.testing.TestingHelpers;
import io.crate.types.DataTypes;
import org.junit.After;
import org.junit.Before;

import java.util.ArrayList;
import java.util.List;
import java.util.UUID;

public class BaseTransportExecutorTest extends SQLTransportIntegrationTest {

    Setup setup = new Setup(sqlExecutor);

    static {
        ClassLoader.getSystemClassLoader().setDefaultAssertionStatus(true);
    }

    TransportExecutor executor;
    DocSchemaInfo docSchemaInfo;

    TableIdent charactersIdent = new TableIdent(null, "characters");
    TableIdent booksIdent = new TableIdent(null, "books");

    Reference idRef = new Reference(new ReferenceInfo(
            new ReferenceIdent(charactersIdent, "id"), RowGranularity.DOC, DataTypes.INTEGER));
    Reference nameRef = new Reference(new ReferenceInfo(
            new ReferenceIdent(charactersIdent, "name"), RowGranularity.DOC, DataTypes.STRING));
    Reference femaleRef = TestingHelpers.createReference(charactersIdent.name(), new ColumnIdent("female"), DataTypes.BOOLEAN);

    TableIdent partedTable = new TableIdent("doc", "parted");
    Reference partedIdRef = new Reference(new ReferenceInfo(
            new ReferenceIdent(partedTable, "id"), RowGranularity.DOC, DataTypes.INTEGER));
    Reference partedNameRef = new Reference(new ReferenceInfo(
            new ReferenceIdent(partedTable, "name"), RowGranularity.DOC, DataTypes.STRING));
    Reference partedDateRef = new Reference(new ReferenceInfo(
            new ReferenceIdent(partedTable, "date"), RowGranularity.PARTITION, DataTypes.TIMESTAMP));

    public static ESGetNode newGetNode(TableInfo tableInfo, List<Symbol> outputs, List<String> singleStringKeys, int executionNodeId) {
        QuerySpec querySpec = new QuerySpec();
        querySpec.outputs(outputs);
        List<List<Symbol>> keys = new ArrayList<>(singleStringKeys.size());
        for (String v : singleStringKeys) {
            keys.add(ImmutableList.<Symbol>of(Literal.newLiteral(v)));
        }
        WhereClause whereClause = new WhereClause(null, new DocKeys(keys, false, -1, null), null);
        querySpec.where(whereClause);
        return new ESGetNode(executionNodeId, tableInfo, querySpec, UUID.randomUUID());
    }

    @Before
    public void transportSetUp() {
        executor = internalCluster().getInstance(TransportExecutor.class);
        docSchemaInfo = internalCluster().getInstance(DocSchemaInfo.class);
    }

    @After
    public void transportTearDown() {
        executor = null;
        docSchemaInfo = null;
    }
}

<code block>


package io.crate.executor.transport;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.Lists;
import com.google.common.util.concurrent.ListenableFuture;
import io.crate.Constants;
import io.crate.analyze.OrderBy;
import io.crate.analyze.WhereClause;
import io.crate.core.collections.Bucket;
import io.crate.executor.Job;
import io.crate.executor.RowCountResult;
import io.crate.executor.Task;
import io.crate.executor.TaskResult;
import io.crate.executor.transport.task.KillTask;
import io.crate.executor.transport.task.SymbolBasedUpsertByIdTask;
import io.crate.executor.transport.task.elasticsearch.ESDeleteByQueryTask;
import io.crate.executor.transport.task.elasticsearch.ESGetTask;
import io.crate.metadata.*;
import io.crate.metadata.doc.DocSysColumns;
import io.crate.metadata.doc.DocTableInfo;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.aggregation.impl.CountAggregation;
import io.crate.operation.operator.EqOperator;
import io.crate.operation.projectors.TopN;
import io.crate.operation.scalar.DateTruncFunction;
import io.crate.planner.*;
import io.crate.planner.node.dml.ESDeleteByQueryNode;
import io.crate.planner.node.dml.SymbolBasedUpsertByIdNode;
import io.crate.planner.node.dml.Upsert;
import io.crate.planner.node.dql.*;
import io.crate.planner.node.management.KillPlan;
import io.crate.planner.projection.*;
import io.crate.planner.symbol.*;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.cluster.routing.operation.plain.Preference;
import org.elasticsearch.common.collect.MapBuilder;
import org.elasticsearch.search.SearchHits;
import org.junit.Before;
import org.junit.Rule;
import org.junit.Test;
import org.junit.rules.ExpectedException;

import java.util.*;

import static io.crate.testing.TestingHelpers.isRow;
import static java.util.Arrays.asList;
import static org.hamcrest.Matchers.*;
import static org.hamcrest.core.Is.is;

public class TransportExecutorTest extends BaseTransportExecutorTest {

    @Rule
    public ExpectedException expectedException = ExpectedException.none();

    @Before
    public void setup() {
    }

    protected ESGetNode newGetNode(String tableName, List<Symbol> outputs, String singleStringKey, int executionNodeId) {
        return newGetNode(tableName, outputs, Collections.singletonList(singleStringKey), executionNodeId);
    }

    protected ESGetNode newGetNode(String tableName, List<Symbol> outputs, List<String> singleStringKeys, int executionNodeId) {
        return newGetNode(docSchemaInfo.getTableInfo(tableName), outputs, singleStringKeys, executionNodeId);
    }

    @Test
    public void testESGetTask() throws Exception {
        setup.setUpCharacters();


        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef);
        Planner.Context ctx = new Planner.Context(clusterService());
        ESGetNode node = newGetNode("characters", outputs, "2", ctx.nextExecutionNodeId());
        Plan plan = new IterablePlan(UUID.randomUUID(), node);
        Job job = executor.newJob(plan);


        assertThat(job.tasks().size(), is(1));
        Task task = job.tasks().get(0);
        assertThat(task, instanceOf(ESGetTask.class));


        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, contains(isRow(2, "Ford")));
    }

    @Test
    public void testESGetTaskWithDynamicReference() throws Exception {
        setup.setUpCharacters();

        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, new DynamicReference(
                new ReferenceIdent(new TableIdent(null, "characters"), "foo"), RowGranularity.DOC));
        Planner.Context ctx = new Planner.Context(clusterService());
        ESGetNode node = newGetNode("characters", outputs, "2", ctx.nextExecutionNodeId());
        Plan plan = new IterablePlan(UUID.randomUUID(), node);
        Job job = executor.newJob(plan);
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, contains(isRow(2, null)));
    }

    @Test
    public void testESMultiGet() throws Exception {
        setup.setUpCharacters();
        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef);
        Planner.Context ctx = new Planner.Context(clusterService());
        ESGetNode node = newGetNode("characters", outputs, asList("1", "2"), ctx.nextExecutionNodeId());
        Plan plan = new IterablePlan(UUID.randomUUID(), node);
        Job job = executor.newJob(plan);
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();

        assertThat(objects.size(), is(2));
    }

    @Test
    public void testQTFTask() throws Exception {

        setup.setUpCharacters();
        DocTableInfo characters = docSchemaInfo.getTableInfo("characters");
        ReferenceInfo docIdRefInfo = characters.getReferenceInfo(new ColumnIdent(DocSysColumns.DOCID.name()));
        List<Symbol> collectSymbols = Lists.<Symbol>newArrayList(new Reference(docIdRefInfo));
        List<Symbol> outputSymbols = Lists.<Symbol>newArrayList(idRef, nameRef);

        Planner.Context ctx = new Planner.Context(clusterService());

        UUID jobId = UUID.randomUUID();
        CollectNode collectNode = PlanNodeBuilder.collect(
                jobId,
                characters,
                ctx,
                WhereClause.MATCH_ALL,
                collectSymbols,
                ImmutableList.<Projection>of(),
                null,
                Constants.DEFAULT_SELECT_LIMIT
        );
        collectNode.keepContextForFetcher(true);

        FetchProjection fetchProjection = getFetchProjection((DocTableInfo) characters, (List<Symbol>) collectSymbols, (List<Symbol>) outputSymbols, (CollectNode) collectNode, ctx);

        MergeNode localMergeNode = PlanNodeBuilder.localMerge(
                jobId,
                ImmutableList.<Projection>of(fetchProjection),
                collectNode,
                ctx);

        Plan plan = new QueryThenFetch(collectNode, localMergeNode, jobId);

        Job job = executor.newJob(plan);
        assertThat(job.tasks().size(), is(1));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, containsInAnyOrder(
                isRow(1, "Arthur"),
                isRow(4, "Arthur"),
                isRow(2, "Ford"),
                isRow(3, "Trillian")
        ));
    }

    @Test
    public void testQTFTaskWithFilter() throws Exception {

        setup.setUpCharacters();
        DocTableInfo characters = docSchemaInfo.getTableInfo("characters");
        ReferenceInfo docIdRefInfo = characters.getReferenceInfo(new ColumnIdent(DocSysColumns.DOCID.name()));
        List<Symbol> collectSymbols = Lists.<Symbol>newArrayList(new Reference(docIdRefInfo));
        List<Symbol> outputSymbols = Lists.<Symbol>newArrayList(idRef, nameRef);

        Function whereClause = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.STRING, DataTypes.STRING)),
                DataTypes.BOOLEAN),
                Arrays.<Symbol>asList(nameRef, Literal.newLiteral("Ford")));

        Planner.Context ctx = new Planner.Context(clusterService());

        UUID jobId = UUID.randomUUID();
        CollectNode collectNode = PlanNodeBuilder.collect(
                jobId,
                characters,
                ctx,
                new WhereClause(whereClause),
                collectSymbols,
                ImmutableList.<Projection>of(),
                null,
                Constants.DEFAULT_SELECT_LIMIT
        );
        collectNode.keepContextForFetcher(true);

        FetchProjection fetchProjection = getFetchProjection(characters, collectSymbols, outputSymbols, collectNode, ctx);

        MergeNode localMergeNode = PlanNodeBuilder.localMerge(
                jobId,
                ImmutableList.<Projection>of(fetchProjection),
                collectNode,
                ctx);

        Plan plan = new QueryThenFetch(collectNode, localMergeNode, jobId);

        Job job = executor.newJob(plan);
        assertThat(job.tasks().size(), is(1));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, contains(isRow(2, "Ford")));
    }

    private FetchProjection getFetchProjection(DocTableInfo characters, List<Symbol> collectSymbols, List<Symbol> outputSymbols, CollectNode collectNode, Planner.Context ctx) {
        return new FetchProjection(
                collectNode.executionNodeId(),
                new InputColumn(0, DataTypes.STRING), collectSymbols, outputSymbols,
                characters.partitionedByColumns(),
                collectNode.executionNodes(),
                5,
                false,
                ctx.jobSearchContextIdToNode(),
                ctx.jobSearchContextIdToShard()
        );
    }

    @Test
    public void testQTFTaskOrdered() throws Exception {

        setup.setUpCharacters();
        DocTableInfo characters = docSchemaInfo.getTableInfo("characters");

        OrderBy orderBy = new OrderBy(Arrays.<Symbol>asList(nameRef, femaleRef),
                new boolean[]{false, false},
                new Boolean[]{false, false});

        ReferenceInfo docIdRefInfo = characters.getReferenceInfo(new ColumnIdent(DocSysColumns.DOCID.name()));

        List<Symbol> collectSymbols = Lists.<Symbol>newArrayList(new Reference(docIdRefInfo), nameRef, femaleRef);
        List<Symbol> outputSymbols = Lists.<Symbol>newArrayList(idRef, nameRef);

        MergeProjection mergeProjection = new MergeProjection(
                collectSymbols,
                orderBy.orderBySymbols(),
                orderBy.reverseFlags(),
                orderBy.nullsFirst()
        );
        Planner.Context ctx = new Planner.Context(clusterService());

        UUID jobId = UUID.randomUUID();
        CollectNode collectNode = PlanNodeBuilder.collect(
                jobId,
                characters,
                ctx,
                WhereClause.MATCH_ALL,
                collectSymbols,
                ImmutableList.<Projection>of(),
                orderBy,
                Constants.DEFAULT_SELECT_LIMIT
        );
        collectNode.projections(ImmutableList.<Projection>of(mergeProjection));
        collectNode.keepContextForFetcher(true);

        FetchProjection fetchProjection = getFetchProjection(characters, collectSymbols, outputSymbols, collectNode, ctx);

        MergeNode localMergeNode = PlanNodeBuilder.sortedLocalMerge(
                jobId,
                ImmutableList.<Projection>of(fetchProjection),
                orderBy,
                collectSymbols,
                null,
                collectNode,
                ctx);

        Plan plan = new QueryThenFetch(collectNode, localMergeNode, jobId);

        Job job = executor.newJob(plan);
        assertThat(job.tasks().size(), is(1));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, contains(
                isRow(1, "Arthur"),
                isRow(4, "Arthur"),
                isRow(2, "Ford"),
                isRow(3, "Trillian")
        ));
    }

    @Test
    public void testQTFTaskWithFunction() throws Exception {

        execute("create table searchf (id int primary key, date timestamp) with (number_of_replicas=0)");
        ensureGreen();
        execute("insert into searchf (id, date) values (1, '1980-01-01'), (2, '1980-01-02')");
        refresh();

        Reference id_ref = new Reference(new ReferenceInfo(
                new ReferenceIdent(
                        new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "searchf"),
                        "id"),
                RowGranularity.DOC,
                DataTypes.INTEGER
        ));
        Reference date_ref = new Reference(new ReferenceInfo(
                new ReferenceIdent(
                        new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "searchf"),
                        "date"),
                RowGranularity.DOC,
                DataTypes.TIMESTAMP
        ));
        Function function = new Function(new FunctionInfo(
                new FunctionIdent(DateTruncFunction.NAME, Arrays.<DataType>asList(DataTypes.STRING, DataTypes.TIMESTAMP)),
                DataTypes.TIMESTAMP
        ), Arrays.asList(Literal.newLiteral("month"), date_ref));
        Function whereClause = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.INTEGER, DataTypes.INTEGER)),
                DataTypes.BOOLEAN),
                Arrays.asList(id_ref, Literal.newLiteral(2))
        );

        DocTableInfo searchf = docSchemaInfo.getTableInfo("searchf");
        ReferenceInfo docIdRefInfo = searchf.getReferenceInfo(new ColumnIdent(DocSysColumns.DOCID.name()));

        Planner.Context ctx = new Planner.Context(clusterService());
        List<Symbol> collectSymbols = ImmutableList.<Symbol>of(new Reference(docIdRefInfo));
        UUID jobId = UUID.randomUUID();
        CollectNode collectNode = PlanNodeBuilder.collect(
                jobId,
                searchf,
                ctx,
                new WhereClause(whereClause),
                collectSymbols,
                ImmutableList.<Projection>of(),
                null,
                Constants.DEFAULT_SELECT_LIMIT
        );
        collectNode.keepContextForFetcher(true);

        TopNProjection topN = new TopNProjection(2, TopN.NO_OFFSET);
        topN.outputs(Collections.<Symbol>singletonList(new InputColumn(0)));

        FetchProjection fetchProjection = getFetchProjection(searchf, collectSymbols, Arrays.asList(id_ref, function), collectNode, ctx);

        MergeNode mergeNode = PlanNodeBuilder.localMerge(
                jobId,
                ImmutableList.of(topN, fetchProjection),
                collectNode,
                ctx);
        Plan plan = new QueryThenFetch(collectNode, mergeNode, jobId);

        Job job = executor.newJob(plan);
        assertThat(job.tasks().size(), is(1));

        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, contains(isRow(2, 315532800000L)));
    }

    @Test
    public void testQTFTaskPartitioned() throws Exception {
        setup.setUpPartitionedTableWithName();
        DocTableInfo parted = docSchemaInfo.getTableInfo("parted");
        Planner.Context ctx = new Planner.Context(clusterService());

        ReferenceInfo docIdRefInfo = parted.getReferenceInfo(new ColumnIdent(DocSysColumns.DOCID.name()));
        List<Symbol> collectSymbols = Lists.<Symbol>newArrayList(new Reference(docIdRefInfo));
        List<Symbol> outputSymbols =  Arrays.<Symbol>asList(partedIdRef, partedNameRef, partedDateRef);

        UUID jobId = UUID.randomUUID();
        CollectNode collectNode = PlanNodeBuilder.collect(
                jobId,
                parted,
                ctx,
                WhereClause.MATCH_ALL,
                collectSymbols,
                ImmutableList.<Projection>of(),
                null,
                Constants.DEFAULT_SELECT_LIMIT
        );
        collectNode.keepContextForFetcher(true);

        FetchProjection fetchProjection = getFetchProjection(parted, collectSymbols, outputSymbols, collectNode, ctx);

        MergeNode localMergeNode = PlanNodeBuilder.localMerge(
                jobId,
                ImmutableList.<Projection>of(fetchProjection),
                collectNode,
                ctx);

        Plan plan = new QueryThenFetch(collectNode, localMergeNode, jobId);
        Job job = executor.newJob(plan);

        assertThat(job.tasks().size(), is(1));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, containsInAnyOrder(
                isRow(3, "Ford", 1396388720242L),
                isRow(1, "Trillian", null),
                isRow(2, null, 0L)
        ));
    }

    @Test
    public void testESDeleteByQueryTask() throws Exception {
        setup.setUpCharacters();

        Function whereClause = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.STRING, DataTypes.STRING)),
                DataTypes.BOOLEAN),
                Arrays.<Symbol>asList(idRef, Literal.newLiteral(2)));

        ESDeleteByQueryNode node = new ESDeleteByQueryNode(
                1,
                ImmutableList.of(new String[]{"characters"}),
                ImmutableList.of(new WhereClause(whereClause)));
        Plan plan = new IterablePlan(UUID.randomUUID(), node);
        Job job = executor.newJob(plan);
        ESDeleteByQueryTask task = (ESDeleteByQueryTask) job.tasks().get(0);

        task.start();
        TaskResult taskResult = task.result().get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(-1L)));


        execute("select * from characters where id = 2");
        assertThat(response.rowCount(), is(0L));
    }

    @Test
    public void testInsertWithSymbolBasedUpsertByIdTask() throws Exception {
        execute("create table characters (id int primary key, name string)");
        ensureGreen();


        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(),
                false,
                false,
                null,
                new Reference[]{idRef, nameRef});
        updateNode.add("characters", "99", "99", null, null, new Object[]{99, new BytesRef("Marvin")});

        Plan plan = new IterablePlan(UUID.randomUUID(), updateNode);
        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));

        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(1L)));


        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef);
        ESGetNode getNode = newGetNode("characters", outputs, "99", ctx.nextExecutionNodeId());
        plan = new IterablePlan(UUID.randomUUID(), getNode);
        job = executor.newJob(plan);
        result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();

        assertThat(objects, contains(isRow(99, "Marvin")));
    }

    @Test
    public void testInsertIntoPartitionedTableWithSymbolBasedUpsertByIdTask() throws Exception {
        execute("create table parted (" +
                "  id int, " +
                "  name string, " +
                "  date timestamp" +
                ") partitioned by (date)");
        ensureGreen();


        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(),
                true,
                false,
                null,
                new Reference[]{idRef, nameRef});

        PartitionName partitionName = new PartitionName("parted", Arrays.asList(new BytesRef("13959981214861")));
        updateNode.add(partitionName.stringValue(), "123", "123", null, null, new Object[]{0L, new BytesRef("Trillian")});

        Plan plan = new IterablePlan(UUID.randomUUID(), updateNode);
        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));

        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket indexResult = taskResult.rows();
        assertThat(indexResult, contains(isRow(1L)));

        refresh();

        assertTrue(
                client().admin().indices().prepareExists(partitionName.stringValue())
                        .execute().actionGet().isExists()
        );
        assertTrue(
                client().admin().indices().prepareAliasesExist("parted")
                        .execute().actionGet().exists()
        );
        SearchHits hits = client().prepareSearch(partitionName.stringValue())
                .setTypes(Constants.DEFAULT_MAPPING_TYPE)
                .addFields("id", "name")
                .setQuery(new MapBuilder<String, Object>()
                                .put("match_all", new HashMap<String, Object>())
                                .map()
                ).execute().actionGet().getHits();
        assertThat(hits.getTotalHits(), is(1L));
        assertThat((Integer) hits.getHits()[0].field("id").getValues().get(0), is(0));
        assertThat((String) hits.getHits()[0].field("name").getValues().get(0), is("Trillian"));
    }

    @Test
    public void testInsertMultiValuesWithSymbolBasedUpsertByIdTask() throws Exception {
        execute("create table characters (id int primary key, name string)");
        ensureGreen();


        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(),
                false,
                false,
                null,
                new Reference[]{idRef, nameRef});

        updateNode.add("characters", "99", "99", null, null, new Object[]{99, new BytesRef("Marvin")});
        updateNode.add("characters", "42", "42", null, null, new Object[]{42, new BytesRef("Deep Thought")});

        Plan plan = new IterablePlan(UUID.randomUUID(), updateNode);
        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));

        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(2L)));


        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef);
        ESGetNode getNode = newGetNode("characters", outputs, Arrays.asList("99", "42"), ctx.nextExecutionNodeId());
        plan = new IterablePlan(UUID.randomUUID(), getNode);
        job = executor.newJob(plan);
        result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();

        assertThat(objects, contains(
                isRow(99, "Marvin"),
                isRow(42, "Deep Thought")
        ));
    }

    @Test
    public void testUpdateWithSymbolBasedUpsertByIdTask() throws Exception {
        setup.setUpCharacters();


        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(), false, false, new String[]{nameRef.ident().columnIdent().fqn()}, null);
        updateNode.add("characters", "1", "1", new Symbol[]{Literal.newLiteral("Vogon lyric fan")}, null);
        Plan plan = new IterablePlan(UUID.randomUUID(), updateNode);

        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(1L)));


        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef);
        ESGetNode getNode = newGetNode("characters", outputs, "1", ctx.nextExecutionNodeId());
        plan = new IterablePlan(UUID.randomUUID(), getNode);
        job = executor.newJob(plan);
        result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();

        assertThat(objects, contains(isRow(1, "Vogon lyric fan")));
    }

    @Test
    public void testInsertOnDuplicateWithSymbolBasedUpsertByIdTask() throws Exception {
        setup.setUpCharacters();

        Object[] missingAssignments = new Object[]{5, new BytesRef("Zaphod Beeblebrox"), false};
        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(),
                false,
                false,
                new String[]{nameRef.ident().columnIdent().fqn()},
                new Reference[]{idRef, nameRef, femaleRef});

        updateNode.add("characters", "5", "5", new Symbol[]{Literal.newLiteral("Zaphod Beeblebrox")}, null, missingAssignments);
        Plan plan = new IterablePlan(UUID.randomUUID(), updateNode);
        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(1L)));


        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef, femaleRef);
        ESGetNode getNode = newGetNode("characters", outputs, "5", ctx.nextExecutionNodeId());
        plan = new IterablePlan(UUID.randomUUID(), getNode);
        job = executor.newJob(plan);
        result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();
        assertThat(objects, contains(isRow(5, "Zaphod Beeblebrox", false)));

    }

    @Test
    public void testUpdateOnDuplicateWithSymbolBasedUpsertByIdTask() throws Exception {
        setup.setUpCharacters();

        Object[] missingAssignments = new Object[]{1, new BytesRef("Zaphod Beeblebrox"), true};
        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(),
                false,
                false,
                new String[]{femaleRef.ident().columnIdent().fqn()},
                new Reference[]{idRef, nameRef, femaleRef});
        updateNode.add("characters", "1", "1", new Symbol[]{Literal.newLiteral(true)}, null, missingAssignments);
        Plan plan = new IterablePlan(UUID.randomUUID(), updateNode);
        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(1L)));


        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef, femaleRef);
        ESGetNode getNode = newGetNode("characters", outputs, "1", ctx.nextExecutionNodeId());
        plan = new IterablePlan(UUID.randomUUID(), getNode);
        job = executor.newJob(plan);
        result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();

        assertThat(objects, contains(isRow(1, "Arthur", true)));
    }

    @Test
    public void testBulkUpdateByQueryTask() throws Exception {
        setup.setUpCharacters();


        List<Plan> childNodes = new ArrayList<>();
        Planner.Context plannerContext = new Planner.Context(clusterService());

        TableInfo tableInfo = docSchemaInfo.getTableInfo("characters");
        Reference uidReference = new Reference(
                new ReferenceInfo(
                        new ReferenceIdent(tableInfo.ident(), "_uid"),
                        RowGranularity.DOC, DataTypes.STRING));


        Function whereClause1 = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.BOOLEAN, DataTypes.BOOLEAN)),
                DataTypes.BOOLEAN),
                Arrays.<Symbol>asList(femaleRef, Literal.newLiteral(true)));

        UpdateProjection updateProjection = new UpdateProjection(
                new InputColumn(0, DataTypes.STRING),
                new String[]{"name"},
                new Symbol[]{Literal.newLiteral("Zaphod Beeblebrox")},
                null);

        UUID jobId = UUID.randomUUID();
        CollectNode collectNode1 = PlanNodeBuilder.collect(
                jobId,
                tableInfo,
                plannerContext,
                new WhereClause(whereClause1),
                ImmutableList.<Symbol>of(uidReference),
                ImmutableList.<Projection>of(updateProjection),
                null,
                Preference.PRIMARY.type()
        );
        MergeNode mergeNode1 = PlanNodeBuilder.localMerge(
                jobId,
                ImmutableList.<Projection>of(CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION), collectNode1,
                plannerContext);
        childNodes.add(new CollectAndMerge(collectNode1, mergeNode1, UUID.randomUUID()));


        Function whereClause2 = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.BOOLEAN, DataTypes.BOOLEAN)),
                DataTypes.BOOLEAN),
                Arrays.<Symbol>asList(femaleRef, Literal.newLiteral(true)));

        CollectNode collectNode2 = PlanNodeBuilder.collect(
                jobId,
                tableInfo,
                plannerContext,
                new WhereClause(whereClause2),
                ImmutableList.<Symbol>of(uidReference),
                ImmutableList.<Projection>of(updateProjection),
                null,
                Preference.PRIMARY.type()
        );
        MergeNode mergeNode2 = PlanNodeBuilder.localMerge(
                jobId,
                ImmutableList.<Projection>of(CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION), collectNode2,
                plannerContext);
        childNodes.add(new CollectAndMerge(collectNode2, mergeNode2, UUID.randomUUID()));

        Upsert plan = new Upsert(childNodes, UUID.randomUUID());
        Job job = executor.newJob(plan);

        assertThat(job.tasks().size(), is(1));
        assertThat(job.tasks().get(0), instanceOf(ExecutionNodesTask.class));
        List<? extends ListenableFuture<TaskResult>> results = executor.execute(job);
        assertThat(results.size(), is(2));

        for (int i = 0; i < results.size(); i++) {
            TaskResult result = results.get(i).get();
            assertThat(result, instanceOf(RowCountResult.class));

            assertThat(((RowCountResult)result).rowCount(), is(2L));
        }
    }

    @Test
    public void testKillTask() throws Exception {
        Job job = executor.newJob(new KillPlan(UUID.randomUUID()));
        assertThat(job.tasks(), hasSize(1));
        assertThat(job.tasks().get(0), instanceOf(KillTask.class));

        List<? extends ListenableFuture<TaskResult>> results = executor.execute(job);
        assertThat(results, hasSize(1));
        results.get(0).get();
    }
}

<code block>


package io.crate.executor.transport;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.Sets;
import io.crate.core.collections.TreeMapBuilder;
import io.crate.metadata.Routing;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.node.ExecutionNodeGrouper;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.MergeNode;
import org.hamcrest.Matchers;
import org.junit.Test;

import java.util.*;

import static org.hamcrest.Matchers.is;
import static org.junit.Assert.assertThat;

public class ExecutionNodesTaskTest {


    @Test
    public void testGroupByServer() throws Exception {

        Routing twoNodeRouting = new Routing(TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder()
                .put("node1", TreeMapBuilder.<String, List<Integer>>newMapBuilder().put("t1", Arrays.asList(1, 2)).map())
                .put("node2", TreeMapBuilder.<String, List<Integer>>newMapBuilder().put("t1", Arrays.asList(3, 4)).map())
                .map());

        UUID jobId = UUID.randomUUID();
        CollectNode c1 = new CollectNode(jobId, 1, "c1", twoNodeRouting);

        MergeNode m1 = new MergeNode(jobId, 2, "merge1", 2);
        m1.executionNodes(Sets.newHashSet("node3", "node4"));

        MergeNode m2 = new MergeNode(jobId, 3, "merge2", 2);

        m2.executionNodes(Sets.newHashSet("node1", "node3"));

        Map<String, Collection<ExecutionNode>> groupByServer = ExecutionNodeGrouper.groupByServer("node1", ImmutableList.<List<ExecutionNode>>of(ImmutableList.<ExecutionNode>of(c1, m1, m2)));

        assertThat(groupByServer.containsKey("node1"), is(true));
        assertThat(groupByServer.get("node1"), Matchers.<ExecutionNode>containsInAnyOrder(c1, m2));

        assertThat(groupByServer.containsKey("node2"), is(true));
        assertThat(groupByServer.get("node2"), Matchers.<ExecutionNode>containsInAnyOrder(c1));

        assertThat(groupByServer.containsKey("node3"), is(true));
        assertThat(groupByServer.get("node3"), Matchers.<ExecutionNode>containsInAnyOrder(m1, m2));

        assertThat(groupByServer.containsKey("node4"), is(true));
        assertThat(groupByServer.get("node4"), Matchers.<ExecutionNode>containsInAnyOrder(m1));
    }


    @Test
    public void testDetectsHasDirectResponse() throws Exception {
        CollectNode c1 = new CollectNode(UUID.randomUUID(), 1, "c1");
        c1.downstreamNodes(Collections.singletonList("foo"));

        assertThat(ExecutionNodesTask.hasDirectResponse(ImmutableList.<List<ExecutionNode>>of(ImmutableList.<ExecutionNode>of(c1))), is(false));

        CollectNode c2 = new CollectNode(UUID.randomUUID(), 1, "c1");
        c2.downstreamNodes(Collections.singletonList(ExecutionNode.DIRECT_RETURN_DOWNSTREAM_NODE));
        assertThat(ExecutionNodesTask.hasDirectResponse(ImmutableList.<List<ExecutionNode>>of(ImmutableList.<ExecutionNode>of(c2))), is(true));
    }
}
<code block>


package io.crate.executor.transport;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableMap;
import com.google.common.collect.ImmutableSet;
import com.google.common.util.concurrent.Futures;
import com.google.common.util.concurrent.ListenableFuture;
import io.crate.Constants;
import io.crate.core.collections.Bucket;
import io.crate.executor.Job;
import io.crate.executor.TaskResult;
import io.crate.integrationtests.SQLTransportIntegrationTest;
import io.crate.metadata.PartitionName;
import io.crate.metadata.TableIdent;
import io.crate.planner.IterablePlan;
import io.crate.planner.Plan;
import io.crate.planner.node.PlanNode;
import io.crate.planner.node.ddl.CreateTableNode;
import io.crate.planner.node.ddl.ESClusterUpdateSettingsNode;
import io.crate.planner.node.ddl.ESCreateTemplateNode;
import io.crate.planner.node.ddl.ESDeletePartitionNode;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.action.admin.indices.alias.Alias;
import org.elasticsearch.action.admin.indices.exists.indices.IndicesExistsRequest;
import org.elasticsearch.action.admin.indices.template.get.GetIndexTemplatesResponse;
import org.elasticsearch.cluster.metadata.IndexTemplateMetaData;
import org.elasticsearch.cluster.settings.ClusterDynamicSettings;
import org.elasticsearch.cluster.settings.DynamicSettings;
import org.elasticsearch.common.inject.Key;
import org.elasticsearch.common.settings.ImmutableSettings;
import org.elasticsearch.common.settings.Settings;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;

import java.util.Arrays;
import java.util.List;
import java.util.Map;
import java.util.UUID;

import static io.crate.testing.TestingHelpers.isRow;
import static org.elasticsearch.common.settings.ImmutableSettings.Builder.EMPTY_SETTINGS;
import static org.hamcrest.Matchers.contains;
import static org.hamcrest.Matchers.is;

public class TransportExecutorDDLTest extends SQLTransportIntegrationTest {

    static {
        ClassLoader.getSystemClassLoader().setDefaultAssertionStatus(true);
    }

    private TransportExecutor executor;

    private final static Map<String, Object> TEST_MAPPING = ImmutableMap.<String, Object>of(
            "properties", ImmutableMap.of(
                    "id", ImmutableMap.builder()
                            .put("type", "integer")
                            .put("store", false)
                            .put("index", "not_analyzed")
                            .put("doc_values", true).build(),
                    "name", ImmutableMap.builder()
                            .put("type", "string")
                            .put("store", false)
                            .put("index", "not_analyzed")
                            .put("doc_values", true).build(),
                    "names", ImmutableMap.builder()
                            .put("type", "string")
                            .put("store", false)
                            .put("index", "not_analyzed")
                            .put("doc_values", false).build()
            ));
    private final static Map<String, Object> TEST_PARTITIONED_MAPPING = ImmutableMap.<String, Object>of(
            "_meta", ImmutableMap.of(
                    "partitioned_by", ImmutableList.of(Arrays.asList("name", "string"))
            ),
            "properties", ImmutableMap.of(
                "id", ImmutableMap.builder()
                    .put("type", "integer")
                    .put("store", false)
                    .put("index", "not_analyzed")
                    .put("doc_values", true).build(),
                "names", ImmutableMap.builder()
                    .put("type", "string")
                    .put("store", false)
                    .put("index", "not_analyzed")
                    .put("doc_values", false).build()
            ));
    private final static Settings TEST_SETTINGS = ImmutableSettings.settingsBuilder()
            .put("number_of_replicas", 0)
            .put("number_of_shards", 2).build();

    @Before
    public void transportSetup() {
        executor = internalCluster().getInstance(TransportExecutor.class);
    }

    @Override
    @After
    public void tearDown() throws Exception {
        super.tearDown();
        client().admin().cluster().prepareUpdateSettings()
                .setPersistentSettingsToRemove(ImmutableSet.of("persistent.level"))
                .setTransientSettingsToRemove(ImmutableSet.of("persistent.level", "transient.uptime"))
                .execute().actionGet();
    }

    @Test
    public void testCreateTableTask() throws Exception {
        CreateTableNode createTableNode = CreateTableNode.createTableNode(
                new TableIdent(null, "test"),
                false,
                TEST_SETTINGS,
                TEST_MAPPING
        );
        Plan plan = new IterablePlan(UUID.randomUUID(), createTableNode);

        Job job = executor.newJob(plan);
        List<? extends ListenableFuture<TaskResult>> futures = executor.execute(job);
        ListenableFuture<List<TaskResult>> listenableFuture = Futures.allAsList(futures);
        Bucket rows = listenableFuture.get().get(0).rows();
        assertThat(rows, contains(isRow(1L)));
        execute("select * from information_schema.tables where table_name = 'test' and number_of_replicas = 0 and number_of_shards = 2");
        assertThat(response.rowCount(), is(1L));

        execute("select count(*) from information_schema.columns where table_name = 'test'");
        assertThat((Long)response.rows()[0][0], is(3L));
    }

    @Test
    public void testCreateTableWithOrphanedPartitions() throws Exception {
        String partitionName = new PartitionName("test", Arrays.asList(new BytesRef("foo"))).stringValue();
        client().admin().indices().prepareCreate(partitionName)
                .addMapping(Constants.DEFAULT_MAPPING_TYPE, TEST_PARTITIONED_MAPPING)
                .setSettings(TEST_SETTINGS)
                .execute().actionGet();
        ensureGreen();
        CreateTableNode createTableNode = CreateTableNode.createTableNode(
                new TableIdent(null, "test"),
                false,
                TEST_SETTINGS,
                TEST_MAPPING
        );
        Plan plan = new IterablePlan(UUID.randomUUID(), createTableNode);

        Job job = executor.newJob(plan);
        List<? extends ListenableFuture<TaskResult>> futures = executor.execute(job);
        ListenableFuture<List<TaskResult>> listenableFuture = Futures.allAsList(futures);
        Bucket objects = listenableFuture.get().get(0).rows();
        assertThat(objects, contains(isRow(1L)));

        execute("select * from information_schema.tables where table_name = 'test' and number_of_replicas = 0 and number_of_shards = 2");
        assertThat(response.rowCount(), is(1L));

        execute("select count(*) from information_schema.columns where table_name = 'test'");
        assertThat((Long)response.rows()[0][0], is(3L));


        assertThat(client().admin().indices().exists(new IndicesExistsRequest(partitionName)).actionGet().isExists(), is(false));
    }

    @Test
    public void testCreateTableWithOrphanedAlias() throws Exception {
        String partitionName = new PartitionName("test", Arrays.asList(new BytesRef("foo"))).stringValue();
        client().admin().indices().prepareCreate(partitionName)
                .addMapping(Constants.DEFAULT_MAPPING_TYPE, TEST_PARTITIONED_MAPPING)
                .setSettings(TEST_SETTINGS)
                .addAlias(new Alias("test"))
                .execute().actionGet();
        ensureGreen();
        CreateTableNode createTableNode = CreateTableNode.createTableNode(
                new TableIdent(null, "test"),
                false,
                TEST_SETTINGS,
                TEST_MAPPING
        );
        Plan plan = new IterablePlan(UUID.randomUUID(), createTableNode);

        Job job = executor.newJob(plan);
        List<? extends ListenableFuture<TaskResult>> futures = executor.execute(job);
        ListenableFuture<List<TaskResult>> listenableFuture = Futures.allAsList(futures);
        Bucket objects = listenableFuture.get().get(0).rows();
        assertThat(objects, contains(isRow(1L)));

        execute("select * from information_schema.tables where table_name = 'test' and number_of_replicas = 0 and number_of_shards = 2");
        assertThat(response.rowCount(), is(1L));

        execute("select count(*) from information_schema.columns where table_name = 'test'");
        assertThat((Long) response.rows()[0][0], is(3L));


        assertThat(client().admin().cluster().prepareState().execute().actionGet()
                .getState().metaData().aliases().containsKey("test"), is(false));

        assertThat(client().admin().indices().exists(new IndicesExistsRequest(partitionName)).actionGet().isExists(), is(false));
    }

    @Test
    public void testDeletePartitionTask() throws Exception {
        execute("create table t (id integer primary key, name string) partitioned by (id)");
        ensureYellow();

        execute("insert into t (id, name) values (1, 'Ford')");
        assertThat(response.rowCount(), is(1L));
        ensureYellow();

        execute("select * from information_schema.table_partitions where table_name = 't'");
        assertThat(response.rowCount(), is(1L));

        String partitionName = new PartitionName("t", ImmutableList.of(new BytesRef("1"))).stringValue();
        ESDeletePartitionNode deleteIndexNode = new ESDeletePartitionNode(partitionName);
        Plan plan = new IterablePlan(UUID.randomUUID(), deleteIndexNode);

        Job job = executor.newJob(plan);
        List<? extends ListenableFuture<TaskResult>> futures = executor.execute(job);
        ListenableFuture<List<TaskResult>> listenableFuture = Futures.allAsList(futures);
        Bucket objects = listenableFuture.get().get(0).rows();
        assertThat(objects, contains(isRow(-1L)));

        execute("select * from information_schema.table_partitions where table_name = 't'");
        assertThat(response.rowCount(), is(0L));
    }


    @Test
    public void testDeletePartitionTaskClosed() throws Exception {
        execute("create table t (id integer primary key, name string) partitioned by (id)");
        ensureYellow();

        execute("insert into t (id, name) values (1, 'Ford')");
        assertThat(response.rowCount(), is(1L));
        ensureYellow();

        String partitionName = new PartitionName("t", ImmutableList.of(new BytesRef("1"))).stringValue();
        assertTrue(client().admin().indices().prepareClose(partitionName).execute().actionGet().isAcknowledged());

        ESDeletePartitionNode deleteIndexNode = new ESDeletePartitionNode(partitionName);
        Plan plan = new IterablePlan(UUID.randomUUID(), deleteIndexNode);

        Job job = executor.newJob(plan);
        List<? extends ListenableFuture<TaskResult>> futures = executor.execute(job);
        ListenableFuture<List<TaskResult>> listenableFuture = Futures.allAsList(futures);
        Bucket objects = listenableFuture.get().get(0).rows();
        assertThat(objects, contains(isRow(-1L)));

        execute("select * from information_schema.table_partitions where table_name = 't'");
        assertThat(response.rowCount(), is(0L));
    }

    @Test
    public void testClusterUpdateSettingsTask() throws Exception {
        final String persistentSetting = "persistent.level";
        final String transientSetting = "transient.uptime";


        Key<DynamicSettings> dynamicSettingsKey = Key.get(DynamicSettings.class, ClusterDynamicSettings.class);
        for (DynamicSettings settings : internalCluster().getInstances(dynamicSettingsKey)) {
            settings.addDynamicSetting(persistentSetting);
            settings.addDynamicSetting(transientSetting);
        }


        Settings persistentSettings = ImmutableSettings.builder()
                .put(persistentSetting, "panic")
                .build();

        ESClusterUpdateSettingsNode node = new ESClusterUpdateSettingsNode(persistentSettings);

        Bucket objects = executePlanNode(node);

        assertThat(objects, contains(isRow(1L)));
        assertEquals("panic", client().admin().cluster().prepareState().execute().actionGet().getState().metaData().persistentSettings().get(persistentSetting));


        Settings transientSettings = ImmutableSettings.builder()
                .put(transientSetting, "123")
                .build();

        node = new ESClusterUpdateSettingsNode(EMPTY_SETTINGS, transientSettings);
        objects = executePlanNode(node);

        assertThat(objects, contains(isRow(1L)));
        assertEquals("123", client().admin().cluster().prepareState().execute().actionGet().getState().metaData().transientSettings().get(transientSetting));


        persistentSettings = ImmutableSettings.builder()
                .put(persistentSetting, "normal")
                .build();
        transientSettings = ImmutableSettings.builder()
                .put(transientSetting, "243")
                .build();

        node = new ESClusterUpdateSettingsNode(persistentSettings, transientSettings);
        objects = executePlanNode(node);

        assertThat(objects, contains(isRow(1L)));
        assertEquals("normal", client().admin().cluster().prepareState().execute().actionGet().getState().metaData().persistentSettings().get(persistentSetting));
        assertEquals("243", client().admin().cluster().prepareState().execute().actionGet().getState().metaData().transientSettings().get(transientSetting));
    }

    private Bucket executePlanNode(PlanNode node) throws InterruptedException, java.util.concurrent.ExecutionException {
        Plan plan = new IterablePlan(UUID.randomUUID(), node);
        Job job = executor.newJob(plan);
        List<? extends ListenableFuture<TaskResult>> futures = executor.execute(job);
        ListenableFuture<List<TaskResult>> listenableFuture = Futures.allAsList(futures);
        return listenableFuture.get().get(0).rows();
    }

    @Test
    public void testCreateIndexTemplateTask() throws Exception {
        Settings indexSettings = ImmutableSettings.builder()
                .put("number_of_replicas", 0)
                .put("number_of_shards", 2)
                .build();
        Map<String, Object> mapping = ImmutableMap.<String, Object>of(
                "properties", ImmutableMap.of(
                        "id", ImmutableMap.builder()
                                .put("type", "integer")
                                .put("store", false)
                                .put("index", "not_analyzed")
                                .put("doc_values", true).build(),
                        "name", ImmutableMap.builder()
                                .put("type", "string")
                                .put("store", false)
                                .put("index", "not_analyzed")
                                .put("doc_values", true).build(),
                        "names", ImmutableMap.builder()
                                .put("type", "string")
                                .put("store", false)
                                .put("index", "not_analyzed")
                                .put("doc_values", false).build()
                ),
                "_meta", ImmutableMap.of(
                        "partitioned_by", ImmutableList.<List<String>>of(
                                ImmutableList.of("name", "string")
                        )
                )
        );
        String templateName = PartitionName.templateName(null, "partitioned");
        String templatePrefix = PartitionName.templateName(null, "partitioned") + "*";
        final String alias = "aliasName";

        ESCreateTemplateNode planNode = new ESCreateTemplateNode(
                templateName,
                templatePrefix,
                indexSettings,
                mapping,
                alias);

        Bucket objects = executePlanNode(planNode);
        assertThat(objects, contains(isRow(1L)));

        refresh();

        GetIndexTemplatesResponse response = client().admin().indices()
                .prepareGetTemplates(".partitioned.partitioned.").execute().actionGet();

        assertThat(response.getIndexTemplates().size(), is(1));
        IndexTemplateMetaData templateMeta = response.getIndexTemplates().get(0);
        assertThat(templateMeta.getName(), is(".partitioned.partitioned."));
        assertThat(templateMeta.mappings().get(Constants.DEFAULT_MAPPING_TYPE).string(),
                is("{\"default\":" +
                        "{\"properties\":{" +
                        "\"id\":{\"type\":\"integer\",\"store\":false,\"index\":\"not_analyzed\",\"doc_values\":true}," +
                        "\"name\":{\"type\":\"string\",\"store\":false,\"index\":\"not_analyzed\",\"doc_values\":true}," +
                        "\"names\":{\"type\":\"string\",\"store\":false,\"index\":\"not_analyzed\",\"doc_values\":false}" +
                        "}," +
                        "\"_meta\":{" +
                        "\"partitioned_by\":[[\"name\",\"string\"]]" +
                        "}}}"));
        assertThat(templateMeta.template(), is(".partitioned.partitioned.*"));
        assertThat(templateMeta.settings().toDelimitedString(','),
                is("index.number_of_replicas=0,index.number_of_shards=2,"));
        assertThat(templateMeta.aliases().get(alias).alias(), is(alias));
    }
}

<code block>
package io.crate.planner;

import com.google.common.collect.Iterables;
import io.crate.Constants;
import io.crate.analyze.Analyzer;
import io.crate.analyze.BaseAnalyzerTest;
import io.crate.analyze.ParameterContext;
import io.crate.analyze.WhereClause;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.core.collections.TreeMapBuilder;
import io.crate.exceptions.UnsupportedFeatureException;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.*;
import io.crate.metadata.blob.BlobSchemaInfo;
import io.crate.metadata.blob.BlobTableInfo;
import io.crate.metadata.doc.DocSysColumns;
import io.crate.metadata.sys.SysClusterTableInfo;
import io.crate.metadata.sys.SysNodesTableInfo;
import io.crate.metadata.sys.SysSchemaInfo;
import io.crate.metadata.sys.SysShardsTableInfo;
import io.crate.metadata.table.ColumnPolicy;
import io.crate.metadata.table.SchemaInfo;
import io.crate.metadata.table.TableInfo;
import io.crate.metadata.table.TestingTableInfo;
import io.crate.operation.aggregation.impl.AggregationImplModule;
import io.crate.operation.operator.OperatorModule;
import io.crate.operation.predicate.PredicateModule;
import io.crate.operation.projectors.FetchProjector;
import io.crate.operation.scalar.ScalarFunctionModule;
import io.crate.planner.node.PlanNode;
import io.crate.planner.node.ddl.DropTableNode;
import io.crate.planner.node.ddl.ESClusterUpdateSettingsNode;
import io.crate.planner.node.dml.*;
import io.crate.planner.node.dql.*;
import io.crate.planner.node.management.KillPlan;
import io.crate.planner.projection.*;
import io.crate.planner.symbol.*;
import io.crate.sql.parser.SqlParser;
import io.crate.test.integration.CrateUnitTest;
import io.crate.testing.TestingHelpers;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.action.admin.indices.template.put.TransportPutIndexTemplateAction;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.cluster.ClusterState;
import org.elasticsearch.cluster.metadata.IndexTemplateMetaData;
import org.elasticsearch.cluster.metadata.MetaData;
import org.elasticsearch.cluster.node.DiscoveryNode;
import org.elasticsearch.cluster.node.DiscoveryNodes;
import org.elasticsearch.common.collect.ImmutableOpenMap;
import org.elasticsearch.common.inject.Injector;
import org.elasticsearch.common.inject.ModulesBuilder;
import org.elasticsearch.index.shard.ShardId;
import org.elasticsearch.threadpool.ThreadPool;
import org.hamcrest.Matchers;
import org.hamcrest.core.Is;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;

import java.util.*;
import java.util.concurrent.TimeUnit;

import static io.crate.testing.TestingHelpers.*;
import static org.hamcrest.Matchers.*;
import static org.hamcrest.core.Is.is;
import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;

public class PlannerTest extends CrateUnitTest {

    private Analyzer analyzer;
    private Planner planner;
    Routing shardRouting = new Routing(TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder()
            .put("nodeOne", TreeMapBuilder.<String, List<Integer>>newMapBuilder().put("t1", Arrays.asList(1, 2)).map())
            .put("nodeTow", TreeMapBuilder.<String, List<Integer>>newMapBuilder().put("t1", Arrays.asList(3, 4)).map())
            .map());

    Routing nodesRouting = new Routing(TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder()
            .put("nodeOne", TreeMapBuilder.<String, List<Integer>>newMapBuilder().map())
            .put("nodeTow", TreeMapBuilder.<String, List<Integer>>newMapBuilder().map())
            .map());

    final Routing partedRouting = new Routing(TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder()
            .put("nodeOne", TreeMapBuilder.<String, List<Integer>>newMapBuilder().put(".partitioned.parted.04232chj", Arrays.asList(1, 2)).map())
            .put("nodeTow", TreeMapBuilder.<String, List<Integer>>newMapBuilder().map())
            .map());

    final Routing clusteredPartedRouting = new Routing(TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder()
            .put("nodeOne", TreeMapBuilder.<String, List<Integer>>newMapBuilder().put(".partitioned.clustered_parted.04732cpp6ks3ed1o60o30c1g",  Arrays.asList(1, 2)).map())
            .put("nodeTwo", TreeMapBuilder.<String, List<Integer>>newMapBuilder().put(".partitioned.clustered_parted.04732cpp6ksjcc9i60o30c1g",  Arrays.asList(3)).map())
            .map());

    private ClusterService clusterService;

    private final static String LOCAL_NODE_ID = "foo";
    private ThreadPool threadPool;


    @Before
    public void prepare() throws Exception {
        threadPool = TestingHelpers.newMockedThreadPool();
        Injector injector = new ModulesBuilder()
                .add(new TestModule())
                .add(new AggregationImplModule())
                .add(new ScalarFunctionModule())
                .add(new PredicateModule())
                .add(new OperatorModule())
                .createInjector();
        analyzer = injector.getInstance(Analyzer.class);
        planner = injector.getInstance(Planner.class);
    }

    @After
    public void after() throws Exception {
        threadPool.shutdown();
        threadPool.awaitTermination(1, TimeUnit.SECONDS);
    }


    class TestModule extends MetaDataModule {

        @Override
        protected void configure() {
            bind(ThreadPool.class).toInstance(threadPool);
            clusterService = mock(ClusterService.class);
            DiscoveryNode localNode = mock(DiscoveryNode.class);
            when(localNode.id()).thenReturn(LOCAL_NODE_ID);
            when(clusterService.localNode()).thenReturn(localNode);
            ClusterState clusterState = mock(ClusterState.class);
            MetaData metaData = mock(MetaData.class);
            when(metaData.concreteAllOpenIndices()).thenReturn(new String[0]);
            when(metaData.getTemplates()).thenReturn(ImmutableOpenMap.<String, IndexTemplateMetaData>of());
            when(metaData.templates()).thenReturn(ImmutableOpenMap.<String, IndexTemplateMetaData>of());
            when(clusterState.metaData()).thenReturn(metaData);
            DiscoveryNodes nodes = mock(DiscoveryNodes.class);
            DiscoveryNode node = mock(DiscoveryNode.class);
            when(clusterService.state()).thenReturn(clusterState);
            when(clusterState.nodes()).thenReturn(nodes);
            ImmutableOpenMap<String, DiscoveryNode> dataNodes =
                    ImmutableOpenMap.<String, DiscoveryNode>builder().fPut("foo", node).build();
            when(nodes.dataNodes()).thenReturn(dataNodes);
            when(nodes.localNodeId()).thenReturn(LOCAL_NODE_ID);
            FulltextAnalyzerResolver fulltextAnalyzerResolver = mock(FulltextAnalyzerResolver.class);
            bind(FulltextAnalyzerResolver.class).toInstance(fulltextAnalyzerResolver);
            bind(ClusterService.class).toInstance(clusterService);
            bind(TransportPutIndexTemplateAction.class).toInstance(mock(TransportPutIndexTemplateAction.class));
            super.configure();
        }

        @Override
        protected void bindSchemas() {
            super.bindSchemas();
            SchemaInfo schemaInfo = mock(SchemaInfo.class);
            TableIdent userTableIdent = new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "users");
            TableInfo userTableInfo = TestingTableInfo.builder(userTableIdent, RowGranularity.DOC, shardRouting)
                    .add("name", DataTypes.STRING, null)
                    .add("id", DataTypes.LONG, null)
                    .add("date", DataTypes.TIMESTAMP, null)
                    .add("text", DataTypes.STRING, null, ReferenceInfo.IndexType.ANALYZED)
                    .add("no_index", DataTypes.STRING, null, ReferenceInfo.IndexType.NO)
                    .addPrimaryKey("id")
                    .clusteredBy("id")
                    .build();
            when(userTableInfo.schemaInfo().name()).thenReturn(ReferenceInfos.DEFAULT_SCHEMA_NAME);
            TableIdent charactersTableIdent = new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "characters");
            TableInfo charactersTableInfo = TestingTableInfo.builder(charactersTableIdent, RowGranularity.DOC, shardRouting)
                    .add("name", DataTypes.STRING, null)
                    .add("id", DataTypes.STRING, null)
                    .addPrimaryKey("id")
                    .clusteredBy("id")
                    .build();
            when(charactersTableInfo.schemaInfo().name()).thenReturn(ReferenceInfos.DEFAULT_SCHEMA_NAME);
            TableIdent partedTableIdent = new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "parted");
            TableInfo partedTableInfo = TestingTableInfo.builder(partedTableIdent, RowGranularity.DOC, partedRouting)
                    .add("name", DataTypes.STRING, null)
                    .add("id", DataTypes.STRING, null)
                    .add("date", DataTypes.TIMESTAMP, null, true)
                    .addPartitions(
                            new PartitionName("parted", new ArrayList<BytesRef>(){{add(null);}}).stringValue(), 
                            new PartitionName("parted", Arrays.asList(new BytesRef("0"))).stringValue(),
                            new PartitionName("parted", Arrays.asList(new BytesRef("123"))).stringValue()
                    )
                    .addPrimaryKey("id")
                    .addPrimaryKey("date")
                    .clusteredBy("id")
                    .build();
            when(partedTableInfo.schemaInfo().name()).thenReturn(ReferenceInfos.DEFAULT_SCHEMA_NAME);
            TableIdent emptyPartedTableIdent = new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "empty_parted");
            TableInfo emptyPartedTableInfo = TestingTableInfo.builder(partedTableIdent, RowGranularity.DOC, shardRouting)
                    .add("name", DataTypes.STRING, null)
                    .add("id", DataTypes.STRING, null)
                    .add("date", DataTypes.TIMESTAMP, null, true)
                    .addPrimaryKey("id")
                    .addPrimaryKey("date")
                    .clusteredBy("id")
                    .build();
            TableIdent multiplePartitionedTableIdent= new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "multi_parted");
            TableInfo multiplePartitionedTableInfo = new TestingTableInfo.Builder(
                    multiplePartitionedTableIdent, RowGranularity.DOC, new Routing())
                    .add("id", DataTypes.INTEGER, null)
                    .add("date", DataTypes.TIMESTAMP, null, true)
                    .add("num", DataTypes.LONG, null)
                    .add("obj", DataTypes.OBJECT, null, ColumnPolicy.DYNAMIC)
                    .add("obj", DataTypes.STRING, Arrays.asList("name"), true)

                    .addPartitions(
                            new PartitionName("multi_parted", Arrays.asList(new BytesRef("1395874800000"), new BytesRef("0"))).stringValue(),
                            new PartitionName("multi_parted", Arrays.asList(new BytesRef("1395961200000"), new BytesRef("-100"))).stringValue(),
                            new PartitionName("multi_parted", Arrays.asList(null, new BytesRef("-100"))).stringValue())
                    .build();
            TableIdent clusteredByParitionedIdent = new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "clustered_parted");
            TableInfo clusteredByPartitionedTableInfo = new TestingTableInfo.Builder(
                    multiplePartitionedTableIdent, RowGranularity.DOC, clusteredPartedRouting)
                    .add("id", DataTypes.INTEGER, null)
                    .add("date", DataTypes.TIMESTAMP, null, true)
                    .add("city", DataTypes.STRING, null)
                    .clusteredBy("city")
                    .addPartitions(
                            new PartitionName("clustered_parted", Arrays.asList(new BytesRef("1395874800000"))).stringValue(),
                            new PartitionName("clustered_parted", Arrays.asList(new BytesRef("1395961200000"))).stringValue())
                    .build();
            when(emptyPartedTableInfo.schemaInfo().name()).thenReturn(ReferenceInfos.DEFAULT_SCHEMA_NAME);
            when(schemaInfo.getTableInfo(charactersTableIdent.name())).thenReturn(charactersTableInfo);
            when(schemaInfo.getTableInfo(userTableIdent.name())).thenReturn(userTableInfo);
            when(schemaInfo.getTableInfo(partedTableIdent.name())).thenReturn(partedTableInfo);
            when(schemaInfo.getTableInfo(emptyPartedTableIdent.name())).thenReturn(emptyPartedTableInfo);
            when(schemaInfo.getTableInfo(multiplePartitionedTableIdent.name())).thenReturn(multiplePartitionedTableInfo);
            when(schemaInfo.getTableInfo(clusteredByParitionedIdent.name())).thenReturn(clusteredByPartitionedTableInfo);
            when(schemaInfo.getTableInfo(BaseAnalyzerTest.IGNORED_NESTED_TABLE_IDENT.name())).thenReturn(BaseAnalyzerTest.IGNORED_NESTED_TABLE_INFO);
            schemaBinder.addBinding(ReferenceInfos.DEFAULT_SCHEMA_NAME).toInstance(schemaInfo);
            schemaBinder.addBinding(SysSchemaInfo.NAME).toInstance(mockSysSchemaInfo());
            schemaBinder.addBinding(BlobSchemaInfo.NAME).toInstance(mockBlobSchemaInfo());
        }

        private SchemaInfo mockBlobSchemaInfo(){
            BlobSchemaInfo blobSchemaInfo = mock(BlobSchemaInfo.class);
            BlobTableInfo tableInfo = mock(BlobTableInfo.class);
            when(blobSchemaInfo.getTableInfo("screenshots")).thenReturn(tableInfo);
            when(tableInfo.schemaInfo()).thenReturn(blobSchemaInfo);
            return blobSchemaInfo;
        }

        private SchemaInfo mockSysSchemaInfo() {
            SchemaInfo schemaInfo = mock(SchemaInfo.class);
            when(schemaInfo.name()).thenReturn(SysSchemaInfo.NAME);
            when(schemaInfo.systemSchema()).thenReturn(true);

            TableInfo sysClusterTableInfo = TestingTableInfo.builder(
                    SysClusterTableInfo.IDENT,


                    RowGranularity.DOC,
                    SysClusterTableInfo.ROUTING
            ).schemaInfo(schemaInfo).add("name", DataTypes.STRING, null).schemaInfo(schemaInfo).build();
            when(schemaInfo.getTableInfo(sysClusterTableInfo.ident().name())).thenReturn(sysClusterTableInfo);

            TableInfo sysNodesTableInfo = TestingTableInfo.builder(
                    SysNodesTableInfo.IDENT,
                    RowGranularity.NODE,
                    nodesRouting)
                    .schemaInfo(schemaInfo)
                    .add("name", DataTypes.STRING, null).schemaInfo(schemaInfo).build();

            when(schemaInfo.getTableInfo(sysNodesTableInfo.ident().name())).thenReturn(sysNodesTableInfo);

            TableInfo sysShardsTableInfo = TestingTableInfo.builder(
                    SysShardsTableInfo.IDENT,
                    RowGranularity.SHARD,
                    nodesRouting
            ).add("id", DataTypes.INTEGER, null)
             .add("table_name", DataTypes.STRING, null)
             .schemaInfo(schemaInfo).build();
            when(schemaInfo.getTableInfo(sysShardsTableInfo.ident().name())).thenReturn(sysShardsTableInfo);
            when(schemaInfo.systemSchema()).thenReturn(true);
            return schemaInfo;
        }
    }

    private Plan plan(String statement) {
        return planner.plan(analyzer.analyze(SqlParser.createStatement(statement),
                new ParameterContext(new Object[0], new Object[0][], ReferenceInfos.DEFAULT_SCHEMA_NAME)));
    }

    @Test
    public void testGroupByWithAggregationStringLiteralArguments() {
        CollectNode collectNode = ((DistributedGroupBy) plan("select count('foo'), name from users group by name")).collectNode();


        GroupProjection groupProjection = (GroupProjection) collectNode.projections().get(0);
        Aggregation aggregation = groupProjection.values().get(0);
    }

    @Test
    public void testGroupByWithAggregationPlan() throws Exception {
        DistributedGroupBy distributedGroupBy = (DistributedGroupBy) plan(
                "select count(*), name from users group by name");


        CollectNode collectNode = distributedGroupBy.collectNode();
        assertThat(collectNode.hasDistributingDownstreams(), is(true));
        assertThat(collectNode.downstreamNodes().size(), is(2));
        assertThat(collectNode.maxRowGranularity(), is(RowGranularity.DOC));
        assertThat(collectNode.executionNodes().size(), is(2));
        assertThat(collectNode.toCollect().size(), is(1));
        assertThat(collectNode.projections().size(), is(1));
        assertThat(collectNode.projections().get(0), instanceOf(GroupProjection.class));
        assertThat(collectNode.outputTypes().size(), is(2));
        assertEquals(DataTypes.STRING, collectNode.outputTypes().get(0));
        assertEquals(DataTypes.UNDEFINED, collectNode.outputTypes().get(1));

        MergeNode mergeNode = distributedGroupBy.reducerMergeNode();

        assertThat(mergeNode.numUpstreams(), is(2));
        assertThat(mergeNode.executionNodes().size(), is(2));
        assertEquals(mergeNode.inputTypes(), collectNode.outputTypes());
        assertThat(mergeNode.projections().size(), is(2)); 
        assertThat(mergeNode.projections().get(1), instanceOf(TopNProjection.class));

        assertThat(mergeNode.projections().get(0), instanceOf(GroupProjection.class));
        GroupProjection groupProjection = (GroupProjection) mergeNode.projections().get(0);
        InputColumn inputColumn = (InputColumn) groupProjection.values().get(0).inputs().get(0);
        assertThat(inputColumn.index(), is(1));

        assertThat(mergeNode.outputTypes().size(), is(2));
        assertEquals(DataTypes.LONG, mergeNode.outputTypes().get(0));
        assertEquals(DataTypes.STRING, mergeNode.outputTypes().get(1));

        MergeNode localMerge = distributedGroupBy.localMergeNode();

        assertThat(localMerge.numUpstreams(), is(2));
        assertThat(localMerge.executionNodes().size(), is(1));
        assertThat(Iterables.getOnlyElement(localMerge.executionNodes()), is(LOCAL_NODE_ID));
        assertEquals(mergeNode.outputTypes(), localMerge.inputTypes());

        assertThat(localMerge.projections().get(0), instanceOf(TopNProjection.class));
        TopNProjection topN = (TopNProjection) localMerge.projections().get(0);
        assertThat(topN.outputs().size(), is(2));

        assertEquals(DataTypes.LONG, localMerge.outputTypes().get(0));
        assertEquals(DataTypes.STRING, localMerge.outputTypes().get(1));

    }

    @Test
    public void testGetPlan() throws Exception {
        IterablePlan plan = (IterablePlan)  plan("select name from users where id = 1");
        Iterator<PlanNode> iterator = plan.iterator();
        ESGetNode node = (ESGetNode) iterator.next();
        assertThat(node.tableInfo().ident().name(), is("users"));
        assertThat(node.docKeys().getOnlyKey(), isDocKey(1L));
        assertThat(node.outputs().size(), is(1));
    }

    @Test
    public void testGetWithVersion() throws Exception{
        expectedException.expect(VersionInvalidException.class);
        expectedException.expectMessage("\"_version\" column is not valid in the WHERE clause of a SELECT statement");
        plan("select name from users where id = 1 and _version = 1");
    }

    @Test
    public void testGetPlanStringLiteral() throws Exception {
        IterablePlan plan = (IterablePlan) plan("select name from characters where id = 'one'");
        Iterator<PlanNode> iterator = plan.iterator();
        ESGetNode node = (ESGetNode) iterator.next();
        assertThat(node.tableInfo().ident().name(), is("characters"));
        assertThat(node.docKeys().getOnlyKey(), isDocKey("one"));
        assertFalse(iterator.hasNext());
        assertThat(node.outputs().size(), is(1));
    }

    @Test
    public void testGetPlanPartitioned() throws Exception {
        IterablePlan plan = (IterablePlan) plan("select name, date from parted where id = 'one' and date = 0");
        Iterator<PlanNode> iterator = plan.iterator();
        PlanNode node = iterator.next();
        assertThat(node, instanceOf(ESGetNode.class));
        ESGetNode getNode = (ESGetNode) node;
        assertThat(getNode.tableInfo().ident().name(), is("parted"));
        assertThat(getNode.docKeys().getOnlyKey(), isDocKey("one", 0L));


        assertEquals(DataTypes.STRING, getNode.outputTypes().get(0));
        assertEquals(DataTypes.TIMESTAMP, getNode.outputTypes().get(1));
    }

    @Test
    public void testMultiGetPlan() throws Exception {
        IterablePlan plan = (IterablePlan) plan("select name from users where id in (1, 2)");
        Iterator<PlanNode> iterator = plan.iterator();
        ESGetNode node = (ESGetNode) iterator.next();
        assertThat(node.docKeys().size(), is(2));
        assertThat(node.docKeys(), containsInAnyOrder(isDocKey(1L), isDocKey(2L)));
    }

    @Test
    public void testDeletePlan() throws Exception {
        IterablePlan plan = (IterablePlan) plan("delete from users where id = 1");
        Iterator<PlanNode> iterator = plan.iterator();
        ESDeleteNode node = (ESDeleteNode) iterator.next();
        assertThat(node.tableInfo().ident().name(), is("users"));
        assertThat(node.docKeys().size(), is(1));
        assertThat(node.docKeys().get(0), isDocKey(1L));
        assertFalse(iterator.hasNext());
    }

    @Test
    public void testMultiDeletePlan() throws Exception {
        IterablePlan plan = (IterablePlan) plan("delete from users where id in (1, 2)");
        Iterator<PlanNode> iterator = plan.iterator();
        assertThat(iterator.next(), instanceOf(ESDeleteByQueryNode.class));
    }

    @Test
    public void testGroupByWithAggregationAndLimit() throws Exception {
        DistributedGroupBy distributedGroupBy = (DistributedGroupBy) plan(
                "select count(*), name from users group by name limit 1 offset 1");


        MergeNode mergeNode = distributedGroupBy.reducerMergeNode();
        assertThat(mergeNode.projections().get(0), instanceOf(GroupProjection.class));
        assertThat(mergeNode.projections().get(1), instanceOf(TopNProjection.class));



        TopNProjection topN = (TopNProjection) mergeNode.projections().get(1);
        assertThat(topN.limit(), is(2));
        assertThat(topN.offset(), is(0));


        DQLPlanNode dqlPlanNode = distributedGroupBy.localMergeNode();
        assertThat(dqlPlanNode.projections().get(0), instanceOf(TopNProjection.class));
        topN = (TopNProjection) dqlPlanNode.projections().get(0);
        assertThat(topN.limit(), is(1));
        assertThat(topN.offset(), is(1));
        assertThat(topN.outputs().get(0), instanceOf(InputColumn.class));
        assertThat(((InputColumn) topN.outputs().get(0)).index(), is(0));
        assertThat(topN.outputs().get(1), instanceOf(InputColumn.class));
        assertThat(((InputColumn) topN.outputs().get(1)).index(), is(1));
    }

    @Test
    public void testGlobalAggregationPlan() throws Exception {
        GlobalAggregate globalAggregate = (GlobalAggregate) plan("select count(name) from users");
        CollectNode collectNode = globalAggregate.collectNode();

        assertEquals(DataTypes.UNDEFINED, collectNode.outputTypes().get(0));
        assertThat(collectNode.maxRowGranularity(), is(RowGranularity.DOC));
        assertThat(collectNode.projections().size(), is(1));
        assertThat(collectNode.projections().get(0), instanceOf(AggregationProjection.class));

        MergeNode mergeNode = globalAggregate.mergeNode();

        assertEquals(DataTypes.UNDEFINED, mergeNode.inputTypes().get(0));
        assertEquals(DataTypes.LONG, mergeNode.outputTypes().get(0));
    }

    @Test
    public void testGroupByOnNodeLevel() throws Exception {
        NonDistributedGroupBy planNode = (NonDistributedGroupBy) plan(
                "select count(*), name from sys.nodes group by name");
        CollectNode collectNode = planNode.collectNode();
        assertFalse(collectNode.hasDistributingDownstreams());
        assertEquals(DataTypes.STRING, collectNode.outputTypes().get(0));
        assertEquals(DataTypes.UNDEFINED, collectNode.outputTypes().get(1));

        MergeNode mergeNode = planNode.localMergeNode();
        assertThat(mergeNode.numUpstreams(), is(2));
        assertThat(mergeNode.projections().size(), is(2));

        assertEquals(DataTypes.LONG, mergeNode.outputTypes().get(0));
        assertEquals(DataTypes.STRING, mergeNode.outputTypes().get(1));

        GroupProjection groupProjection = (GroupProjection) mergeNode.projections().get(0);
        assertThat(groupProjection.keys().size(), is(1));
        assertThat(((InputColumn) groupProjection.outputs().get(0)).index(), is(0));
        assertThat(groupProjection.outputs().get(1), is(instanceOf(Aggregation.class)));
        assertThat(((Aggregation) groupProjection.outputs().get(1)).functionIdent().name(), is("count"));
        assertThat(((Aggregation) groupProjection.outputs().get(1)).fromStep(), is(Aggregation.Step.PARTIAL));
        assertThat(((Aggregation)groupProjection.outputs().get(1)).toStep(), is(Aggregation.Step.FINAL));

        TopNProjection projection = (TopNProjection) mergeNode.projections().get(1);
        assertThat(((InputColumn) projection.outputs().get(0)).index(), is(1));
        assertThat(((InputColumn) projection.outputs().get(1)).index(), is(0));

    }

    @Test
    public void testShardPlan() throws Exception {
        QueryAndFetch planNode = (QueryAndFetch) plan("select id from sys.shards order by id limit 10");
        CollectNode collectNode = planNode.collectNode();

        assertEquals(DataTypes.INTEGER, collectNode.outputTypes().get(0));
        assertThat(collectNode.maxRowGranularity(), is(RowGranularity.SHARD));

        MergeNode mergeNode = planNode.localMergeNode();

        assertThat(mergeNode.inputTypes().size(), is(1));
        assertEquals(DataTypes.INTEGER, mergeNode.inputTypes().get(0));
        assertThat(mergeNode.outputTypes().size(), is(1));
        assertEquals(DataTypes.INTEGER, mergeNode.outputTypes().get(0));

        assertThat(mergeNode.numUpstreams(), is(2));
    }

    @Test
    public void testQueryThenFetchPlan() throws Exception {
        Plan plan = plan("select name from users where name = 'x' order by id limit 10");
        assertThat(plan, instanceOf(QueryThenFetch.class));
        CollectNode collectNode = ((QueryThenFetch) plan).collectNode();
        assertTrue(collectNode.whereClause().hasQuery());
        assertFalse(collectNode.isPartitioned());

        DQLPlanNode resultNode = ((QueryThenFetch) plan).resultNode();
        assertThat(resultNode.outputTypes().size(), is(1));
        assertEquals(DataTypes.STRING, resultNode.outputTypes().get(0));

        assertThat(resultNode, instanceOf(MergeNode.class));
        MergeNode mergeNode = (MergeNode) resultNode;
        assertTrue(mergeNode.finalProjection().isPresent());

        Projection lastProjection = mergeNode.finalProjection().get();
        assertThat(lastProjection, instanceOf(FetchProjection.class));
        FetchProjection fetchProjection = (FetchProjection) lastProjection;
        assertThat(fetchProjection.outputs().size(), is(1));
        assertThat(fetchProjection.outputs().get(0), isReference("_doc['name']"));
    }

    @Test
    public void testQueryThenFetchPlanNoFetch() throws Exception {


        Plan plan = plan("select name from users where name = 'x' order by name limit 10");
        assertThat(plan, instanceOf(QueryThenFetch.class));
        CollectNode collectNode = ((QueryThenFetch) plan).collectNode();
        assertTrue(collectNode.whereClause().hasQuery());
        assertFalse(collectNode.isPartitioned());

        DQLPlanNode resultNode = ((QueryThenFetch) plan).resultNode();
        assertThat(resultNode.outputTypes().size(), is(1));
        assertEquals(DataTypes.STRING, resultNode.outputTypes().get(0));

        assertThat(resultNode, instanceOf(MergeNode.class));
        MergeNode mergeNode = (MergeNode) resultNode;
        assertTrue(mergeNode.finalProjection().isPresent());

        Projection lastProjection = mergeNode.finalProjection().get();
        assertThat(lastProjection, instanceOf(TopNProjection.class));
        TopNProjection topNProjection = (TopNProjection) lastProjection;
        assertThat(topNProjection.outputs().size(), is(1));
    }

    @Test
    public void testQueryThenFetchPlanDefaultLimit() throws Exception {
        QueryThenFetch plan = (QueryThenFetch)plan("select name from users");
        CollectNode collectNode = plan.collectNode();
        assertThat(collectNode.limit(), is(Constants.DEFAULT_SELECT_LIMIT));

        MergeNode mergeNode = plan.mergeNode();
        assertThat(mergeNode.projections().size(), is(2));
        assertThat(mergeNode.finalProjection().get(), instanceOf(FetchProjection.class));
        TopNProjection topN = (TopNProjection)mergeNode.projections().get(0);
        assertThat(topN.limit(), is(Constants.DEFAULT_SELECT_LIMIT));
        assertThat(topN.offset(), is(0));
        assertNull(topN.orderBy());

        FetchProjection fetchProjection = (FetchProjection)mergeNode.projections().get(1);
        assertThat(fetchProjection.bulkSize(), is(FetchProjector.NO_BULK_REQUESTS));


        plan = (QueryThenFetch)plan("select name from users offset 20");
        collectNode = plan.collectNode();
        assertThat(collectNode.limit(), is(Constants.DEFAULT_SELECT_LIMIT + 20));

        mergeNode = plan.mergeNode();
        assertThat(mergeNode.projections().size(), is(2));
        assertThat(mergeNode.finalProjection().get(), instanceOf(FetchProjection.class));
        topN = (TopNProjection)mergeNode.projections().get(0);
        assertThat(topN.limit(), is(Constants.DEFAULT_SELECT_LIMIT));
        assertThat(topN.offset(), is(20));
        assertNull(topN.orderBy());

        fetchProjection = (FetchProjection)mergeNode.projections().get(1);
        assertThat(fetchProjection.bulkSize(), is(FetchProjector.NO_BULK_REQUESTS));
    }

    @Test
    public void testQueryThenFetchPlanHighLimit() throws Exception {
        QueryThenFetch plan = (QueryThenFetch)plan("select name from users limit 100000");
        CollectNode collectNode = plan.collectNode();
        assertThat(collectNode.limit(), is(100_000));

        MergeNode mergeNode = plan.mergeNode();
        assertThat(mergeNode.projections().size(), is(2));
        assertThat(mergeNode.finalProjection().get(), instanceOf(FetchProjection.class));
        TopNProjection topN = (TopNProjection)mergeNode.projections().get(0);
        assertThat(topN.limit(), is(100_000));
        assertThat(topN.offset(), is(0));
        assertNull(topN.orderBy());

        FetchProjection fetchProjection = (FetchProjection)mergeNode.projections().get(1);
        assertThat(fetchProjection.bulkSize(), is(Constants.DEFAULT_SELECT_LIMIT));


        plan = (QueryThenFetch)plan("select name from users limit 100000 offset 20");
        collectNode = plan.collectNode();
        assertThat(collectNode.limit(), is(100_000 + 20));

        mergeNode = plan.mergeNode();
        assertThat(mergeNode.projections().size(), is(2));
        assertThat(mergeNode.finalProjection().get(), instanceOf(FetchProjection.class));
        topN = (TopNProjection)mergeNode.projections().get(0);
        assertThat(topN.limit(), is(100_000));
        assertThat(topN.offset(), is(20));
        assertNull(topN.orderBy());

        fetchProjection = (FetchProjection)mergeNode.projections().get(1);
        assertThat(fetchProjection.bulkSize(), is(Constants.DEFAULT_SELECT_LIMIT));
    }

    @Test
    public void testQueryThenFetchPlanPartitioned() throws Exception {
        Plan plan = plan("select id, name, date from parted where date > 0 and name = 'x' order by id limit 10");
        assertThat(plan, instanceOf(QueryThenFetch.class));
        CollectNode collectNode = ((QueryThenFetch) plan).collectNode();

        List<String> indices = new ArrayList<>();
        Map<String, Map<String, List<Integer>>> locations = collectNode.routing().locations();
        for (Map.Entry<String, Map<String, List<Integer>>> entry : locations.entrySet()) {
            indices.addAll(entry.getValue().keySet());
        }
        assertThat(indices, Matchers.contains(
                new PartitionName("parted", Arrays.asList(new BytesRef("123"))).stringValue()));

        assertTrue(collectNode.whereClause().hasQuery());
        assertTrue(collectNode.isPartitioned());

        DQLPlanNode resultNode = ((QueryThenFetch) plan).resultNode();
        assertThat(resultNode.outputTypes().size(), is(3));
    }

    @Test
    public void testQueryThenFetchPlanFunction() throws Exception {
        Plan plan = plan("select format('Hi, my name is %s', name), name from users where name = 'x' order by id limit 10");
        assertThat(plan, instanceOf(QueryThenFetch.class));
        CollectNode collectNode = ((QueryThenFetch) plan).collectNode();

        assertTrue(collectNode.whereClause().hasQuery());
        assertFalse(collectNode.isPartitioned());

        DQLPlanNode resultNode = ((QueryThenFetch) plan).resultNode();
        assertThat(resultNode.outputTypes().size(), is(2));
        assertEquals(DataTypes.STRING, resultNode.outputTypes().get(0));
        assertEquals(DataTypes.STRING, resultNode.outputTypes().get(1));

        assertThat(resultNode, instanceOf(MergeNode.class));
        MergeNode mergeNode = (MergeNode) resultNode;
        assertTrue(mergeNode.finalProjection().isPresent());

        Projection lastProjection = mergeNode.finalProjection().get();
        assertThat(lastProjection, instanceOf(FetchProjection.class));
        FetchProjection fetchProjection = (FetchProjection) lastProjection;
        assertThat(fetchProjection.outputs().size(), is(2));
        assertThat(fetchProjection.outputs().get(0), isFunction("format"));
        assertThat(fetchProjection.outputs().get(1), isReference("_doc['name']"));

    }

    @Test
    public void testInsertPlan() throws Exception {
        Upsert plan = (Upsert) plan("insert into users (id, name) values (42, 'Deep Thought')");

        assertThat(plan.nodes().size(), is(1));

        PlanNode next = ((IterablePlan) plan.nodes().get(0)).iterator().next();
        assertThat(next, instanceOf(SymbolBasedUpsertByIdNode.class));

        SymbolBasedUpsertByIdNode updateNode = (SymbolBasedUpsertByIdNode)next;

        assertThat(updateNode.insertColumns().length, is(2));
        Reference idRef = updateNode.insertColumns()[0];
        assertThat(idRef.ident().columnIdent().fqn(), is("id"));
        Reference nameRef = updateNode.insertColumns()[1];
        assertThat(nameRef.ident().columnIdent().fqn(), is("name"));

        assertThat(updateNode.items().size(), is(1));
        SymbolBasedUpsertByIdNode.Item item = updateNode.items().get(0);
        assertThat(item.index(), is("users"));
        assertThat(item.id(), is("42"));
        assertThat(item.routing(), is("42"));

        assertThat(item.insertValues().length, is(2));
        assertThat((Long)item.insertValues()[0], is(42L));
        assertThat((BytesRef) item.insertValues()[1], is(new BytesRef("Deep Thought")));
    }

    @Test
    public void testInsertPlanMultipleValues() throws Exception {
        Upsert plan = (Upsert) plan("insert into users (id, name) values (42, 'Deep Thought'), (99, 'Marvin')");

        assertThat(plan.nodes().size(), is(1));

        PlanNode next = ((IterablePlan) plan.nodes().get(0)).iterator().next();
        assertThat(next, instanceOf(SymbolBasedUpsertByIdNode.class));

        SymbolBasedUpsertByIdNode updateNode = (SymbolBasedUpsertByIdNode)next;

        assertThat(updateNode.insertColumns().length, is(2));
        Reference idRef = updateNode.insertColumns()[0];
        assertThat(idRef.ident().columnIdent().fqn(), is("id"));
        Reference nameRef = updateNode.insertColumns()[1];
        assertThat(nameRef.ident().columnIdent().fqn(), is("name"));

        assertThat(updateNode.items().size(), is(2));

        SymbolBasedUpsertByIdNode.Item item1 = updateNode.items().get(0);
        assertThat(item1.index(), is("users"));
        assertThat(item1.id(), is("42"));
        assertThat(item1.routing(), is("42"));
        assertThat(item1.insertValues().length, is(2));
        assertThat((Long)item1.insertValues()[0], is(42L));
        assertThat((BytesRef)item1.insertValues()[1], is(new BytesRef("Deep Thought")));

        SymbolBasedUpsertByIdNode.Item item2 = updateNode.items().get(1);
        assertThat(item2.index(), is("users"));
        assertThat(item2.id(), is("99"));
        assertThat(item2.routing(), is("99"));
        assertThat(item2.insertValues().length, is(2));
        assertThat((Long)item2.insertValues()[0], is(99L));
        assertThat((BytesRef) item2.insertValues()[1], is(new BytesRef("Marvin")));
    }

    @Test
    public void testCountDistinctPlan() throws Exception {
        GlobalAggregate globalAggregate = (GlobalAggregate) plan("select count(distinct name) from users");

        CollectNode collectNode = globalAggregate.collectNode();
        Projection projection = collectNode.projections().get(0);
        assertThat(projection, instanceOf(AggregationProjection.class));
        AggregationProjection aggregationProjection = (AggregationProjection)projection;
        assertThat(aggregationProjection.aggregations().size(), is(1));

        Aggregation aggregation = aggregationProjection.aggregations().get(0);
        assertThat(aggregation.toStep(), is(Aggregation.Step.PARTIAL));
        Symbol aggregationInput = aggregation.inputs().get(0);
        assertThat(aggregationInput.symbolType(), is(SymbolType.INPUT_COLUMN));

        assertThat(collectNode.toCollect().get(0), instanceOf(Reference.class));
        assertThat(((Reference) collectNode.toCollect().get(0)).info().ident().columnIdent().name(), is("name"));

        MergeNode mergeNode = globalAggregate.mergeNode();
        assertThat(mergeNode.projections().size(), is(2));
        Projection projection1 = mergeNode.projections().get(1);
        assertThat(projection1, instanceOf(TopNProjection.class));
        Symbol collection_count = projection1.outputs().get(0);
        assertThat(collection_count, instanceOf(Function.class));
    }

    @Test
    public void testNonDistributedGroupByOnClusteredColumn() throws Exception {
        NonDistributedGroupBy planNode = (NonDistributedGroupBy) plan(
                "select count(*), id from users group by id limit 20");
        CollectNode collectNode = planNode.collectNode();
        assertFalse(collectNode.hasDistributingDownstreams());
        assertThat(collectNode.projections().size(), is(2));
        assertThat(collectNode.projections().get(1), instanceOf(TopNProjection.class));
        assertThat(collectNode.projections().get(0).requiredGranularity(), is(RowGranularity.SHARD));
        MergeNode mergeNode = planNode.localMergeNode();
        assertThat(mergeNode.projections().size(), is(1));
    }

    @Test
    public void testNonDistributedGroupByOnClusteredColumnSorted() throws Exception {
        NonDistributedGroupBy planNode = (NonDistributedGroupBy) plan(
                "select count(*), id from users group by id order by 1 desc nulls last limit 20");
        CollectNode collectNode = planNode.collectNode();
        assertFalse(collectNode.hasDistributingDownstreams());
        assertThat(collectNode.projections().size(), is(2));
        assertThat(collectNode.projections().get(1), instanceOf(TopNProjection.class));
        assertThat(((TopNProjection)collectNode.projections().get(1)).orderBy().size(), is(1));

        assertThat(collectNode.projections().get(0).requiredGranularity(), is(RowGranularity.SHARD));
        MergeNode mergeNode = planNode.localMergeNode();
        assertThat(mergeNode.projections().size(), is(1));
        TopNProjection projection = (TopNProjection)mergeNode.projections().get(0);
        assertThat(projection.orderBy(), is(nullValue()));
        assertThat(mergeNode.sortedInputOutput(), is(true));
        assertThat(mergeNode.orderByIndices().length, is(1));
        assertThat(mergeNode.orderByIndices()[0], is(0));
        assertThat(mergeNode.reverseFlags()[0], is(true));
        assertThat(mergeNode.nullsFirst()[0], is(false));
    }

    @Test
    public void testNonDistributedGroupByOnClusteredColumnSortedScalar() throws Exception {
        NonDistributedGroupBy planNode = (NonDistributedGroupBy) plan(
                "select count(*) + 1, id from users group by id order by count(*) + 1 limit 20");
        CollectNode collectNode = planNode.collectNode();
        assertFalse(collectNode.hasDistributingDownstreams());
        assertThat(collectNode.projections().size(), is(2));
        assertThat(collectNode.projections().get(1), instanceOf(TopNProjection.class));
        assertThat(((TopNProjection)collectNode.projections().get(1)).orderBy().size(), is(1));

        assertThat(collectNode.projections().get(0).requiredGranularity(), is(RowGranularity.SHARD));
        MergeNode mergeNode = planNode.localMergeNode();
        assertThat(mergeNode.projections().size(), is(1));
        TopNProjection projection = (TopNProjection)mergeNode.projections().get(0);
        assertThat(projection.orderBy(), is(nullValue()));
        assertThat(mergeNode.sortedInputOutput(), is(true));
        assertThat(mergeNode.orderByIndices().length, is(1));
        assertThat(mergeNode.orderByIndices()[0], is(0));
        assertThat(mergeNode.reverseFlags()[0], is(false));
        assertThat(mergeNode.nullsFirst()[0], is(nullValue()));
    }

    @Test
    public void testNoDistributedGroupByOnAllPrimaryKeys() throws Exception {
        NonDistributedGroupBy planNode = (NonDistributedGroupBy) plan(
                "select count(*), id, date from empty_parted group by id, date limit 20");
        CollectNode collectNode = planNode.collectNode();
        assertFalse(collectNode.hasDistributingDownstreams());
        assertThat(collectNode.projections().size(), is(2));
        assertThat(collectNode.projections().get(0), instanceOf(GroupProjection.class));
        assertThat(collectNode.projections().get(0).requiredGranularity(), is(RowGranularity.SHARD));
        assertThat(collectNode.projections().get(1), instanceOf(TopNProjection.class));
        MergeNode mergeNode = planNode.localMergeNode();
        assertThat(mergeNode.projections().size(), is(1));
        assertThat(mergeNode.projections().get(0), instanceOf(TopNProjection.class));
    }

    @Test
    public void testNonDistributedGroupByAggregationsWrappedInScalar() throws Exception {
        DistributedGroupBy planNode = (DistributedGroupBy) plan(
                "select (count(*) + 1), id from empty_parted group by id");
        CollectNode collectNode = planNode.collectNode();
        assertThat(collectNode.projections().size(), is(1));
        assertThat(collectNode.projections().get(0), instanceOf(GroupProjection.class));

        TopNProjection topNProjection = (TopNProjection) planNode.reducerMergeNode().projections().get(1);
        assertThat(topNProjection.limit(), is(Constants.DEFAULT_SELECT_LIMIT));
        assertThat(topNProjection.offset(), is(0));

        MergeNode mergeNode = planNode.localMergeNode();
        assertThat(mergeNode.projections().size(), is(1));
        assertThat(mergeNode.projections().get(0), instanceOf(TopNProjection.class));
    }

    @Test
    public void testGroupByWithOrderOnAggregate() throws Exception {
        DistributedGroupBy distributedGroupBy = (DistributedGroupBy) plan(
                "select count(*), name from users group by name order by count(*)");



        MergeNode mergeNode = distributedGroupBy.localMergeNode();
        assertThat(mergeNode.projections().size(), is(1));

        TopNProjection topNProjection = (TopNProjection)mergeNode.projections().get(0);
        Symbol orderBy = topNProjection.orderBy().get(0);
        assertThat(orderBy, instanceOf(InputColumn.class));

        assertThat(orderBy.valueType(), Is.<DataType>is(DataTypes.LONG));
    }

    @Test
    public void testHandlerSideRouting() throws Exception {

        QueryAndFetch plan = (QueryAndFetch) plan("select * from sys.cluster");
    }

    @Test
    public void testHandlerSideRoutingGroupBy() throws Exception {
        NonDistributedGroupBy planNode = (NonDistributedGroupBy) plan(
                "select count(*) from sys.cluster group by name");

        CollectNode collectNode = planNode.collectNode();
        assertThat(collectNode.toCollect().get(0), instanceOf(Reference.class));
        assertThat(collectNode.toCollect().size(), is(1));

        MergeNode mergeNode = planNode.localMergeNode();
        assertThat(mergeNode.projections().size(), is(2));
        assertThat(mergeNode.projections().get(0), instanceOf(GroupProjection.class));
        assertThat(mergeNode.projections().get(1), instanceOf(TopNProjection.class));
    }

    @Test
    public void testCountDistinctWithGroupBy() throws Exception {
        DistributedGroupBy distributedGroupBy = (DistributedGroupBy) plan(
                "select count(distinct id), name from users group by name order by count(distinct id)");
        CollectNode collectNode = distributedGroupBy.collectNode();


        assertThat(collectNode.toCollect().get(0), instanceOf(Reference.class));
        assertThat(collectNode.toCollect().size(), is(2));
        assertThat(((Reference)collectNode.toCollect().get(0)).info().ident().columnIdent().name(), is("id"));
        assertThat(((Reference)collectNode.toCollect().get(1)).info().ident().columnIdent().name(), is("name"));
        Projection projection = collectNode.projections().get(0);
        assertThat(projection, instanceOf(GroupProjection.class));
        GroupProjection groupProjection = (GroupProjection)projection;
        Symbol groupKey = groupProjection.keys().get(0);
        assertThat(groupKey, instanceOf(InputColumn.class));
        assertThat(((InputColumn)groupKey).index(), is(1));
        assertThat(groupProjection.values().size(), is(1));

        Aggregation aggregation = groupProjection.values().get(0);
        assertThat(aggregation.toStep(), is(Aggregation.Step.PARTIAL));
        Symbol aggregationInput = aggregation.inputs().get(0);
        assertThat(aggregationInput.symbolType(), is(SymbolType.INPUT_COLUMN));




        MergeNode mergeNode = distributedGroupBy.reducerMergeNode();
        assertThat(mergeNode.projections().size(), is(2));
        Projection groupProjection1 = mergeNode.projections().get(0);
        assertThat(groupProjection1, instanceOf(GroupProjection.class));
        groupProjection = (GroupProjection)groupProjection1;
        assertThat(groupProjection.keys().get(0), instanceOf(InputColumn.class));
        assertThat(((InputColumn)groupProjection.keys().get(0)).index(), is(0));

        assertThat(groupProjection.values().get(0), instanceOf(Aggregation.class));
        Aggregation aggregationStep2 = groupProjection.values().get(0);
        assertThat(aggregationStep2.toStep(), is(Aggregation.Step.FINAL));

        TopNProjection topNProjection = (TopNProjection)mergeNode.projections().get(1);
        Symbol collection_count = topNProjection.outputs().get(0);
        assertThat(collection_count, instanceOf(Function.class));



        MergeNode localMergeNode = distributedGroupBy.localMergeNode();
        assertThat(localMergeNode.projections().size(), is(1));
        Projection localTopN = localMergeNode.projections().get(0);
        assertThat(localTopN, instanceOf(TopNProjection.class));
    }

    @Test
    public void testUpdateByQueryPlan() throws Exception {
        Upsert plan = (Upsert) plan("update users set name='Vogon lyric fan'");
        assertThat(plan.nodes().size(), is(1));

        CollectAndMerge planNode = (CollectAndMerge) plan.nodes().get(0);

        CollectNode collectNode = planNode.collectNode();
        assertThat(collectNode.routing(), is(shardRouting));
        assertFalse(collectNode.whereClause().noMatch());
        assertFalse(collectNode.whereClause().hasQuery());
        assertThat(collectNode.projections().size(), is(1));
        assertThat(collectNode.projections().get(0), instanceOf(UpdateProjection.class));
        assertThat(collectNode.toCollect().size(), is(1));
        assertThat(collectNode.toCollect().get(0), instanceOf(Reference.class));
        assertThat(((Reference)collectNode.toCollect().get(0)).info().ident().columnIdent().fqn(), is("_uid"));

        UpdateProjection updateProjection = (UpdateProjection)collectNode.projections().get(0);
        assertThat(updateProjection.uidSymbol(), instanceOf(InputColumn.class));

        assertThat(updateProjection.assignmentsColumns()[0], is("name"));
        Symbol symbol = updateProjection.assignments()[0];
        assertThat(symbol, isLiteral("Vogon lyric fan", DataTypes.STRING));

        MergeNode mergeNode = planNode.localMergeNode();
        assertThat(mergeNode.projections().size(), is(1));
        assertThat(mergeNode.projections().get(0), instanceOf(AggregationProjection.class));

        assertThat(mergeNode.outputTypes().size(), is(1));
    }

    @Test
    public void testUpdateByIdPlan() throws Exception {
        Upsert planNode = (Upsert) plan("update users set name='Vogon lyric fan' where id=1");
        assertThat(planNode.nodes().size(), is(1));

        PlanNode next = ((IterablePlan) planNode.nodes().get(0)).iterator().next();
        assertThat(next, instanceOf(SymbolBasedUpsertByIdNode.class));

        SymbolBasedUpsertByIdNode updateNode = (SymbolBasedUpsertByIdNode) next;
        assertThat(updateNode.items().size(), is(1));

        assertThat(updateNode.updateColumns()[0], is("name"));

        SymbolBasedUpsertByIdNode.Item item = updateNode.items().get(0);
        assertThat(item.index(), is("users"));
        assertThat(item.id(), is("1"));

        Symbol symbol = item.updateAssignments()[0];
        assertThat(symbol, isLiteral("Vogon lyric fan", DataTypes.STRING));
    }

    @Test
    public void testUpdatePlanWithMultiplePrimaryKeyValues() throws Exception {
        Upsert planNode =  (Upsert) plan("update users set name='Vogon lyric fan' where id in (1,2,3)");
        assertThat(planNode.nodes().size(), is(1));

        PlanNode next = ((IterablePlan) planNode.nodes().get(0)).iterator().next();

        assertThat(next, instanceOf(SymbolBasedUpsertByIdNode.class));
        SymbolBasedUpsertByIdNode updateNode = (SymbolBasedUpsertByIdNode) next;

        List<String> ids = new ArrayList<>(3);
        for (SymbolBasedUpsertByIdNode.Item item : updateNode.items()) {
            ids.add(item.id());
            assertThat(item.updateAssignments().length, is(1));
            assertThat(item.updateAssignments()[0], isLiteral("Vogon lyric fan", DataTypes.STRING));
        }

        assertThat(ids, containsInAnyOrder("1", "2", "3"));
    }

    @Test
    public void testUpdatePlanWithMultiplePrimaryKeyValuesPartitioned() throws Exception {
        Upsert planNode =  (Upsert) plan("update parted set name='Vogon lyric fan' where " +
                "(id=2 and date = 0) OR" +
                "(id=3 and date=123)");
        assertThat(planNode.nodes().size(), is(1));

        PlanNode next = ((IterablePlan) planNode.nodes().get(0)).iterator().next();

        assertThat(next, instanceOf(SymbolBasedUpsertByIdNode.class));
        SymbolBasedUpsertByIdNode updateNode = (SymbolBasedUpsertByIdNode) next;

        List<String> partitions = new ArrayList<>(2);
        List<String> ids = new ArrayList<>(2);
        for (SymbolBasedUpsertByIdNode.Item item : updateNode.items()) {
            partitions.add(item.index());
            ids.add(item.id());
            assertThat(item.updateAssignments().length, is(1));
            assertThat(item.updateAssignments()[0], isLiteral("Vogon lyric fan", DataTypes.STRING));
        }
        assertThat(ids, containsInAnyOrder("AgEyATA=", "AgEzAzEyMw==")); 
        assertThat(partitions, containsInAnyOrder(".partitioned.parted.04130", ".partitioned.parted.04232chj"));
    }

    @Test
    public void testCopyFromPlan() throws Exception {
        CollectAndMerge plan = (CollectAndMerge) plan("copy users from '/path/to/file.extension'");
        assertThat(plan.collectNode(), instanceOf(FileUriCollectNode.class));

        FileUriCollectNode collectNode = (FileUriCollectNode)plan.collectNode();
        assertThat((BytesRef) ((Literal) collectNode.targetUri()).value(),
                is(new BytesRef("/path/to/file.extension")));
    }

    @Test
    public void testCopyFromNumReadersSetting() throws Exception {
        CollectAndMerge plan = (CollectAndMerge) plan("copy users from '/path/to/file.extension' with (num_readers=1)");
        assertThat(plan.collectNode(), instanceOf(FileUriCollectNode.class));
        FileUriCollectNode collectNode = (FileUriCollectNode) plan.collectNode();
        assertThat(collectNode.executionNodes().size(), is(1));
    }

    @Test
    public void testCopyFromPlanWithParameters() throws Exception {
        CollectAndMerge plan = (CollectAndMerge) plan("copy users from '/path/to/file.ext' with (bulk_size=30, compression='gzip', shared=true)");
        assertThat(plan.collectNode(), instanceOf(FileUriCollectNode.class));
        FileUriCollectNode collectNode = (FileUriCollectNode)plan.collectNode();
        SourceIndexWriterProjection indexWriterProjection = (SourceIndexWriterProjection) collectNode.projections().get(0);
        assertThat(indexWriterProjection.bulkActions(), is(30));
        assertThat(collectNode.compression(), is("gzip"));
        assertThat(collectNode.sharedStorage(), is(true));


        plan = (CollectAndMerge) plan("copy users from '/path/to/file.ext'");
        collectNode = (FileUriCollectNode)plan.collectNode();
        assertNull(collectNode.compression());
        assertNull(collectNode.sharedStorage());
    }

    @Test
    public void testCopyToWithColumnsReferenceRewrite() throws Exception {
        CollectAndMerge plan = (CollectAndMerge) plan("copy users (name) to '/file.ext'");
        CollectNode node = plan.collectNode();
        Reference nameRef = (Reference)node.toCollect().get(0);

        assertThat(nameRef.info().ident().columnIdent().name(), is(DocSysColumns.DOC.name()));
        assertThat(nameRef.info().ident().columnIdent().path().get(0), is("name"));
    }

    @Test
    public void testCopyToWithNonExistentPartitionClause() throws Exception {
        CollectAndMerge plan = (CollectAndMerge) plan("copy parted partition (date=0) to '/foo.txt' ");
        assertFalse(plan.collectNode().routing().hasLocations());
    }

    @Test (expected = IllegalArgumentException.class)
    public void testCopyFromPlanWithInvalidParameters() throws Exception {
        plan("copy users from '/path/to/file.ext' with (bulk_size=-28)");
    }

    @Test
    public void testShardSelect() throws Exception {
        QueryAndFetch planNode = (QueryAndFetch) plan("select id from sys.shards");
        CollectNode collectNode = planNode.collectNode();
        assertTrue(collectNode.isRouted());
        assertThat(collectNode.maxRowGranularity(), is(RowGranularity.SHARD));
    }

    @Test
    public void testDropTable() throws Exception {
        IterablePlan plan = (IterablePlan) plan("drop table users");
        Iterator<PlanNode> iterator = plan.iterator();
        PlanNode planNode = iterator.next();
        assertThat(planNode, instanceOf(DropTableNode.class));

        DropTableNode node = (DropTableNode) planNode;
        assertThat(node.tableInfo().ident().name(), is("users"));
    }

    @Test
    public void testDropTableIfExistsWithUnknownSchema() throws Exception {
        Plan plan = plan("drop table if exists unknown_schema.unknwon_table");
        assertThat(plan, instanceOf(NoopPlan.class));
    }

    @Test
    public void testDropTableIfExists() throws Exception {
        IterablePlan plan = (IterablePlan) plan("drop table if exists users");
        Iterator<PlanNode> iterator = plan.iterator();
        PlanNode planNode = iterator.next();
        assertThat(planNode, instanceOf(DropTableNode.class));

        DropTableNode node = (DropTableNode) planNode;
        assertThat(node.tableInfo().ident().name(), is("users"));
    }

    @Test
    public void testDropTableIfExistsNonExistentTableCreatesNoop() throws Exception {
        Plan plan = plan("drop table if exists groups");
        assertThat(plan, instanceOf(NoopPlan.class));
    }


    @Test
    public void testDropPartitionedTable() throws Exception {
        IterablePlan plan = (IterablePlan) plan("drop table parted");
        Iterator<PlanNode> iterator = plan.iterator();
        PlanNode planNode = iterator.next();

        assertThat(planNode, instanceOf(DropTableNode.class));
        DropTableNode node = (DropTableNode) planNode;
        assertThat(node.tableInfo().ident().name(), is("parted"));

        assertFalse(iterator.hasNext());
    }

    @Test
    public void testDropBlobTableIfExistsCreatesIterablePlan() throws Exception {
        Plan plan = plan("drop blob table if exists screenshots");
        assertThat(plan, instanceOf(IterablePlan.class));
    }

    @Test
    public void testDropNonExistentBlobTableCreatesNoop() throws Exception {
        Plan plan = plan("drop blob table if exists unknown");
        assertThat(plan, instanceOf(NoopPlan.class));
    }

    @Test
    public void testGlobalCountPlan() throws Exception {
        CountPlan plan = (CountPlan) plan("select count(*) from users");
        assertThat(plan, instanceOf(PlannedAnalyzedRelation.class));

        assertThat(plan.countNode().whereClause(), equalTo(WhereClause.MATCH_ALL));

        assertThat(plan.mergeNode().projections().size(), is(1));
        assertThat(plan.mergeNode().projections().get(0), instanceOf(AggregationProjection.class));
    }

    @Test
    public void testSetPlan() throws Exception {
        IterablePlan plan = (IterablePlan) plan("set GLOBAL PERSISTENT stats.jobs_log_size=1024");
        Iterator<PlanNode> iterator = plan.iterator();
        PlanNode planNode = iterator.next();
        assertThat(planNode, instanceOf(ESClusterUpdateSettingsNode.class));

        ESClusterUpdateSettingsNode node = (ESClusterUpdateSettingsNode) planNode;

        assertThat(node.transientSettings().toDelimitedString(','), is("stats.jobs_log_size=1024,"));
        assertThat(node.persistentSettings().toDelimitedString(','), is("stats.jobs_log_size=1024,"));

        plan = (IterablePlan)  plan("set GLOBAL TRANSIENT stats.enabled=false,stats.jobs_log_size=0");
        iterator = plan.iterator();
        planNode = iterator.next();
        assertThat(planNode, instanceOf(ESClusterUpdateSettingsNode.class));

        node = (ESClusterUpdateSettingsNode) planNode;
        assertThat(node.persistentSettings().getAsMap().size(), is(0));
        assertThat(node.transientSettings().toDelimitedString(','), is("stats.enabled=false,stats.jobs_log_size=0,"));
    }

    @Test
    public void testInsertFromSubQueryNonDistributedGroupBy() throws Exception {
        InsertFromSubQuery planNode = (InsertFromSubQuery) plan(
                "insert into users (id, name) (select name, count(*) from sys.nodes group by name)");
        NonDistributedGroupBy nonDistributedGroupBy = (NonDistributedGroupBy)planNode.innerPlan();
        MergeNode mergeNode = nonDistributedGroupBy.localMergeNode();
        assertThat(mergeNode.projections().size(), is(3));
        assertThat(mergeNode.projections().get(0), instanceOf(GroupProjection.class));
        assertThat(mergeNode.projections().get(1), instanceOf(TopNProjection.class));
        assertThat(mergeNode.projections().get(2), instanceOf(ColumnIndexWriterProjection.class));
        assertThat(planNode.handlerMergeNode().isPresent(), is(false));
    }

    @Test (expected = UnsupportedFeatureException.class)
    public void testInsertFromSubQueryDistributedGroupByWithLimit() throws Exception {
        IterablePlan plan = (IterablePlan) plan("insert into users (id, name) (select name, count(*) from users group by name order by name limit 10)");
        Iterator<PlanNode> iterator = plan.iterator();
        PlanNode planNode = iterator.next();
        assertThat(planNode, instanceOf(CollectNode.class));

        planNode = iterator.next();
        assertThat(planNode, instanceOf(MergeNode.class));
        MergeNode mergeNode = (MergeNode)planNode;
        assertThat(mergeNode.projections().size(), is(2));
        assertThat(mergeNode.projections().get(1), instanceOf(TopNProjection.class));

        planNode = iterator.next();
        assertThat(planNode, instanceOf(MergeNode.class));
        mergeNode = (MergeNode)planNode;
        assertThat(mergeNode.projections().size(), is(2));
        assertThat(mergeNode.projections().get(0), instanceOf(TopNProjection.class));
        assertThat(((TopNProjection)mergeNode.projections().get(0)).limit(), is(10));

        assertThat(mergeNode.projections().get(1), instanceOf(ColumnIndexWriterProjection.class));

        assertThat(iterator.hasNext(), is(false));
    }

    @Test
    public void testInsertFromSubQueryDistributedGroupByWithoutLimit() throws Exception {
        InsertFromSubQuery planNode = (InsertFromSubQuery) plan(
                "insert into users (id, name) (select name, count(*) from users group by name)");
        DistributedGroupBy groupBy = (DistributedGroupBy)planNode.innerPlan();
        MergeNode mergeNode = groupBy.reducerMergeNode();
        assertThat(mergeNode.projections().size(), is(2));
        assertThat(mergeNode.projections().get(1), instanceOf(ColumnIndexWriterProjection.class));
        ColumnIndexWriterProjection projection = (ColumnIndexWriterProjection)mergeNode.projections().get(1);
        assertThat(projection.primaryKeys().size(), is(1));
        assertThat(projection.primaryKeys().get(0).fqn(), is("id"));
        assertThat(projection.columnReferences().size(), is(2));
        assertThat(projection.columnReferences().get(0).ident().columnIdent().fqn(), is("id"));
        assertThat(projection.columnReferences().get(1).ident().columnIdent().fqn(), is("name"));

        assertNotNull(projection.clusteredByIdent());
        assertThat(projection.clusteredByIdent().fqn(), is("id"));
        assertThat(projection.tableIdent().fqn(), is("users"));
        assertThat(projection.partitionedBySymbols().isEmpty(), is(true));

        MergeNode localMergeNode = planNode.handlerMergeNode().get();
        assertThat(localMergeNode.projections().size(), is(1));
        assertThat(localMergeNode.projections().get(0), instanceOf(AggregationProjection.class));
        assertThat(localMergeNode.finalProjection().get().outputs().size(), is(1));

    }

    @Test
    public void testInsertFromSubQueryDistributedGroupByPartitioned() throws Exception {
        InsertFromSubQuery planNode = (InsertFromSubQuery) plan(
                "insert into parted (id, date) (select id, date from users group by id, date)");
        DistributedGroupBy groupBy = (DistributedGroupBy)planNode.innerPlan();
        MergeNode mergeNode = groupBy.reducerMergeNode();
        assertThat(mergeNode.projections().size(), is(2));
        assertThat(mergeNode.projections().get(1), instanceOf(ColumnIndexWriterProjection.class));
        ColumnIndexWriterProjection projection = (ColumnIndexWriterProjection)mergeNode.projections().get(1);
        assertThat(projection.primaryKeys().size(), is(2));
        assertThat(projection.primaryKeys().get(0).fqn(), is("id"));
        assertThat(projection.primaryKeys().get(1).fqn(), is("date"));

        assertThat(projection.columnReferences().size(), is(1));
        assertThat(projection.columnReferences().get(0).ident().columnIdent().fqn(), is("id"));

        assertThat(projection.partitionedBySymbols().size(), is(1));
        assertThat(((InputColumn) projection.partitionedBySymbols().get(0)).index(), is(1));

        assertNotNull(projection.clusteredByIdent());
        assertThat(projection.clusteredByIdent().fqn(), is("id"));
        assertThat(projection.tableIdent().fqn(), is("parted"));

        MergeNode localMergeNode = planNode.handlerMergeNode().get();

        assertThat(localMergeNode.projections().size(), is(1));
        assertThat(localMergeNode.projections().get(0), instanceOf(AggregationProjection.class));
        assertThat(localMergeNode.finalProjection().get().outputs().size(), is(1));

    }

    @Test
    public void testInsertFromSubQueryGlobalAggregate() throws Exception {
        InsertFromSubQuery planNode = (InsertFromSubQuery) plan(
                "insert into users (name, id) (select arbitrary(name), count(*) from users)");
        GlobalAggregate globalAggregate = (GlobalAggregate)planNode.innerPlan();
        MergeNode mergeNode = globalAggregate.mergeNode();
        assertThat(mergeNode.projections().size(), is(3));
        assertThat(mergeNode.projections().get(1), instanceOf(TopNProjection.class));
        assertThat(mergeNode.projections().get(2), instanceOf(ColumnIndexWriterProjection.class));
        ColumnIndexWriterProjection projection = (ColumnIndexWriterProjection)mergeNode.projections().get(2);

        assertThat(projection.columnReferences().size(), is(2));
        assertThat(projection.columnReferences().get(0).ident().columnIdent().fqn(), is("name"));
        assertThat(projection.columnReferences().get(1).ident().columnIdent().fqn(), is("id"));

        assertThat(projection.columnSymbols().size(), is(2));
        assertThat(((InputColumn)projection.columnSymbols().get(0)).index(), is(0));
        assertThat(((InputColumn)projection.columnSymbols().get(1)).index(), is(1));

        assertNotNull(projection.clusteredByIdent());
        assertThat(projection.clusteredByIdent().fqn(), is("id"));
        assertThat(projection.tableIdent().fqn(), is("users"));
        assertThat(projection.partitionedBySymbols().isEmpty(), is(true));

        assertThat(planNode.handlerMergeNode().isPresent(), is(false));
    }

    @Test
    public void testInsertFromSubQueryESGet() throws Exception {


        InsertFromSubQuery planNode = (InsertFromSubQuery) plan(
                "insert into users (date, id, name) (select date, id, name from users where id=1)");
        QueryAndFetch queryAndFetch = (QueryAndFetch)planNode.innerPlan();
        CollectNode collectNode = queryAndFetch.collectNode();

        assertThat(collectNode.projections().size(), is(1));
        assertThat(collectNode.projections().get(0), instanceOf(ColumnIndexWriterProjection.class));
        ColumnIndexWriterProjection projection = (ColumnIndexWriterProjection)collectNode.projections().get(0);

        assertThat(projection.columnReferences().size(), is(3));
        assertThat(projection.columnReferences().get(0).ident().columnIdent().fqn(), is("date"));
        assertThat(projection.columnReferences().get(1).ident().columnIdent().fqn(), is("id"));
        assertThat(projection.columnReferences().get(2).ident().columnIdent().fqn(), is("name"));
        assertThat(((InputColumn) projection.ids().get(0)).index(), is(1));
        assertThat(((InputColumn)projection.clusteredBy()).index(), is(1));
        assertThat(projection.partitionedBySymbols().isEmpty(), is(true));

        assertThat(planNode.handlerMergeNode().isPresent(), is(true));
    }

    @Test (expected = UnsupportedFeatureException.class)
    public void testInsertFromSubQueryWithLimit() throws Exception {
        Plan plan = plan("insert into users (date, id, name) (select date, id, name from users limit 10)");
        assertThat(plan, instanceOf(QueryThenFetch.class));
        CollectNode collectNode = ((QueryThenFetch) plan).collectNode();
        assertTrue(collectNode.whereClause().hasQuery());
        assertFalse(collectNode.isPartitioned());

        DQLPlanNode resultNode = ((QueryThenFetch) plan).resultNode();
        assertThat(resultNode.outputTypes().size(), is(1));
        assertEquals(DataTypes.STRING, resultNode.outputTypes().get(0));

        assertThat(resultNode, instanceOf(MergeNode.class));
        MergeNode mergeNode = (MergeNode) resultNode;
        assertTrue(mergeNode.finalProjection().isPresent());

        assertThat(mergeNode.projections().size(), is(2));
        assertThat(mergeNode.projections().get(1), instanceOf(ColumnIndexWriterProjection.class));
    }

    @Test (expected = UnsupportedFeatureException.class)
    public void testInsertFromSubQueryWithOffset() throws Exception {
        plan("insert into users (date, id, name) (select date, id, name from users offset 10)");
    }

    @Test (expected = UnsupportedFeatureException.class)
    public void testInsertFromSubQueryWithOrderBy() throws Exception {
        plan("insert into users (date, id, name) (select date, id, name from users order by id)");
    }

    @Test
    public void testInsertFromSubQueryWithoutLimit() throws Exception {
        InsertFromSubQuery planNode = (InsertFromSubQuery) plan(
                "insert into users (date, id, name) (select date, id, name from users)");
        QueryAndFetch queryAndFetch = (QueryAndFetch)planNode.innerPlan();
        CollectNode collectNode = queryAndFetch.collectNode();
        assertThat(collectNode.projections().size(), is(1));
        assertThat(collectNode.projections().get(0), instanceOf(ColumnIndexWriterProjection.class));
        assertNull(queryAndFetch.localMergeNode());

        MergeNode localMergeNode = planNode.handlerMergeNode().get();

        assertThat(localMergeNode.projections().size(), is(1));
        assertThat(localMergeNode.projections().get(0), instanceOf(AggregationProjection.class));
    }

    @Test
    public void testGroupByHaving() throws Exception {
        DistributedGroupBy distributedGroupBy = (DistributedGroupBy) plan(
                "select avg(date), name from users group by name having min(date) > '1970-01-01'");
        CollectNode collectNode = distributedGroupBy.collectNode();
        assertThat(collectNode.projections().size(), is(1));
        assertThat(collectNode.projections().get(0), instanceOf(GroupProjection.class));

        MergeNode mergeNode = distributedGroupBy.reducerMergeNode();

        assertThat(mergeNode.projections().size(), is(3));


        assertThat(mergeNode.projections().get(0), instanceOf(GroupProjection.class));
        GroupProjection groupProjection = (GroupProjection)mergeNode.projections().get(0);
        assertThat(groupProjection.values().size(), is(2));


        assertThat(mergeNode.projections().get(1), instanceOf(FilterProjection.class));
        FilterProjection filterProjection = (FilterProjection)mergeNode.projections().get(1);


        assertThat(mergeNode.projections().get(2), instanceOf(TopNProjection.class));
        TopNProjection topN = (TopNProjection)mergeNode.projections().get(2);
        assertThat(topN.outputs().get(0).valueType(), Is.<DataType>is(DataTypes.DOUBLE));
        assertThat(topN.outputs().get(1).valueType(), Is.<DataType>is(DataTypes.STRING));
        assertThat(topN.limit(), is(Constants.DEFAULT_SELECT_LIMIT));
    }

    @Test
    public void testInsertFromQueryWithPartitionedColumn() throws Exception {
        InsertFromSubQuery planNode = (InsertFromSubQuery) plan(
                "insert into users (id, date) (select id, date from parted)");
        QueryAndFetch queryAndFetch = (QueryAndFetch)planNode.innerPlan();
        CollectNode collectNode = queryAndFetch.collectNode();
        List<Symbol> toCollect = collectNode.toCollect();
        assertThat(toCollect.size(), is(2));
        assertThat(toCollect.get(0), isFunction("toLong"));
        assertThat(((Function) toCollect.get(0)).arguments().get(0), isReference("_doc['id']"));
        assertThat((Reference) toCollect.get(1), equalTo(new Reference(new ReferenceInfo(
                new ReferenceIdent(new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "parted"), "date"), RowGranularity.PARTITION, DataTypes.TIMESTAMP))));
    }

    @Test
    public void testGroupByHavingInsertInto() throws Exception {
        InsertFromSubQuery planNode = (InsertFromSubQuery) plan(
                "insert into users (id, name) (select name, count(*) from users group by name having count(*) > 3)");
        DistributedGroupBy groupByNode = (DistributedGroupBy)planNode.innerPlan();
        MergeNode mergeNode = groupByNode.reducerMergeNode();
        assertThat(mergeNode.projections().size(), is(3));
        assertThat(mergeNode.projections().get(0), instanceOf(GroupProjection.class));
        assertThat(mergeNode.projections().get(1), instanceOf(FilterProjection.class));
        assertThat(mergeNode.projections().get(2), instanceOf(ColumnIndexWriterProjection.class));

        FilterProjection filterProjection = (FilterProjection)mergeNode.projections().get(1);
        assertThat(filterProjection.outputs().size(), is(2));
        assertThat(filterProjection.outputs().get(0), instanceOf(InputColumn.class));
        assertThat(filterProjection.outputs().get(1), instanceOf(InputColumn.class));

        InputColumn inputColumn = (InputColumn)filterProjection.outputs().get(0);
        assertThat(inputColumn.index(), is(0));
        inputColumn = (InputColumn)filterProjection.outputs().get(1);
        assertThat(inputColumn.index(), is(1));
        MergeNode localMergeNode = planNode.handlerMergeNode().get();

        assertThat(localMergeNode.projections().size(), is(1));
        assertThat(localMergeNode.projections().get(0), instanceOf(AggregationProjection.class));
        assertThat(localMergeNode.finalProjection().get().outputs().size(), is(1));

    }

    @Test
    public void testGroupByHavingNonDistributed() throws Exception {
        NonDistributedGroupBy planNode = (NonDistributedGroupBy) plan(
                "select id from users group by id having id > 0");
        CollectNode collectNode = planNode.collectNode();
        assertThat(collectNode.projections().size(), is(2));
        assertThat(collectNode.projections().get(0), instanceOf(GroupProjection.class));
        assertThat(collectNode.projections().get(1), instanceOf(FilterProjection.class));

        FilterProjection filterProjection = (FilterProjection)collectNode.projections().get(1);
        assertThat(filterProjection.requiredGranularity(), is(RowGranularity.SHARD));
        assertThat(filterProjection.outputs().size(), is(1));
        assertThat(filterProjection.outputs().get(0), instanceOf(InputColumn.class));
        InputColumn inputColumn = (InputColumn)filterProjection.outputs().get(0);
        assertThat(inputColumn.index(), is(0));

        MergeNode localMergeNode = planNode.localMergeNode();

        assertThat(localMergeNode.projections().size(), is(1));
        assertThat(localMergeNode.projections().get(0), instanceOf(TopNProjection.class));
    }

    @Test
    public void testGlobalAggregationHaving() throws Exception {
        GlobalAggregate globalAggregate = (GlobalAggregate) plan(
                "select avg(date) from users having min(date) > '1970-01-01'");
        CollectNode collectNode = globalAggregate.collectNode();
        assertThat(collectNode.projections().size(), is(1));
        assertThat(collectNode.projections().get(0), instanceOf(AggregationProjection.class));

        MergeNode localMergeNode = globalAggregate.mergeNode();

        assertThat(localMergeNode.projections().size(), is(3));
        assertThat(localMergeNode.projections().get(0), instanceOf(AggregationProjection.class));
        assertThat(localMergeNode.projections().get(1), instanceOf(FilterProjection.class));
        assertThat(localMergeNode.projections().get(2), instanceOf(TopNProjection.class));

        AggregationProjection aggregationProjection = (AggregationProjection)localMergeNode.projections().get(0);
        assertThat(aggregationProjection.aggregations().size(), is(2));

        FilterProjection filterProjection = (FilterProjection)localMergeNode.projections().get(1);
        assertThat(filterProjection.outputs().size(), is(2));
        assertThat(filterProjection.outputs().get(0), instanceOf(InputColumn.class));
        InputColumn inputColumn = (InputColumn)filterProjection.outputs().get(0);
        assertThat(inputColumn.index(), is(0));

        TopNProjection topNProjection = (TopNProjection)localMergeNode.projections().get(2);
        assertThat(topNProjection.outputs().size(), is(1));
    }

    @Test
    public void testCountOnPartitionedTable() throws Exception {
        CountPlan plan = (CountPlan) plan("select count(*) from parted where date = 123");
        assertThat(plan, instanceOf(PlannedAnalyzedRelation.class));
        assertThat(plan.countNode().whereClause().partitions(), containsInAnyOrder(".partitioned.parted.04232chj"));
    }

    @Test(expected = UnsupportedOperationException.class)
    public void testSelectPartitionedTableOrderByPartitionedColumn() throws Exception {
        plan("select name from parted order by date");
    }

    @Test(expected = UnsupportedOperationException.class)
    public void testSelectPartitionedTableOrderByPartitionedColumnInFunction() throws Exception {
        plan("select name from parted order by year(date)");
    }

    @Test(expected = UnsupportedOperationException.class)
    public void testSelectOrderByPartitionedNestedColumn() throws Exception {
        plan("select id from multi_parted order by obj['name']");
    }

    @Test(expected = UnsupportedOperationException.class)
    public void testSelectOrderByPartitionedNestedColumnInFunction() throws Exception {
        plan("select id from multi_parted order by format('abc %s', obj['name'])");
    }

    @Test(expected = UnsupportedFeatureException.class)
    public void testQueryRequiresScalar() throws Exception {

        plan("select * from sys.shards where match(table_name, 'characters')");
    }

    @Test
    public void testGroupByWithHavingAndLimit() throws Exception {
        DistributedGroupBy planNode = (DistributedGroupBy) plan(
                "select count(*), name from users group by name having count(*) > 1 limit 100");;

        MergeNode mergeNode = planNode.reducerMergeNode(); 

        Projection projection = mergeNode.projections().get(1);
        assertThat(projection, instanceOf(FilterProjection.class));
        FilterProjection filterProjection = (FilterProjection) projection;

        Symbol countArgument = ((Function) filterProjection.query()).arguments().get(0);
        assertThat(countArgument, instanceOf(InputColumn.class));
        assertThat(((InputColumn) countArgument).index(), is(1));  


        TopNProjection topN = (TopNProjection) mergeNode.projections().get(2);
        assertThat(topN.outputs().get(0).valueType(), Is.<DataType>is(DataTypes.LONG));
        assertThat(topN.outputs().get(1).valueType(), Is.<DataType>is(DataTypes.STRING));


        MergeNode localMerge = planNode.localMergeNode();


        topN = (TopNProjection) localMerge.projections().get(0);
        assertThat(topN.outputs().get(0).valueType(), Is.<DataType>is(DataTypes.LONG));
        assertThat(topN.outputs().get(1).valueType(), Is.<DataType>is(DataTypes.STRING));
    }

    @Test
    public void testGroupByWithHavingAndNoLimit() throws Exception {
        DistributedGroupBy planNode = (DistributedGroupBy) plan(
                "select count(*), name from users group by name having count(*) > 1");

        MergeNode mergeNode = planNode.reducerMergeNode(); 




        Projection projection = mergeNode.projections().get(1);
        assertThat(projection, instanceOf(FilterProjection.class));
        FilterProjection filterProjection = (FilterProjection) projection;

        Symbol countArgument = ((Function) filterProjection.query()).arguments().get(0);
        assertThat(countArgument, instanceOf(InputColumn.class));
        assertThat(((InputColumn) countArgument).index(), is(1));  

        assertThat(mergeNode.outputTypes().get(0), equalTo((DataType) DataTypes.LONG));
        assertThat(mergeNode.outputTypes().get(1), equalTo((DataType) DataTypes.STRING));

        mergeNode = planNode.localMergeNode();

        assertThat(mergeNode.outputTypes().get(0), equalTo((DataType) DataTypes.LONG));
        assertThat(mergeNode.outputTypes().get(1), equalTo((DataType) DataTypes.STRING));
    }

    @Test
    public void testGroupByWithHavingAndNoSelectListReordering() throws Exception {
        DistributedGroupBy planNode = (DistributedGroupBy) plan(
                "select name, count(*) from users group by name having count(*) > 1");

        MergeNode mergeNode = planNode.reducerMergeNode(); 






        Projection projection = mergeNode.projections().get(1);
        assertThat(projection, instanceOf(FilterProjection.class));
        FilterProjection filterProjection = (FilterProjection) projection;

        Symbol countArgument = ((Function) filterProjection.query()).arguments().get(0);
        assertThat(countArgument, instanceOf(InputColumn.class));
        assertThat(((InputColumn) countArgument).index(), is(1));  


        assertThat(((InputColumn) filterProjection.outputs().get(0)).index(), is(0));
        assertThat(((InputColumn) filterProjection.outputs().get(1)).index(), is(1));

        MergeNode localMerge = planNode.localMergeNode();


        TopNProjection topN = (TopNProjection) localMerge.projections().get(0);
        assertThat(((InputColumn) topN.outputs().get(0)).index(), is(0));
        assertThat(((InputColumn) topN.outputs().get(1)).index(), is(1));
    }

    @Test
    public void testGroupByHavingAndNoSelectListReOrderingWithLimit() throws Exception {
        DistributedGroupBy planNode = (DistributedGroupBy) plan(
                "select name, count(*) from users group by name having count(*) > 1 limit 100");

        MergeNode mergeNode = planNode.reducerMergeNode(); 








        Projection projection = mergeNode.projections().get(1);
        assertThat(projection, instanceOf(FilterProjection.class));
        FilterProjection filterProjection = (FilterProjection) projection;

        Symbol countArgument = ((Function) filterProjection.query()).arguments().get(0);
        assertThat(countArgument, instanceOf(InputColumn.class));
        assertThat(((InputColumn) countArgument).index(), is(1));  


        assertThat(((InputColumn) filterProjection.outputs().get(0)).index(), is(0));
        assertThat(((InputColumn) filterProjection.outputs().get(1)).index(), is(1));


        TopNProjection topN = (TopNProjection) mergeNode.projections().get(2);
        assertThat(((InputColumn) topN.outputs().get(0)).index(), is(0));
        assertThat(((InputColumn) topN.outputs().get(1)).index(), is(1));


        MergeNode localMerge = planNode.localMergeNode();



        topN = (TopNProjection) localMerge.projections().get(0);
        assertThat(((InputColumn) topN.outputs().get(0)).index(), is(0));
        assertThat(((InputColumn) topN.outputs().get(1)).index(), is(1));
    }

    @Test
    public void testOrderByOnAnalyzed() throws Exception {
        expectedException.expect(UnsupportedOperationException.class);
        expectedException.expectMessage("Cannot ORDER BY 'users.text': sorting on analyzed/fulltext columns is not possible");
        plan("select text from users u order by 1");
    }

    @Test
    public void testSortOnUnknownColumn() throws Exception {
        expectedException.expect(UnsupportedOperationException.class);
        expectedException.expectMessage("Cannot ORDER BY 'details['unknown_column']': invalid data type 'null'.");
        plan("select details from ignored_nested order by details['unknown_column']");
    }

    @Test
    public void testOrderByOnIndexOff() throws Exception {
        expectedException.expect(UnsupportedOperationException.class);
        expectedException.expectMessage("Cannot ORDER BY 'users.no_index': sorting on non-indexed columns is not possible");
        plan("select no_index from users u order by 1");
    }

    @Test
    public void testGroupByOnAnalyzed() throws Exception {
        expectedException.expect(IllegalArgumentException.class);
        expectedException.expectMessage("Cannot GROUP BY 'users.text': grouping on analyzed/fulltext columns is not possible");
        plan("select text from users u group by 1");
    }

    @Test
    public void testGroupByOnIndexOff() throws Exception {
        expectedException.expect(IllegalArgumentException.class);
        expectedException.expectMessage("Cannot GROUP BY 'users.no_index': grouping on non-indexed columns is not possible");
        plan("select no_index from users u group by 1");
    }

    @Test
    public void testSelectAnalyzedReferenceInFunctionGroupBy() throws Exception {
        expectedException.expect(IllegalArgumentException.class);
        expectedException.expectMessage("Cannot GROUP BY 'users.text': grouping on analyzed/fulltext columns is not possible");
        plan("select substr(text, 0, 2) from users u group by 1");
    }

    @Test
    public void testSelectAnalyzedReferenceInFunctionAggregation() throws Exception {
        expectedException.expect(IllegalArgumentException.class);
        expectedException.expectMessage("Cannot select analyzed column 'users.text' within grouping or aggregations");
        plan("select min(substr(text, 0, 2)) from users");
    }

    @Test
    public void testSelectNonIndexedReferenceInFunctionGroupBy() throws Exception {
        expectedException.expect(IllegalArgumentException.class);
        expectedException.expectMessage("Cannot GROUP BY 'users.no_index': grouping on non-indexed columns is not possible");
        plan("select substr(no_index, 0, 2) from users u group by 1");
    }

    @Test
    public void testSelectNonIndexedReferenceInFunctionAggregation() throws Exception {
        expectedException.expect(IllegalArgumentException.class);
        expectedException.expectMessage("Cannot select non-indexed column 'users.no_index' within grouping or aggregations");
        plan("select min(substr(no_index, 0, 2)) from users");
    }

    @Test
    public void testGlobalAggregateWithWhereOnPartitionColumn() throws Exception {
        GlobalAggregate globalAggregate = (GlobalAggregate) plan(
                "select min(name) from parted where date > 0");

        WhereClause whereClause = globalAggregate.collectNode().whereClause();
        assertThat(whereClause.partitions().size(), is(1));
        assertThat(whereClause.noMatch(), is(false));
    }

    private void assertNoop(Plan plan){
        assertThat(plan, instanceOf(NoopPlan.class));
    }

    @Test
    public void testHasNoResultFromHaving() throws Exception {
        assertNoop(plan("select min(name) from users having 1 = 2"));
    }

    @Test
    public void testHasNoResultFromLimit() {
        assertNoop(plan("select count(*) from users limit 1 offset 1"));
        assertNoop(plan("select count(*) from users limit 5 offset 1"));
        assertNoop(plan("select count(*) from users limit 0"));
    }

    @Test
    public void testHasNoResultFromQuery() {
        assertNoop(plan("select name from users where false"));
    }

    @Test
    public void testInsertFromValuesWithOnDuplicateKey() throws Exception {
        Upsert plan = (Upsert) plan("insert into users (id, name) values (1, null) on duplicate key update name = values(name)");
        PlanNode planNode = ((IterablePlan) plan.nodes().get(0)).iterator().next();
        assertThat(planNode, instanceOf(SymbolBasedUpsertByIdNode.class));
        SymbolBasedUpsertByIdNode node = (SymbolBasedUpsertByIdNode) planNode;

        assertThat(node.updateColumns(), is(new String[]{ "name" }));

        assertThat(node.insertColumns().length, is(2));
        Reference idRef = node.insertColumns()[0];
        assertThat(idRef.ident().columnIdent().fqn(), is("id"));
        Reference nameRef = node.insertColumns()[1];
        assertThat(nameRef.ident().columnIdent().fqn(), is("name"));

        assertThat(node.items().size(), is(1));
        SymbolBasedUpsertByIdNode.Item item = node.items().get(0);
        assertThat(item.index(), is("users"));
        assertThat(item.id(), is("1"));
        assertThat(item.routing(), is("1"));

        assertThat(item.insertValues().length, is(2));
        assertThat((Long)item.insertValues()[0], is(1L));
        assertNull(item.insertValues()[1]);

        assertThat(item.updateAssignments().length, is(1));
        assertThat(item.updateAssignments()[0], isLiteral(null, DataTypes.STRING));
    }

    @Test
    public void testGroupByOnClusteredByColumnPartitionedOnePartition() throws Exception {

        Plan optimizedPlan = plan("select count(*), city from clustered_parted where date=1395874800000 group by city");
        assertThat(optimizedPlan, instanceOf(NonDistributedGroupBy.class));
        NonDistributedGroupBy optimizedGroupBy = (NonDistributedGroupBy) optimizedPlan;

        assertThat(optimizedGroupBy.collectNode().isPartitioned(), is(true));
        assertThat(optimizedGroupBy.collectNode().projections().size(), is(1));
        assertThat(optimizedGroupBy.collectNode().projections().get(0), instanceOf(GroupProjection.class));

        assertThat(optimizedGroupBy.localMergeNode().projections().size(), is(1));
        assertThat(optimizedGroupBy.localMergeNode().projections().get(0), instanceOf(TopNProjection.class));


        Plan plan = plan("select count(*), city from clustered_parted where date=1395874800000 or date=1395961200000 group by city");
        assertThat(plan, instanceOf(DistributedGroupBy.class));
    }

    @Test
    public void testIndices() throws Exception {
        TableIdent custom = new TableIdent("custom", "table");
        String[] indices = Planner.indices(TestingTableInfo.builder(custom, RowGranularity.DOC, shardRouting).add("id", DataTypes.INTEGER, null).build(), WhereClause.MATCH_ALL);
        assertThat(indices, arrayContainingInAnyOrder("custom.table"));

        indices = Planner.indices(TestingTableInfo.builder(new TableIdent(null, "table"), RowGranularity.DOC, shardRouting).add("id", DataTypes.INTEGER, null).build(), WhereClause.MATCH_ALL);
        assertThat(indices, arrayContainingInAnyOrder("table"));

        indices = Planner.indices(TestingTableInfo.builder(custom, RowGranularity.DOC, shardRouting)
                .add("id", DataTypes.INTEGER, null)
                .add("date", DataTypes.TIMESTAMP, null, true)
                .addPartitions(new PartitionName(custom, Arrays.asList(new BytesRef("0"))).stringValue())
                .addPartitions(new PartitionName(custom, Arrays.asList(new BytesRef("12345"))).stringValue())
                .build(), WhereClause.MATCH_ALL);
        assertThat(indices, arrayContainingInAnyOrder("custom..partitioned.table.04130", "custom..partitioned.table.04332chj6gqg"));
    }

    @Test
    public void testAllocatedJobSearchContextIds() throws Exception {
        Planner.Context plannerContext = new Planner.Context(clusterService);
        CollectNode collectNode = new CollectNode(
                UUID.randomUUID(),
                plannerContext.nextExecutionNodeId(), "collect", shardRouting);

        int shardNum = collectNode.routing().numShards();

        plannerContext.allocateJobSearchContextIds(collectNode.routing());

        java.lang.reflect.Field f = plannerContext.getClass().getDeclaredField("jobSearchContextIdBaseSeq");
        f.setAccessible(true);
        int jobSearchContextIdBaseSeq = (Integer)f.get(plannerContext);

        assertThat(jobSearchContextIdBaseSeq, is(shardNum));
        assertThat(collectNode.routing().jobSearchContextIdBase(), is(jobSearchContextIdBaseSeq-shardNum));

        int idx = 0;
        for (Map.Entry<String, Map<String, List<Integer>>> locations : collectNode.routing().locations().entrySet()) {
            String nodeId = locations.getKey();
            for (Map.Entry<String, List<Integer>> entry : locations.getValue().entrySet()) {
                for (Integer shardId : entry.getValue()) {
                    assertThat(plannerContext.shardId(idx), is(new ShardId(entry.getKey(), shardId)));
                    assertThat(plannerContext.nodeId(idx), is(nodeId));
                    idx++;
                }
            }
        }


        int jobSearchContextIdBase = collectNode.routing().jobSearchContextIdBase();
        plannerContext.allocateJobSearchContextIds(collectNode.routing());
        assertThat(collectNode.routing().jobSearchContextIdBase(), is(jobSearchContextIdBase));
    }

    @Test
    public void testExecutionNodeIdSequence() throws Exception {
        Planner.Context plannerContext = new Planner.Context(clusterService);
        CollectNode collectNode1 = new CollectNode(
                UUID.randomUUID(),
                plannerContext.nextExecutionNodeId(), "collect1", shardRouting);
        CollectNode collectNode2 = new CollectNode(
                UUID.randomUUID(),
                plannerContext.nextExecutionNodeId(), "collect2", shardRouting);

        assertThat(collectNode1.executionNodeId(), is(0));
        assertThat(collectNode2.executionNodeId(), is(1));
    }

    @SuppressWarnings("ConstantConditions")
    @Test
    public void testLimitThatIsBiggerThanPageSizeCausesQTFPUshPlan() throws Exception {
        QueryThenFetch plan = (QueryThenFetch) plan("select * from users limit 2147483647 ");
        assertThat(plan.collectNode().downstreamNodes().size(), is(1));
        assertThat(plan.collectNode().downstreamNodes().get(0), is(LOCAL_NODE_ID));
        assertThat(plan.collectNode().hasDistributingDownstreams(), is(true));
    }

    @Test
    public void testKillPlan() throws Exception {
        Plan killPlan = plan("kill all");
        assertThat(killPlan, instanceOf(KillPlan.class));
    }
}

<code block>


package io.crate.planner.node;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.Sets;
import io.crate.metadata.FunctionIdent;
import io.crate.metadata.FunctionInfo;
import io.crate.operation.aggregation.impl.CountAggregation;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.GroupProjection;
import io.crate.planner.projection.TopNProjection;
import io.crate.planner.symbol.Aggregation;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.test.integration.CrateUnitTest;
import io.crate.testing.TestingHelpers;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.elasticsearch.common.io.stream.BytesStreamInput;
import org.elasticsearch.common.io.stream.BytesStreamOutput;
import org.junit.Test;

import java.util.Arrays;
import java.util.UUID;

import static org.hamcrest.core.Is.is;

public class MergeNodeTest extends CrateUnitTest {


    @Test
    public void testSerialization() throws Exception {
        MergeNode node = new MergeNode(UUID.randomUUID(), 0, "merge", 2);
        node.executionNodes(Sets.newHashSet("node1", "node2"));
        node.inputTypes(Arrays.<DataType>asList(DataTypes.UNDEFINED, DataTypes.STRING));
        node.downstreamNodes(Sets.newHashSet("node3", "node4"));

        Reference nameRef = TestingHelpers.createReference("name", DataTypes.STRING);
        GroupProjection groupProjection = new GroupProjection();
        groupProjection.keys(Arrays.<Symbol>asList(nameRef));
        groupProjection.values(Arrays.asList(
                new Aggregation(
                        new FunctionInfo(new FunctionIdent(CountAggregation.NAME, ImmutableList.<DataType>of()), DataTypes.LONG),
                        ImmutableList.<Symbol>of(),
                        Aggregation.Step.PARTIAL,
                        Aggregation.Step.FINAL
                )
        ));
        TopNProjection topNProjection = new TopNProjection(10, 0);

        node.projections(Arrays.asList(groupProjection, topNProjection));

        BytesStreamOutput output = new BytesStreamOutput();
        node.writeTo(output);


        BytesStreamInput input = new BytesStreamInput(output.bytes());
        MergeNode node2 = new MergeNode();
        node2.readFrom(input);

        assertThat(node.downstreamNodes(), is(node2.downstreamNodes()));
        assertThat(node.numUpstreams(), is(node2.numUpstreams()));
        assertThat(node.executionNodes(), is(node2.executionNodes()));
        assertThat(node.jobId(), is(node2.jobId()));
        assertThat(node.inputTypes(), is(node2.inputTypes()));
        assertThat(node.executionNodeId(), is(node2.executionNodeId()));
    }
}

<code block>


package io.crate.planner.node;

import com.google.common.collect.ImmutableList;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.symbol.Symbol;
import io.crate.planner.symbol.Value;
import io.crate.test.integration.CrateUnitTest;
import io.crate.types.DataTypes;
import org.elasticsearch.common.io.stream.BytesStreamInput;
import org.elasticsearch.common.io.stream.BytesStreamOutput;
import org.junit.Test;

import java.util.Arrays;
import java.util.UUID;

import static org.hamcrest.Matchers.equalTo;
import static org.hamcrest.Matchers.is;

public class CollectNodeTest extends CrateUnitTest {

    @Test
    public void testStreaming() throws Exception {
        CollectNode cn = new CollectNode(UUID.randomUUID(), 0, "cn");
        cn.maxRowGranularity(RowGranularity.DOC);
        cn.downstreamNodes(Arrays.asList("n1", "n2"));
        cn.toCollect(ImmutableList.<Symbol>of(new Value(DataTypes.STRING)));

        BytesStreamOutput out = new BytesStreamOutput();
        cn.writeTo(out);

        BytesStreamInput in = new BytesStreamInput(out.bytes());
        CollectNode cn2 = new CollectNode(UUID.randomUUID(), 1, "collect");
        cn2.readFrom(in);
        assertThat(cn, equalTo(cn2));

        assertThat(cn.toCollect(), is(cn2.toCollect()));
        assertThat(cn.downstreamNodes(), is(cn2.downstreamNodes()));
        assertThat(cn.executionNodes(), is(cn2.executionNodes()));
        assertThat(cn.jobId(), is(cn2.jobId()));
        assertThat(cn.executionNodeId(), is(cn2.executionNodeId()));
        assertThat(cn.maxRowGranularity(), is(cn2.maxRowGranularity()));
    }
}

<code block>


package io.crate.planner.node;

import com.google.common.collect.ImmutableList;
import io.crate.Streamer;
import io.crate.metadata.FunctionIdent;
import io.crate.metadata.FunctionInfo;
import io.crate.metadata.Functions;
import io.crate.metadata.Routing;
import io.crate.operation.aggregation.impl.AggregationImplModule;
import io.crate.operation.aggregation.impl.CountAggregation;
import io.crate.operation.aggregation.impl.MaximumAggregation;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.AggregationProjection;
import io.crate.planner.projection.GroupProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.TopNProjection;
import io.crate.planner.symbol.Aggregation;
import io.crate.planner.symbol.InputColumn;
import io.crate.planner.symbol.Literal;
import io.crate.planner.symbol.Symbol;
import io.crate.test.integration.CrateUnitTest;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.elasticsearch.common.inject.Injector;
import org.elasticsearch.common.inject.ModulesBuilder;
import org.junit.Before;
import org.junit.Test;

import java.util.*;

import static org.hamcrest.Matchers.instanceOf;
import static org.hamcrest.Matchers.is;

public class StreamerVisitorTest extends CrateUnitTest {

    private StreamerVisitor visitor;
    private FunctionInfo maxInfo;
    private FunctionInfo countInfo;

    final static Routing EMPTY_ROUTING = new Routing(new TreeMap<String, Map<String, List<Integer>>>());

    @Before
    public void prepare() {
        Injector injector = new ModulesBuilder()
                .add(new AggregationImplModule())
                .createInjector();
        Functions functions = injector.getInstance(Functions.class);
        visitor = new StreamerVisitor(functions);
        maxInfo = new FunctionInfo(new FunctionIdent(MaximumAggregation.NAME, Arrays.<DataType>asList(DataTypes.INTEGER)), DataTypes.INTEGER);
        countInfo = new FunctionInfo(new FunctionIdent(CountAggregation.NAME, ImmutableList.<DataType>of()), DataTypes.LONG);
    }

    @Test
    public void testGetOutputStreamersFromCollectNode() throws Exception {
        CollectNode collectNode = new CollectNode(UUID.randomUUID(), 0, "bla", EMPTY_ROUTING);
        collectNode.outputTypes(Arrays.<DataType>asList(DataTypes.BOOLEAN, DataTypes.FLOAT, DataTypes.OBJECT));
        StreamerVisitor.Context ctx = visitor.processPlanNode(collectNode);
        Streamer<?>[] streamers = ctx.outputStreamers();
        assertThat(streamers.length, is(3));
        assertThat(streamers[0], instanceOf(DataTypes.BOOLEAN.streamer().getClass()));
        assertThat(streamers[1], instanceOf(DataTypes.FLOAT.streamer().getClass()));
        assertThat(streamers[2], instanceOf(DataTypes.OBJECT.streamer().getClass()));


        ctx = visitor.processExecutionNode(collectNode);
        streamers = ctx.outputStreamers();
        assertThat(streamers.length, is(3));
        assertThat(streamers[0], instanceOf(DataTypes.BOOLEAN.streamer().getClass()));
        assertThat(streamers[1], instanceOf(DataTypes.FLOAT.streamer().getClass()));
        assertThat(streamers[2], instanceOf(DataTypes.OBJECT.streamer().getClass()));
    }

    @Test
    public void testGetOutputStreamersFromCollectNodeWithWrongNull() throws Exception {

        CollectNode collectNode = new CollectNode(UUID.randomUUID(), 0, "bla", EMPTY_ROUTING);
        collectNode.outputTypes(Arrays.<DataType>asList(DataTypes.BOOLEAN, null, DataTypes.OBJECT));
        StreamerVisitor.Context ctx = visitor.processPlanNode(collectNode);

        assertEquals(DataTypes.UNDEFINED.streamer(), ctx.outputStreamers()[1]);


        ctx = visitor.processExecutionNode(collectNode);

        assertEquals(DataTypes.UNDEFINED.streamer(), ctx.outputStreamers()[1]);
    }

    @Test
    public void testGetOutputStreamersFromCollectNodeWithAggregations() throws Exception {
        CollectNode collectNode = new CollectNode(UUID.randomUUID(), 0, "bla", EMPTY_ROUTING);
        collectNode.outputTypes(Arrays.<DataType>asList(DataTypes.BOOLEAN, null, null, DataTypes.DOUBLE));
        AggregationProjection aggregationProjection = new AggregationProjection();
        aggregationProjection.aggregations(Arrays.asList( 
                new Aggregation(maxInfo, Arrays.<Symbol>asList(new InputColumn(0)), Aggregation.Step.ITER, Aggregation.Step.FINAL),
                new Aggregation(maxInfo, Arrays.<Symbol>asList(new InputColumn(1)), Aggregation.Step.ITER, Aggregation.Step.PARTIAL)
        ));
        collectNode.projections(Arrays.<Projection>asList(aggregationProjection));
        StreamerVisitor.Context ctx = visitor.processPlanNode(collectNode);
        Streamer<?>[] streamers = ctx.outputStreamers();
        assertThat(streamers.length, is(4));
        assertThat(streamers[0], instanceOf(DataTypes.BOOLEAN.streamer().getClass()));
        assertThat(streamers[1], instanceOf(DataTypes.INTEGER.streamer().getClass()));
        assertThat(streamers[2], instanceOf(DataTypes.INTEGER.streamer().getClass()));
        assertThat(streamers[3], instanceOf(DataTypes.DOUBLE.streamer().getClass()));
    }

    @Test
    public void testGetOutputStreamersFromCollectNodeWithGroupAndTopNProjection() throws Exception {
        CollectNode collectNode = new CollectNode(UUID.randomUUID(), 0, "mynode", EMPTY_ROUTING);
        collectNode.outputTypes(Arrays.<DataType>asList(DataTypes.UNDEFINED));
        GroupProjection groupProjection = new GroupProjection(
                Arrays.<Symbol>asList(Literal.newLiteral("key")),
                Arrays.asList(new Aggregation(
                        countInfo,
                        ImmutableList.<Symbol>of(),
                        Aggregation.Step.PARTIAL, Aggregation.Step.FINAL))
        );
        collectNode.projections(Arrays.<Projection>asList(groupProjection, new TopNProjection(10,0)));
        StreamerVisitor.Context ctx = visitor.processPlanNode(collectNode);
        Streamer<?>[] streamers = ctx.outputStreamers();
        assertThat(streamers.length, is(1));
        assertThat(streamers[0], instanceOf(DataTypes.LONG.streamer().getClass()));
    }

    @Test
    public void testGetInputStreamersForMergeNode() throws Exception {
        MergeNode mergeNode = new MergeNode(UUID.randomUUID(), 0, "mÃ¶rtsch", 2);
        mergeNode.inputTypes(Arrays.<DataType>asList(DataTypes.BOOLEAN, DataTypes.SHORT, DataTypes.TIMESTAMP));
        StreamerVisitor.Context ctx = visitor.processPlanNode(mergeNode);
        Streamer<?>[] streamers = ctx.inputStreamers();
        assertThat(streamers.length, is(3));
        assertThat(streamers[0], instanceOf(DataTypes.BOOLEAN.streamer().getClass()));
        assertThat(streamers[1], instanceOf(DataTypes.SHORT.streamer().getClass()));
        assertThat(streamers[2], instanceOf(DataTypes.TIMESTAMP.streamer().getClass()));
    }

    @Test(expected= IllegalStateException.class)
    public void testGetInputStreamersForMergeNodeWithWrongNull() throws Exception {
        MergeNode mergeNode = new MergeNode(UUID.randomUUID(), 0, "mÃ¶rtsch", 2);
        mergeNode.inputTypes(Arrays.<DataType>asList(DataTypes.BOOLEAN, null, DataTypes.TIMESTAMP));
        visitor.processPlanNode(mergeNode);
    }

    @Test
    public void testGetInputStreamersForMergeNodeWithAggregations() throws Exception {
        MergeNode mergeNode = new MergeNode(UUID.randomUUID(), 0, "mÃ¶rtsch", 2);
        mergeNode.inputTypes(Arrays.<DataType>asList(DataTypes.UNDEFINED, DataTypes.TIMESTAMP));
        AggregationProjection aggregationProjection = new AggregationProjection();
        aggregationProjection.aggregations(Arrays.asList(
                new Aggregation(maxInfo, Arrays.<Symbol>asList(new InputColumn(0)), Aggregation.Step.PARTIAL, Aggregation.Step.FINAL)
        ));
        mergeNode.projections(Arrays.<Projection>asList(aggregationProjection));
        StreamerVisitor.Context ctx = visitor.processPlanNode(mergeNode);
        Streamer<?>[] streamers = ctx.inputStreamers();
        assertThat(streamers.length, is(2));
        assertThat(streamers[0], instanceOf(DataTypes.INTEGER.streamer().getClass()));
        assertThat(streamers[1], instanceOf(DataTypes.TIMESTAMP.streamer().getClass()));
    }

    @Test
    public void testOutputStreamerFromGroupByMergeNode() throws Exception {


        MergeNode mergeNode = new MergeNode(UUID.randomUUID(), 0, "mÃ¶rtsch", 2);
        mergeNode.inputTypes(Arrays.<DataType>asList(DataTypes.STRING, DataTypes.UNDEFINED));
        GroupProjection groupProjection = new GroupProjection(
                Arrays.<Symbol>asList(Literal.newLiteral("key")),
                Arrays.asList(new Aggregation(
                        countInfo,
                        ImmutableList.<Symbol>of(),
                        Aggregation.Step.PARTIAL, Aggregation.Step.FINAL))
        );

        TopNProjection topNProjection = new TopNProjection(2, 0);
        topNProjection.outputs(Arrays.<Symbol>asList(
                new InputColumn(1),
                new InputColumn(0)
        ));

        mergeNode.projections(Arrays.asList(groupProjection, topNProjection));
        mergeNode.outputTypes(Arrays.<DataType>asList(DataTypes.LONG, DataTypes.STRING));
        StreamerVisitor.Context context = visitor.processPlanNode(mergeNode);
        assertSame(DataTypes.STRING.streamer(), context.outputStreamers()[1]);
        assertSame(DataTypes.LONG.streamer(), context.outputStreamers()[0]);
    }
}

<code block>


package io.crate.planner.node.dql;

import io.crate.analyze.WhereClause;
import io.crate.core.collections.TreeMapBuilder;
import io.crate.metadata.Routing;
import io.crate.planner.node.ExecutionNode;
import io.crate.test.integration.CrateUnitTest;
import org.elasticsearch.common.io.stream.BytesStreamInput;
import org.elasticsearch.common.io.stream.BytesStreamOutput;
import org.junit.Test;

import java.util.*;

import static org.hamcrest.Matchers.*;

public class CountNodeTest extends CrateUnitTest {

    @Test
    public void testStreaming() throws Exception {
        Routing routing = new Routing(
                TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder()
                        .put("n1", TreeMapBuilder.<String, List<Integer>>newMapBuilder()
                                .put("i1", Arrays.asList(1, 2))
                                .put("i2", Arrays.asList(1, 2)).map())
                        .put("n2", TreeMapBuilder.<String, List<Integer>>newMapBuilder()
                                .put("i1", Collections.singletonList(3)).map()).map());
        UUID jobId = UUID.randomUUID();
        CountNode countNode = new CountNode(jobId, 1, routing, WhereClause.MATCH_ALL);

        BytesStreamOutput out = new BytesStreamOutput(10);
        countNode.writeTo(out);

        BytesStreamInput in = new BytesStreamInput(out.bytes());

        CountNode streamedNode = CountNode.FACTORY.create();
        streamedNode.readFrom(in);

        assertThat(streamedNode.jobId(), is(jobId));
        assertThat(streamedNode.executionNodeId(), is(1));
        assertThat(streamedNode.downstreamNodes(), contains(ExecutionNode.DIRECT_RETURN_DOWNSTREAM_NODE));
        assertThat(streamedNode.executionNodes(), containsInAnyOrder("n1", "n2"));
        assertThat(streamedNode.routing(), equalTo(routing));
    }
}
<code block>


package io.crate.metadata;

import com.google.common.base.MoreObjects;
import com.google.common.base.Preconditions;
import io.crate.metadata.table.ColumnPolicy;
import io.crate.planner.RowGranularity;
import io.crate.types.DataTypes;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;

import javax.annotation.Nullable;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;


public class IndexReferenceInfo extends ReferenceInfo {

    public static class Builder {
        private ReferenceIdent ident;
        private IndexType indexType = IndexType.ANALYZED;
        private List<ReferenceInfo> columns = new ArrayList<>();
        private String analyzer = null;

        public Builder ident(ReferenceIdent ident) {
            this.ident = ident;
            return this;
        }

        public Builder indexType(IndexType indexType) {
            this.indexType = indexType;
            return this;
        }

        public Builder addColumn(ReferenceInfo info) {
            this.columns.add(info);
            return this;
        }

        public IndexReferenceInfo build() {
            Preconditions.checkNotNull(ident, "ident is null");
            return new IndexReferenceInfo(ident, indexType, columns, analyzer);
        }
    }

    private String analyzer;
    private List<ReferenceInfo> columns;

    public IndexReferenceInfo(ReferenceIdent ident,
                         IndexType indexType,
                         List<ReferenceInfo> columns,
                         @Nullable String analyzer) {
        super(ident, RowGranularity.DOC, DataTypes.STRING, ColumnPolicy.DYNAMIC, indexType);
        this.columns = MoreObjects.firstNonNull(columns, Collections.<ReferenceInfo>emptyList());
        this.analyzer = analyzer;
    }

    @Override
    public boolean equals(Object o) {
        if (this == o) return true;
        if (o == null || getClass() != o.getClass()) return false;
        if (!super.equals(o)) return false;

        IndexReferenceInfo that = (IndexReferenceInfo) o;

        if (analyzer != null ? !analyzer.equals(that.analyzer) : that.analyzer != null)
            return false;
        if (!columns.equals(that.columns)) return false;

        return true;
    }

    @Override
    public int hashCode() {
        int result = super.hashCode();
        result = 31 * result + (analyzer != null ? analyzer.hashCode() : 0);
        result = 31 * result + columns.hashCode();
        return result;
    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        super.writeTo(out);

    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        super.readFrom(in);
    }

    @Override
    public String toString() {
        return MoreObjects.toStringHelper(this).add("ident", ident()).toString();
    }
}

<code block>


package io.crate.metadata.table;

import com.google.common.base.Predicate;
import com.google.common.base.Predicates;
import com.google.common.collect.ImmutableMap;
import io.crate.analyze.TableParameterInfo;
import io.crate.analyze.WhereClause;
import io.crate.metadata.*;
import io.crate.planner.RowGranularity;
import io.crate.planner.symbol.DynamicReference;
import org.apache.lucene.util.BytesRef;

import javax.annotation.Nullable;
import java.util.Collection;
import java.util.List;

public interface TableInfo extends Iterable<ReferenceInfo> {


    public static final String NULL_NODE_ID = "";
    public static final Predicate<String> IS_NOT_NULL_NODE_ID = Predicates.not(Predicates.equalTo(TableInfo.NULL_NODE_ID));


    public SchemaInfo schemaInfo();


    @Nullable
    public ReferenceInfo getReferenceInfo(ColumnIdent columnIdent);


    public Collection<ReferenceInfo> columns();

    public List<ReferenceInfo> partitionedByColumns();

    @Nullable
    public IndexReferenceInfo indexColumn(ColumnIdent ident);

    public RowGranularity rowGranularity();

    public TableIdent ident();

    public Routing getRouting(WhereClause whereClause, @Nullable String preference);

    public List<ColumnIdent> primaryKey();

    public int numberOfShards();

    public BytesRef numberOfReplicas();

    public boolean hasAutoGeneratedPrimaryKey();

    @Nullable
    public ColumnIdent clusteredBy();


    public boolean isAlias();

    public String[] concreteIndices();

    public List<PartitionName> partitions();


    public List<ColumnIdent> partitionedBy();


    public boolean isPartitioned();


    @Nullable
    DynamicReference getDynamic(ColumnIdent ident, boolean forWrite);


    public ColumnPolicy columnPolicy();

    public TableParameterInfo tableParameterInfo();

    public ImmutableMap<String, Object> tableParameters();

}

<code block>
package io.crate.metadata.table;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableMap;
import io.crate.analyze.TableParameterInfo;
import io.crate.metadata.ColumnIdent;
import io.crate.metadata.IndexReferenceInfo;
import io.crate.metadata.PartitionName;
import io.crate.metadata.ReferenceInfo;
import io.crate.planner.symbol.DynamicReference;
import org.apache.lucene.util.BytesRef;

import javax.annotation.Nullable;
import java.util.ArrayList;
import java.util.List;

public abstract class AbstractTableInfo implements TableInfo {

    private static final BytesRef ZERO_REPLICAS = new BytesRef("0");
    private final SchemaInfo schemaInfo;

    protected AbstractTableInfo(SchemaInfo schemaInfo) {
        this.schemaInfo = schemaInfo;
    }

    @Override
    public SchemaInfo schemaInfo() {
        return schemaInfo;
    }

    @Override
    public int numberOfShards() {
        return 1;
    }

    @Override
    public BytesRef numberOfReplicas() {
        return ZERO_REPLICAS;
    }

    @Override
    public boolean hasAutoGeneratedPrimaryKey() {
        return false;
    }

    @Override
    public boolean isAlias() {
        return false;
    }

    @Override
    public boolean isPartitioned() {
        return false;
    }

    @Override
    public List<ReferenceInfo> partitionedByColumns() {
        return ImmutableList.of();
    }

    @Nullable
    @Override
    public IndexReferenceInfo indexColumn(ColumnIdent ident) {
        return null;
    }

    @Nullable
    @Override
    public ColumnIdent clusteredBy() {
        return null;
    }

    @Nullable
    public DynamicReference getDynamic(ColumnIdent ident) {
        return getDynamic(ident, false);
    }

    @Nullable
    @Override
    public DynamicReference getDynamic(ColumnIdent ident, boolean forWrite) {
        return null;
    }

    @Override
    public List<PartitionName> partitions() {
        return new ArrayList<>(0);
    }

    @Override
    public List<ColumnIdent> partitionedBy() {
        return ImmutableList.of();
    }

    @Override
    public TableParameterInfo tableParameterInfo() {
        return null;
    }

    @Override
    public ImmutableMap<String, Object> tableParameters() {
        return ImmutableMap.of();
    }

    @Override
    public String toString() {
        return String.format("%s.%s", schemaInfo.name(), ident().name());
    }

}

<code block>


package io.crate.metadata.doc;

import com.google.common.base.MoreObjects;
import com.google.common.collect.*;
import io.crate.Constants;
import io.crate.analyze.TableParameter;
import io.crate.analyze.TableParameterInfo;
import io.crate.core.NumberOfReplicas;
import io.crate.exceptions.TableAliasSchemaException;
import io.crate.metadata.*;
import io.crate.metadata.settings.CrateTableSettings;
import io.crate.metadata.table.ColumnPolicy;
import io.crate.planner.RowGranularity;
import io.crate.types.ArrayType;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.action.admin.indices.alias.Alias;
import org.elasticsearch.action.admin.indices.template.put.PutIndexTemplateRequest;
import org.elasticsearch.action.admin.indices.template.put.TransportPutIndexTemplateAction;
import org.elasticsearch.cluster.metadata.IndexMetaData;
import org.elasticsearch.cluster.metadata.MappingMetaData;
import org.elasticsearch.common.Booleans;
import org.elasticsearch.common.collect.Tuple;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.common.xcontent.XContentHelper;

import java.io.IOException;
import java.util.*;

public class DocIndexMetaData {

    private static final String ID = "_id";
    public static final ColumnIdent ID_IDENT = new ColumnIdent(ID);
    private final IndexMetaData metaData;

    private final MappingMetaData defaultMappingMetaData;
    private final Map<String, Object> defaultMappingMap;

    private final Map<ColumnIdent, IndexReferenceInfo.Builder> indicesBuilder = new HashMap<>();

    private final ImmutableSortedSet.Builder<ReferenceInfo> columnsBuilder = ImmutableSortedSet.orderedBy(new Comparator<ReferenceInfo>() {
        @Override
        public int compare(ReferenceInfo o1, ReferenceInfo o2) {
            return o1.ident().columnIdent().fqn().compareTo(o2.ident().columnIdent().fqn());
        }
    });


    private final ImmutableMap.Builder<ColumnIdent, ReferenceInfo> referencesBuilder = ImmutableSortedMap.naturalOrder();
    private final ImmutableList.Builder<ReferenceInfo> partitionedByColumnsBuilder = ImmutableList.builder();

    private final TableIdent ident;
    private final int numberOfShards;
    private final BytesRef numberOfReplicas;
    private final ImmutableMap<String, Object> tableParameters;
    private Map<String, Object> metaMap;
    private Map<String, Object> metaColumnsMap;
    private Map<String, Object> indicesMap;
    private List<List<String>> partitionedByList;
    private ImmutableList<ReferenceInfo> columns;
    private ImmutableMap<ColumnIdent, IndexReferenceInfo> indices;
    private ImmutableList<ReferenceInfo> partitionedByColumns;
    private ImmutableMap<ColumnIdent, ReferenceInfo> references;
    private ImmutableList<ColumnIdent> primaryKey;
    private ColumnIdent routingCol;
    private ImmutableList<ColumnIdent> partitionedBy;
    private final boolean isAlias;
    private final Set<String> aliases;
    private boolean hasAutoGeneratedPrimaryKey = false;

    private ColumnPolicy columnPolicy = ColumnPolicy.DYNAMIC;

    private final static ImmutableMap<String, DataType> dataTypeMap = ImmutableMap.<String, DataType>builder()
            .put("date", DataTypes.TIMESTAMP)
            .put("string", DataTypes.STRING)
            .put("boolean", DataTypes.BOOLEAN)
            .put("byte", DataTypes.BYTE)
            .put("short", DataTypes.SHORT)
            .put("integer", DataTypes.INTEGER)
            .put("long", DataTypes.LONG)
            .put("float", DataTypes.FLOAT)
            .put("double", DataTypes.DOUBLE)
            .put("ip", DataTypes.IP)
            .put("geo_point", DataTypes.GEO_POINT)
            .put("object", DataTypes.OBJECT)
            .put("nested", DataTypes.OBJECT).build();

    public DocIndexMetaData(IndexMetaData metaData, TableIdent ident) throws IOException {
        this.ident = ident;
        this.metaData = metaData;
        this.isAlias = !metaData.getIndex().equals(ident.esName());
        this.numberOfShards = metaData.numberOfShards();
        final Settings settings = metaData.getSettings();
        this.numberOfReplicas = NumberOfReplicas.fromSettings(settings);
        this.aliases = ImmutableSet.copyOf(metaData.aliases().keys().toArray(String.class));
        this.defaultMappingMetaData = this.metaData.mappingOrDefault(Constants.DEFAULT_MAPPING_TYPE);
        if (defaultMappingMetaData == null) {
            this.defaultMappingMap = new HashMap<>();
        } else {
            this.defaultMappingMap = this.defaultMappingMetaData.sourceAsMap();
        }
        this.tableParameters = TableParameterInfo.tableParametersFromIndexMetaData(metaData);

        prepareCrateMeta();
    }

    @SuppressWarnings("unchecked")
    private static <T> T getNested(Map map, String key) {
        return (T) map.get(key);
    }

    private void prepareCrateMeta() {
        metaMap = getNested(defaultMappingMap, "_meta");
        if (metaMap != null) {
            indicesMap = getNested(metaMap, "indices");
            if (indicesMap == null) {
                indicesMap = ImmutableMap.of();
            }
            metaColumnsMap = getNested(metaMap, "columns");
            if (metaColumnsMap == null) {
                metaColumnsMap = ImmutableMap.of();
            }

            partitionedByList = getNested(metaMap, "partitioned_by");
            if (partitionedByList == null) {
                partitionedByList = ImmutableList.of();
            }
        } else {
            metaMap = new HashMap<>();
            indicesMap = new HashMap<>();
            metaColumnsMap = new HashMap<>();
            partitionedByList = ImmutableList.of();
        }
    }

    private void addPartitioned(ColumnIdent column, DataType type) {
        add(column, type, ColumnPolicy.DYNAMIC, ReferenceInfo.IndexType.NOT_ANALYZED, true);
    }

    private void add(ColumnIdent column, DataType type, ReferenceInfo.IndexType indexType) {
        add(column, type, ColumnPolicy.DYNAMIC, indexType, false);
    }

    private void add(ColumnIdent column, DataType type, ColumnPolicy columnPolicy,
                     ReferenceInfo.IndexType indexType, boolean partitioned) {
        ReferenceInfo info = newInfo(column, type, columnPolicy, indexType);

        if (partitioned || !(partitionedBy != null && partitionedBy.contains(column))) {
            if (info.ident().isColumn()) {
                columnsBuilder.add(info);
            }
            referencesBuilder.put(info.ident().columnIdent(), info);
        }
        if (partitioned) {
            partitionedByColumnsBuilder.add(info);
        }
    }

    private ReferenceInfo newInfo(ColumnIdent column,
                                  DataType type,
                                  ColumnPolicy columnPolicy,
                                  ReferenceInfo.IndexType indexType) {
        RowGranularity granularity = RowGranularity.DOC;
        if (partitionedBy.contains(column)) {
            granularity = RowGranularity.PARTITION;
        }
        return new ReferenceInfo(new ReferenceIdent(ident, column), granularity, type,
                columnPolicy, indexType);
    }


    public static DataType getColumnDataType(Map<String, Object> columnProperties) {
        DataType type;
        String typeName = (String) columnProperties.get("type");

        if (typeName == null) {
            if (columnProperties.containsKey("properties")) {
                type = DataTypes.OBJECT;
            } else {
                return DataTypes.NOT_SUPPORTED;
            }
        } else if (typeName.equalsIgnoreCase("array")) {

            Map<String, Object> innerProperties = getNested(columnProperties, "inner");
            DataType innerType = getColumnDataType(innerProperties);
            type = new ArrayType(innerType);
        } else {
            typeName = typeName.toLowerCase(Locale.ENGLISH);
            type = MoreObjects.firstNonNull(dataTypeMap.get(typeName), DataTypes.NOT_SUPPORTED);
        }
        return type;
    }

    private ReferenceInfo.IndexType getColumnIndexType(Map<String, Object> columnProperties) {
        String indexType = (String) columnProperties.get("index");
        String analyzerName = (String) columnProperties.get("analyzer");
        if (indexType != null) {
            if (indexType.equals(ReferenceInfo.IndexType.NOT_ANALYZED.toString())) {
                return ReferenceInfo.IndexType.NOT_ANALYZED;
            } else if (indexType.equals(ReferenceInfo.IndexType.NO.toString())) {
                return ReferenceInfo.IndexType.NO;
            } else if (indexType.equals(ReferenceInfo.IndexType.ANALYZED.toString())
                    && analyzerName != null && !analyzerName.equals("keyword")) {
                return ReferenceInfo.IndexType.ANALYZED;
            }
        } 
        else if (analyzerName != null && !analyzerName.equals("keyword")) {
            return ReferenceInfo.IndexType.ANALYZED;
        }
        return ReferenceInfo.IndexType.NOT_ANALYZED;
    }

    private ColumnIdent childIdent(ColumnIdent ident, String name) {
        if (ident == null) {
            return new ColumnIdent(name);
        }
        if (ident.isColumn()) {
            return new ColumnIdent(ident.name(), name);
        } else {
            ImmutableList.Builder<String> builder = ImmutableList.builder();
            for (String s : ident.path()) {
                builder.add(s);
            }
            builder.add(name);
            return new ColumnIdent(ident.name(), builder.build());
        }
    }


    @SuppressWarnings("unchecked")
    private void internalExtractColumnDefinitions(ColumnIdent columnIdent,
                                                  Map<String, Object> propertiesMap) {
        if (propertiesMap == null) {
            return;
        }

        for (Map.Entry<String, Object> columnEntry : propertiesMap.entrySet()) {
            Map<String, Object> columnProperties = (Map) columnEntry.getValue();
            DataType columnDataType = getColumnDataType(columnProperties);
            ColumnIdent newIdent = childIdent(columnIdent, columnEntry.getKey());

            columnProperties = furtherColumnProperties(columnProperties);
            ReferenceInfo.IndexType columnIndexType = getColumnIndexType(columnProperties);
            if (columnDataType == DataTypes.OBJECT
                    || (columnDataType.id() == ArrayType.ID
                    && ((ArrayType) columnDataType).innerType() == DataTypes.OBJECT)) {
                ColumnPolicy columnPolicy =
                        ColumnPolicy.of(columnProperties.get("dynamic"));
                add(newIdent, columnDataType, columnPolicy, ReferenceInfo.IndexType.NO, false);

                if (columnProperties.get("properties") != null) {

                    internalExtractColumnDefinitions(newIdent, (Map<String, Object>) columnProperties.get("properties"));
                }
            } else if (columnDataType != DataTypes.NOT_SUPPORTED) {
                List<String> copyToColumns = getNested(columnProperties, "copy_to");


                if (copyToColumns != null) {
                    for (String copyToColumn : copyToColumns) {
                        ColumnIdent targetIdent = ColumnIdent.fromPath(copyToColumn);
                        IndexReferenceInfo.Builder builder = getOrCreateIndexBuilder(targetIdent);
                        builder.addColumn(newInfo(newIdent, columnDataType, ColumnPolicy.DYNAMIC, columnIndexType));
                    }
                }

                if (indicesMap.containsKey(newIdent.fqn())) {
                    IndexReferenceInfo.Builder builder = getOrCreateIndexBuilder(newIdent);
                    builder.indexType(columnIndexType)
                            .ident(new ReferenceIdent(ident, newIdent));
                } else {
                    add(newIdent, columnDataType, columnIndexType);
                }
            }
        }
    }


    private Map<String, Object> furtherColumnProperties(Map<String, Object> columnProperties) {
        if (columnProperties.get("inner") != null) {
            return (Map<String, Object>) columnProperties.get("inner");
        } else {
            return columnProperties;
        }
    }

    private IndexReferenceInfo.Builder getOrCreateIndexBuilder(ColumnIdent ident) {
        IndexReferenceInfo.Builder builder = indicesBuilder.get(ident);
        if (builder == null) {
            builder = new IndexReferenceInfo.Builder();
            indicesBuilder.put(ident, builder);
        }
        return builder;
    }

    private ImmutableList<ColumnIdent> getPrimaryKey() {
        Map<String, Object> metaMap = getNested(defaultMappingMap, "_meta");
        if (metaMap == null) {
            hasAutoGeneratedPrimaryKey = true;
            return ImmutableList.of(ID_IDENT);
        }

        ImmutableList.Builder<ColumnIdent> builder = ImmutableList.builder();
        Object pKeys = metaMap.get("primary_keys");
        if (pKeys == null) {
            hasAutoGeneratedPrimaryKey = true;
            return ImmutableList.of(ID_IDENT);
        }

        if (pKeys instanceof String) {
            builder.add(ColumnIdent.fromPath((String) pKeys));
        } else if (pKeys instanceof Collection) {
            Collection keys = (Collection) pKeys;
            if (keys.isEmpty()) {
                hasAutoGeneratedPrimaryKey = true;
                return ImmutableList.of(ID_IDENT);
            }
            for (Object pkey : keys) {
                builder.add(ColumnIdent.fromPath(pkey.toString()));
            }
        }
        return builder.build();
    }

    private ImmutableList<ColumnIdent> getPartitionedBy() {
        ImmutableList.Builder<ColumnIdent> builder = ImmutableList.builder();
        for (List<String> partitionedByInfo : partitionedByList) {
            builder.add(ColumnIdent.fromPath(partitionedByInfo.get(0)));
        }
        return builder.build();
    }

    private ColumnPolicy getColumnPolicy() {
        Object dynamic = getNested(defaultMappingMap, "dynamic");
        if (ColumnPolicy.STRICT.value().equals(String.valueOf(dynamic).toLowerCase(Locale.ENGLISH))) {
            return ColumnPolicy.STRICT;
        } else if (Booleans.isExplicitFalse(String.valueOf(dynamic))) {
            return ColumnPolicy.IGNORED;
        } else {
            return ColumnPolicy.DYNAMIC;
        }
    }

    private void createColumnDefinitions() {
        Map<String, Object> propertiesMap = getNested(defaultMappingMap, "properties");
        internalExtractColumnDefinitions(null, propertiesMap);
        extractPartitionedByColumns();
    }

    private ImmutableMap<ColumnIdent, IndexReferenceInfo> createIndexDefinitions() {
        ImmutableMap.Builder<ColumnIdent, IndexReferenceInfo> builder = ImmutableMap.builder();
        for (Map.Entry<ColumnIdent, IndexReferenceInfo.Builder> entry : indicesBuilder.entrySet()) {
            builder.put(entry.getKey(), entry.getValue().build());
        }
        indices = builder.build();
        return indices;
    }

    private void extractPartitionedByColumns() {
        for (Tuple<ColumnIdent, DataType> partitioned : PartitionedByMappingExtractor.extractPartitionedByColumns(partitionedByList)) {
            addPartitioned(partitioned.v1(), partitioned.v2());
        }
    }

    private ColumnIdent getRoutingCol() {
        if (defaultMappingMetaData != null) {
            Map<String, Object> metaMap = getNested(defaultMappingMap, "_meta");
            if (metaMap != null) {
                String routingPath = (String) metaMap.get("routing");
                if (routingPath != null) {
                    return ColumnIdent.fromPath(routingPath);
                }
            }
        }
        if (primaryKey.size() == 1) {
            return primaryKey.get(0);
        }
        return ID_IDENT;
    }

    public DocIndexMetaData build() {
        partitionedBy = getPartitionedBy();
        columnPolicy = getColumnPolicy();
        createColumnDefinitions();
        indices = createIndexDefinitions();
        columns = ImmutableList.copyOf(columnsBuilder.build());
        partitionedByColumns = partitionedByColumnsBuilder.build();

        for (Tuple<ColumnIdent, ReferenceInfo> sysColumn : DocSysColumns.forTable(ident)) {
            referencesBuilder.put(sysColumn.v1(), sysColumn.v2());
        }
        references = referencesBuilder.build();
        primaryKey = getPrimaryKey();
        routingCol = getRoutingCol();
        return this;
    }

    public ImmutableMap<ColumnIdent, ReferenceInfo> references() {
        return references;
    }

    public ImmutableList<ReferenceInfo> columns() {
        return columns;
    }

    public ImmutableMap<ColumnIdent, IndexReferenceInfo> indices() {
        return indices;
    }

    public ImmutableList<ReferenceInfo> partitionedByColumns() {
        return partitionedByColumns;
    }

    public ImmutableList<ColumnIdent> primaryKey() {
        return primaryKey;
    }

    public ColumnIdent routingCol() {
        return routingCol;
    }


    public boolean schemaEquals(DocIndexMetaData other) {
        if (this == other) return true;
        if (other == null) return false;



        if (columns != null ? !columns.equals(other.columns) : other.columns != null) return false;
        if (primaryKey != null ? !primaryKey.equals(other.primaryKey) : other.primaryKey != null) return false;
        if (indices != null ? !indices.equals(other.indices) : other.indices != null) return false;
        if (references != null ? !references.equals(other.references) : other.references != null) return false;
        if (routingCol != null ? !routingCol.equals(other.routingCol) : other.routingCol != null) return false;

        return true;
    }

    protected DocIndexMetaData merge(DocIndexMetaData other,
                                     TransportPutIndexTemplateAction transportPutIndexTemplateAction,
                                     boolean thisIsCreatedFromTemplate) throws IOException {
        if (schemaEquals(other)) {
            return this;
        } else if (thisIsCreatedFromTemplate) {
            if (this.references.size() < other.references.size()) {



                updateTemplate(other, transportPutIndexTemplateAction, this.metaData.settings());

                return new DocIndexMetaData(IndexMetaData.builder(other.metaData).settings(this.metaData.settings()).build(), other.ident).build();
            } else if (references().size() == other.references().size() &&
                    !references().keySet().equals(other.references().keySet())) {
                XContentHelper.update(defaultMappingMap, other.defaultMappingMap, false);

                updateTemplate(this, transportPutIndexTemplateAction, this.metaData.settings());
                return this;
            }

            return this;
        } else {
            throw new TableAliasSchemaException(other.ident.name());
        }
    }

    private void updateTemplate(DocIndexMetaData md,
                                TransportPutIndexTemplateAction transportPutIndexTemplateAction,
                                Settings updateSettings) {
        String templateName = PartitionName.templateName(ident.schema(), ident.name());
        PutIndexTemplateRequest request = new PutIndexTemplateRequest(templateName)
                .mapping(Constants.DEFAULT_MAPPING_TYPE, md.defaultMappingMap)
                .create(false)
                .settings(updateSettings)
                .template(templateName + "*");
        for (String alias : md.aliases()) {
            request = request.alias(new Alias(alias));
        }
        transportPutIndexTemplateAction.execute(request);
    }


    public String concreteIndexName() {
        return metaData.index();
    }

    public boolean isAlias() {
        return isAlias;
    }

    public Set<String> aliases() {
        return aliases;
    }

    public boolean hasAutoGeneratedPrimaryKey() {
        return hasAutoGeneratedPrimaryKey;
    }

    public int numberOfShards() {
        return numberOfShards;
    }

    public BytesRef numberOfReplicas() {
        return numberOfReplicas;
    }

    public ImmutableList<ColumnIdent> partitionedBy() {
        return partitionedBy;
    }

    public ColumnPolicy columnPolicy() {
        return columnPolicy;
    }

    public ImmutableMap<String, Object> tableParameters() {
        return tableParameters;
    }
}

<code block>


package io.crate.metadata.doc;

import io.crate.Constants;
import io.crate.exceptions.TableUnknownException;
import io.crate.exceptions.UnhandledServerException;
import io.crate.metadata.PartitionName;
import io.crate.metadata.TableIdent;
import org.elasticsearch.action.admin.indices.template.put.TransportPutIndexTemplateAction;
import org.elasticsearch.action.support.IndicesOptions;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.cluster.metadata.IndexMetaData;
import org.elasticsearch.cluster.metadata.IndexTemplateMetaData;
import org.elasticsearch.cluster.metadata.MetaData;
import org.elasticsearch.common.logging.ESLogger;
import org.elasticsearch.common.logging.Loggers;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.indices.IndexMissingException;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import java.util.Locale;
import java.util.concurrent.ExecutorService;

import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;

public class DocTableInfoBuilder {

    private final TableIdent ident;
    private ExecutorService executorService;
    private final boolean checkAliasSchema;
    private final DocSchemaInfo docSchemaInfo;
    private final ClusterService clusterService;
    private final TransportPutIndexTemplateAction transportPutIndexTemplateAction;
    private final MetaData metaData;
    private String[] concreteIndices;
    private static final ESLogger logger = Loggers.getLogger(DocTableInfoBuilder.class);

    public DocTableInfoBuilder(DocSchemaInfo docSchemaInfo,
                               TableIdent ident,
                               ClusterService clusterService,
                               TransportPutIndexTemplateAction transportPutIndexTemplateAction,
                               ExecutorService executorService,
                               boolean checkAliasSchema) {
        this.docSchemaInfo = docSchemaInfo;
        this.clusterService = clusterService;
        this.transportPutIndexTemplateAction = transportPutIndexTemplateAction;
        this.ident = ident;
        this.executorService = executorService;
        this.metaData = clusterService.state().metaData();
        this.checkAliasSchema = checkAliasSchema;
    }

    private DocIndexMetaData docIndexMetaData() {
        DocIndexMetaData docIndexMetaData;
        String templateName = PartitionName.templateName(ident.schema(), ident.name());
        boolean createdFromTemplate = false;
        if (metaData.getTemplates().containsKey(templateName)) {
            docIndexMetaData = buildDocIndexMetaDataFromTemplate(ident.esName(), templateName);
            createdFromTemplate = true;
            concreteIndices = metaData.concreteIndices(IndicesOptions.lenientExpandOpen(), ident.esName());
        } else {
            try {
                concreteIndices = metaData.concreteIndices(IndicesOptions.strictExpandOpen(), ident.esName());
                if (concreteIndices.length == 0) {
                    throw new TableUnknownException(ident);
                }
                docIndexMetaData = buildDocIndexMetaData(concreteIndices[0]);
            } catch (IndexMissingException ex) {
                throw new TableUnknownException(ident.fqn(), ex);
            }
        }

        if ((!createdFromTemplate && concreteIndices.length == 1) || !checkAliasSchema) {
            return docIndexMetaData;
        }
        for (int i = 0; i < concreteIndices.length; i++) {
            try {
                docIndexMetaData = docIndexMetaData.merge(
                        buildDocIndexMetaData(concreteIndices[i]),
                        transportPutIndexTemplateAction,
                        createdFromTemplate);
            } catch (IOException e) {
                throw new UnhandledServerException("Unable to merge/build new DocIndexMetaData", e);
            }
        }
        return docIndexMetaData;
    }

    private DocIndexMetaData buildDocIndexMetaData(String index) {
        DocIndexMetaData docIndexMetaData;
        try {
            docIndexMetaData = new DocIndexMetaData(metaData.index(index), ident);
        } catch (IOException e) {
            throw new UnhandledServerException("Unable to build DocIndexMetaData", e);
        }
        return docIndexMetaData.build();
    }

    private DocIndexMetaData buildDocIndexMetaDataFromTemplate(String index, String templateName) {
        IndexTemplateMetaData indexTemplateMetaData = metaData.getTemplates().get(templateName);
        DocIndexMetaData docIndexMetaData;
        try {
            IndexMetaData.Builder builder = new IndexMetaData.Builder(index);
            builder.putMapping(Constants.DEFAULT_MAPPING_TYPE,
                    indexTemplateMetaData.getMappings().get(Constants.DEFAULT_MAPPING_TYPE).toString());
            Settings settings = indexTemplateMetaData.settings();
            builder.settings(settings);

            builder.numberOfShards(settings.getAsInt(SETTING_NUMBER_OF_SHARDS, 5));
            builder.numberOfReplicas(settings.getAsInt(SETTING_NUMBER_OF_REPLICAS, 1));
            docIndexMetaData = new DocIndexMetaData(builder.build(), ident);
        } catch (IOException e) {
            throw new UnhandledServerException("Unable to build DocIndexMetaData from template", e);
        }
        return docIndexMetaData.build();
    }

    public DocTableInfo build() {
        DocIndexMetaData md = docIndexMetaData();

        List<PartitionName> partitions = new ArrayList<>();
        if (md.partitionedBy().size() > 0) {
            for(String index : concreteIndices) {
                if (PartitionName.isPartition(index, ident.schema(), ident.name())) {
                    try {
                        PartitionName partitionName = PartitionName.fromString(index, ident.schema(), ident.name());
                        partitions.add(partitionName);
                    } catch (IllegalArgumentException e) {

                        logger.warn(String.format(Locale.ENGLISH, "Cannot build partition %s of index %s", index, ident.esName()));
                    }
                }
            }
        }

        return new DocTableInfo(
                docSchemaInfo,
                ident,
                md.columns(),
                md.partitionedByColumns(),
                md.indices(),
                md.references(), md.primaryKey(), md.routingCol(),
                md.isAlias(), md.hasAutoGeneratedPrimaryKey(),
                concreteIndices, clusterService,
                md.numberOfShards(), md.numberOfReplicas(),
                md.tableParameters(),
                md.partitionedBy(),
                partitions,
                md.columnPolicy(),
                executorService);
    }

}

<code block>


package io.crate.metadata.doc;

import com.google.common.base.Throwables;
import com.google.common.collect.ImmutableMap;
import com.google.common.util.concurrent.SettableFuture;
import io.crate.analyze.AlterPartitionedTableParameterInfo;
import io.crate.analyze.TableParameterInfo;
import io.crate.analyze.WhereClause;
import io.crate.exceptions.UnavailableShardsException;
import io.crate.metadata.*;
import io.crate.metadata.table.AbstractDynamicTableInfo;
import io.crate.metadata.table.ColumnPolicy;
import io.crate.planner.RowGranularity;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.cluster.ClusterChangedEvent;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.cluster.ClusterState;
import org.elasticsearch.cluster.ClusterStateObserver;
import org.elasticsearch.cluster.routing.GroupShardsIterator;
import org.elasticsearch.cluster.routing.ShardIterator;
import org.elasticsearch.cluster.routing.ShardRouting;
import org.elasticsearch.common.logging.ESLogger;
import org.elasticsearch.common.logging.Loggers;
import org.elasticsearch.common.unit.TimeValue;
import org.elasticsearch.index.shard.ShardId;
import org.elasticsearch.indices.IndexMissingException;

import javax.annotation.Nullable;
import java.util.*;
import java.util.concurrent.*;


public class DocTableInfo extends AbstractDynamicTableInfo {

    private static final TimeValue ROUTING_FETCH_TIMEOUT = new TimeValue(5, TimeUnit.SECONDS);

    private final List<ReferenceInfo> columns;
    private final List<ReferenceInfo> partitionedByColumns;
    private final Map<ColumnIdent, IndexReferenceInfo> indexColumns;
    private final ImmutableMap<ColumnIdent, ReferenceInfo> references;
    private final TableIdent ident;
    private final List<ColumnIdent> primaryKeys;
    private final ColumnIdent clusteredBy;
    private final String[] concreteIndices;
    private final List<ColumnIdent> partitionedBy;
    private final int numberOfShards;
    private final BytesRef numberOfReplicas;
    private final ImmutableMap<String, Object> tableParameters;
    private ExecutorService executorService;
    private final ClusterService clusterService;
    private final TableParameterInfo tableParameterInfo;
    private static final ESLogger logger = Loggers.getLogger(DocTableInfo.class);

    private final String[] indices;
    private final List<PartitionName> partitions;

    private final boolean isAlias;
    private final boolean hasAutoGeneratedPrimaryKey;
    private final boolean isPartitioned;

    private final ColumnPolicy columnPolicy;

    public DocTableInfo(DocSchemaInfo schemaInfo,
                        TableIdent ident,
                        List<ReferenceInfo> columns,
                        List<ReferenceInfo> partitionedByColumns,
                        ImmutableMap<ColumnIdent, IndexReferenceInfo> indexColumns,
                        ImmutableMap<ColumnIdent, ReferenceInfo> references,
                        List<ColumnIdent> primaryKeys,
                        ColumnIdent clusteredBy,
                        boolean isAlias,
                        boolean hasAutoGeneratedPrimaryKey,
                        String[] concreteIndices,
                        ClusterService clusterService,
                        int numberOfShards,
                        BytesRef numberOfReplicas,
                        ImmutableMap<String, Object> tableParameters,
                        List<ColumnIdent> partitionedBy,
                        List<PartitionName> partitions,
                        ColumnPolicy columnPolicy,
                        ExecutorService executorService) {
        super(schemaInfo);
        this.clusterService = clusterService;
        this.columns = columns;
        this.partitionedByColumns = partitionedByColumns;
        this.indexColumns = indexColumns;
        this.references = references;
        this.ident = ident;
        this.primaryKeys = primaryKeys;
        this.clusteredBy = clusteredBy;
        this.concreteIndices = concreteIndices;
        this.numberOfShards = numberOfShards;
        this.numberOfReplicas = numberOfReplicas;
        this.tableParameters = tableParameters;
        this.executorService = executorService;
        indices = new String[]{ident.esName()};
        this.isAlias = isAlias;
        this.hasAutoGeneratedPrimaryKey = hasAutoGeneratedPrimaryKey;
        isPartitioned = !partitionedByColumns.isEmpty();
        this.partitionedBy = partitionedBy;
        this.partitions = partitions;
        this.columnPolicy = columnPolicy;
        if (isPartitioned) {
            tableParameterInfo = new AlterPartitionedTableParameterInfo();
        } else {
            tableParameterInfo = new TableParameterInfo();
        }
    }

    @Override
    @Nullable
    public ReferenceInfo getReferenceInfo(ColumnIdent columnIdent) {
        return references.get(columnIdent);
    }

    @Override
    public Collection<ReferenceInfo> columns() {
        return columns;
    }

    @Override
    public RowGranularity rowGranularity() {
        return RowGranularity.DOC;
    }

    @Override
    public TableIdent ident() {
        return ident;
    }

    private void processShardRouting(Map<String, Map<String, List<Integer>>> locations, ShardRouting shardRouting) {
        String node = shardRouting.currentNodeId();
        Map<String, List<Integer>> nodeMap = locations.get(node);
        if (nodeMap == null) {
            nodeMap = new TreeMap<>();
            locations.put(shardRouting.currentNodeId(), nodeMap);
        }

        List<Integer> shards = nodeMap.get(shardRouting.getIndex());
        if (shards == null) {
            shards = new ArrayList<>();
            nodeMap.put(shardRouting.getIndex(), shards);
        }
        shards.add(shardRouting.id());
    }

    private GroupShardsIterator getShardIterators(WhereClause whereClause,
                                                  @Nullable String preference,
                                                  ClusterState clusterState) throws IndexMissingException {
        String[] routingIndices = concreteIndices;
        if (whereClause.partitions().size() > 0) {
            routingIndices = whereClause.partitions().toArray(new String[whereClause.partitions().size()]);
        }

        Map<String, Set<String>> routingMap = null;
        if (whereClause.clusteredBy().isPresent()) {
            routingMap = clusterState.metaData().resolveSearchRouting(
                    whereClause.routingValues(), routingIndices);
        }
        return clusterService.operationRouting().searchShards(
                clusterState,
                indices,
                routingIndices,
                routingMap,
                preference
        );
    }

    public Routing getRouting(ClusterState state, WhereClause whereClause, String preference, final List<ShardId> missingShards) {
        final Map<String, Map<String, List<Integer>>> locations = new TreeMap<>();
        GroupShardsIterator shardIterators;
        try {
            shardIterators = getShardIterators(whereClause, preference, state);
        } catch (IndexMissingException e) {
            return new Routing();
        }

        fillLocationsFromShardIterators(locations, shardIterators, missingShards);

        if (missingShards.isEmpty()) {
            return new Routing(locations);
        } else {
            return null;
        }
    }

    @Override
    public Routing getRouting(final WhereClause whereClause, @Nullable final String preference) {
        Routing routing = getRouting(clusterService.state(), whereClause, preference, new ArrayList<ShardId>(0));
        if (routing != null) return routing;

        ClusterStateObserver observer = new ClusterStateObserver(clusterService, ROUTING_FETCH_TIMEOUT, logger);
        final SettableFuture<Routing> routingSettableFuture = SettableFuture.create();
        observer.waitForNextChange(
                new FetchRoutingListener(routingSettableFuture, whereClause, preference),
                new ClusterStateObserver.ChangePredicate() {

                    @Override
                    public boolean apply(ClusterState previousState, ClusterState.ClusterStateStatus previousStatus, ClusterState newState, ClusterState.ClusterStateStatus newStatus) {
                        return validate(newState);
                    }

                    @Override
                    public boolean apply(ClusterChangedEvent changedEvent) {
                        return validate(changedEvent.state());
                    }

                    private boolean validate(ClusterState state) {
                        final Map<String, Map<String, List<Integer>>> locations = new TreeMap<>();

                        GroupShardsIterator shardIterators;
                        try {
                            shardIterators = getShardIterators(whereClause, preference, state);
                        } catch (IndexMissingException e) {
                            return true;
                        }

                        final List<ShardId> missingShards = new ArrayList<>(0);
                        fillLocationsFromShardIterators(locations, shardIterators, missingShards);

                        return missingShards.isEmpty();
                    }

                });

        try {
            return routingSettableFuture.get();
        } catch (ExecutionException e) {
            throw Throwables.propagate(e.getCause());
        } catch (Exception e) {
            throw Throwables.propagate(e);
        }
    }

    private void fillLocationsFromShardIterators(Map<String, Map<String, List<Integer>>> locations,
                                                 GroupShardsIterator shardIterators,
                                                 List<ShardId> missingShards) {
        ShardRouting shardRouting;
        for (ShardIterator shardIterator : shardIterators) {
            shardRouting = shardIterator.nextOrNull();
            if (shardRouting != null) {
                if (shardRouting.active()) {
                    processShardRouting(locations, shardRouting);
                } else {
                    missingShards.add(shardIterator.shardId());
                }
            } else {
                if (isPartitioned) {

                    missingShards.add(shardIterator.shardId());
                } else {
                    throw new UnavailableShardsException(shardIterator.shardId());
                }
            }
        }
    }

    public List<ColumnIdent> primaryKey() {
        return primaryKeys;
    }

    @Override
    public int numberOfShards() {
        return numberOfShards;
    }

    @Override
    public BytesRef numberOfReplicas() {
        return numberOfReplicas;
    }

    @Override
    public boolean hasAutoGeneratedPrimaryKey() {
        return hasAutoGeneratedPrimaryKey;
    }

    @Override
    public ColumnIdent clusteredBy() {
        return clusteredBy;
    }

    @Override
    public boolean isAlias() {
        return isAlias;
    }

    @Override
    public String[] concreteIndices() {
        return concreteIndices;
    }


    public List<ReferenceInfo> partitionedByColumns() {
        return partitionedByColumns;
    }


    @Override
    public List<ColumnIdent> partitionedBy() {
        return partitionedBy;
    }

    @Override
    public List<PartitionName> partitions() {
        return partitions;
    }

    @Override
    public boolean isPartitioned() {
        return isPartitioned;
    }

    @Override
    public IndexReferenceInfo indexColumn(ColumnIdent ident) {
        return indexColumns.get(ident);
    }

    @Override
    public Iterator<ReferenceInfo> iterator() {
        return references.values().iterator();
    }

    @Override
    public ColumnPolicy columnPolicy() {
        return columnPolicy;
    }

    @Override
    public TableParameterInfo tableParameterInfo () {
        return tableParameterInfo;
    }

    @Override
    public ImmutableMap<String, Object> tableParameters() {
        return tableParameters;
    }

    private class FetchRoutingListener implements ClusterStateObserver.Listener {

        private final SettableFuture<Routing> routingFuture;
        private final WhereClause whereClause;
        private final String preference;
        Future<?> innerTaskFuture;

        public FetchRoutingListener(SettableFuture<Routing> routingFuture, WhereClause whereClause, String preference) {
            this.routingFuture = routingFuture;
            this.whereClause = whereClause;
            this.preference = preference;
        }

        @Override
        public void onNewClusterState(final ClusterState state) {
            try {
                innerTaskFuture = executorService.submit(new Runnable() {
                    @Override
                    public void run() {
                        final List<ShardId> missingShards = new ArrayList<>(0);
                        Routing routing = getRouting(state, whereClause, preference, missingShards);
                        if (routing == null) {
                            routingFuture.setException(new UnavailableShardsException(missingShards.get(0)));
                        } else {
                            routingFuture.set(routing);
                        }
                    }
                });
            } catch (RejectedExecutionException e) {
                routingFuture.setException(e);
            }
        }

        @Override
        public void onClusterServiceClose() {
            if (innerTaskFuture != null) {
                innerTaskFuture.cancel(true);
            }
            routingFuture.setException(new IllegalStateException("ClusterService closed"));
        }

        @Override
        public void onTimeout(TimeValue timeout) {
            if (innerTaskFuture != null) {
                innerTaskFuture.cancel(true);
            }
            routingFuture.setException(new IllegalStateException("Fetching table info routing timed out."));
        }
    }
}

<code block>


package io.crate.metadata.blob;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableMap;
import io.crate.analyze.AlterBlobTableParameterInfo;
import io.crate.analyze.TableParameterInfo;
import io.crate.analyze.WhereClause;
import io.crate.metadata.*;
import io.crate.metadata.table.ColumnPolicy;
import io.crate.metadata.table.SchemaInfo;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.RowGranularity;
import io.crate.planner.symbol.DynamicReference;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.action.NoShardAvailableActionException;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.cluster.routing.GroupShardsIterator;
import org.elasticsearch.cluster.routing.ShardIterator;
import org.elasticsearch.cluster.routing.ShardRouting;
import org.elasticsearch.common.Strings;
import org.elasticsearch.common.collect.Tuple;
import org.elasticsearch.index.shard.ShardId;

import javax.annotation.Nullable;
import java.util.*;

public class BlobTableInfo implements TableInfo {

    private final BlobSchemaInfo blobSchemaInfo;
    private final TableIdent ident;
    private final int numberOfShards;
    private final BytesRef numberOfReplicas;
    private final ClusterService clusterService;
    private final String index;
    private final LinkedHashSet<ReferenceInfo> columns = new LinkedHashSet<>();
    private final BytesRef blobsPath;
    private final TableParameterInfo tableParameterInfo;
    private ImmutableMap<String,Object> tableParameters;

    public static final Map<ColumnIdent, ReferenceInfo> INFOS = new LinkedHashMap<>();

    private static final ImmutableList<ColumnIdent> primaryKey = ImmutableList.of(
            new ColumnIdent("digest"));
    private final static List<Tuple<String, DataType>> staticColumns = ImmutableList.<Tuple<String,DataType>>builder()
                .add(new Tuple<String, DataType>("digest", DataTypes.STRING))
                .add(new Tuple<String, DataType>("last_modified", DataTypes.TIMESTAMP))
                .build();

    public BlobTableInfo(BlobSchemaInfo blobSchemaInfo,
                         TableIdent ident,
                         String index,
                         ClusterService clusterService,
                         int numberOfShards,
                         BytesRef numberOfReplicas,
                         ImmutableMap<String, Object> tableParameters,
                         BytesRef blobsPath) {
        this.blobSchemaInfo = blobSchemaInfo;
        this.ident = ident;
        this.index = index;
        this.clusterService = clusterService;
        this.numberOfShards = numberOfShards;
        this.numberOfReplicas = numberOfReplicas;
        this.blobsPath = blobsPath;
        this.tableParameterInfo = new AlterBlobTableParameterInfo();
        this.tableParameters = tableParameters;

        registerStaticColumns();
    }

    @Override
    public SchemaInfo schemaInfo() {
        return blobSchemaInfo;
    }

    @Nullable
    @Override
    public ReferenceInfo getReferenceInfo(ColumnIdent columnIdent) {
        return INFOS.get(columnIdent);
    }

    @Override
    public Collection<ReferenceInfo> columns() {
        return columns;
    }

    @Override
    public List<ReferenceInfo> partitionedByColumns() {
        return ImmutableList.of();
    }

    @Override
    public IndexReferenceInfo indexColumn(ColumnIdent ident) {
        return null;
    }

    @Override
    public RowGranularity rowGranularity() {
        return RowGranularity.DOC;
    }

    @Override
    public TableIdent ident() {
        return ident;
    }

    private void processShardRouting(Map<String, Map<String, List<Integer>>> locations, ShardRouting shardRouting, ShardId shardId) {
        String node;
        if (shardRouting == null) {
            throw new NoShardAvailableActionException(shardId);
        }
        node = shardRouting.currentNodeId();
        Map<String, List<Integer>> nodeMap = locations.get(node);
        if (nodeMap == null) {
            nodeMap = new TreeMap<>();
            locations.put(shardRouting.currentNodeId(), nodeMap);
        }

        List<Integer> shards = nodeMap.get(shardRouting.getIndex());
        if (shards == null) {
            shards = new ArrayList<>();
            nodeMap.put(shardRouting.getIndex(), shards);
        }
        shards.add(shardRouting.id());
    }

    @Override
    public Routing getRouting(WhereClause whereClause, @Nullable String preference) {
        Map<String, Map<String, List<Integer>>> locations = new TreeMap<>();
        GroupShardsIterator shardIterators = clusterService.operationRouting().searchShards(
                clusterService.state(),
                Strings.EMPTY_ARRAY,
                new String[]{index},
                null,
                preference
        );
        ShardRouting shardRouting;
        for (ShardIterator shardIterator : shardIterators) {
            shardRouting = shardIterator.nextOrNull();
            processShardRouting(locations, shardRouting, shardIterator.shardId());
        }

        return new Routing(locations);
    }

    @Override
    public List<ColumnIdent> primaryKey() {
        return primaryKey;
    }

    @Override
    public int numberOfShards() {
        return numberOfShards;
    }

    @Override
    public BytesRef numberOfReplicas() {
        return numberOfReplicas;
    }

    @Override
    public boolean hasAutoGeneratedPrimaryKey() {
        return false;
    }

    @Nullable
    @Override
    public ColumnIdent clusteredBy() {
        return primaryKey.get(0);
    }

    @Override
    public boolean isAlias() {
        return false;
    }

    @Override
    public String[] concreteIndices() {
        return Strings.EMPTY_ARRAY;
    }

    @Override
    public boolean isPartitioned() {
        return false;
    }

    @Override
    public List<PartitionName> partitions() {
        return ImmutableList.of();
    }

    @Override
    public List<ColumnIdent> partitionedBy() {
        return ImmutableList.of();
    }

    @Nullable
    @Override
    public DynamicReference getDynamic(ColumnIdent ident, boolean forWrite) {
        return null;
    }

    @Override
    public ColumnPolicy columnPolicy() {
        return ColumnPolicy.STRICT;
    }

    public DynamicReference getDynamic(ColumnIdent ident) {
        return null;
    }

    @Override
    public Iterator<ReferenceInfo> iterator() {
        return columns.iterator();
    }

    private void registerStaticColumns() {
        for (Tuple<String, DataType> column : staticColumns) {
            ReferenceInfo info = new ReferenceInfo(new ReferenceIdent(ident(), column.v1(), null),
                    RowGranularity.DOC, column.v2());
            if (info.ident().isColumn()) {
                columns.add(info);
            }
            INFOS.put(info.ident().columnIdent(), info);
        }
    }

    public BytesRef blobsPath() {
        return blobsPath;
    }

    @Override
    public TableParameterInfo tableParameterInfo() {
        return tableParameterInfo;
    }

    @Override
    public ImmutableMap<String, Object> tableParameters() {
        return tableParameters;
    }
}

<code block>


package io.crate.metadata.table;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableMap;
import io.crate.analyze.AlterPartitionedTableParameterInfo;
import io.crate.analyze.TableParameterInfo;
import io.crate.analyze.WhereClause;
import io.crate.exceptions.ColumnUnknownException;
import io.crate.metadata.*;
import io.crate.metadata.doc.DocIndexMetaData;
import io.crate.metadata.doc.DocSysColumns;
import io.crate.planner.RowGranularity;
import io.crate.planner.symbol.DynamicReference;
import io.crate.types.DataType;
import org.mockito.Answers;

import javax.annotation.Nullable;
import java.util.Collection;
import java.util.Iterator;
import java.util.List;
import java.util.Map;

import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;

public class TestingTableInfo extends AbstractDynamicTableInfo {

    private final Routing routing;
    private final ColumnIdent clusteredBy;

    public static Builder builder(TableIdent ident, RowGranularity granularity, Routing routing) {
        return new Builder(ident, granularity, routing);
    }

    public static class Builder {

        private final ImmutableList.Builder<ReferenceInfo> columns = ImmutableList.builder();
        private final ImmutableMap.Builder<ColumnIdent, ReferenceInfo> references = ImmutableMap.builder();
        private final ImmutableList.Builder<ReferenceInfo> partitionedByColumns = ImmutableList.builder();
        private final ImmutableList.Builder<ColumnIdent> primaryKey = ImmutableList.builder();
        private final ImmutableList.Builder<ColumnIdent> partitionedBy = ImmutableList.builder();
        private final ImmutableList.Builder<PartitionName> partitions = ImmutableList.builder();
        private final ImmutableMap.Builder<ColumnIdent, IndexReferenceInfo> indexColumns = ImmutableMap.builder();
        private ColumnIdent clusteredBy;


        private final RowGranularity granularity;
        private final TableIdent ident;
        private final Routing routing;
        private boolean isAlias = false;
        private ColumnPolicy columnPolicy = ColumnPolicy.DYNAMIC;

        private SchemaInfo schemaInfo = mock(SchemaInfo.class, Answers.RETURNS_MOCKS.get());

        public Builder(TableIdent ident, RowGranularity granularity, Routing routing) {
            this.granularity = granularity;
            this.routing = routing;
            this.ident = ident;
        }

        private ReferenceInfo genInfo(ColumnIdent columnIdent, DataType type) {
            return new ReferenceInfo(
                    new ReferenceIdent(ident, columnIdent.name(), columnIdent.path()),
                    RowGranularity.DOC, type
            );
        }

        private void addDocSysColumns() {
            for (Map.Entry<ColumnIdent, DataType> entry : DocSysColumns.COLUMN_IDENTS.entrySet()) {
                references.put(
                        entry.getKey(),
                        genInfo(entry.getKey(), entry.getValue())
                );
            }
        }

        public Builder add(String column, DataType type, List<String> path) {
            return add(column, type, path, ColumnPolicy.DYNAMIC);
        }
        public Builder add(String column, DataType type, List<String> path, ColumnPolicy columnPolicy) {
            return add(column, type, path, columnPolicy, ReferenceInfo.IndexType.NOT_ANALYZED, false);
        }
        public Builder add(String column, DataType type, List<String> path, ReferenceInfo.IndexType indexType) {
            return add(column, type, path, ColumnPolicy.DYNAMIC, indexType, false);
        }
        public Builder add(String column, DataType type, List<String> path,
                           boolean partitionBy) {
            return add(column, type, path, ColumnPolicy.DYNAMIC,
                    ReferenceInfo.IndexType.NOT_ANALYZED, partitionBy);
        }

        public Builder add(String column, DataType type, List<String> path,
                           ColumnPolicy columnPolicy, ReferenceInfo.IndexType indexType,
                           boolean partitionBy) {
            RowGranularity rowGranularity = granularity;
            if (partitionBy) {
                rowGranularity = RowGranularity.PARTITION;
            }
            ReferenceInfo info = new ReferenceInfo(new ReferenceIdent(ident, column, path),
                    rowGranularity, type, columnPolicy, indexType);
            if (info.ident().isColumn()) {
                columns.add(info);
            }
            references.put(info.ident().columnIdent(), info);
            if (partitionBy) {
                partitionedByColumns.add(info);
                partitionedBy.add(info.ident().columnIdent());
            }
            return this;
        }

        public Builder addIndex(ColumnIdent columnIdent, ReferenceInfo.IndexType indexType) {
            IndexReferenceInfo.Builder builder = new IndexReferenceInfo.Builder()
                    .ident(new ReferenceIdent(ident, columnIdent))
                    .indexType(indexType);
            indexColumns.put(columnIdent, builder.build());
            return this;
        }

        public Builder addPrimaryKey(String column) {
            primaryKey.add(ColumnIdent.fromPath(column));
            return this;
        }

        public Builder clusteredBy(String clusteredBy) {
            this.clusteredBy = ColumnIdent.fromPath(clusteredBy);
            return this;
        }

        public Builder isAlias(boolean isAlias) {
            this.isAlias = isAlias;
            return this;
        }

        public Builder schemaInfo(SchemaInfo schemaInfo) {
            this.schemaInfo = schemaInfo;
            return this;
        }

        public Builder addPartitions(String... partitionNames) {
            for (String partitionName : partitionNames) {
                PartitionName partition = PartitionName.fromString(partitionName, ident.schema(), ident.name());
                partitions.add(partition);
            }
            return this;
        }

        public TableInfo build() {
            addDocSysColumns();
            return new TestingTableInfo(
                    columns.build(),
                    partitionedByColumns.build(),
                    indexColumns.build(),
                    references.build(),
                    ident,
                    granularity,
                    routing,
                    primaryKey.build(),
                    clusteredBy,
                    isAlias,
                    partitionedBy.build(),
                    partitions.build(),
                    columnPolicy,
                    schemaInfo == null ? mock(SchemaInfo.class, Answers.RETURNS_MOCKS.get()) : schemaInfo);
        }

    }


    private final List<ReferenceInfo> columns;
    private final List<ReferenceInfo> partitionedByColumns;
    private final Map<ColumnIdent, IndexReferenceInfo> indexColumns;
    private final Map<ColumnIdent, ReferenceInfo> references;
    private final TableIdent ident;
    private final RowGranularity granularity;
    private final List<ColumnIdent> primaryKey;
    private final boolean isAlias;
    private final boolean hasAutoGeneratedPrimaryKey;
    private final List<ColumnIdent> partitionedBy;
    private final List<PartitionName> partitions;
    private final ColumnPolicy columnPolicy;
    private final TableParameterInfo tableParameterInfo;


    public TestingTableInfo(List<ReferenceInfo> columns,
                            List<ReferenceInfo> partitionedByColumns,
                            Map<ColumnIdent, IndexReferenceInfo> indexColumns,
                            Map<ColumnIdent, ReferenceInfo> references,
                            TableIdent ident, RowGranularity granularity,
                            Routing routing,
                            List<ColumnIdent> primaryKey,
                            ColumnIdent clusteredBy,
                            boolean isAlias,
                            List<ColumnIdent> partitionedBy,
                            List<PartitionName> partitions,
                            ColumnPolicy columnPolicy,
                            SchemaInfo schemaInfo
                            ) {
        super(schemaInfo);
        this.columns = columns;
        this.partitionedByColumns = partitionedByColumns;
        this.indexColumns = indexColumns;
        this.references = references;
        this.ident = ident;
        this.granularity = granularity;
        this.routing = routing;
        if (primaryKey == null || primaryKey.isEmpty()){
            this.primaryKey = ImmutableList.of(DocIndexMetaData.ID_IDENT);
            this.hasAutoGeneratedPrimaryKey = true;
        } else {
            this.primaryKey = primaryKey;
            this.hasAutoGeneratedPrimaryKey = false;
        }
        this.clusteredBy = clusteredBy;
        this.isAlias = isAlias;
        this.columnPolicy = columnPolicy;
        this.partitionedBy = partitionedBy;
        this.partitions = partitions;
        if (partitionedByColumns.isEmpty()) {
            tableParameterInfo = new TableParameterInfo();
        } else {
            tableParameterInfo = new AlterPartitionedTableParameterInfo();
        }
    }

    @Override
    public ReferenceInfo getReferenceInfo(ColumnIdent columnIdent) {
        return references.get(columnIdent);
    }

    @Override
    public Collection<ReferenceInfo> columns() {
        return columns;
    }


    @Override
    public List<ReferenceInfo> partitionedByColumns() {
        return partitionedByColumns;
    }

    @Override
    public IndexReferenceInfo indexColumn(ColumnIdent ident) {
        return indexColumns.get(ident);
    }

    @Override
    public boolean isPartitioned() {
        return !partitionedByColumns.isEmpty();
    }

    @Override
    public RowGranularity rowGranularity() {
        return granularity;
    }

    @Override
    public TableIdent ident() {
        return ident;
    }

    @Override
    public Routing getRouting(WhereClause whereClause, @Nullable String preference) {
        return routing;
    }

    @Override
    public List<ColumnIdent> primaryKey() {
        return primaryKey;
    }

    @Override
    public boolean hasAutoGeneratedPrimaryKey() {
        return hasAutoGeneratedPrimaryKey;
    }

    @Override
    public ColumnIdent clusteredBy() {
        return clusteredBy;
    }

    @Override
    public boolean isAlias() {
        return isAlias;
    }

    @Override
    public String[] concreteIndices() {
        return new String[]{ident.esName()};
    }

    @Override
    public DynamicReference getDynamic(ColumnIdent ident) {
        if (!ident.isColumn()) {
            ColumnIdent parentIdent = ident.getParent();
            ReferenceInfo parentInfo = getReferenceInfo(parentIdent);
            if (parentInfo != null && parentInfo.columnPolicy() == ColumnPolicy.STRICT) {
                throw new ColumnUnknownException(ident.sqlFqn());
            }
        }
        return new DynamicReference(new ReferenceIdent(ident(), ident), rowGranularity());
    }

    @Override
    public Iterator<ReferenceInfo> iterator() {
        return references.values().iterator();
    }

    @Override
    public List<ColumnIdent> partitionedBy() {
        return partitionedBy;
    }

    @Override
    public List<PartitionName> partitions() {
        return partitions;
    }

    @Override
    public ColumnPolicy columnPolicy() {
        return columnPolicy;
    }

    @Override
    public TableParameterInfo tableParameterInfo () {
        return tableParameterInfo;
    }

    @Override
    public SchemaInfo schemaInfo() {
        final SchemaInfo schemaInfo = super.schemaInfo();
        when(schemaInfo.name()).thenReturn(ident.schema());
        return schemaInfo;
    }
}

<code block>


package io.crate.analyze;

import com.google.common.collect.HashMultimap;
import com.google.common.collect.ImmutableMap;
import com.google.common.collect.Multimap;
import io.crate.metadata.*;
import io.crate.metadata.table.TableInfo;
import io.crate.sql.tree.*;
import io.crate.types.*;
import org.apache.lucene.analysis.*;
import org.elasticsearch.common.Nullable;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.index.analysis.CustomAnalyzer;

import java.util.*;

public class MetaDataToASTNodeResolver {


    public MetaDataToASTNodeResolver() {}

    public static CreateTable resolveCreateTable(TableInfo info) {
        Extractor extractor = new Extractor(info);
        return extractor.extractCreateTable();
    }

    private static class Extractor {

        private final TableInfo tableInfo;

        public Extractor(TableInfo tableInfo) {
            this.tableInfo = tableInfo;
        }

        private Table extractTable() {
            return new Table(QualifiedName.of(tableInfo.ident().fqn()), false);
        }

        private List<TableElement> extractTableElements() {
            List<TableElement> elements = new ArrayList<>();

            elements.addAll(extractColumnDefinitions(null));

            PrimaryKeyConstraint pk = extractPrimaryKeyConstraint();
            if (pk != null) elements.add(pk);

            elements.addAll(extractIndexDefinitions());
            return elements;
        }

        private List<ColumnDefinition> extractColumnDefinitions(@Nullable ColumnIdent parent) {
            Iterator<ReferenceInfo> referenceInfoIterator = tableInfo.iterator();
            List<ColumnDefinition> elements = new ArrayList<>();
            while (referenceInfoIterator.hasNext()) {
                ReferenceInfo info = referenceInfoIterator.next();
                ColumnIdent ident = info.ident().columnIdent();
                if (ident.isSystemColumn()) continue;
                if (parent != null && !ident.isChildOf(parent)) continue;
                if (parent == null && !ident.path().isEmpty()) continue;

                ColumnType columnType = null;
                if (info.type().equals(DataTypes.OBJECT)) {
                    columnType = new ObjectColumnType(info.columnPolicy().value(), extractColumnDefinitions(ident));
                } else if (info.type().id() == ArrayType.ID) {
                    DataType innerType = ((CollectionType)info.type()).innerType();
                    ColumnType innerColumnType = null;
                    if (innerType.equals(DataTypes.OBJECT)) {
                        innerColumnType = new ObjectColumnType(info.columnPolicy().value(), extractColumnDefinitions(ident));
                    } else {
                        innerColumnType = new ColumnType(innerType.getName());
                    }
                    columnType = CollectionColumnType.array(innerColumnType);
                } else if (info.type().id() == SetType.ID) {
                    ColumnType innerColumnType = new ColumnType(((CollectionType) info.type()).innerType().getName());
                    columnType = CollectionColumnType.set(innerColumnType);
                } else {
                    columnType = new ColumnType(info.type().getName());
                }

                String columnName = ident.isColumn() ? ident.name() : ident.path().get(ident.path().size()-1);
                List<ColumnConstraint> constraints = new ArrayList<>();
                if (info.indexType().equals(ReferenceInfo.IndexType.NO)) {
                    constraints.add(IndexColumnConstraint.OFF);
                }
                ColumnDefinition column = new ColumnDefinition(columnName, columnType, constraints);
                elements.add(column);
            }
            return elements;
        }

        private PrimaryKeyConstraint extractPrimaryKeyConstraint() {
            if (!tableInfo.primaryKey().isEmpty()) {
                if (tableInfo.primaryKey().size() == 1 && tableInfo.primaryKey().get(0).isSystemColumn()) {
                    return null;
                }
                return new PrimaryKeyConstraint(expressionsFromColumns(tableInfo.primaryKey()));
            }
            return null;
        }

        private List<IndexDefinition> extractIndexDefinitions() {
            List<IndexDefinition> elements = new ArrayList<>();
            Iterator indexColumns = tableInfo.indexColumns();
            if (indexColumns != null) {
                while (indexColumns.hasNext()) {
                    IndexReferenceInfo indexRef = (IndexReferenceInfo) indexColumns.next();
                    String name = indexRef.ident().columnIdent().name();
                    List<Expression> columns = expressionsFromReferenceInfos(indexRef.columns());
                    if (indexRef.indexType().equals(ReferenceInfo.IndexType.ANALYZED)) {
                        String analyzer = indexRef.analyzer();
                        GenericProperties properties = new GenericProperties();
                        if (analyzer != null) {
                            properties.add(new GenericProperty(FulltextAnalyzerResolver.CustomType.ANALYZER.getName(), new StringLiteral(analyzer)));
                        }
                        elements.add(new IndexDefinition(name, "fulltext", columns, properties));
                    } else if (indexRef.indexType().equals(ReferenceInfo.IndexType.NOT_ANALYZED)) {
                        elements.add(new IndexDefinition(name, "plain", columns, null));
                    }

                }
            }
            return elements;
        }

        private List<CrateTableOption> extractTableOptions() {
            List<CrateTableOption> options = new ArrayList<>();

            Expression clusterColumn = null;
            if (tableInfo.clusteredBy() != null) {
                clusterColumn = expressionFromColumn(tableInfo.clusteredBy());
            }
            Expression numShards = new LongLiteral(Integer.toString(tableInfo.numberOfShards()));
            options.add(new ClusteredBy(clusterColumn, numShards));

            if (!tableInfo.partitionedBy().isEmpty()) {
                options.add(new PartitionedBy(expressionsFromColumns(tableInfo.partitionedBy())));
            }
            return options;
        }

        private GenericProperties extractTableProperties() {
            GenericProperties properties = new GenericProperties();
            ImmutableMap<String, Object> tableParameters = tableInfo.tableParameters();
            for (Map.Entry<String, Object> entry : tableParameters.entrySet()) {
                properties.add(new GenericProperty(entry.getKey(), literalFromObject(entry.getValue())));
            }
            return properties;
        }

        private static Expression literalFromObject(Object value) {
            Expression expression = null;
            if (value == null) {
                expression = new NullLiteral();
            } else if (value instanceof String) {
                expression = new StringLiteral((String) value);
            } else if (value instanceof Number) {
                if (value instanceof Float || value instanceof Double) {
                    expression = new DoubleLiteral(value.toString());
                } else if (value instanceof Short || value instanceof Integer || value instanceof Long){
                    expression = new LongLiteral(value.toString());
                }
            } else if (value instanceof Boolean) {
                expression = new BooleanLiteral(value.toString());
            } else if (value instanceof Object[]) {
                List<Expression> expressions = new ArrayList<>();
                for (Object o : (Object[]) value) {
                    expressions.add(literalFromObject(o));
                }
                expression = new ArrayLiteral(expressions);
            } else if (value instanceof Map) {
                Multimap<String, Expression> map = HashMultimap.create();
                @SuppressWarnings("unchecked") Map<String, Object> valueMap = (Map<String, Object>) value;
                for (String key : valueMap.keySet()) {
                    map.put(key, literalFromObject(valueMap.get(key)));
                }
                expression = new ObjectLiteral(map);
            }
            return expression;
        }

        private boolean extractIfNotExists() {
            return true;
        }

        private CreateTable extractCreateTable() {
            Table table = extractTable();
            List<TableElement> tableElements = extractTableElements();
            List<CrateTableOption> tableOptions = extractTableOptions();
            GenericProperties tableProperties = extractTableProperties();
            boolean ifNotExists = extractIfNotExists();
            return new CreateTable(table, tableElements, tableOptions, tableProperties, ifNotExists);
        }


        private Expression expressionFromColumn(ColumnIdent ident) {
            Expression fqn = new QualifiedNameReference(QualifiedName.of(ident.getRoot().fqn()));
            for (String child : ident.path()) {
                fqn = new SubscriptExpression(fqn, literalFromObject(child));
            }
            return fqn;
        }

        private List<Expression> expressionsFromReferenceInfos(List<ReferenceInfo> columns) {
            List<Expression> expressions = new ArrayList<>(columns.size());
            for (ReferenceInfo ident : columns) {
                expressions.add(expressionFromColumn(ident.ident().columnIdent()));
            }
            return expressions;
        }

        private List<Expression> expressionsFromColumns(List<ColumnIdent> columns) {
            List<Expression> expressions = new ArrayList<>(columns.size());
            for (ColumnIdent ident : columns) {
                expressions.add(expressionFromColumn(ident));
            }
            return expressions;
        }

    }
}

<code block>


package io.crate.metadata;

import com.google.common.base.MoreObjects;
import com.google.common.base.Preconditions;
import io.crate.metadata.table.ColumnPolicy;
import io.crate.planner.RowGranularity;
import io.crate.types.DataTypes;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;

import javax.annotation.Nullable;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;


public class IndexReferenceInfo extends ReferenceInfo {

    public static class Builder {
        private ReferenceIdent ident;
        private IndexType indexType = IndexType.ANALYZED;
        private List<ReferenceInfo> columns = new ArrayList<>();
        private String analyzer = null;

        public Builder ident(ReferenceIdent ident) {
            this.ident = ident;
            return this;
        }

        public Builder indexType(IndexType indexType) {
            this.indexType = indexType;
            return this;
        }

        public Builder addColumn(ReferenceInfo info) {
            this.columns.add(info);
            return this;
        }

        public Builder analyzer(String name) {
            this.analyzer = name;
            return this;
        }

        public IndexReferenceInfo build() {
            Preconditions.checkNotNull(ident, "ident is null");
            return new IndexReferenceInfo(ident, indexType, columns, analyzer);
        }
    }

    private String analyzer;
    private List<ReferenceInfo> columns;

    public IndexReferenceInfo(ReferenceIdent ident,
                         IndexType indexType,
                         List<ReferenceInfo> columns,
                         @Nullable String analyzer) {
        super(ident, RowGranularity.DOC, DataTypes.STRING, ColumnPolicy.DYNAMIC, indexType);
        this.columns = MoreObjects.firstNonNull(columns, Collections.<ReferenceInfo>emptyList());
        this.analyzer = analyzer;
    }

    public List<ReferenceInfo> columns() {
        return columns;
    }

    public String analyzer() {
        return analyzer;
    }

    @Override
    public boolean equals(Object o) {
        if (this == o) return true;
        if (o == null || getClass() != o.getClass()) return false;
        if (!super.equals(o)) return false;

        IndexReferenceInfo that = (IndexReferenceInfo) o;

        if (analyzer != null ? !analyzer.equals(that.analyzer) : that.analyzer != null)
            return false;
        if (!columns.equals(that.columns)) return false;

        return true;
    }

    @Override
    public int hashCode() {
        int result = super.hashCode();
        result = 31 * result + (analyzer != null ? analyzer.hashCode() : 0);
        result = 31 * result + columns.hashCode();
        return result;
    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        super.writeTo(out);

    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        super.readFrom(in);
    }

    @Override
    public String toString() {
        return MoreObjects.toStringHelper(this).add("ident", ident()).toString();
    }
}

<code block>


package io.crate.metadata.table;

import com.google.common.base.Predicate;
import com.google.common.base.Predicates;
import com.google.common.collect.ImmutableMap;
import io.crate.analyze.TableParameterInfo;
import io.crate.analyze.WhereClause;
import io.crate.metadata.*;
import io.crate.planner.RowGranularity;
import io.crate.planner.symbol.DynamicReference;
import org.apache.lucene.util.BytesRef;

import javax.annotation.Nullable;
import java.util.Collection;
import java.util.Iterator;
import java.util.List;

public interface TableInfo extends Iterable<ReferenceInfo> {


    public static final String NULL_NODE_ID = "";
    public static final Predicate<String> IS_NOT_NULL_NODE_ID = Predicates.not(Predicates.equalTo(TableInfo.NULL_NODE_ID));


    public SchemaInfo schemaInfo();


    @Nullable
    public ReferenceInfo getReferenceInfo(ColumnIdent columnIdent);


    public Collection<ReferenceInfo> columns();

    public List<ReferenceInfo> partitionedByColumns();

    @Nullable
    public IndexReferenceInfo indexColumn(ColumnIdent ident);

    @Nullable
    public Iterator<IndexReferenceInfo> indexColumns();

    public RowGranularity rowGranularity();

    public TableIdent ident();

    public Routing getRouting(WhereClause whereClause, @Nullable String preference);

    public List<ColumnIdent> primaryKey();

    public int numberOfShards();

    public BytesRef numberOfReplicas();

    public boolean hasAutoGeneratedPrimaryKey();

    @Nullable
    public ColumnIdent clusteredBy();


    public boolean isAlias();

    public String[] concreteIndices();

    public List<PartitionName> partitions();


    public List<ColumnIdent> partitionedBy();


    public boolean isPartitioned();


    @Nullable
    DynamicReference getDynamic(ColumnIdent ident, boolean forWrite);


    public ColumnPolicy columnPolicy();

    public TableParameterInfo tableParameterInfo();

    public ImmutableMap<String, Object> tableParameters();

}

<code block>
package io.crate.metadata.table;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableMap;
import io.crate.analyze.TableParameterInfo;
import io.crate.metadata.ColumnIdent;
import io.crate.metadata.IndexReferenceInfo;
import io.crate.metadata.PartitionName;
import io.crate.metadata.ReferenceInfo;
import io.crate.planner.symbol.DynamicReference;
import org.apache.lucene.util.BytesRef;

import javax.annotation.Nullable;
import java.util.*;

public abstract class AbstractTableInfo implements TableInfo {

    private static final BytesRef ZERO_REPLICAS = new BytesRef("0");
    private final SchemaInfo schemaInfo;

    protected AbstractTableInfo(SchemaInfo schemaInfo) {
        this.schemaInfo = schemaInfo;
    }

    @Override
    public SchemaInfo schemaInfo() {
        return schemaInfo;
    }

    @Override
    public int numberOfShards() {
        return 1;
    }

    @Override
    public BytesRef numberOfReplicas() {
        return ZERO_REPLICAS;
    }

    @Override
    public boolean hasAutoGeneratedPrimaryKey() {
        return false;
    }

    @Override
    public boolean isAlias() {
        return false;
    }

    @Override
    public boolean isPartitioned() {
        return false;
    }

    @Override
    public List<ReferenceInfo> partitionedByColumns() {
        return ImmutableList.of();
    }

    @Nullable
    @Override
    public IndexReferenceInfo indexColumn(ColumnIdent ident) {
        return null;
    }

    @Nullable
    @Override
    public Iterator<IndexReferenceInfo> indexColumns() {
        return null;
    }

    @Nullable
    @Override
    public ColumnIdent clusteredBy() {
        return null;
    }

    @Nullable
    public DynamicReference getDynamic(ColumnIdent ident) {
        return getDynamic(ident, false);
    }

    @Nullable
    @Override
    public DynamicReference getDynamic(ColumnIdent ident, boolean forWrite) {
        return null;
    }

    @Override
    public List<PartitionName> partitions() {
        return new ArrayList<>(0);
    }

    @Override
    public List<ColumnIdent> partitionedBy() {
        return ImmutableList.of();
    }

    @Override
    public TableParameterInfo tableParameterInfo() {
        return null;
    }

    @Override
    public ImmutableMap<String, Object> tableParameters() {
        return ImmutableMap.of();
    }

    @Override
    public String toString() {
        return String.format("%s.%s", schemaInfo.name(), ident().name());
    }

}

<code block>


package io.crate.metadata.doc;

import com.google.common.base.MoreObjects;
import com.google.common.collect.*;
import io.crate.Constants;
import io.crate.analyze.TableParameter;
import io.crate.analyze.TableParameterInfo;
import io.crate.core.NumberOfReplicas;
import io.crate.exceptions.TableAliasSchemaException;
import io.crate.metadata.*;
import io.crate.metadata.settings.CrateTableSettings;
import io.crate.metadata.table.ColumnPolicy;
import io.crate.planner.RowGranularity;
import io.crate.types.ArrayType;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.action.admin.indices.alias.Alias;
import org.elasticsearch.action.admin.indices.template.put.PutIndexTemplateRequest;
import org.elasticsearch.action.admin.indices.template.put.TransportPutIndexTemplateAction;
import org.elasticsearch.cluster.metadata.IndexMetaData;
import org.elasticsearch.cluster.metadata.MappingMetaData;
import org.elasticsearch.common.Booleans;
import org.elasticsearch.common.collect.Tuple;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.common.xcontent.XContentHelper;

import java.io.IOException;
import java.util.*;

public class DocIndexMetaData {

    private static final String ID = "_id";
    public static final ColumnIdent ID_IDENT = new ColumnIdent(ID);
    private final IndexMetaData metaData;

    private final MappingMetaData defaultMappingMetaData;
    private final Map<String, Object> defaultMappingMap;

    private final Map<ColumnIdent, IndexReferenceInfo.Builder> indicesBuilder = new HashMap<>();

    private final ImmutableSortedSet.Builder<ReferenceInfo> columnsBuilder = ImmutableSortedSet.orderedBy(new Comparator<ReferenceInfo>() {
        @Override
        public int compare(ReferenceInfo o1, ReferenceInfo o2) {
            return o1.ident().columnIdent().fqn().compareTo(o2.ident().columnIdent().fqn());
        }
    });


    private final ImmutableMap.Builder<ColumnIdent, ReferenceInfo> referencesBuilder = ImmutableSortedMap.naturalOrder();
    private final ImmutableList.Builder<ReferenceInfo> partitionedByColumnsBuilder = ImmutableList.builder();

    private final TableIdent ident;
    private final int numberOfShards;
    private final BytesRef numberOfReplicas;
    private final ImmutableMap<String, Object> tableParameters;
    private Map<String, Object> metaMap;
    private Map<String, Object> metaColumnsMap;
    private Map<String, Object> indicesMap;
    private List<List<String>> partitionedByList;
    private ImmutableList<ReferenceInfo> columns;
    private ImmutableMap<ColumnIdent, IndexReferenceInfo> indices;
    private ImmutableList<ReferenceInfo> partitionedByColumns;
    private ImmutableMap<ColumnIdent, ReferenceInfo> references;
    private ImmutableList<ColumnIdent> primaryKey;
    private ColumnIdent routingCol;
    private ImmutableList<ColumnIdent> partitionedBy;
    private final boolean isAlias;
    private final Set<String> aliases;
    private boolean hasAutoGeneratedPrimaryKey = false;

    private ColumnPolicy columnPolicy = ColumnPolicy.DYNAMIC;

    private final static ImmutableMap<String, DataType> dataTypeMap = ImmutableMap.<String, DataType>builder()
            .put("date", DataTypes.TIMESTAMP)
            .put("string", DataTypes.STRING)
            .put("boolean", DataTypes.BOOLEAN)
            .put("byte", DataTypes.BYTE)
            .put("short", DataTypes.SHORT)
            .put("integer", DataTypes.INTEGER)
            .put("long", DataTypes.LONG)
            .put("float", DataTypes.FLOAT)
            .put("double", DataTypes.DOUBLE)
            .put("ip", DataTypes.IP)
            .put("geo_point", DataTypes.GEO_POINT)
            .put("object", DataTypes.OBJECT)
            .put("nested", DataTypes.OBJECT).build();

    public DocIndexMetaData(IndexMetaData metaData, TableIdent ident) throws IOException {
        this.ident = ident;
        this.metaData = metaData;
        this.isAlias = !metaData.getIndex().equals(ident.esName());
        this.numberOfShards = metaData.numberOfShards();
        final Settings settings = metaData.getSettings();
        this.numberOfReplicas = NumberOfReplicas.fromSettings(settings);
        this.aliases = ImmutableSet.copyOf(metaData.aliases().keys().toArray(String.class));
        this.defaultMappingMetaData = this.metaData.mappingOrDefault(Constants.DEFAULT_MAPPING_TYPE);
        if (defaultMappingMetaData == null) {
            this.defaultMappingMap = new HashMap<>();
        } else {
            this.defaultMappingMap = this.defaultMappingMetaData.sourceAsMap();
        }
        this.tableParameters = TableParameterInfo.tableParametersFromIndexMetaData(metaData);

        prepareCrateMeta();
    }

    @SuppressWarnings("unchecked")
    private static <T> T getNested(Map map, String key) {
        return (T) map.get(key);
    }

    private void prepareCrateMeta() {
        metaMap = getNested(defaultMappingMap, "_meta");
        if (metaMap != null) {
            indicesMap = getNested(metaMap, "indices");
            if (indicesMap == null) {
                indicesMap = ImmutableMap.of();
            }
            metaColumnsMap = getNested(metaMap, "columns");
            if (metaColumnsMap == null) {
                metaColumnsMap = ImmutableMap.of();
            }
            partitionedByList = getNested(metaMap, "partitioned_by");
            if (partitionedByList == null) {
                partitionedByList = ImmutableList.of();
            }
        } else {
            metaMap = new HashMap<>();
            indicesMap = new HashMap<>();
            metaColumnsMap = new HashMap<>();
            partitionedByList = ImmutableList.of();
        }
    }

    private void addPartitioned(ColumnIdent column, DataType type) {
        add(column, type, ColumnPolicy.DYNAMIC, ReferenceInfo.IndexType.NOT_ANALYZED, true);
    }

    private void add(ColumnIdent column, DataType type, ReferenceInfo.IndexType indexType) {
        add(column, type, ColumnPolicy.DYNAMIC, indexType, false);
    }

    private void add(ColumnIdent column, DataType type, ColumnPolicy columnPolicy,
                     ReferenceInfo.IndexType indexType, boolean partitioned) {
        ReferenceInfo info = newInfo(column, type, columnPolicy, indexType);

        if (partitioned || !(partitionedBy != null && partitionedBy.contains(column))) {
            if (info.ident().isColumn()) {
                columnsBuilder.add(info);
            }
            referencesBuilder.put(info.ident().columnIdent(), info);
        }
        if (partitioned) {
            partitionedByColumnsBuilder.add(info);
        }
    }

    private ReferenceInfo newInfo(ColumnIdent column,
                                  DataType type,
                                  ColumnPolicy columnPolicy,
                                  ReferenceInfo.IndexType indexType) {
        RowGranularity granularity = RowGranularity.DOC;
        if (partitionedBy.contains(column)) {
            granularity = RowGranularity.PARTITION;
        }
        return new ReferenceInfo(new ReferenceIdent(ident, column), granularity, type,
                columnPolicy, indexType);
    }


    public static DataType getColumnDataType(Map<String, Object> columnProperties) {
        DataType type;
        String typeName = (String) columnProperties.get("type");

        if (typeName == null) {
            if (columnProperties.containsKey("properties")) {
                type = DataTypes.OBJECT;
            } else {
                return DataTypes.NOT_SUPPORTED;
            }
        } else if (typeName.equalsIgnoreCase("array")) {

            Map<String, Object> innerProperties = getNested(columnProperties, "inner");
            DataType innerType = getColumnDataType(innerProperties);
            type = new ArrayType(innerType);
        } else {
            typeName = typeName.toLowerCase(Locale.ENGLISH);
            type = MoreObjects.firstNonNull(dataTypeMap.get(typeName), DataTypes.NOT_SUPPORTED);
        }
        return type;
    }

    private ReferenceInfo.IndexType getColumnIndexType(Map<String, Object> columnProperties) {
        String indexType = (String) columnProperties.get("index");
        String analyzerName = (String) columnProperties.get("analyzer");
        if (indexType != null) {
            if (indexType.equals(ReferenceInfo.IndexType.NOT_ANALYZED.toString())) {
                return ReferenceInfo.IndexType.NOT_ANALYZED;
            } else if (indexType.equals(ReferenceInfo.IndexType.NO.toString())) {
                return ReferenceInfo.IndexType.NO;
            } else if (indexType.equals(ReferenceInfo.IndexType.ANALYZED.toString())
                    && analyzerName != null && !analyzerName.equals("keyword")) {
                return ReferenceInfo.IndexType.ANALYZED;
            }
        } 
        else if (analyzerName != null && !analyzerName.equals("keyword")) {
            return ReferenceInfo.IndexType.ANALYZED;
        }
        return ReferenceInfo.IndexType.NOT_ANALYZED;
    }

    private ColumnIdent childIdent(ColumnIdent ident, String name) {
        if (ident == null) {
            return new ColumnIdent(name);
        }
        if (ident.isColumn()) {
            return new ColumnIdent(ident.name(), name);
        } else {
            ImmutableList.Builder<String> builder = ImmutableList.builder();
            for (String s : ident.path()) {
                builder.add(s);
            }
            builder.add(name);
            return new ColumnIdent(ident.name(), builder.build());
        }
    }


    @SuppressWarnings("unchecked")
    private void internalExtractColumnDefinitions(ColumnIdent columnIdent,
                                                  Map<String, Object> propertiesMap) {
        if (propertiesMap == null) {
            return;
        }

        for (Map.Entry<String, Object> columnEntry : propertiesMap.entrySet()) {
            Map<String, Object> columnProperties = (Map) columnEntry.getValue();
            DataType columnDataType = getColumnDataType(columnProperties);
            ColumnIdent newIdent = childIdent(columnIdent, columnEntry.getKey());

            columnProperties = furtherColumnProperties(columnProperties);
            ReferenceInfo.IndexType columnIndexType = getColumnIndexType(columnProperties);
            if (columnDataType == DataTypes.OBJECT
                    || (columnDataType.id() == ArrayType.ID
                    && ((ArrayType) columnDataType).innerType() == DataTypes.OBJECT)) {
                ColumnPolicy columnPolicy =
                        ColumnPolicy.of(columnProperties.get("dynamic"));
                add(newIdent, columnDataType, columnPolicy, ReferenceInfo.IndexType.NO, false);

                if (columnProperties.get("properties") != null) {

                    internalExtractColumnDefinitions(newIdent, (Map<String, Object>) columnProperties.get("properties"));
                }
            } else if (columnDataType != DataTypes.NOT_SUPPORTED) {
                List<String> copyToColumns = getNested(columnProperties, "copy_to");


                if (copyToColumns != null) {
                    for (String copyToColumn : copyToColumns) {
                        ColumnIdent targetIdent = ColumnIdent.fromPath(copyToColumn);
                        IndexReferenceInfo.Builder builder = getOrCreateIndexBuilder(targetIdent);
                        builder.addColumn(newInfo(newIdent, columnDataType, ColumnPolicy.DYNAMIC, columnIndexType));
                    }
                }

                if (indicesMap.containsKey(newIdent.fqn())) {
                    IndexReferenceInfo.Builder builder = getOrCreateIndexBuilder(newIdent);
                    builder.indexType(columnIndexType)
                            .ident(new ReferenceIdent(ident, newIdent))
                            .analyzer((String) columnProperties.get("analyzer"));
                } else {
                    add(newIdent, columnDataType, columnIndexType);
                }
            }
        }
    }


    private Map<String, Object> furtherColumnProperties(Map<String, Object> columnProperties) {
        if (columnProperties.get("inner") != null) {
            return (Map<String, Object>) columnProperties.get("inner");
        } else {
            return columnProperties;
        }
    }

    private IndexReferenceInfo.Builder getOrCreateIndexBuilder(ColumnIdent ident) {
        IndexReferenceInfo.Builder builder = indicesBuilder.get(ident);
        if (builder == null) {
            builder = new IndexReferenceInfo.Builder();
            indicesBuilder.put(ident, builder);
        }
        return builder;
    }

    private ImmutableList<ColumnIdent> getPrimaryKey() {
        Map<String, Object> metaMap = getNested(defaultMappingMap, "_meta");
        if (metaMap == null) {
            hasAutoGeneratedPrimaryKey = true;
            return ImmutableList.of(ID_IDENT);
        }

        ImmutableList.Builder<ColumnIdent> builder = ImmutableList.builder();
        Object pKeys = metaMap.get("primary_keys");
        if (pKeys == null) {
            hasAutoGeneratedPrimaryKey = true;
            return ImmutableList.of(ID_IDENT);
        }

        if (pKeys instanceof String) {
            builder.add(ColumnIdent.fromPath((String) pKeys));
        } else if (pKeys instanceof Collection) {
            Collection keys = (Collection) pKeys;
            if (keys.isEmpty()) {
                hasAutoGeneratedPrimaryKey = true;
                return ImmutableList.of(ID_IDENT);
            }
            for (Object pkey : keys) {
                builder.add(ColumnIdent.fromPath(pkey.toString()));
            }
        }
        return builder.build();
    }

    private ImmutableList<ColumnIdent> getPartitionedBy() {
        ImmutableList.Builder<ColumnIdent> builder = ImmutableList.builder();
        for (List<String> partitionedByInfo : partitionedByList) {
            builder.add(ColumnIdent.fromPath(partitionedByInfo.get(0)));
        }
        return builder.build();
    }

    private ColumnPolicy getColumnPolicy() {
        Object dynamic = getNested(defaultMappingMap, "dynamic");
        if (ColumnPolicy.STRICT.value().equals(String.valueOf(dynamic).toLowerCase(Locale.ENGLISH))) {
            return ColumnPolicy.STRICT;
        } else if (Booleans.isExplicitFalse(String.valueOf(dynamic))) {
            return ColumnPolicy.IGNORED;
        } else {
            return ColumnPolicy.DYNAMIC;
        }
    }

    private void createColumnDefinitions() {
        Map<String, Object> propertiesMap = getNested(defaultMappingMap, "properties");
        internalExtractColumnDefinitions(null, propertiesMap);
        extractPartitionedByColumns();
    }

    private ImmutableMap<ColumnIdent, IndexReferenceInfo> createIndexDefinitions() {
        ImmutableMap.Builder<ColumnIdent, IndexReferenceInfo> builder = ImmutableMap.builder();
        for (Map.Entry<ColumnIdent, IndexReferenceInfo.Builder> entry : indicesBuilder.entrySet()) {
            builder.put(entry.getKey(), entry.getValue().build());
        }
        indices = builder.build();
        return indices;
    }

    private void extractPartitionedByColumns() {
        for (Tuple<ColumnIdent, DataType> partitioned : PartitionedByMappingExtractor.extractPartitionedByColumns(partitionedByList)) {
            addPartitioned(partitioned.v1(), partitioned.v2());
        }
    }

    private ColumnIdent getRoutingCol() {
        if (defaultMappingMetaData != null) {
            Map<String, Object> metaMap = getNested(defaultMappingMap, "_meta");
            if (metaMap != null) {
                String routingPath = (String) metaMap.get("routing");
                if (routingPath != null) {
                    return ColumnIdent.fromPath(routingPath);
                }
            }
        }
        if (primaryKey.size() == 1) {
            return primaryKey.get(0);
        }
        return ID_IDENT;
    }

    public DocIndexMetaData build() {
        partitionedBy = getPartitionedBy();
        columnPolicy = getColumnPolicy();
        createColumnDefinitions();
        indices = createIndexDefinitions();
        columns = ImmutableList.copyOf(columnsBuilder.build());
        partitionedByColumns = partitionedByColumnsBuilder.build();

        for (Tuple<ColumnIdent, ReferenceInfo> sysColumn : DocSysColumns.forTable(ident)) {
            referencesBuilder.put(sysColumn.v1(), sysColumn.v2());
        }
        references = referencesBuilder.build();
        primaryKey = getPrimaryKey();
        routingCol = getRoutingCol();
        return this;
    }

    public ImmutableMap<ColumnIdent, ReferenceInfo> references() {
        return references;
    }

    public ImmutableList<ReferenceInfo> columns() {
        return columns;
    }

    public ImmutableMap<ColumnIdent, IndexReferenceInfo> indices() {
        return indices;
    }

    public ImmutableList<ReferenceInfo> partitionedByColumns() {
        return partitionedByColumns;
    }

    public ImmutableList<ColumnIdent> primaryKey() {
        return primaryKey;
    }

    public ColumnIdent routingCol() {
        return routingCol;
    }


    public boolean schemaEquals(DocIndexMetaData other) {
        if (this == other) return true;
        if (other == null) return false;



        if (columns != null ? !columns.equals(other.columns) : other.columns != null) return false;
        if (primaryKey != null ? !primaryKey.equals(other.primaryKey) : other.primaryKey != null) return false;
        if (indices != null ? !indices.equals(other.indices) : other.indices != null) return false;
        if (references != null ? !references.equals(other.references) : other.references != null) return false;
        if (routingCol != null ? !routingCol.equals(other.routingCol) : other.routingCol != null) return false;

        return true;
    }

    protected DocIndexMetaData merge(DocIndexMetaData other,
                                     TransportPutIndexTemplateAction transportPutIndexTemplateAction,
                                     boolean thisIsCreatedFromTemplate) throws IOException {
        if (schemaEquals(other)) {
            return this;
        } else if (thisIsCreatedFromTemplate) {
            if (this.references.size() < other.references.size()) {



                updateTemplate(other, transportPutIndexTemplateAction, this.metaData.settings());

                return new DocIndexMetaData(IndexMetaData.builder(other.metaData).settings(this.metaData.settings()).build(), other.ident).build();
            } else if (references().size() == other.references().size() &&
                    !references().keySet().equals(other.references().keySet())) {
                XContentHelper.update(defaultMappingMap, other.defaultMappingMap, false);

                updateTemplate(this, transportPutIndexTemplateAction, this.metaData.settings());
                return this;
            }

            return this;
        } else {
            throw new TableAliasSchemaException(other.ident.name());
        }
    }

    private void updateTemplate(DocIndexMetaData md,
                                TransportPutIndexTemplateAction transportPutIndexTemplateAction,
                                Settings updateSettings) {
        String templateName = PartitionName.templateName(ident.schema(), ident.name());
        PutIndexTemplateRequest request = new PutIndexTemplateRequest(templateName)
                .mapping(Constants.DEFAULT_MAPPING_TYPE, md.defaultMappingMap)
                .create(false)
                .settings(updateSettings)
                .template(templateName + "*");
        for (String alias : md.aliases()) {
            request = request.alias(new Alias(alias));
        }
        transportPutIndexTemplateAction.execute(request);
    }


    public String concreteIndexName() {
        return metaData.index();
    }

    public boolean isAlias() {
        return isAlias;
    }

    public Set<String> aliases() {
        return aliases;
    }

    public boolean hasAutoGeneratedPrimaryKey() {
        return hasAutoGeneratedPrimaryKey;
    }

    public int numberOfShards() {
        return numberOfShards;
    }

    public BytesRef numberOfReplicas() {
        return numberOfReplicas;
    }

    public ImmutableList<ColumnIdent> partitionedBy() {
        return partitionedBy;
    }

    public ColumnPolicy columnPolicy() {
        return columnPolicy;
    }

    public ImmutableMap<String, Object> tableParameters() {
        return tableParameters;
    }
}

<code block>


package io.crate.metadata.doc;

import io.crate.Constants;
import io.crate.exceptions.TableUnknownException;
import io.crate.exceptions.UnhandledServerException;
import io.crate.metadata.PartitionName;
import io.crate.metadata.TableIdent;
import org.elasticsearch.action.admin.indices.template.put.TransportPutIndexTemplateAction;
import org.elasticsearch.action.support.IndicesOptions;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.cluster.metadata.IndexMetaData;
import org.elasticsearch.cluster.metadata.IndexTemplateMetaData;
import org.elasticsearch.cluster.metadata.MetaData;
import org.elasticsearch.common.logging.ESLogger;
import org.elasticsearch.common.logging.Loggers;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.indices.IndexMissingException;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import java.util.Locale;
import java.util.concurrent.ExecutorService;

import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_REPLICAS;
import static org.elasticsearch.cluster.metadata.IndexMetaData.SETTING_NUMBER_OF_SHARDS;

public class DocTableInfoBuilder {

    private final TableIdent ident;
    private ExecutorService executorService;
    private final boolean checkAliasSchema;
    private final DocSchemaInfo docSchemaInfo;
    private final ClusterService clusterService;
    private final TransportPutIndexTemplateAction transportPutIndexTemplateAction;
    private final MetaData metaData;
    private String[] concreteIndices;
    private static final ESLogger logger = Loggers.getLogger(DocTableInfoBuilder.class);

    public DocTableInfoBuilder(DocSchemaInfo docSchemaInfo,
                               TableIdent ident,
                               ClusterService clusterService,
                               TransportPutIndexTemplateAction transportPutIndexTemplateAction,
                               ExecutorService executorService,
                               boolean checkAliasSchema) {
        this.docSchemaInfo = docSchemaInfo;
        this.clusterService = clusterService;
        this.transportPutIndexTemplateAction = transportPutIndexTemplateAction;
        this.ident = ident;
        this.executorService = executorService;
        this.metaData = clusterService.state().metaData();
        this.checkAliasSchema = checkAliasSchema;
    }

    private DocIndexMetaData docIndexMetaData() {
        DocIndexMetaData docIndexMetaData;
        String templateName = PartitionName.templateName(ident.schema(), ident.name());
        boolean createdFromTemplate = false;
        if (metaData.getTemplates().containsKey(templateName)) {
            docIndexMetaData = buildDocIndexMetaDataFromTemplate(ident.esName(), templateName);
            createdFromTemplate = true;
            concreteIndices = metaData.concreteIndices(IndicesOptions.lenientExpandOpen(), ident.esName());
        } else {
            try {
                concreteIndices = metaData.concreteIndices(IndicesOptions.strictExpandOpen(), ident.esName());
                if (concreteIndices.length == 0) {
                    throw new TableUnknownException(ident);
                }
                docIndexMetaData = buildDocIndexMetaData(concreteIndices[0]);
            } catch (IndexMissingException ex) {
                throw new TableUnknownException(ident.fqn(), ex);
            }
        }

        if ((!createdFromTemplate && concreteIndices.length == 1) || !checkAliasSchema) {
            return docIndexMetaData;
        }
        for (int i = 0; i < concreteIndices.length; i++) {
            try {
                docIndexMetaData = docIndexMetaData.merge(
                        buildDocIndexMetaData(concreteIndices[i]),
                        transportPutIndexTemplateAction,
                        createdFromTemplate);
            } catch (IOException e) {
                throw new UnhandledServerException("Unable to merge/build new DocIndexMetaData", e);
            }
        }
        return docIndexMetaData;
    }

    private DocIndexMetaData buildDocIndexMetaData(String index) {
        DocIndexMetaData docIndexMetaData;
        try {
            docIndexMetaData = new DocIndexMetaData(metaData.index(index), ident);
        } catch (IOException e) {
            throw new UnhandledServerException("Unable to build DocIndexMetaData", e);
        }
        return docIndexMetaData.build();
    }

    private DocIndexMetaData buildDocIndexMetaDataFromTemplate(String index, String templateName) {
        IndexTemplateMetaData indexTemplateMetaData = metaData.getTemplates().get(templateName);
        DocIndexMetaData docIndexMetaData;
        try {
            IndexMetaData.Builder builder = new IndexMetaData.Builder(index);
            builder.putMapping(Constants.DEFAULT_MAPPING_TYPE,
                    indexTemplateMetaData.getMappings().get(Constants.DEFAULT_MAPPING_TYPE).toString());
            Settings settings = indexTemplateMetaData.settings();
            builder.settings(settings);

            builder.numberOfShards(settings.getAsInt(SETTING_NUMBER_OF_SHARDS, 5));
            builder.numberOfReplicas(settings.getAsInt(SETTING_NUMBER_OF_REPLICAS, 1));
            docIndexMetaData = new DocIndexMetaData(builder.build(), ident);
        } catch (IOException e) {
            throw new UnhandledServerException("Unable to build DocIndexMetaData from template", e);
        }
        return docIndexMetaData.build();
    }

    public DocTableInfo build() {
        DocIndexMetaData md = docIndexMetaData();

        List<PartitionName> partitions = new ArrayList<>();
        if (md.partitionedBy().size() > 0) {
            for(String index : concreteIndices) {
                if (PartitionName.isPartition(index, ident.schema(), ident.name())) {
                    try {
                        PartitionName partitionName = PartitionName.fromString(index, ident.schema(), ident.name());
                        partitions.add(partitionName);
                    } catch (IllegalArgumentException e) {

                        logger.warn(String.format(Locale.ENGLISH, "Cannot build partition %s of index %s", index, ident.esName()));
                    }
                }
            }
        }
        return new DocTableInfo(
                docSchemaInfo,
                ident,
                md.columns(),
                md.partitionedByColumns(),
                md.indices(),
                md.references(), md.primaryKey(), md.routingCol(),
                md.isAlias(), md.hasAutoGeneratedPrimaryKey(),
                concreteIndices, clusterService,
                md.numberOfShards(), md.numberOfReplicas(),
                md.tableParameters(),
                md.partitionedBy(),
                partitions,
                md.columnPolicy(),
                executorService);
    }

}

<code block>


package io.crate.metadata.doc;

import com.google.common.base.Throwables;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableMap;
import com.google.common.util.concurrent.SettableFuture;
import io.crate.analyze.AlterPartitionedTableParameterInfo;
import io.crate.analyze.TableParameterInfo;
import io.crate.analyze.WhereClause;
import io.crate.exceptions.UnavailableShardsException;
import io.crate.metadata.*;
import io.crate.metadata.table.AbstractDynamicTableInfo;
import io.crate.metadata.table.ColumnPolicy;
import io.crate.planner.RowGranularity;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.cluster.ClusterChangedEvent;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.cluster.ClusterState;
import org.elasticsearch.cluster.ClusterStateObserver;
import org.elasticsearch.cluster.routing.GroupShardsIterator;
import org.elasticsearch.cluster.routing.ShardIterator;
import org.elasticsearch.cluster.routing.ShardRouting;
import org.elasticsearch.common.logging.ESLogger;
import org.elasticsearch.common.logging.Loggers;
import org.elasticsearch.common.unit.TimeValue;
import org.elasticsearch.index.shard.ShardId;
import org.elasticsearch.indices.IndexMissingException;

import javax.annotation.Nullable;
import java.util.*;
import java.util.concurrent.*;


public class DocTableInfo extends AbstractDynamicTableInfo {

    private static final TimeValue ROUTING_FETCH_TIMEOUT = new TimeValue(5, TimeUnit.SECONDS);

    private final List<ReferenceInfo> columns;
    private final List<ReferenceInfo> partitionedByColumns;
    private final Map<ColumnIdent, IndexReferenceInfo> indexColumns;
    private final ImmutableMap<ColumnIdent, ReferenceInfo> references;
    private final TableIdent ident;
    private final List<ColumnIdent> primaryKeys;
    private final ColumnIdent clusteredBy;
    private final String[] concreteIndices;
    private final List<ColumnIdent> partitionedBy;
    private final int numberOfShards;
    private final BytesRef numberOfReplicas;
    private final ImmutableMap<String, Object> tableParameters;
    private ExecutorService executorService;
    private final ClusterService clusterService;
    private final TableParameterInfo tableParameterInfo;
    private static final ESLogger logger = Loggers.getLogger(DocTableInfo.class);

    private final String[] indices;
    private final List<PartitionName> partitions;

    private final boolean isAlias;
    private final boolean hasAutoGeneratedPrimaryKey;
    private final boolean isPartitioned;

    private final ColumnPolicy columnPolicy;

    public DocTableInfo(DocSchemaInfo schemaInfo,
                        TableIdent ident,
                        List<ReferenceInfo> columns,
                        List<ReferenceInfo> partitionedByColumns,
                        ImmutableMap<ColumnIdent, IndexReferenceInfo> indexColumns,
                        ImmutableMap<ColumnIdent, ReferenceInfo> references,
                        List<ColumnIdent> primaryKeys,
                        ColumnIdent clusteredBy,
                        boolean isAlias,
                        boolean hasAutoGeneratedPrimaryKey,
                        String[] concreteIndices,
                        ClusterService clusterService,
                        int numberOfShards,
                        BytesRef numberOfReplicas,
                        ImmutableMap<String, Object> tableParameters,
                        List<ColumnIdent> partitionedBy,
                        List<PartitionName> partitions,
                        ColumnPolicy columnPolicy,
                        ExecutorService executorService) {
        super(schemaInfo);
        this.clusterService = clusterService;
        this.columns = columns;
        this.partitionedByColumns = partitionedByColumns;
        this.indexColumns = indexColumns;
        this.references = references;
        this.ident = ident;
        this.primaryKeys = primaryKeys;
        this.clusteredBy = clusteredBy;
        this.concreteIndices = concreteIndices;
        this.numberOfShards = numberOfShards;
        this.numberOfReplicas = numberOfReplicas;
        this.tableParameters = tableParameters;
        this.executorService = executorService;
        indices = new String[]{ident.esName()};
        this.isAlias = isAlias;
        this.hasAutoGeneratedPrimaryKey = hasAutoGeneratedPrimaryKey;
        isPartitioned = !partitionedByColumns.isEmpty();
        this.partitionedBy = partitionedBy;
        this.partitions = partitions;
        this.columnPolicy = columnPolicy;
        if (isPartitioned) {
            tableParameterInfo = new AlterPartitionedTableParameterInfo();
        } else {
            tableParameterInfo = new TableParameterInfo();
        }
    }

    @Override
    @Nullable
    public ReferenceInfo getReferenceInfo(ColumnIdent columnIdent) {
        return references.get(columnIdent);
    }

    @Override
    public Collection<ReferenceInfo> columns() {
        return columns;
    }

    @Override
    public RowGranularity rowGranularity() {
        return RowGranularity.DOC;
    }

    @Override
    public TableIdent ident() {
        return ident;
    }

    private void processShardRouting(Map<String, Map<String, List<Integer>>> locations, ShardRouting shardRouting) {
        String node = shardRouting.currentNodeId();
        Map<String, List<Integer>> nodeMap = locations.get(node);
        if (nodeMap == null) {
            nodeMap = new TreeMap<>();
            locations.put(shardRouting.currentNodeId(), nodeMap);
        }

        List<Integer> shards = nodeMap.get(shardRouting.getIndex());
        if (shards == null) {
            shards = new ArrayList<>();
            nodeMap.put(shardRouting.getIndex(), shards);
        }
        shards.add(shardRouting.id());
    }

    private GroupShardsIterator getShardIterators(WhereClause whereClause,
                                                  @Nullable String preference,
                                                  ClusterState clusterState) throws IndexMissingException {
        String[] routingIndices = concreteIndices;
        if (whereClause.partitions().size() > 0) {
            routingIndices = whereClause.partitions().toArray(new String[whereClause.partitions().size()]);
        }

        Map<String, Set<String>> routingMap = null;
        if (whereClause.clusteredBy().isPresent()) {
            routingMap = clusterState.metaData().resolveSearchRouting(
                    whereClause.routingValues(), routingIndices);
        }
        return clusterService.operationRouting().searchShards(
                clusterState,
                indices,
                routingIndices,
                routingMap,
                preference
        );
    }

    public Routing getRouting(ClusterState state, WhereClause whereClause, String preference, final List<ShardId> missingShards) {
        final Map<String, Map<String, List<Integer>>> locations = new TreeMap<>();
        GroupShardsIterator shardIterators;
        try {
            shardIterators = getShardIterators(whereClause, preference, state);
        } catch (IndexMissingException e) {
            return new Routing();
        }

        fillLocationsFromShardIterators(locations, shardIterators, missingShards);

        if (missingShards.isEmpty()) {
            return new Routing(locations);
        } else {
            return null;
        }
    }

    @Override
    public Routing getRouting(final WhereClause whereClause, @Nullable final String preference) {
        Routing routing = getRouting(clusterService.state(), whereClause, preference, new ArrayList<ShardId>(0));
        if (routing != null) return routing;

        ClusterStateObserver observer = new ClusterStateObserver(clusterService, ROUTING_FETCH_TIMEOUT, logger);
        final SettableFuture<Routing> routingSettableFuture = SettableFuture.create();
        observer.waitForNextChange(
                new FetchRoutingListener(routingSettableFuture, whereClause, preference),
                new ClusterStateObserver.ChangePredicate() {

                    @Override
                    public boolean apply(ClusterState previousState, ClusterState.ClusterStateStatus previousStatus, ClusterState newState, ClusterState.ClusterStateStatus newStatus) {
                        return validate(newState);
                    }

                    @Override
                    public boolean apply(ClusterChangedEvent changedEvent) {
                        return validate(changedEvent.state());
                    }

                    private boolean validate(ClusterState state) {
                        final Map<String, Map<String, List<Integer>>> locations = new TreeMap<>();

                        GroupShardsIterator shardIterators;
                        try {
                            shardIterators = getShardIterators(whereClause, preference, state);
                        } catch (IndexMissingException e) {
                            return true;
                        }

                        final List<ShardId> missingShards = new ArrayList<>(0);
                        fillLocationsFromShardIterators(locations, shardIterators, missingShards);

                        return missingShards.isEmpty();
                    }

                });

        try {
            return routingSettableFuture.get();
        } catch (ExecutionException e) {
            throw Throwables.propagate(e.getCause());
        } catch (Exception e) {
            throw Throwables.propagate(e);
        }
    }

    private void fillLocationsFromShardIterators(Map<String, Map<String, List<Integer>>> locations,
                                                 GroupShardsIterator shardIterators,
                                                 List<ShardId> missingShards) {
        ShardRouting shardRouting;
        for (ShardIterator shardIterator : shardIterators) {
            shardRouting = shardIterator.nextOrNull();
            if (shardRouting != null) {
                if (shardRouting.active()) {
                    processShardRouting(locations, shardRouting);
                } else {
                    missingShards.add(shardIterator.shardId());
                }
            } else {
                if (isPartitioned) {

                    missingShards.add(shardIterator.shardId());
                } else {
                    throw new UnavailableShardsException(shardIterator.shardId());
                }
            }
        }
    }

    public List<ColumnIdent> primaryKey() {
        return primaryKeys;
    }

    @Override
    public int numberOfShards() {
        return numberOfShards;
    }

    @Override
    public BytesRef numberOfReplicas() {
        return numberOfReplicas;
    }

    @Override
    public boolean hasAutoGeneratedPrimaryKey() {
        return hasAutoGeneratedPrimaryKey;
    }

    @Override
    public ColumnIdent clusteredBy() {
        return clusteredBy;
    }

    @Override
    public boolean isAlias() {
        return isAlias;
    }

    @Override
    public String[] concreteIndices() {
        return concreteIndices;
    }


    public List<ReferenceInfo> partitionedByColumns() {
        return partitionedByColumns;
    }


    @Override
    public List<ColumnIdent> partitionedBy() {
        return partitionedBy;
    }

    @Override
    public List<PartitionName> partitions() {
        return partitions;
    }

    @Override
    public boolean isPartitioned() {
        return isPartitioned;
    }

    @Override
    public IndexReferenceInfo indexColumn(ColumnIdent ident) {
        return indexColumns.get(ident);
    }

    @Override
    public Iterator<IndexReferenceInfo> indexColumns() {
        return indexColumns.values().iterator();
    }

    @Override
    public Iterator<ReferenceInfo> iterator() {
        return references.values().iterator();
    }

    @Override
    public ColumnPolicy columnPolicy() {
        return columnPolicy;
    }

    @Override
    public TableParameterInfo tableParameterInfo () {
        return tableParameterInfo;
    }

    @Override
    public ImmutableMap<String, Object> tableParameters() {
        return tableParameters;
    }

    private class FetchRoutingListener implements ClusterStateObserver.Listener {

        private final SettableFuture<Routing> routingFuture;
        private final WhereClause whereClause;
        private final String preference;
        Future<?> innerTaskFuture;

        public FetchRoutingListener(SettableFuture<Routing> routingFuture, WhereClause whereClause, String preference) {
            this.routingFuture = routingFuture;
            this.whereClause = whereClause;
            this.preference = preference;
        }

        @Override
        public void onNewClusterState(final ClusterState state) {
            try {
                innerTaskFuture = executorService.submit(new Runnable() {
                    @Override
                    public void run() {
                        final List<ShardId> missingShards = new ArrayList<>(0);
                        Routing routing = getRouting(state, whereClause, preference, missingShards);
                        if (routing == null) {
                            routingFuture.setException(new UnavailableShardsException(missingShards.get(0)));
                        } else {
                            routingFuture.set(routing);
                        }
                    }
                });
            } catch (RejectedExecutionException e) {
                routingFuture.setException(e);
            }
        }

        @Override
        public void onClusterServiceClose() {
            if (innerTaskFuture != null) {
                innerTaskFuture.cancel(true);
            }
            routingFuture.setException(new IllegalStateException("ClusterService closed"));
        }

        @Override
        public void onTimeout(TimeValue timeout) {
            if (innerTaskFuture != null) {
                innerTaskFuture.cancel(true);
            }
            routingFuture.setException(new IllegalStateException("Fetching table info routing timed out."));
        }
    }
}

<code block>


package io.crate.metadata.blob;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableMap;
import io.crate.analyze.AlterBlobTableParameterInfo;
import io.crate.analyze.TableParameterInfo;
import io.crate.analyze.WhereClause;
import io.crate.metadata.*;
import io.crate.metadata.table.ColumnPolicy;
import io.crate.metadata.table.SchemaInfo;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.RowGranularity;
import io.crate.planner.symbol.DynamicReference;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.action.NoShardAvailableActionException;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.cluster.routing.GroupShardsIterator;
import org.elasticsearch.cluster.routing.ShardIterator;
import org.elasticsearch.cluster.routing.ShardRouting;
import org.elasticsearch.common.Strings;
import org.elasticsearch.common.collect.Tuple;
import org.elasticsearch.index.shard.ShardId;

import javax.annotation.Nullable;
import java.util.*;

public class BlobTableInfo implements TableInfo {

    private final BlobSchemaInfo blobSchemaInfo;
    private final TableIdent ident;
    private final int numberOfShards;
    private final BytesRef numberOfReplicas;
    private final ClusterService clusterService;
    private final String index;
    private final LinkedHashSet<ReferenceInfo> columns = new LinkedHashSet<>();
    private final BytesRef blobsPath;
    private final TableParameterInfo tableParameterInfo;
    private ImmutableMap<String,Object> tableParameters;

    public static final Map<ColumnIdent, ReferenceInfo> INFOS = new LinkedHashMap<>();

    private static final ImmutableList<ColumnIdent> primaryKey = ImmutableList.of(
            new ColumnIdent("digest"));
    private final static List<Tuple<String, DataType>> staticColumns = ImmutableList.<Tuple<String,DataType>>builder()
                .add(new Tuple<String, DataType>("digest", DataTypes.STRING))
                .add(new Tuple<String, DataType>("last_modified", DataTypes.TIMESTAMP))
                .build();

    public BlobTableInfo(BlobSchemaInfo blobSchemaInfo,
                         TableIdent ident,
                         String index,
                         ClusterService clusterService,
                         int numberOfShards,
                         BytesRef numberOfReplicas,
                         ImmutableMap<String, Object> tableParameters,
                         BytesRef blobsPath) {
        this.blobSchemaInfo = blobSchemaInfo;
        this.ident = ident;
        this.index = index;
        this.clusterService = clusterService;
        this.numberOfShards = numberOfShards;
        this.numberOfReplicas = numberOfReplicas;
        this.blobsPath = blobsPath;
        this.tableParameterInfo = new AlterBlobTableParameterInfo();
        this.tableParameters = tableParameters;

        registerStaticColumns();
    }

    @Override
    public SchemaInfo schemaInfo() {
        return blobSchemaInfo;
    }

    @Nullable
    @Override
    public ReferenceInfo getReferenceInfo(ColumnIdent columnIdent) {
        return INFOS.get(columnIdent);
    }

    @Override
    public Collection<ReferenceInfo> columns() {
        return columns;
    }

    @Override
    public List<ReferenceInfo> partitionedByColumns() {
        return ImmutableList.of();
    }

    @Override
    public IndexReferenceInfo indexColumn(ColumnIdent ident) {
        return null;
    }

    @Nullable
    @Override
    public Iterator<IndexReferenceInfo> indexColumns() {
        return null;
    }

    @Override
    public RowGranularity rowGranularity() {
        return RowGranularity.DOC;
    }

    @Override
    public TableIdent ident() {
        return ident;
    }

    private void processShardRouting(Map<String, Map<String, List<Integer>>> locations, ShardRouting shardRouting, ShardId shardId) {
        String node;
        if (shardRouting == null) {
            throw new NoShardAvailableActionException(shardId);
        }
        node = shardRouting.currentNodeId();
        Map<String, List<Integer>> nodeMap = locations.get(node);
        if (nodeMap == null) {
            nodeMap = new TreeMap<>();
            locations.put(shardRouting.currentNodeId(), nodeMap);
        }

        List<Integer> shards = nodeMap.get(shardRouting.getIndex());
        if (shards == null) {
            shards = new ArrayList<>();
            nodeMap.put(shardRouting.getIndex(), shards);
        }
        shards.add(shardRouting.id());
    }

    @Override
    public Routing getRouting(WhereClause whereClause, @Nullable String preference) {
        Map<String, Map<String, List<Integer>>> locations = new TreeMap<>();
        GroupShardsIterator shardIterators = clusterService.operationRouting().searchShards(
                clusterService.state(),
                Strings.EMPTY_ARRAY,
                new String[]{index},
                null,
                preference
        );
        ShardRouting shardRouting;
        for (ShardIterator shardIterator : shardIterators) {
            shardRouting = shardIterator.nextOrNull();
            processShardRouting(locations, shardRouting, shardIterator.shardId());
        }

        return new Routing(locations);
    }

    @Override
    public List<ColumnIdent> primaryKey() {
        return primaryKey;
    }

    @Override
    public int numberOfShards() {
        return numberOfShards;
    }

    @Override
    public BytesRef numberOfReplicas() {
        return numberOfReplicas;
    }

    @Override
    public boolean hasAutoGeneratedPrimaryKey() {
        return false;
    }

    @Nullable
    @Override
    public ColumnIdent clusteredBy() {
        return primaryKey.get(0);
    }

    @Override
    public boolean isAlias() {
        return false;
    }

    @Override
    public String[] concreteIndices() {
        return Strings.EMPTY_ARRAY;
    }

    @Override
    public boolean isPartitioned() {
        return false;
    }

    @Override
    public List<PartitionName> partitions() {
        return ImmutableList.of();
    }

    @Override
    public List<ColumnIdent> partitionedBy() {
        return ImmutableList.of();
    }

    @Nullable
    @Override
    public DynamicReference getDynamic(ColumnIdent ident, boolean forWrite) {
        return null;
    }

    @Override
    public ColumnPolicy columnPolicy() {
        return ColumnPolicy.STRICT;
    }

    public DynamicReference getDynamic(ColumnIdent ident) {
        return null;
    }

    @Override
    public Iterator<ReferenceInfo> iterator() {
        return columns.iterator();
    }

    private void registerStaticColumns() {
        for (Tuple<String, DataType> column : staticColumns) {
            ReferenceInfo info = new ReferenceInfo(new ReferenceIdent(ident(), column.v1(), null),
                    RowGranularity.DOC, column.v2());
            if (info.ident().isColumn()) {
                columns.add(info);
            }
            INFOS.put(info.ident().columnIdent(), info);
        }
    }

    public BytesRef blobsPath() {
        return blobsPath;
    }

    @Override
    public TableParameterInfo tableParameterInfo() {
        return tableParameterInfo;
    }

    @Override
    public ImmutableMap<String, Object> tableParameters() {
        return tableParameters;
    }
}

<code block>


package io.crate.analyze;

import com.google.common.collect.ImmutableList;
import io.crate.core.collections.TreeMapBuilder;
import io.crate.metadata.*;
import io.crate.metadata.table.ColumnPolicy;
import io.crate.metadata.table.TableInfo;
import io.crate.metadata.table.TestingTableInfo;
import io.crate.planner.RowGranularity;
import io.crate.sql.SqlFormatter;
import io.crate.sql.tree.CreateTable;
import io.crate.test.integration.CrateUnitTest;
import io.crate.types.ArrayType;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.junit.Test;

import java.util.*;

public class MetaDataToASTNodeResolverTest extends CrateUnitTest {

    static TableIdent TEST_DOC_TABLE_IDENT = new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "users");

    static Routing SHARD_ROUTING = new Routing(TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder()
            .put("crate1", TreeMapBuilder.<String, List<Integer>>newMapBuilder().put("t1", Arrays.asList(1, 2)).map())
            .put("crate2", TreeMapBuilder.<String, List<Integer>>newMapBuilder().put("t1", Arrays.asList(3, 4)).map())
            .map());

    static ReferenceInfo COL_TEXT = new ReferenceInfo(new ReferenceIdent(TEST_DOC_TABLE_IDENT, "text", null),
            RowGranularity.DOC, DataTypes.STRING, null, ReferenceInfo.IndexType.ANALYZED);

    static ReferenceInfo COL_NAME = new ReferenceInfo(new ReferenceIdent(TEST_DOC_TABLE_IDENT, "name", null),
            RowGranularity.DOC, DataTypes.STRING, null, ReferenceInfo.IndexType.NOT_ANALYZED);

    @Test
    public void testBuildCreateTableColumns() throws Exception {
        TableIdent ident = new TableIdent("doc", "test");
        TableInfo tableInfo = TestingTableInfo.builder(ident, RowGranularity.DOC, SHARD_ROUTING)
                .add("bytes", DataTypes.BYTE, null)
                .add("strings", DataTypes.STRING, null)
                .add("shorts", DataTypes.SHORT, null)
                .add("floats", DataTypes.FLOAT, null)
                .add("doubles", DataTypes.DOUBLE, null)
                .add("ints", DataTypes.INTEGER, null)
                .add("longs", DataTypes.LONG, null)
                .add("timestamp", DataTypes.TIMESTAMP, null)
                .add("arr_simple", new ArrayType(DataTypes.STRING), null)
                .add("arr_geo_point", new ArrayType(DataTypes.GEO_POINT), null)
                .add("arr_obj", new ArrayType(DataTypes.OBJECT), null, ColumnPolicy.STRICT)
                .add("arr_obj", DataTypes.LONG, Arrays.asList("col_1"))
                .add("arr_obj", DataTypes.STRING, Arrays.asList("col_2"))
                .add("obj", DataTypes.OBJECT, null, ColumnPolicy.DYNAMIC)
                .add("obj", DataTypes.LONG, Arrays.asList("col_1"))
                .add("obj", DataTypes.STRING, Arrays.asList("col_2"))
                .build();
        CreateTable node = MetaDataToASTNodeResolver.resolveCreateTable(tableInfo);
        assertEquals("CREATE TABLE IF NOT EXISTS \"test\" (\n" +
                "   \"bytes\" BYTE,\n" +
                "   \"strings\" STRING,\n" +
                "   \"shorts\" SHORT,\n" +
                "   \"floats\" FLOAT,\n" +
                "   \"doubles\" DOUBLE,\n" +
                "   \"ints\" INTEGER,\n" +
                "   \"longs\" LONG,\n" +
                "   \"timestamp\" TIMESTAMP,\n" +
                "   \"arr_simple\" ARRAY(STRING),\n" +
                "   \"arr_geo_point\" ARRAY(GEO_POINT),\n" +
                "   \"arr_obj\" ARRAY(OBJECT (STRICT) AS (\n" +
                "      \"col_1\" LONG,\n" +
                "      \"col_2\" STRING\n" +
                "   )),\n" +
                "   \"obj\" OBJECT (DYNAMIC) AS (\n" +
                "      \"col_1\" LONG,\n" +
                "      \"col_2\" STRING\n" +
                "   )\n" +
                ")\n" +
                "CLUSTERED INTO 1 SHARDS", SqlFormatter.formatSql(node));
    }

    @Test
    public void testBuildCreateTablePrimaryKey() throws Exception {
        TableIdent ident = new TableIdent("myschema", "test");
        TableInfo tableInfo = TestingTableInfo.builder(ident, RowGranularity.DOC, SHARD_ROUTING)
                .add("pk_col_one", DataTypes.LONG, null)
                .add("pk_col_two", DataTypes.LONG, null)
                .addPrimaryKey("pk_col_one")
                .addPrimaryKey("pk_col_two")
                .build();
        CreateTable node = MetaDataToASTNodeResolver.resolveCreateTable(tableInfo);
        assertEquals("CREATE TABLE IF NOT EXISTS \"myschema\".\"test\" (\n" +
                "   \"pk_col_one\" LONG,\n" +
                "   \"pk_col_two\" LONG,\n" +
                "   PRIMARY KEY (\"pk_col_one\", \"pk_col_two\")\n" +
                ")\n" +
                "CLUSTERED INTO 1 SHARDS", SqlFormatter.formatSql(node));
    }

    @Test
    public void testBuildCreateTableParameters() throws Exception {
        TableIdent ident = new TableIdent("myschema", "test");
        TableInfo tableInfo = TestingTableInfo.builder(ident, RowGranularity.DOC, SHARD_ROUTING)
                .add("id", DataTypes.LONG, null)
                .addParameter("refresh_interval", 10000L)
                .addParameter("param_array", new String[]{"foo", "bar"})
                .addParameter("param_obj", new HashMap() {{
                    put("foo", "bar");
                    put("int", 42);
                }})
                .numberOfReplicas("0-all")
                .build();
        CreateTable node = MetaDataToASTNodeResolver.resolveCreateTable(tableInfo);
        assertEquals("CREATE TABLE IF NOT EXISTS \"myschema\".\"test\" (\n" +
                "   \"id\" LONG\n" +
                ")\n" +
                "CLUSTERED INTO 1 SHARDS\n" +
                "WITH (\n" +
                "   number_of_replicas = '0-all',\n" +
                "   param_array = ['foo','bar'],\n" +
                "   param_obj = {foo: 'bar', int: 42},\n" +
                "   refresh_interval = 10000\n" +
                ")", SqlFormatter.formatSql(node));
    }

    @Test
    public void testBuildCreateTableClusteredByPartitionedBy() throws Exception {
        TableIdent ident = new TableIdent("myschema", "test");
        TableInfo tableInfo = TestingTableInfo.builder(ident, RowGranularity.DOC, SHARD_ROUTING)
                .add("id", DataTypes.LONG, null)
                .add("partition_column", DataTypes.STRING, null, true)
                .add("cluster_column", DataTypes.STRING, null)
                .clusteredBy("cluster_column")
                .build();
        CreateTable node = MetaDataToASTNodeResolver.resolveCreateTable(tableInfo);
        assertEquals("CREATE TABLE IF NOT EXISTS \"myschema\".\"test\" (\n" +
                "   \"id\" LONG,\n" +
                "   \"partition_column\" STRING,\n" +
                "   \"cluster_column\" STRING\n" +
                ")\n" +
                "CLUSTERED BY (\"cluster_column\") INTO 1 SHARDS\n" +
                "PARTITIONED BY (\"partition_column\")", SqlFormatter.formatSql(node));
    }

    @Test
    public void testBuildCreateTableIndexes() throws Exception {
        TableIdent ident = new TableIdent("myschema", "test");
        ReferenceInfo colA = new ReferenceInfo(new ReferenceIdent(ident, "col_a", null),
                RowGranularity.DOC, DataTypes.STRING, null, ReferenceInfo.IndexType.ANALYZED);
        ReferenceInfo colB = new ReferenceInfo(new ReferenceIdent(ident, "col_b", null),
                RowGranularity.DOC, DataTypes.STRING, null, ReferenceInfo.IndexType.ANALYZED);
        ReferenceInfo colC = new ReferenceInfo(new ReferenceIdent(ident, "col_c", null),
                RowGranularity.DOC, DataTypes.STRING, null, ReferenceInfo.IndexType.NO);
        ReferenceInfo colD = new ReferenceInfo(new ReferenceIdent(ident, "col_d", null),
                RowGranularity.DOC, DataTypes.OBJECT);
        ReferenceInfo colE = new ReferenceInfo(new ReferenceIdent(ident, "col_d", Arrays.asList("a")),
                RowGranularity.DOC, DataTypes.STRING, null, ReferenceInfo.IndexType.ANALYZED);

        TableInfo tableInfo = TestingTableInfo.builder(ident, RowGranularity.DOC, SHARD_ROUTING)
                .add("id", DataTypes.LONG, null)
                .add(colA)
                .add(colB)
                .add(colC)
                .add(colD)
                .add(colE)
                .addIndex(new ColumnIdent("col_a_plain"), ReferenceInfo.IndexType.NOT_ANALYZED,
                        ImmutableList.of(colA))
                .addIndex(new ColumnIdent("col_b_ft"), ReferenceInfo.IndexType.ANALYZED,
                        ImmutableList.of(colB))
                .addIndex(new ColumnIdent("col_a_col_b_ft"), ReferenceInfo.IndexType.ANALYZED,
                        ImmutableList.of(colA, colB), "english")
                .addIndex(new ColumnIdent("col_d_a_ft"), ReferenceInfo.IndexType.ANALYZED,
                        ImmutableList.of(colE), "custom_analyzer")
                .build();
        CreateTable node = MetaDataToASTNodeResolver.resolveCreateTable(tableInfo);
        assertEquals("CREATE TABLE IF NOT EXISTS \"myschema\".\"test\" (\n" +
                "   \"id\" LONG,\n" +
                "   \"col_a\" STRING,\n" +
                "   \"col_b\" STRING,\n" +
                "   \"col_c\" STRING INDEX OFF,\n" +
                "   \"col_d\" OBJECT (DYNAMIC) AS (\n" +
                "      \"a\" STRING\n" +
                "   ),\n" +
                "   INDEX \"col_a_plain\" USING PLAIN (\"col_a\"),\n" +
                "   INDEX \"col_b_ft\" USING FULLTEXT (\"col_b\"),\n" +
                "   INDEX \"col_a_col_b_ft\" USING FULLTEXT (\"col_a\", \"col_b\") WITH (\n" +
                "      analyzer = 'english'\n" +
                "   ),\n" +
                "   INDEX \"col_d_a_ft\" USING FULLTEXT (\"col_d\"['a']) WITH (\n" +
                "      analyzer = 'custom_analyzer'\n" +
                "   )\n" +
                ")\n" +
                "CLUSTERED INTO 1 SHARDS", SqlFormatter.formatSql(node));
    }
}
<code block>


package io.crate.metadata.table;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableMap;
import io.crate.analyze.AlterPartitionedTableParameterInfo;
import io.crate.analyze.TableParameterInfo;
import io.crate.analyze.WhereClause;
import io.crate.exceptions.ColumnUnknownException;
import io.crate.metadata.*;
import io.crate.metadata.doc.DocIndexMetaData;
import io.crate.metadata.doc.DocSysColumns;
import io.crate.planner.RowGranularity;
import io.crate.planner.symbol.DynamicReference;
import io.crate.types.DataType;
import org.hamcrest.generator.qdox.parser.Builder;
import org.mockito.Answers;

import javax.annotation.Nullable;
import java.util.Collection;
import java.util.Iterator;
import java.util.List;
import java.util.Map;

import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;

public class TestingTableInfo extends AbstractDynamicTableInfo {


    private final Routing routing;
    private final ColumnIdent clusteredBy;
    private final ImmutableMap<String, Object> tableParameters;

    public static Builder builder(TableIdent ident, RowGranularity granularity, Routing routing) {
        return new Builder(ident, granularity, routing);
    }

    public static class Builder {

        private final ImmutableList.Builder<ReferenceInfo> columns = ImmutableList.builder();
        private final ImmutableMap.Builder<ColumnIdent, ReferenceInfo> references = ImmutableMap.builder();
        private final ImmutableList.Builder<ReferenceInfo> partitionedByColumns = ImmutableList.builder();
        private final ImmutableList.Builder<ColumnIdent> primaryKey = ImmutableList.builder();
        private final ImmutableList.Builder<ColumnIdent> partitionedBy = ImmutableList.builder();
        private final ImmutableList.Builder<PartitionName> partitions = ImmutableList.builder();
        private final ImmutableMap.Builder<ColumnIdent, IndexReferenceInfo> indexColumns = ImmutableMap.builder();
        private final ImmutableMap.Builder<String, Object> parameters = ImmutableMap.builder();
        private ColumnIdent clusteredBy;


        private final RowGranularity granularity;
        private final TableIdent ident;
        private final Routing routing;
        private boolean isAlias = false;
        private ColumnPolicy columnPolicy = ColumnPolicy.DYNAMIC;

        private SchemaInfo schemaInfo = mock(SchemaInfo.class, Answers.RETURNS_MOCKS.get());

        public Builder(TableIdent ident, RowGranularity granularity, Routing routing) {
            this.granularity = granularity;
            this.routing = routing;
            this.ident = ident;
        }

        private ReferenceInfo genInfo(ColumnIdent columnIdent, DataType type) {
            return new ReferenceInfo(
                    new ReferenceIdent(ident, columnIdent.name(), columnIdent.path()),
                    RowGranularity.DOC, type
            );
        }

        private void addDocSysColumns() {
            for (Map.Entry<ColumnIdent, DataType> entry : DocSysColumns.COLUMN_IDENTS.entrySet()) {
                references.put(
                        entry.getKey(),
                        genInfo(entry.getKey(), entry.getValue())
                );
            }
        }

        public Builder add(String column, DataType type, List<String> path) {
            return add(column, type, path, ColumnPolicy.DYNAMIC);
        }
        public Builder add(String column, DataType type, List<String> path, ColumnPolicy columnPolicy) {
            return add(column, type, path, columnPolicy, ReferenceInfo.IndexType.NOT_ANALYZED, false);
        }
        public Builder add(String column, DataType type, List<String> path, ReferenceInfo.IndexType indexType) {
            return add(column, type, path, ColumnPolicy.DYNAMIC, indexType, false);
        }
        public Builder add(String column, DataType type, List<String> path,
                           boolean partitionBy) {
            return add(column, type, path, ColumnPolicy.DYNAMIC,
                    ReferenceInfo.IndexType.NOT_ANALYZED, partitionBy);
        }

        public Builder add(String column, DataType type, List<String> path,
                           ColumnPolicy columnPolicy, ReferenceInfo.IndexType indexType,
                           boolean partitionBy) {
            RowGranularity rowGranularity = granularity;
            if (partitionBy) {
                rowGranularity = RowGranularity.PARTITION;
            }
            ReferenceInfo info = new ReferenceInfo(new ReferenceIdent(ident, column, path),
                    rowGranularity, type, columnPolicy, indexType);
            return add(info, partitionBy);
        }

        public Builder add(ReferenceInfo info, boolean partitionBy) {
            if (info.ident().isColumn()) {
                columns.add(info);
            }
            references.put(info.ident().columnIdent(), info);
            if (partitionBy) {
                partitionedByColumns.add(info);
                partitionedBy.add(info.ident().columnIdent());
            }
            return this;
        }

        public Builder add(ReferenceInfo info) {
            return add(info, false);
        }

        public Builder addIndex(ColumnIdent columnIdent, ReferenceInfo.IndexType indexType) {
            return addIndex(columnIdent, indexType, ImmutableList.<ReferenceInfo>of());
        }

        public Builder addIndex(ColumnIdent columnIdent, ReferenceInfo.IndexType indexType, List<ReferenceInfo> columns) {
            return addIndex(columnIdent, indexType, columns, null);
        }

        public Builder addIndex(ColumnIdent columnIdent, ReferenceInfo.IndexType indexType, List<ReferenceInfo> columns, String analyzer) {
            IndexReferenceInfo.Builder builder = new IndexReferenceInfo.Builder()
                    .ident(new ReferenceIdent(ident, columnIdent))
                    .indexType(indexType)
                    .analyzer(analyzer);
            for (ReferenceInfo column : columns) {
                builder.addColumn(column);
            }
            indexColumns.put(columnIdent, builder.build());
            return this;
        }

        public Builder addPrimaryKey(String column) {
            primaryKey.add(ColumnIdent.fromPath(column));
            return this;
        }

        public Builder clusteredBy(String clusteredBy) {
            this.clusteredBy = ColumnIdent.fromPath(clusteredBy);
            return this;
        }

        public Builder isAlias(boolean isAlias) {
            this.isAlias = isAlias;
            return this;
        }

        public Builder schemaInfo(SchemaInfo schemaInfo) {
            this.schemaInfo = schemaInfo;
            return this;
        }

        public Builder addPartitions(String... partitionNames) {
            for (String partitionName : partitionNames) {
                PartitionName partition = PartitionName.fromString(partitionName, ident.schema(), ident.name());
                partitions.add(partition);
            }
            return this;
        }

        public Builder numberOfReplicas(Object s) {
            addParameter("number_of_replicas", s);
            return this;
        }

        public Builder addParameter(String key, Object value) {
            parameters.put(key, value);
            return this;
        }

        public TableInfo build() {
            addDocSysColumns();
            return new TestingTableInfo(
                    columns.build(),
                    partitionedByColumns.build(),
                    indexColumns.build(),
                    references.build(),
                    ident,
                    granularity,
                    routing,
                    primaryKey.build(),
                    clusteredBy,
                    isAlias,
                    partitionedBy.build(),
                    partitions.build(),
                    columnPolicy,
                    schemaInfo == null ? mock(SchemaInfo.class, Answers.RETURNS_MOCKS.get()) : schemaInfo,
                    parameters.build()
            );
        }

    }


    private final List<ReferenceInfo> columns;
    private final List<ReferenceInfo> partitionedByColumns;
    private final Map<ColumnIdent, IndexReferenceInfo> indexColumns;
    private final Map<ColumnIdent, ReferenceInfo> references;
    private final TableIdent ident;
    private final RowGranularity granularity;
    private final List<ColumnIdent> primaryKey;
    private final boolean isAlias;
    private final boolean hasAutoGeneratedPrimaryKey;
    private final List<ColumnIdent> partitionedBy;
    private final List<PartitionName> partitions;
    private final ColumnPolicy columnPolicy;
    private final TableParameterInfo tableParameterInfo;


    public TestingTableInfo(List<ReferenceInfo> columns,
                            List<ReferenceInfo> partitionedByColumns,
                            Map<ColumnIdent, IndexReferenceInfo> indexColumns,
                            Map<ColumnIdent, ReferenceInfo> references,
                            TableIdent ident, RowGranularity granularity,
                            Routing routing,
                            List<ColumnIdent> primaryKey,
                            ColumnIdent clusteredBy,
                            boolean isAlias,
                            List<ColumnIdent> partitionedBy,
                            List<PartitionName> partitions,
                            ColumnPolicy columnPolicy,
                            SchemaInfo schemaInfo,
                            ImmutableMap<String, Object> tableParameters
                            ) {
        super(schemaInfo);
        this.columns = columns;
        this.partitionedByColumns = partitionedByColumns;
        this.indexColumns = indexColumns;
        this.references = references;
        this.ident = ident;
        this.granularity = granularity;
        this.routing = routing;
        if (primaryKey == null || primaryKey.isEmpty()){
            this.primaryKey = ImmutableList.of(DocIndexMetaData.ID_IDENT);
            this.hasAutoGeneratedPrimaryKey = true;
        } else {
            this.primaryKey = primaryKey;
            this.hasAutoGeneratedPrimaryKey = false;
        }
        this.clusteredBy = clusteredBy;
        this.isAlias = isAlias;
        this.columnPolicy = columnPolicy;
        this.partitionedBy = partitionedBy;
        this.partitions = partitions;
        if (partitionedByColumns.isEmpty()) {
            tableParameterInfo = new TableParameterInfo();
        } else {
            tableParameterInfo = new AlterPartitionedTableParameterInfo();
        }
        this.tableParameters = tableParameters;
    }

    @Override
    public ReferenceInfo getReferenceInfo(ColumnIdent columnIdent) {
        return references.get(columnIdent);
    }

    @Override
    public Collection<ReferenceInfo> columns() {
        return columns;
    }


    @Override
    public List<ReferenceInfo> partitionedByColumns() {
        return partitionedByColumns;
    }

    @Override
    public IndexReferenceInfo indexColumn(ColumnIdent ident) {
        return indexColumns.get(ident);
    }

    @Override
    public Iterator<IndexReferenceInfo> indexColumns() {
        return indexColumns.values().iterator();
    }

    @Override
    public boolean isPartitioned() {
        return !partitionedByColumns.isEmpty();
    }

    @Override
    public RowGranularity rowGranularity() {
        return granularity;
    }

    @Override
    public TableIdent ident() {
        return ident;
    }

    @Override
    public Routing getRouting(WhereClause whereClause, @Nullable String preference) {
        return routing;
    }

    @Override
    public List<ColumnIdent> primaryKey() {
        return primaryKey;
    }

    @Override
    public boolean hasAutoGeneratedPrimaryKey() {
        return hasAutoGeneratedPrimaryKey;
    }

    @Override
    public ColumnIdent clusteredBy() {
        return clusteredBy;
    }

    @Override
    public boolean isAlias() {
        return isAlias;
    }

    @Override
    public String[] concreteIndices() {
        return new String[]{ident.esName()};
    }

    @Override
    public DynamicReference getDynamic(ColumnIdent ident) {
        if (!ident.isColumn()) {
            ColumnIdent parentIdent = ident.getParent();
            ReferenceInfo parentInfo = getReferenceInfo(parentIdent);
            if (parentInfo != null && parentInfo.columnPolicy() == ColumnPolicy.STRICT) {
                throw new ColumnUnknownException(ident.sqlFqn());
            }
        }
        return new DynamicReference(new ReferenceIdent(ident(), ident), rowGranularity());
    }

    @Override
    public Iterator<ReferenceInfo> iterator() {
        return references.values().iterator();
    }

    @Override
    public List<ColumnIdent> partitionedBy() {
        return partitionedBy;
    }

    @Override
    public List<PartitionName> partitions() {
        return partitions;
    }

    @Override
    public ColumnPolicy columnPolicy() {
        return columnPolicy;
    }

    @Override
    public ImmutableMap<String, Object> tableParameters() {
        return tableParameters;
    }

    @Override
    public TableParameterInfo tableParameterInfo () {
        return tableParameterInfo;
    }

    @Override
    public SchemaInfo schemaInfo() {
        final SchemaInfo schemaInfo = super.schemaInfo();
        when(schemaInfo.name()).thenReturn(ident.schema());
        return schemaInfo;
    }
}

<code block>


package io.crate.planner;

import com.carrotsearch.hppc.IntObjectOpenHashMap;
import com.carrotsearch.hppc.procedures.ObjectProcedure;
import com.google.common.base.Preconditions;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.Lists;
import io.crate.analyze.*;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.analyze.relations.TableRelation;
import io.crate.analyze.where.DocKeys;
import io.crate.core.collections.TreeMapBuilder;
import io.crate.exceptions.UnhandledServerException;
import io.crate.metadata.*;
import io.crate.metadata.doc.DocSysColumns;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.aggregation.impl.CountAggregation;
import io.crate.planner.consumer.ConsumerContext;
import io.crate.planner.consumer.ConsumingPlanner;
import io.crate.planner.consumer.UpdateConsumer;
import io.crate.planner.node.ddl.*;
import io.crate.planner.node.dml.ESDeleteByQueryNode;
import io.crate.planner.node.dml.ESDeleteNode;
import io.crate.planner.node.dml.SymbolBasedUpsertByIdNode;
import io.crate.planner.node.dml.Upsert;
import io.crate.planner.node.dql.CollectAndMerge;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.FileUriCollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.management.KillPlan;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.SourceIndexWriterProjection;
import io.crate.planner.projection.WriterProjection;
import io.crate.planner.symbol.InputColumn;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.cluster.node.DiscoveryNodes;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.inject.Singleton;
import org.elasticsearch.common.settings.ImmutableSettings;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.index.shard.ShardId;

import javax.annotation.Nullable;
import java.io.IOException;
import java.util.*;
import java.util.concurrent.atomic.AtomicInteger;

import static com.google.common.base.MoreObjects.firstNonNull;

@Singleton
public class Planner extends AnalyzedStatementVisitor<Planner.Context, Plan> {

    private final ConsumingPlanner consumingPlanner;
    private final ClusterService clusterService;
    private UpdateConsumer updateConsumer;

    public static class Context {

        private final IntObjectOpenHashMap<ShardId> jobSearchContextIdToShard = new IntObjectOpenHashMap<>();
        private final IntObjectOpenHashMap<String> jobSearchContextIdToNode = new IntObjectOpenHashMap<>();
        private final ClusterService clusterService;
        private int jobSearchContextIdBaseSeq = 0;
        private int executionNodeId = 0;

        public Context(ClusterService clusterService) {
            this.clusterService = clusterService;
        }

        public ClusterService clusterService() {
            return clusterService;
        }


        public void allocateJobSearchContextIds(Routing routing) {
            if (routing.jobSearchContextIdBase() > -1 || routing.hasLocations() == false
                    || routing.numShards() == 0) {
                return;
            }
            int jobSearchContextId = jobSearchContextIdBaseSeq;
            jobSearchContextIdBaseSeq += routing.numShards();
            routing.jobSearchContextIdBase(jobSearchContextId);
            for (Map.Entry<String, Map<String, List<Integer>>> nodeEntry : routing.locations().entrySet()) {
                String nodeId = nodeEntry.getKey();
                Map<String, List<Integer>> nodeRouting = nodeEntry.getValue();
                if (nodeRouting != null) {
                    for (Map.Entry<String, List<Integer>> entry : nodeRouting.entrySet()) {
                        for (Integer shardId : entry.getValue()) {
                            jobSearchContextIdToShard.put(jobSearchContextId, new ShardId(entry.getKey(), shardId));
                            jobSearchContextIdToNode.put(jobSearchContextId, nodeId);
                            jobSearchContextId++;
                        }
                    }
                }
            }
        }

        @Nullable
        public ShardId shardId(int jobSearchContextId) {
            return jobSearchContextIdToShard.get(jobSearchContextId);
        }

        public IntObjectOpenHashMap<ShardId> jobSearchContextIdToShard() {
            return jobSearchContextIdToShard;
        }

        @Nullable
        public String nodeId(int jobSearchContextId) {
            return jobSearchContextIdToNode.get(jobSearchContextId);
        }

        public IntObjectOpenHashMap<String> jobSearchContextIdToNode() {
            return jobSearchContextIdToNode;
        }

        public int nextExecutionNodeId() {
            return executionNodeId++;
        }
    }

    @Inject
    public Planner(ClusterService clusterService, ConsumingPlanner consumingPlanner, UpdateConsumer updateConsumer) {
        this.clusterService = clusterService;
        this.updateConsumer = updateConsumer;
        this.consumingPlanner = consumingPlanner;
    }


    public Plan plan(Analysis analysis) {
        AnalyzedStatement analyzedStatement = analysis.analyzedStatement();
        return process(analyzedStatement, new Context(clusterService));
    }

    @Override
    protected Plan visitAnalyzedStatement(AnalyzedStatement analyzedStatement, Context context) {
        throw new UnsupportedOperationException(String.format("AnalyzedStatement \"%s\" not supported.", analyzedStatement));
    }

    @Override
    protected Plan visitSelectStatement(SelectAnalyzedStatement statement, Context context) {
        return consumingPlanner.plan(statement.relation(), context);
    }

    @Override
    protected Plan visitInsertFromValuesStatement(InsertFromValuesAnalyzedStatement statement, Context context) {
        Preconditions.checkState(!statement.sourceMaps().isEmpty(), "no values given");
        return processInsertStatement(statement, context);
    }

    @Override
    protected Plan visitInsertFromSubQueryStatement(InsertFromSubQueryAnalyzedStatement statement, Context context) {
        return consumingPlanner.plan(statement, context);
    }

    @Override
    protected Plan visitUpdateStatement(UpdateAnalyzedStatement statement, Context context) {
        ConsumerContext consumerContext = new ConsumerContext(statement, context);
        if (updateConsumer.consume(statement, consumerContext)) {
            return ((PlannedAnalyzedRelation) consumerContext.rootRelation()).plan();
        }
        throw new IllegalArgumentException("Couldn't plan Update statement");
    }

    @Override
    protected Plan visitDeleteStatement(DeleteAnalyzedStatement analyzedStatement, Context context) {
        IterablePlan plan = new IterablePlan();
        TableRelation tableRelation = analyzedStatement.analyzedRelation();
        List<WhereClause> whereClauses = new ArrayList<>(analyzedStatement.whereClauses().size());
        List<DocKeys.DocKey> docKeys = new ArrayList<>(analyzedStatement.whereClauses().size());
        for (WhereClause whereClause : analyzedStatement.whereClauses()) {
            if (whereClause.noMatch()) {
                continue;
            }
            if (whereClause.docKeys().isPresent() && whereClause.docKeys().get().size() == 1) {
                docKeys.add(whereClause.docKeys().get().getOnlyKey());
            } else if (!whereClause.noMatch()) {
                whereClauses.add(whereClause);
            }
        }
        if (!docKeys.isEmpty()) {
            plan.add(new ESDeleteNode(context.nextExecutionNodeId(), tableRelation.tableInfo(), docKeys));
        } else if (!whereClauses.isEmpty()) {
            createESDeleteByQueryNode(tableRelation.tableInfo(), whereClauses, plan, context);
        }

        if (plan.isEmpty()) {
            return NoopPlan.INSTANCE;
        }
        return plan;
    }

    @Override
    protected Plan visitCopyStatement(final CopyAnalyzedStatement analysis, Context context) {
        switch (analysis.mode()) {
            case FROM:
                return copyFromPlan(analysis, context);
            case TO:
                return copyToPlan(analysis, context);
            default:
                throw new UnsupportedOperationException("mode not supported: " + analysis.mode());
        }
    }

    private Plan copyToPlan(CopyAnalyzedStatement analysis, Context context) {
        TableInfo tableInfo = analysis.table();
        WriterProjection projection = new WriterProjection();
        projection.uri(analysis.uri());
        projection.isDirectoryUri(analysis.directoryUri());
        projection.settings(analysis.settings());

        List<Symbol> outputs;
        if (analysis.selectedColumns() != null && !analysis.selectedColumns().isEmpty()) {
            outputs = new ArrayList<>(analysis.selectedColumns().size());
            List<Symbol> columnSymbols = new ArrayList<>(analysis.selectedColumns().size());
            for (int i = 0; i < analysis.selectedColumns().size(); i++) {
                outputs.add(DocReferenceConverter.convertIfPossible(analysis.selectedColumns().get(i), analysis.table()));
                columnSymbols.add(new InputColumn(i, null));
            }
            projection.inputs(columnSymbols);
        } else {
            Reference sourceRef;
            if (analysis.table().isPartitioned() && analysis.partitionIdent() == null) {

                sourceRef = new Reference(analysis.table().getReferenceInfo(DocSysColumns.DOC));
                Map<ColumnIdent, Symbol> overwrites = new HashMap<>();
                for (ReferenceInfo referenceInfo : analysis.table().partitionedByColumns()) {
                    overwrites.put(referenceInfo.ident().columnIdent(), new Reference(referenceInfo));
                }
                projection.overwrites(overwrites);
            } else {
                sourceRef = new Reference(analysis.table().getReferenceInfo(DocSysColumns.RAW));
            }
            outputs = ImmutableList.<Symbol>of(sourceRef);
        }
        CollectNode collectNode = PlanNodeBuilder.collect(
                tableInfo,
                context,
                WhereClause.MATCH_ALL,
                outputs,
                ImmutableList.<Projection>of(projection),
                analysis.partitionIdent()
        );

        MergeNode mergeNode = PlanNodeBuilder.localMerge(
                ImmutableList.<Projection>of(CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION), collectNode, context);
        return new CollectAndMerge(collectNode, mergeNode);
    }

    private Plan copyFromPlan(CopyAnalyzedStatement analysis, Context context) {


        TableInfo table = analysis.table();
        int clusteredByPrimaryKeyIdx = table.primaryKey().indexOf(analysis.table().clusteredBy());
        List<String> partitionedByNames;
        String partitionIdent = null;

        List<BytesRef> partitionValues;
        if (analysis.partitionIdent() == null) {

            if (table.isPartitioned()) {
                partitionedByNames = Lists.newArrayList(
                        Lists.transform(table.partitionedBy(), ColumnIdent.GET_FQN_NAME_FUNCTION));
            } else {
                partitionedByNames = Collections.emptyList();
            }
            partitionValues = ImmutableList.of();
        } else {
            assert table.isPartitioned() : "table must be partitioned if partitionIdent is set";

            PartitionName partitionName = PartitionName.fromPartitionIdent(table.ident().schema(), table.ident().name(), analysis.partitionIdent());
            partitionValues = partitionName.values();

            partitionIdent = partitionName.ident();
            partitionedByNames = Collections.emptyList();
        }

        SourceIndexWriterProjection sourceIndexWriterProjection = new SourceIndexWriterProjection(
                table.ident(),
                partitionIdent,
                new Reference(table.getReferenceInfo(DocSysColumns.RAW)),
                table.primaryKey(),
                table.partitionedBy(),
                partitionValues,
                table.clusteredBy(),
                clusteredByPrimaryKeyIdx,
                analysis.settings(),
                null,
                partitionedByNames.size() > 0 ? partitionedByNames.toArray(new String[partitionedByNames.size()]) : null,
                table.isPartitioned() 
        );
        List<Projection> projections = Arrays.<Projection>asList(sourceIndexWriterProjection);
        partitionedByNames.removeAll(Lists.transform(table.primaryKey(), ColumnIdent.GET_FQN_NAME_FUNCTION));
        int referencesSize = table.primaryKey().size() + partitionedByNames.size() + 1;
        referencesSize = clusteredByPrimaryKeyIdx == -1 ? referencesSize + 1 : referencesSize;

        List<Symbol> toCollect = new ArrayList<>(referencesSize);

        for (ColumnIdent primaryKey : table.primaryKey()) {
            toCollect.add(new Reference(table.getReferenceInfo(primaryKey)));
        }


        for (String partitionedColumn : partitionedByNames) {
            toCollect.add(
                    new Reference(table.getReferenceInfo(ColumnIdent.fromPath(partitionedColumn)))
            );
        }

        if (clusteredByPrimaryKeyIdx == -1) {
            toCollect.add(
                    new Reference(table.getReferenceInfo(table.clusteredBy())));
        }

        if (table.isPartitioned() && analysis.partitionIdent() == null) {
            toCollect.add(new Reference(table.getReferenceInfo(DocSysColumns.DOC)));
        } else {
            toCollect.add(new Reference(table.getReferenceInfo(DocSysColumns.RAW)));
        }

        DiscoveryNodes allNodes = clusterService.state().nodes();
        FileUriCollectNode collectNode = new FileUriCollectNode(
                context.nextExecutionNodeId(),
                "copyFrom",
                generateRouting(allNodes, analysis.settings().getAsInt("num_readers", allNodes.getSize())),
                analysis.uri(),
                toCollect,
                projections,
                analysis.settings().get("compression", null),
                analysis.settings().getAsBoolean("shared", null)
        );
        PlanNodeBuilder.setOutputTypes(collectNode);

        return new CollectAndMerge(collectNode, PlanNodeBuilder.localMerge(
                ImmutableList.<Projection>of(CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION), collectNode, context));
    }

    private Routing generateRouting(DiscoveryNodes allNodes, int maxNodes) {
        final AtomicInteger counter = new AtomicInteger(maxNodes);
        final Map<String, Map<String, List<Integer>>> locations = new TreeMap<>();
        allNodes.dataNodes().keys().forEach(new ObjectProcedure<String>() {
            @Override
            public void apply(String value) {
                if (counter.getAndDecrement() > 0) {
                    locations.put(value, TreeMapBuilder.<String, List<Integer>>newMapBuilder().map());
                }
            }
        });
        return new Routing(locations);
    }

    @Override
    protected Plan visitDDLAnalyzedStatement(AbstractDDLAnalyzedStatement statement, Context context) {
        return new IterablePlan(new GenericDDLNode(statement));
    }

    @Override
    public Plan visitDropBlobTableStatement(DropBlobTableAnalyzedStatement analysis, Context context) {
        if (analysis.noop()) {
            return NoopPlan.INSTANCE;
        }
        return visitDDLAnalyzedStatement(analysis, context);
    }

    @Override
    protected Plan visitDropTableStatement(DropTableAnalyzedStatement analysis, Context context) {
        if (analysis.noop()) {
            return NoopPlan.INSTANCE;
        }
        return new IterablePlan(new DropTableNode(analysis.table()));
    }

    @Override
    protected Plan visitCreateTableStatement(CreateTableAnalyzedStatement analysis, Context context) {
        if (analysis.noOp()) {
            return NoopPlan.INSTANCE;
        }
        TableIdent tableIdent = analysis.tableIdent();

        CreateTableNode createTableNode;
        if (analysis.isPartitioned()) {
            createTableNode = CreateTableNode.createPartitionedTableNode(
                    tableIdent,
                    analysis.ifNotExists(),
                    analysis.tableParameter().settings().getByPrefix("index."),
                    analysis.mapping(),
                    analysis.templateName(),
                    analysis.templatePrefix()
            );
        } else {
            createTableNode = CreateTableNode.createTableNode(
                    tableIdent,
                    analysis.ifNotExists(),
                    analysis.tableParameter().settings(),
                    analysis.mapping()
            );
        }
        return new IterablePlan(createTableNode);
    }

    @Override
    protected Plan visitCreateAnalyzerStatement(CreateAnalyzerAnalyzedStatement analysis, Context context) {
        Settings analyzerSettings;
        try {
            analyzerSettings = analysis.buildSettings();
        } catch (IOException ioe) {
            throw new UnhandledServerException("Could not build analyzer Settings", ioe);
        }

        ESClusterUpdateSettingsNode node = new ESClusterUpdateSettingsNode(analyzerSettings);
        return new IterablePlan(node);
    }

    @Override
    public Plan visitSetStatement(SetAnalyzedStatement analysis, Context context) {
        ESClusterUpdateSettingsNode node = null;
        if (analysis.isReset()) {

            if (analysis.settingsToRemove() != null) {
                node = new ESClusterUpdateSettingsNode(analysis.settingsToRemove(), analysis.settingsToRemove());
            }
        } else {
            if (analysis.settings() != null) {
                if (analysis.isPersistent()) {
                    node = new ESClusterUpdateSettingsNode(analysis.settings());
                } else {
                    node = new ESClusterUpdateSettingsNode(ImmutableSettings.EMPTY, analysis.settings());
                }
            }
        }
        return node != null ? new IterablePlan(node) : NoopPlan.INSTANCE;
    }

    @Override
    public Plan visitKillAnalyzedStatement(KillAnalyzedStatement analysis, Context context) {
        return KillPlan.INSTANCE;
    }

    private void createESDeleteByQueryNode(TableInfo tableInfo,
                                           List<WhereClause> whereClauses,
                                           IterablePlan plan,
                                           Context context) {

        List<String[]> indicesList = new ArrayList<>(whereClauses.size());
        for (WhereClause whereClause : whereClauses) {
            String[] indices = indices(tableInfo, whereClauses.get(0));
            if (indices.length > 0) {
                if (!whereClause.hasQuery() && tableInfo.isPartitioned()) {
                    plan.add(new ESDeletePartitionNode(indices));
                } else {
                    indicesList.add(indices);
                }
            }
        }



        if (!indicesList.isEmpty()) {
            plan.add(new ESDeleteByQueryNode(context.nextExecutionNodeId(), indicesList, whereClauses));
        }
    }

    private Upsert processInsertStatement(InsertFromValuesAnalyzedStatement analysis, Context context) {
        String[] onDuplicateKeyAssignmentsColumns = null;
        if (analysis.onDuplicateKeyAssignmentsColumns().size() > 0) {
            onDuplicateKeyAssignmentsColumns = analysis.onDuplicateKeyAssignmentsColumns().get(0);
        }
        SymbolBasedUpsertByIdNode upsertByIdNode = new SymbolBasedUpsertByIdNode(
                context.nextExecutionNodeId(),
                analysis.tableInfo().isPartitioned(),
                analysis.isBulkRequest(),
                onDuplicateKeyAssignmentsColumns,
                analysis.columns().toArray(new Reference[analysis.columns().size()])
        );
        if (analysis.tableInfo().isPartitioned()) {
            List<String> partitions = analysis.generatePartitions();
            String[] indices = partitions.toArray(new String[partitions.size()]);
            for (int i = 0; i < indices.length; i++) {
                Symbol[] onDuplicateKeyAssignments = null;
                if (analysis.onDuplicateKeyAssignmentsColumns().size() > i) {
                    onDuplicateKeyAssignments = analysis.onDuplicateKeyAssignments().get(i);
                }
                upsertByIdNode.add(
                        indices[i],
                        analysis.ids().get(i),
                        analysis.routingValues().get(i),
                        onDuplicateKeyAssignments,
                        null,
                        analysis.sourceMaps().get(i));
            }
        } else {
            for (int i = 0; i < analysis.ids().size(); i++) {
                Symbol[] onDuplicateKeyAssignments = null;
                if (analysis.onDuplicateKeyAssignments().size() > i) {
                    onDuplicateKeyAssignments = analysis.onDuplicateKeyAssignments().get(i);
                }
                upsertByIdNode.add(
                        analysis.tableInfo().ident().esName(),
                        analysis.ids().get(i),
                        analysis.routingValues().get(i),
                        onDuplicateKeyAssignments,
                        null,
                        analysis.sourceMaps().get(i));
            }
        }

        return new Upsert(ImmutableList.<Plan>of(new IterablePlan(upsertByIdNode)));
    }

    static List<DataType> extractDataTypes(List<Projection> projections, @Nullable List<DataType> inputTypes) {
        if (projections.size() == 0) {
            return inputTypes;
        }
        int projectionIdx = projections.size() - 1;
        Projection lastProjection = projections.get(projectionIdx);
        List<DataType> types = new ArrayList<>(lastProjection.outputs().size());
        List<DataType> dataTypes = firstNonNull(inputTypes, ImmutableList.<DataType>of());

        for (int c = 0; c < lastProjection.outputs().size(); c++) {
            types.add(resolveType(projections, projectionIdx, c, dataTypes));
        }
        return types;
    }

    private static DataType resolveType(List<Projection> projections, int projectionIdx, int columnIdx, List<DataType> inputTypes) {
        Projection projection = projections.get(projectionIdx);
        Symbol symbol = projection.outputs().get(columnIdx);
        DataType type = symbol.valueType();
        if (type == null || (type.equals(DataTypes.UNDEFINED) && symbol instanceof InputColumn)) {
            if (projectionIdx > 0) {
                if (symbol instanceof InputColumn) {
                    columnIdx = ((InputColumn) symbol).index();
                }
                return resolveType(projections, projectionIdx - 1, columnIdx, inputTypes);
            } else {
                assert symbol instanceof InputColumn; 
                return inputTypes.get(((InputColumn) symbol).index());
            }
        }

        return type;
    }



    public static String[] indices(TableInfo tableInfo, WhereClause whereClause) {
        String[] indices;

        if (whereClause.noMatch()) {
            indices = org.elasticsearch.common.Strings.EMPTY_ARRAY;
        } else if (!tableInfo.isPartitioned()) {

            indices = new String[]{tableInfo.ident().esName()};
        } else if (whereClause.partitions().isEmpty()) {
            if (whereClause.noMatch()) {
                return new String[0];
            }


            indices = new String[tableInfo.partitions().size()];
            int i = 0;
            for (PartitionName partitionName: tableInfo.partitions()) {
                indices[i] = partitionName.stringValue();
                i++;
            }
        } else {
            indices = whereClause.partitions().toArray(new String[whereClause.partitions().size()]);
        }
        return indices;
    }
}


<code block>


package io.crate.planner.consumer;

import com.google.common.collect.ImmutableList;
import io.crate.analyze.UpdateAnalyzedStatement;
import io.crate.analyze.VersionRewriter;
import io.crate.analyze.WhereClause;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.analyze.relations.TableRelation;
import io.crate.analyze.where.DocKeys;
import io.crate.metadata.PartitionName;
import io.crate.metadata.ReferenceIdent;
import io.crate.metadata.ReferenceInfo;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.aggregation.impl.CountAggregation;
import io.crate.planner.*;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dml.SymbolBasedUpsertByIdNode;
import io.crate.planner.node.dml.Upsert;
import io.crate.planner.node.dql.CollectAndMerge;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.UpdateProjection;
import io.crate.planner.symbol.InputColumn;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.planner.symbol.ValueSymbolVisitor;
import io.crate.types.DataTypes;
import org.elasticsearch.cluster.routing.operation.plain.Preference;
import org.elasticsearch.common.collect.Tuple;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.inject.Singleton;

import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;
import java.util.Map;

@Singleton
public class UpdateConsumer implements Consumer {

    private final Visitor visitor;

    @Inject
    public UpdateConsumer() {
        visitor = new Visitor();
    }

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        PlannedAnalyzedRelation plannedAnalyzedRelation = visitor.process(rootRelation, context);
        if (plannedAnalyzedRelation == null) {
            return false;
        }
        context.rootRelation(plannedAnalyzedRelation);
        return true;
    }

    class Visitor extends AnalyzedRelationVisitor<ConsumerContext, PlannedAnalyzedRelation> {

        @Override
        public PlannedAnalyzedRelation visitUpdateAnalyzedStatement(UpdateAnalyzedStatement statement, ConsumerContext context) {
            assert statement.sourceRelation() instanceof TableRelation : "sourceRelation of update statement must be a TableRelation";
            TableRelation tableRelation = (TableRelation) statement.sourceRelation();
            TableInfo tableInfo = tableRelation.tableInfo();

            if (tableInfo.schemaInfo().systemSchema() || tableInfo.rowGranularity() != RowGranularity.DOC) {
                return null;
            }

            List<Plan> childNodes = new ArrayList<>(statement.nestedStatements().size());
            SymbolBasedUpsertByIdNode upsertByIdNode = null;
            for (UpdateAnalyzedStatement.NestedAnalyzedStatement nestedAnalysis : statement.nestedStatements()) {
                WhereClause whereClause = nestedAnalysis.whereClause();
                if (whereClause.noMatch()){
                    continue;
                }
                if (whereClause.docKeys().isPresent()) {
                    if (upsertByIdNode == null) {
                        Tuple<String[], Symbol[]> assignments = convertAssignments(nestedAnalysis.assignments());
                        upsertByIdNode = new SymbolBasedUpsertByIdNode(context.plannerContext().nextExecutionNodeId(), false, statement.nestedStatements().size() > 1, assignments.v1(), null);
                        childNodes.add(new IterablePlan(upsertByIdNode));
                    }
                    upsertById(nestedAnalysis, tableInfo, whereClause, upsertByIdNode);
                } else {
                    Plan plan = upsertByQuery(nestedAnalysis, context, tableInfo, whereClause);
                    if (plan != null) {
                        childNodes.add(plan);
                    }
                }
            }
            if (childNodes.size() > 0){
                return new Upsert(childNodes);
            } else {
                return new NoopPlannedAnalyzedRelation(statement);
            }
        }

        private Plan upsertByQuery(UpdateAnalyzedStatement.NestedAnalyzedStatement nestedAnalysis,
                                   ConsumerContext consumerContext,
                                   TableInfo tableInfo,
                                   WhereClause whereClause) {

            Symbol versionSymbol = null;
            if(whereClause.hasVersions()){
                versionSymbol = VersionRewriter.get(whereClause.query());
                whereClause = new WhereClause(whereClause.query(), whereClause.docKeys().orNull(), whereClause.partitions());
            }


            if (!whereClause.noMatch() || !(tableInfo.isPartitioned() && whereClause.partitions().isEmpty())) {

                Reference uidReference = new Reference(
                        new ReferenceInfo(
                                new ReferenceIdent(tableInfo.ident(), "_uid"),
                                RowGranularity.DOC, DataTypes.STRING));

                Tuple<String[], Symbol[]> assignments = convertAssignments(nestedAnalysis.assignments());

                Long version = null;
                if (versionSymbol != null){
                    version = ValueSymbolVisitor.LONG.process(versionSymbol);
                }

                UpdateProjection updateProjection = new UpdateProjection(
                        new InputColumn(0, DataTypes.STRING),
                        assignments.v1(),
                        assignments.v2(),
                        version);

                CollectNode collectNode = PlanNodeBuilder.collect(
                        tableInfo,
                        consumerContext.plannerContext(),
                        whereClause,
                        ImmutableList.<Symbol>of(uidReference),
                        ImmutableList.<Projection>of(updateProjection),
                        null,
                        Preference.PRIMARY.type()
                );
                MergeNode mergeNode = PlanNodeBuilder.localMerge(
                        ImmutableList.<Projection>of(CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION), collectNode,
                        consumerContext.plannerContext());
                return new CollectAndMerge(collectNode, mergeNode);
            } else {
                return null;
            }
        }

        private void upsertById(UpdateAnalyzedStatement.NestedAnalyzedStatement nestedAnalysis,
                                             TableInfo tableInfo,
                                             WhereClause whereClause,
                                             SymbolBasedUpsertByIdNode upsertByIdNode) {
            String[] indices = Planner.indices(tableInfo, whereClause);
            assert tableInfo.isPartitioned() || indices.length == 1;

            Tuple<String[], Symbol[]> assignments = convertAssignments(nestedAnalysis.assignments());


            for (DocKeys.DocKey key : whereClause.docKeys().get()) {
                String index;
                if (key.partitionValues().isPresent()) {
                    index = new PartitionName(tableInfo.ident(), key.partitionValues().get()).stringValue();
                } else {
                    index = indices[0];
                }
                upsertByIdNode.add(
                        index,
                        key.id(),
                        key.routing(),
                        assignments.v2(),
                        key.version().orNull());
            }
        }


        private Tuple<String[], Symbol[]> convertAssignments(Map<Reference, Symbol> assignments) {
            String[] assignmentColumns = new String[assignments.size()];
            Symbol[] assignmentSymbols = new Symbol[assignments.size()];
            Iterator<Reference> it = assignments.keySet().iterator();
            int i = 0;
            while(it.hasNext()) {
                Reference key = it.next();
                assignmentColumns[i] = key.ident().columnIdent().fqn();
                assignmentSymbols[i] = assignments.get(key);
                i++;
            }
            return new Tuple<>(assignmentColumns, assignmentSymbols);
        }

        @Override
        protected PlannedAnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, ConsumerContext context) {
            return null;
        }
    }
}

<code block>

package io.crate.planner.consumer;

import com.google.common.collect.ImmutableList;
import io.crate.analyze.*;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.Routing;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.GroupByConsumer;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.dql.NonDistributedGroupBy;
import io.crate.planner.projection.GroupProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.builder.ProjectionBuilder;
import io.crate.planner.projection.builder.SplitPoints;
import io.crate.planner.symbol.Aggregation;
import io.crate.planner.symbol.Symbol;

import java.util.ArrayList;
import java.util.List;

public class NonDistributedGroupByConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        Context ctx = new Context(context);
        context.rootRelation(VISITOR.process(context.rootRelation(), ctx));
        return ctx.result;
    }

    private static class Context {
        ConsumerContext consumerContext;
        boolean result = false;

        public Context(ConsumerContext context) {
            this.consumerContext = context;
        }
    }

    private static class Visitor extends AnalyzedRelationVisitor<Context, AnalyzedRelation> {

        @Override
        public AnalyzedRelation visitQueriedTable(QueriedTable table, Context context) {
            if (table.querySpec().groupBy() == null) {
                return table;
            }
            TableInfo tableInfo = table.tableRelation().tableInfo();

            if (table.querySpec().where().hasVersions()) {
                context.consumerContext.validationException(new VersionInvalidException());
                return table;
            }

            Routing routing = tableInfo.getRouting(table.querySpec().where(), null);

            if (GroupByConsumer.requiresDistribution(tableInfo, routing) && !(tableInfo.schemaInfo().systemSchema())) {
                return table;
            }

            context.result = true;
            return nonDistributedGroupBy(table, context);
        }

        @Override
        public AnalyzedRelation visitInsertFromQuery(InsertFromSubQueryAnalyzedStatement insertFromSubQueryAnalyzedStatement, Context context) {
            InsertFromSubQueryConsumer.planInnerRelation(insertFromSubQueryAnalyzedStatement, context, this);
            return insertFromSubQueryAnalyzedStatement;

        }

        @Override
        protected AnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, Context context) {
            return relation;
        }


        private AnalyzedRelation nonDistributedGroupBy(QueriedTable table, Context context) {
            TableInfo tableInfo = table.tableRelation().tableInfo();

            GroupByConsumer.validateGroupBySymbols(table.tableRelation(), table.querySpec().groupBy());
            List<Symbol> groupBy = table.querySpec().groupBy();

            ProjectionBuilder projectionBuilder = new ProjectionBuilder(table.querySpec());
            SplitPoints splitPoints = projectionBuilder.getSplitPoints();


            GroupProjection groupProjection = projectionBuilder.groupProjection(
                    splitPoints.leaves(),
                    table.querySpec().groupBy(),
                    splitPoints.aggregates(),
                    Aggregation.Step.ITER,
                    Aggregation.Step.PARTIAL);

            CollectNode collectNode = PlanNodeBuilder.collect(
                    tableInfo,
                    context.consumerContext.plannerContext(),
                    table.querySpec().where(),
                    splitPoints.leaves(),
                    ImmutableList.<Projection>of(groupProjection)
            );

            List<Symbol> collectOutputs = new ArrayList<>(
                    groupBy.size() +
                            splitPoints.aggregates().size());
            collectOutputs.addAll(groupBy);
            collectOutputs.addAll(splitPoints.aggregates());


            OrderBy orderBy = table.querySpec().orderBy();
            if (orderBy != null) {
                table.tableRelation().validateOrderBy(orderBy);
            }

            List<Projection> projections = new ArrayList<>();
            projections.add(projectionBuilder.groupProjection(
                    collectOutputs,
                    table.querySpec().groupBy(),
                    splitPoints.aggregates(),
                    Aggregation.Step.PARTIAL,
                    Aggregation.Step.FINAL
            ));

            HavingClause havingClause = table.querySpec().having();
            if (havingClause != null) {
                if (havingClause.noMatch()) {
                    return new NoopPlannedAnalyzedRelation(table);
                } else if (havingClause.hasQuery()){
                    projections.add(projectionBuilder.filterProjection(
                            collectOutputs,
                            havingClause.query()
                    ));
                }
            }


            boolean outputsMatch = table.querySpec().outputs().size() == collectOutputs.size() &&
                                    collectOutputs.containsAll(table.querySpec().outputs());
            if (context.consumerContext.rootRelation() == table || !outputsMatch){
                projections.add(projectionBuilder.topNProjection(
                        collectOutputs,
                        orderBy,
                        table.querySpec().offset(),
                        table.querySpec().limit(),
                        table.querySpec().outputs()
                ));
            }
            MergeNode localMergeNode = PlanNodeBuilder.localMerge(projections, collectNode,
                    context.consumerContext.plannerContext());
            return new NonDistributedGroupBy(collectNode, localMergeNode);
        }
    }

}

<code block>


package io.crate.planner.consumer;

import io.crate.analyze.QueriedTable;
import io.crate.analyze.QuerySpec;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.Routing;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.aggregation.impl.CountAggregation;
import io.crate.planner.Planner;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.CountNode;
import io.crate.planner.node.dql.CountPlan;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.Function;
import io.crate.planner.symbol.Symbol;
import io.crate.types.DataType;
import io.crate.types.DataTypes;

import java.util.Collections;
import java.util.List;

import static com.google.common.base.MoreObjects.firstNonNull;

public class CountConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        AnalyzedRelation analyzedRelation = VISITOR.process(rootRelation, context);
        if (analyzedRelation != null) {
            context.rootRelation(analyzedRelation);
            return true;
        }
        return false;
    }

    private static class Visitor extends AnalyzedRelationVisitor<ConsumerContext, PlannedAnalyzedRelation> {

        @Override
        public PlannedAnalyzedRelation visitQueriedTable(QueriedTable table, ConsumerContext context) {
            QuerySpec querySpec = table.querySpec();
            if (!querySpec.hasAggregates() || querySpec.groupBy()!=null) {
                return null;
            }
            TableInfo tableInfo = table.tableRelation().tableInfo();
            if (tableInfo.schemaInfo().systemSchema()) {
                return null;
            }
            if (!hasOnlyGlobalCount(querySpec.outputs())) {
                return null;
            }
            if(querySpec.where().hasVersions()){
                context.validationException(new VersionInvalidException());
                return null;
            }

            if (firstNonNull(querySpec.limit(), 1) < 1 ||
                    querySpec.offset() > 0){
                return new NoopPlannedAnalyzedRelation(table);
            }

            Routing routing = tableInfo.getRouting(querySpec.where(), null);
            Planner.Context plannerContext = context.plannerContext();
            CountNode countNode = new CountNode(plannerContext.nextExecutionNodeId(), routing, querySpec.where());
            MergeNode mergeNode = new MergeNode(
                    plannerContext.nextExecutionNodeId(),
                    "count-merge",
                    countNode.executionNodes().size());
            mergeNode.inputTypes(Collections.<DataType>singletonList(DataTypes.LONG));
            mergeNode.projections(Collections.<Projection>singletonList(
                    CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION));
            return new CountPlan(countNode, mergeNode);
        }

        @Override
        protected PlannedAnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, ConsumerContext context) {
            return null;
        }

        private boolean hasOnlyGlobalCount(List<Symbol> symbols) {
            if (symbols.size() != 1) {
                return false;
            }
            Symbol symbol = symbols.get(0);
            if (!(symbol instanceof Function)) {
                return false;
            }
            Function function = (Function) symbol;
            return function.info().equals(CountAggregation.COUNT_STAR_FUNCTION);
        }
    }
}

<code block>


package io.crate.planner.consumer;

import com.google.common.collect.ImmutableList;
import io.crate.Constants;
import io.crate.analyze.*;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.TableRelation;
import io.crate.exceptions.UnsupportedFeatureException;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.DocReferenceConverter;
import io.crate.metadata.FunctionIdent;
import io.crate.metadata.Functions;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.aggregation.impl.SumAggregation;
import io.crate.operation.predicate.MatchPredicate;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.node.dql.QueryAndFetch;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.AggregationProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.TopNProjection;
import io.crate.planner.symbol.*;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import io.crate.types.LongType;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;

import static com.google.common.base.MoreObjects.firstNonNull;

public class QueryAndFetchConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        Context ctx = new Context(context);
        context.rootRelation(VISITOR.process(context.rootRelation(), ctx));
        return ctx.result;
    }

    private static class Context {
        ConsumerContext consumerContext;
        boolean result = false;

        public Context(ConsumerContext context) {
            this.consumerContext = context;
        }
    }

    private static class Visitor extends AnalyzedRelationVisitor<Context, AnalyzedRelation> {

        @Override
        public AnalyzedRelation visitQueriedTable(QueriedTable table, Context context) {
            TableRelation tableRelation = table.tableRelation();
            if(table.querySpec().where().hasVersions()){
                context.consumerContext.validationException(new VersionInvalidException());
                return table;
            }
            TableInfo tableInfo = tableRelation.tableInfo();

            if (tableInfo.schemaInfo().systemSchema() && table.querySpec().where().hasQuery()) {
                ensureNoLuceneOnlyPredicates(table.querySpec().where().query());
            }
            if (table.querySpec().hasAggregates()) {
                context.result = true;
                return GlobalAggregateConsumer.globalAggregates(table, tableRelation, table.querySpec().where(), context.consumerContext);
            } else {
               context.result = true;
               return normalSelect(table, table.querySpec().where(), tableRelation, context);
            }
        }

        @Override
        public AnalyzedRelation visitInsertFromQuery(InsertFromSubQueryAnalyzedStatement insertFromSubQueryAnalyzedStatement,
                                                     Context context) {
            InsertFromSubQueryConsumer.planInnerRelation(insertFromSubQueryAnalyzedStatement, context, this);
            return insertFromSubQueryAnalyzedStatement;
        }

        @Override
        protected AnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, Context context) {
            return relation;
        }

        private void ensureNoLuceneOnlyPredicates(Symbol query) {
            NoPredicateVisitor noPredicateVisitor = new NoPredicateVisitor();
            noPredicateVisitor.process(query, null);
        }

        private static class NoPredicateVisitor extends SymbolVisitor<Void, Void> {
            @Override
            public Void visitFunction(Function symbol, Void context) {
                if (symbol.info().ident().name().equals(MatchPredicate.NAME)) {
                    throw new UnsupportedFeatureException("Cannot use match predicate on system tables");
                }
                for (Symbol argument : symbol.arguments()) {
                    process(argument, context);
                }
                return null;
            }
        }

        private AnalyzedRelation normalSelect(QueriedTable table,
                                              WhereClause whereClause,
                                              TableRelation tableRelation,
                                              Context context){
            QuerySpec querySpec = table.querySpec();
            TableInfo tableInfo = tableRelation.tableInfo();

            List<Symbol> outputSymbols;
            if (tableInfo.schemaInfo().systemSchema()) {
                outputSymbols = tableRelation.resolve(querySpec.outputs());
            } else {
                outputSymbols = new ArrayList<>(querySpec.outputs().size());
                for (Symbol symbol : querySpec.outputs()) {
                    outputSymbols.add(DocReferenceConverter.convertIfPossible(tableRelation.resolve(symbol), tableInfo));
                }
            }
            CollectNode collectNode;
            MergeNode mergeNode = null;
            OrderBy orderBy = querySpec.orderBy();
            if (context.consumerContext.rootRelation() != table) {

                assert !querySpec.isLimited() : "insert from sub query with limit or order by is not supported. " +
                        "Analyzer should have thrown an exception already.";

                ImmutableList<Projection> projections = ImmutableList.<Projection>of();
                collectNode = PlanNodeBuilder.collect(tableInfo,
                        context.consumerContext.plannerContext(),
                        whereClause, outputSymbols, projections);
            } else if (querySpec.isLimited() || orderBy != null) {

                List<Symbol> toCollect;
                List<Symbol> orderByInputColumns = null;
                if (orderBy != null){
                    List<Symbol> orderBySymbols = tableRelation.resolve(orderBy.orderBySymbols());
                    toCollect = new ArrayList<>(outputSymbols.size() + orderBySymbols.size());
                    toCollect.addAll(outputSymbols);

                    for (Symbol orderBySymbol : orderBySymbols) {
                        if (!toCollect.contains(orderBySymbol)) {
                            toCollect.add(orderBySymbol);
                        }
                    }
                    orderByInputColumns = new ArrayList<>();
                    for (Symbol symbol : orderBySymbols) {
                        orderByInputColumns.add(new InputColumn(toCollect.indexOf(symbol), symbol.valueType()));
                    }
                } else {
                    toCollect = new ArrayList<>(outputSymbols.size());
                    toCollect.addAll(outputSymbols);
                }

                List<Symbol> allOutputs = new ArrayList<>(toCollect.size());        
                for (int i = 0; i < toCollect.size(); i++) {
                    allOutputs.add(new InputColumn(i, toCollect.get(i).valueType()));
                }
                List<Symbol> finalOutputs = new ArrayList<>(outputSymbols.size());  
                for (int i = 0; i < outputSymbols.size(); i++) {
                    finalOutputs.add(new InputColumn(i, outputSymbols.get(i).valueType()));
                }



                TopNProjection tnp;
                int limit = firstNonNull(querySpec.limit(), Constants.DEFAULT_SELECT_LIMIT);
                if (orderBy == null){
                    tnp = new TopNProjection(querySpec.offset() + limit, 0);
                } else {
                    tnp = new TopNProjection(querySpec.offset() + limit, 0,
                            orderByInputColumns,
                            orderBy.reverseFlags(),
                            orderBy.nullsFirst()
                    );
                }
                tnp.outputs(allOutputs);
                collectNode = PlanNodeBuilder.collect(tableInfo,
                        context.consumerContext.plannerContext(),
                        whereClause, toCollect, ImmutableList.<Projection>of(tnp));


                tnp = new TopNProjection(limit, querySpec.offset());
                tnp.outputs(finalOutputs);
                if (orderBy == null) {

                    mergeNode = PlanNodeBuilder.localMerge(ImmutableList.<Projection>of(tnp), collectNode,
                            context.consumerContext.plannerContext());
                } else {


                    mergeNode = PlanNodeBuilder.sortedLocalMerge(
                            ImmutableList.<Projection>of(tnp),
                            orderBy,
                            allOutputs,
                            orderByInputColumns,
                            collectNode,
                            context.consumerContext.plannerContext()
                    );
                }
            } else {
                collectNode = PlanNodeBuilder.collect(tableInfo,
                        context.consumerContext.plannerContext(),
                        whereClause, outputSymbols, ImmutableList.<Projection>of());
                mergeNode = PlanNodeBuilder.localMerge(
                        ImmutableList.<Projection>of(), collectNode,
                        context.consumerContext.plannerContext());
            }
            return new QueryAndFetch(collectNode, mergeNode);
        }
    }
}

<code block>


package io.crate.planner.consumer;

import com.google.common.collect.ImmutableList;
import io.crate.analyze.*;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.analyze.relations.TableRelation;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.FunctionInfo;
import io.crate.metadata.ReferenceInfo;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.GlobalAggregate;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.AggregationProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.TopNProjection;
import io.crate.planner.projection.builder.ProjectionBuilder;
import io.crate.planner.projection.builder.SplitPoints;
import io.crate.planner.symbol.*;

import java.util.ArrayList;
import java.util.Collection;
import java.util.List;

import static com.google.common.base.MoreObjects.firstNonNull;


public class GlobalAggregateConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();
    private static final AggregationOutputValidator AGGREGATION_OUTPUT_VALIDATOR = new AggregationOutputValidator();

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        AnalyzedRelation analyzedRelation = VISITOR.process(rootRelation, context);
        if (analyzedRelation != null) {
            context.rootRelation(analyzedRelation);
            return true;
        }
        return false;
    }

    private static class Visitor extends AnalyzedRelationVisitor<ConsumerContext, PlannedAnalyzedRelation> {

        @Override
        public PlannedAnalyzedRelation visitQueriedTable(QueriedTable table, ConsumerContext context) {
            if (table.querySpec().groupBy()!=null || !table.querySpec().hasAggregates()) {
                return null;
            }
            if (firstNonNull(table.querySpec().limit(), 1) < 1 || table.querySpec().offset() > 0){
                return new NoopPlannedAnalyzedRelation(table);
            }

            if (table.querySpec().where().hasVersions()){
                context.validationException(new VersionInvalidException());
                return null;
            }
            return globalAggregates(table, table.tableRelation(),  table.querySpec().where(), context);
        }

        @Override
        public PlannedAnalyzedRelation visitInsertFromQuery(InsertFromSubQueryAnalyzedStatement insertFromSubQueryAnalyzedStatement, ConsumerContext context) {
            InsertFromSubQueryConsumer.planInnerRelation(insertFromSubQueryAnalyzedStatement, context, this);
            return null;
        }

        @Override
        protected PlannedAnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, ConsumerContext context) {
            return null;
        }
    }

    private static boolean noGroupBy(List<Symbol> groupBy) {
        return groupBy == null || groupBy.isEmpty();
    }

    public static PlannedAnalyzedRelation globalAggregates(QueriedTable table,
                                                           TableRelation tableRelation,
                                                           WhereClause whereClause,
                                                           ConsumerContext context) {
        assert noGroupBy(table.querySpec().groupBy()) : "must not have group by clause for global aggregate queries";
        validateAggregationOutputs(tableRelation, table.querySpec().outputs());


        ProjectionBuilder projectionBuilder = new ProjectionBuilder(table.querySpec());
        SplitPoints splitPoints = projectionBuilder.getSplitPoints();

        AggregationProjection ap = projectionBuilder.aggregationProjection(
                splitPoints.leaves(),
                splitPoints.aggregates(),
                Aggregation.Step.ITER,
                Aggregation.Step.PARTIAL);

        CollectNode collectNode = PlanNodeBuilder.collect(
                tableRelation.tableInfo(),
                context.plannerContext(),
                whereClause,
                splitPoints.leaves(),
                ImmutableList.<Projection>of(ap)
        );


        List<Projection> projections = new ArrayList<>();
        projections.add(projectionBuilder.aggregationProjection(
                splitPoints.aggregates(),
                splitPoints.aggregates(),
                Aggregation.Step.PARTIAL,
                Aggregation.Step.FINAL));

        HavingClause havingClause = table.querySpec().having();
        if(havingClause != null){
            if (havingClause.noMatch()) {
                return new NoopPlannedAnalyzedRelation(table);
            } else if (havingClause.hasQuery()){
                projections.add(projectionBuilder.filterProjection(
                        splitPoints.aggregates(),
                        havingClause.query()
                ));
            }
        }

        TopNProjection topNProjection = projectionBuilder.topNProjection(
                splitPoints.aggregates(),
                null, 0, 1,
                table.querySpec().outputs()
                );
        projections.add(topNProjection);
        MergeNode localMergeNode = PlanNodeBuilder.localMerge(projections, collectNode,
                context.plannerContext());
        return new GlobalAggregate(collectNode, localMergeNode);
    }

    private static void validateAggregationOutputs(TableRelation tableRelation, Collection<? extends Symbol> outputSymbols) {
        OutputValidatorContext context = new OutputValidatorContext(tableRelation);
        for (Symbol outputSymbol : outputSymbols) {
            context.insideAggregation = false;
            AGGREGATION_OUTPUT_VALIDATOR.process(outputSymbol, context);
        }
    }

    private static class OutputValidatorContext {
        private final TableRelation tableRelation;
        private boolean insideAggregation = false;

        public OutputValidatorContext(TableRelation tableRelation) {
            this.tableRelation = tableRelation;
        }
    }

    private static class AggregationOutputValidator extends SymbolVisitor<OutputValidatorContext, Void> {

        @Override
        public Void visitFunction(Function symbol, OutputValidatorContext context) {
            context.insideAggregation = context.insideAggregation || symbol.info().type().equals(FunctionInfo.Type.AGGREGATE);
            for (Symbol argument : symbol.arguments()) {
                process(argument, context);
            }
            context.insideAggregation = false;
            return null;
        }

        @Override
        public Void visitReference(Reference symbol, OutputValidatorContext context) {
            if (context.insideAggregation) {
                ReferenceInfo.IndexType indexType = symbol.info().indexType();
                if (indexType == ReferenceInfo.IndexType.ANALYZED) {
                    throw new IllegalArgumentException(String.format(
                            "Cannot select analyzed column '%s' within grouping or aggregations", SymbolFormatter.format(symbol)));
                } else if (indexType == ReferenceInfo.IndexType.NO) {
                    throw new IllegalArgumentException(String.format(
                            "Cannot select non-indexed column '%s' within grouping or aggregations", SymbolFormatter.format(symbol)));
                }
            }
            return null;
        }

        @Override
        public Void visitField(Field field, OutputValidatorContext context) {
            return process(context.tableRelation.resolveField(field), context);
        }

        @Override
        protected Void visitSymbol(Symbol symbol, OutputValidatorContext context) {
            return null;
        }
    }
}

<code block>


package io.crate.planner.consumer;


import io.crate.analyze.OrderBy;
import io.crate.analyze.QueriedTable;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.ESGetNode;

public class ESGetConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        PlannedAnalyzedRelation relation = VISITOR.process(rootRelation, context);
        if (relation == null) {
            return false;
        }
        context.rootRelation(relation);
        return true;
    }

    private static class Visitor extends AnalyzedRelationVisitor<ConsumerContext, PlannedAnalyzedRelation> {

        @Override
        public PlannedAnalyzedRelation visitQueriedTable(QueriedTable table, ConsumerContext context) {
            if (table.querySpec().hasAggregates() || table.querySpec().groupBy()!=null) {
                return null;
            }
            TableInfo tableInfo = table.tableRelation().tableInfo();
            if (tableInfo.isAlias()
                    || tableInfo.schemaInfo().systemSchema()
                    || tableInfo.rowGranularity() != RowGranularity.DOC) {
                return null;
            }

            if (!table.querySpec().where().docKeys().isPresent()) {
                return null;
            }

            if(table.querySpec().where().docKeys().get().withVersions()){
                context.validationException(new VersionInvalidException());
                return null;
            }
            Integer limit = table.querySpec().limit();
            if (limit != null){
                if (limit == 0){
                    return new NoopPlannedAnalyzedRelation(table);
                }
            }

            OrderBy orderBy = table.querySpec().orderBy();
            if (orderBy != null){
                table.tableRelation().validateOrderBy(orderBy);
            }
            return new ESGetNode(context.plannerContext().nextExecutionNodeId(), tableInfo, table.querySpec());
        }

        @Override
        protected PlannedAnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, ConsumerContext context) {
            return null;
        }
    }
}

<code block>


package io.crate.planner.consumer;

import com.google.common.collect.ImmutableList;
import io.crate.Constants;
import io.crate.analyze.OrderBy;
import io.crate.analyze.QueriedTable;
import io.crate.analyze.QuerySpec;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.ColumnIdent;
import io.crate.metadata.DocReferenceConverter;
import io.crate.metadata.ReferenceInfo;
import io.crate.metadata.ScoreReferenceDetector;
import io.crate.metadata.doc.DocSysColumns;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.projectors.FetchProjector;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.dql.QueryThenFetch;
import io.crate.planner.projection.FetchProjection;
import io.crate.planner.projection.MergeProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.TopNProjection;
import io.crate.planner.projection.builder.ProjectionBuilder;
import io.crate.planner.projection.builder.SplitPoints;
import io.crate.planner.symbol.*;
import io.crate.types.DataTypes;

import java.util.ArrayList;
import java.util.Collections;
import java.util.List;

public class QueryThenFetchConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();
    private static final OutputOrderReferenceCollector OUTPUT_ORDER_REFERENCE_COLLECTOR = new OutputOrderReferenceCollector();
    private static final ReferencesCollector REFERENCES_COLLECTOR = new ReferencesCollector();
    private static final ScoreReferenceDetector SCORE_REFERENCE_DETECTOR = new ScoreReferenceDetector();
    private static final ColumnIdent DOC_ID_COLUMN_IDENT = new ColumnIdent(DocSysColumns.DOCID.name());
    private static final InputColumn DEFAULT_DOC_ID_INPUT_COLUMN = new InputColumn(0, DataTypes.STRING);

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        PlannedAnalyzedRelation plannedAnalyzedRelation = VISITOR.process(rootRelation, context);
        if (plannedAnalyzedRelation == null) {
            return false;
        }
        context.rootRelation(plannedAnalyzedRelation);
        return true;
    }

    private static class Visitor extends AnalyzedRelationVisitor<ConsumerContext, PlannedAnalyzedRelation> {

        @Override
        public PlannedAnalyzedRelation visitQueriedTable(QueriedTable table, ConsumerContext context) {
            QuerySpec querySpec = table.querySpec();
            if (querySpec.hasAggregates() || querySpec.groupBy()!=null) {
                return null;
            }
            TableInfo tableInfo = table.tableRelation().tableInfo();
            if (tableInfo.schemaInfo().systemSchema() || tableInfo.rowGranularity() != RowGranularity.DOC) {
                return null;
            }

            if(querySpec.where().hasVersions()){
                context.validationException(new VersionInvalidException());
                return null;
            }

            if (querySpec.where().noMatch()) {
                return new NoopPlannedAnalyzedRelation(table);
            }

            boolean outputsAreAllOrdered = false;
            boolean needFetchProjection = REFERENCES_COLLECTOR.collect(querySpec.outputs()).containsAnyReference();
            List<Projection> collectProjections = new ArrayList<>();
            List<Projection> mergeProjections = new ArrayList<>();
            List<Symbol> collectSymbols = new ArrayList<>();
            List<Symbol> outputSymbols = new ArrayList<>();
            ReferenceInfo docIdRefInfo = tableInfo.getReferenceInfo(DOC_ID_COLUMN_IDENT);

            ProjectionBuilder projectionBuilder = new ProjectionBuilder(querySpec);
            SplitPoints splitPoints = projectionBuilder.getSplitPoints();


            OrderBy orderBy = querySpec.orderBy();
            if (orderBy != null) {
                table.tableRelation().validateOrderBy(orderBy);




                OutputOrderReferenceContext outputOrderContext =
                        OUTPUT_ORDER_REFERENCE_COLLECTOR.collect(splitPoints.leaves());
                outputOrderContext.collectOrderBy = true;
                OUTPUT_ORDER_REFERENCE_COLLECTOR.collect(orderBy.orderBySymbols(), outputOrderContext);
                outputsAreAllOrdered = outputOrderContext.outputsAreAllOrdered();
                if (outputsAreAllOrdered) {
                    collectSymbols = splitPoints.toCollect();
                } else {
                    collectSymbols.addAll(orderBy.orderBySymbols());
                }
            }

            needFetchProjection = needFetchProjection & !outputsAreAllOrdered;

            if (needFetchProjection) {
                collectSymbols.add(0, new Reference(docIdRefInfo));
                for (Symbol symbol : querySpec.outputs()) {

                    if (SCORE_REFERENCE_DETECTOR.detect(symbol) && !collectSymbols.contains(symbol)) {
                        collectSymbols.add(symbol);
                    }
                    outputSymbols.add(DocReferenceConverter.convertIfPossible(symbol, tableInfo));
                }
            } else {

                collectSymbols = splitPoints.toCollect();
            }
            if (orderBy != null) {
                MergeProjection mergeProjection = projectionBuilder.mergeProjection(
                        collectSymbols,
                        orderBy);
                collectProjections.add(mergeProjection);
            }

            Integer limit = querySpec.limit();

            if ( limit == null && context.rootRelation() == table) {
                limit = Constants.DEFAULT_SELECT_LIMIT;
            }

            CollectNode collectNode = PlanNodeBuilder.collect(
                    tableInfo,
                    context.plannerContext(),
                    querySpec.where(),
                    collectSymbols,
                    ImmutableList.<Projection>of(),
                    orderBy,
                    limit == null ? null : limit + querySpec.offset()
            );


            collectNode.keepContextForFetcher(needFetchProjection);
            collectNode.projections(collectProjections);



            TopNProjection topNProjection;
            if (needFetchProjection) {
                topNProjection = projectionBuilder.topNProjection(
                        collectSymbols,
                        null,
                        querySpec.offset(),
                        querySpec.limit(),
                        null);
                mergeProjections.add(topNProjection);



                int bulkSize = FetchProjector.NO_BULK_REQUESTS;
                if (topNProjection.limit() > Constants.DEFAULT_SELECT_LIMIT) {
                    bulkSize = Constants.DEFAULT_SELECT_LIMIT;
                }

                FetchProjection fetchProjection = new FetchProjection(
                        collectNode.executionNodeId(),
                        DEFAULT_DOC_ID_INPUT_COLUMN, collectSymbols, outputSymbols,
                        tableInfo.partitionedByColumns(),
                        collectNode.executionNodes(),
                        bulkSize,
                        querySpec.isLimited(),
                        context.plannerContext().jobSearchContextIdToNode(),
                        context.plannerContext().jobSearchContextIdToShard()
                );
                mergeProjections.add(fetchProjection);
            } else {
                topNProjection = projectionBuilder.topNProjection(
                        collectSymbols,
                        null,
                        querySpec.offset(),
                        querySpec.limit(),
                        querySpec.outputs());
                mergeProjections.add(topNProjection);
            }

            MergeNode localMergeNode;
            if (orderBy != null) {
                localMergeNode = PlanNodeBuilder.sortedLocalMerge(
                        mergeProjections,
                        orderBy,
                        collectSymbols,
                        null,
                        collectNode,
                        context.plannerContext());
            } else {
                localMergeNode = PlanNodeBuilder.localMerge(
                        mergeProjections,
                        collectNode,
                        context.plannerContext());
            }


            if (limit != null && limit + querySpec.offset() > Constants.PAGE_SIZE) {
                collectNode.downstreamNodes(Collections.singletonList(context.plannerContext().clusterService().localNode().id()));
                collectNode.downstreamExecutionNodeId(localMergeNode.executionNodeId());
            }
            return new QueryThenFetch(collectNode, localMergeNode);
        }

        @Override
        protected PlannedAnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, ConsumerContext context) {
            return null;
        }
    }

    static class OutputOrderReferenceContext {

        private List<Reference> outputReferences = new ArrayList<>();
        private List<Reference> orderByReferences = new ArrayList<>();
        public boolean collectOrderBy = false;

        public void addReference(Reference reference) {
            if (collectOrderBy) {
                orderByReferences.add(reference);
            } else {
                outputReferences.add(reference);
            }
        }

        public boolean outputsAreAllOrdered() {
            return orderByReferences.containsAll(outputReferences);
        }

    }

    static class OutputOrderReferenceCollector extends SymbolVisitor<OutputOrderReferenceContext, Void> {

        public OutputOrderReferenceContext collect(List<Symbol> symbols) {
            OutputOrderReferenceContext context = new OutputOrderReferenceContext();
            collect(symbols, context);
            return context;
        }

        public void collect(List<Symbol> symbols, OutputOrderReferenceContext context) {
            for (Symbol symbol : symbols) {
                process(symbol, context);
            }
        }

        @Override
        public Void visitAggregation(Aggregation aggregation, OutputOrderReferenceContext context) {
            for (Symbol symbol : aggregation.inputs()) {
                process(symbol, context);
            }
            return null;
        }

        @Override
        public Void visitReference(Reference symbol, OutputOrderReferenceContext context) {
            context.addReference(symbol);
            return null;
        }

        @Override
        public Void visitDynamicReference(DynamicReference symbol, OutputOrderReferenceContext context) {
            return visitReference(symbol, context);
        }

        @Override
        public Void visitFunction(Function function, OutputOrderReferenceContext context) {
            for (Symbol symbol : function.arguments()) {
                process(symbol, context);
            }
            return null;
        }
    }

    static class ReferencesCollectorContext {
        private List<Reference> outputReferences = new ArrayList<>();

        public void addReference(Reference reference) {
            outputReferences.add(reference);
        }

        public boolean containsAnyReference() {
            return !outputReferences.isEmpty();
        }
    }

    static class ReferencesCollector extends SymbolVisitor<ReferencesCollectorContext, Void> {

        public ReferencesCollectorContext collect(List<Symbol> symbols) {
            ReferencesCollectorContext context = new ReferencesCollectorContext();
            collect(symbols, context);
            return context;
        }

        public void collect(List<Symbol> symbols, ReferencesCollectorContext context) {
            for (Symbol symbol : symbols) {
                process(symbol, context);
            }
        }

        @Override
        public Void visitAggregation(Aggregation aggregation, ReferencesCollectorContext context) {
            for (Symbol symbol : aggregation.inputs()) {
                process(symbol, context);
            }
            return null;
        }

        @Override
        public Void visitReference(Reference symbol, ReferencesCollectorContext context) {
            context.addReference(symbol);
            return null;
        }

        @Override
        public Void visitDynamicReference(DynamicReference symbol, ReferencesCollectorContext context) {
            return visitReference(symbol, context);
        }

        @Override
        public Void visitFunction(Function function, ReferencesCollectorContext context) {
            for (Symbol symbol : function.arguments()) {
                process(symbol, context);
            }
            return null;
        }

    }
}

<code block>


package io.crate.planner.consumer;

import io.crate.analyze.relations.AnalyzedRelation;

public interface Consumer {

    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context);
}

<code block>


package io.crate.planner.consumer;

import com.google.common.base.MoreObjects;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.Lists;
import com.google.common.collect.Sets;
import io.crate.Constants;
import io.crate.analyze.HavingClause;
import io.crate.analyze.InsertFromSubQueryAnalyzedStatement;
import io.crate.analyze.OrderBy;
import io.crate.analyze.QueriedTable;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.Routing;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.DistributedGroupBy;
import io.crate.planner.node.dql.GroupByConsumer;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.GroupProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.TopNProjection;
import io.crate.planner.projection.builder.ProjectionBuilder;
import io.crate.planner.projection.builder.SplitPoints;
import io.crate.planner.symbol.Aggregation;
import io.crate.planner.symbol.Symbol;

import java.util.ArrayList;
import java.util.LinkedList;
import java.util.List;

public class DistributedGroupByConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        Context ctx = new Context(context);
        context.rootRelation(VISITOR.process(context.rootRelation(), ctx));
        return ctx.result;
    }

    private static class Context {
        ConsumerContext consumerContext;
        boolean result = false;

        public Context(ConsumerContext context) {
            this.consumerContext = context;
        }
    }

    private static class Visitor extends AnalyzedRelationVisitor<Context, AnalyzedRelation> {

        @Override
        public AnalyzedRelation visitQueriedTable(QueriedTable table, Context context) {
            List<Symbol> groupBy = table.querySpec().groupBy();
            if (groupBy == null) {
                return table;
            }

            TableInfo tableInfo = table.tableRelation().tableInfo();
            if(table.querySpec().where().hasVersions()){
                context.consumerContext.validationException(new VersionInvalidException());
                return table;
            }

            Routing routing = tableInfo.getRouting(table.querySpec().where(), null);

            GroupByConsumer.validateGroupBySymbols(table.tableRelation(), table.querySpec().groupBy());

            ProjectionBuilder projectionBuilder = new ProjectionBuilder(table.querySpec());

            SplitPoints splitPoints = projectionBuilder.getSplitPoints();


            GroupProjection groupProjection = projectionBuilder.groupProjection(
                    splitPoints.leaves(),
                    table.querySpec().groupBy(),
                    splitPoints.aggregates(),
                    Aggregation.Step.ITER,
                    Aggregation.Step.PARTIAL);

            CollectNode collectNode = PlanNodeBuilder.distributingCollect(
                    tableInfo,
                    context.consumerContext.plannerContext(),
                    table.querySpec().where(),
                    splitPoints.leaves(),
                    Lists.newArrayList(routing.nodes()),
                    ImmutableList.<Projection>of(groupProjection)
            );



            List<Symbol> collectOutputs = new ArrayList<>(
                    groupBy.size() +
                            splitPoints.aggregates().size());
            collectOutputs.addAll(groupBy);
            collectOutputs.addAll(splitPoints.aggregates());

            List<Projection> reducerProjections = new LinkedList<>();
            reducerProjections.add(projectionBuilder.groupProjection(
                    collectOutputs,
                    table.querySpec().groupBy(),
                    splitPoints.aggregates(),
                    Aggregation.Step.PARTIAL,
                    Aggregation.Step.FINAL)
            );

            OrderBy orderBy = table.querySpec().orderBy();
            if (orderBy != null) {
                table.tableRelation().validateOrderBy(orderBy);
            }

            HavingClause havingClause = table.querySpec().having();
            if (havingClause != null) {
                if (havingClause.noMatch()) {
                    return new NoopPlannedAnalyzedRelation(table);
                } else if (havingClause.hasQuery()) {
                    reducerProjections.add(projectionBuilder.filterProjection(
                            collectOutputs,
                            havingClause.query()
                    ));
                }
            }

            boolean isRootRelation = context.consumerContext.rootRelation() == table;
            if (isRootRelation) {
                reducerProjections.add(projectionBuilder.topNProjection(
                        collectOutputs,
                        orderBy,
                        0,
                        MoreObjects.firstNonNull(table.querySpec().limit(),
                                Constants.DEFAULT_SELECT_LIMIT) + table.querySpec().offset(),
                        table.querySpec().outputs()));
            }
            MergeNode mergeNode = PlanNodeBuilder.distributedMerge(
                    collectNode,
                    context.consumerContext.plannerContext(),
                    reducerProjections
            );


            MergeNode localMergeNode = null;
            String localNodeId = context.consumerContext.plannerContext().clusterService().state().nodes().localNodeId();
            if(isRootRelation) {
                TopNProjection topN = projectionBuilder.topNProjection(
                        table.querySpec().outputs(),
                        orderBy,
                        table.querySpec().offset(),
                        table.querySpec().limit(),
                        null);
                localMergeNode = PlanNodeBuilder.localMerge(ImmutableList.<Projection>of(topN),
                        mergeNode, context.consumerContext.plannerContext());
                localMergeNode.executionNodes(Sets.newHashSet(localNodeId));

                mergeNode.downstreamNodes(localMergeNode.executionNodes());
                mergeNode.downstreamExecutionNodeId(localMergeNode.executionNodeId());
            } else {
                mergeNode.downstreamNodes(Sets.newHashSet(localNodeId));
                mergeNode.downstreamExecutionNodeId(mergeNode.executionNodeId() + 1);
            }
            context.result = true;

            collectNode.downstreamExecutionNodeId(mergeNode.executionNodeId());
            return new DistributedGroupBy(
                    collectNode,
                    mergeNode,
                    localMergeNode
            );
        }

        @Override
        public AnalyzedRelation visitInsertFromQuery(InsertFromSubQueryAnalyzedStatement insertFromSubQueryAnalyzedStatement, Context context) {
            InsertFromSubQueryConsumer.planInnerRelation(insertFromSubQueryAnalyzedStatement, context, this);
            return insertFromSubQueryAnalyzedStatement;
        }

        @Override
        protected AnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, Context context) {
            return relation;
        }

    }
}

<code block>


package io.crate.planner.consumer;

import com.google.common.collect.ImmutableList;
import io.crate.Constants;
import io.crate.analyze.HavingClause;
import io.crate.analyze.InsertFromSubQueryAnalyzedStatement;
import io.crate.analyze.OrderBy;
import io.crate.analyze.QueriedTable;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.TableRelation;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.projectors.TopN;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.GroupByConsumer;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.dql.NonDistributedGroupBy;
import io.crate.planner.projection.FilterProjection;
import io.crate.planner.projection.GroupProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.builder.ProjectionBuilder;
import io.crate.planner.projection.builder.SplitPoints;
import io.crate.planner.symbol.Aggregation;
import io.crate.planner.symbol.Symbol;

import java.util.ArrayList;
import java.util.List;

import static com.google.common.base.MoreObjects.firstNonNull;

public class ReduceOnCollectorGroupByConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        Context ctx = new Context(context);
        context.rootRelation(VISITOR.process(context.rootRelation(), ctx));
        return ctx.result;
    }

    private static class Context {
        ConsumerContext consumerContext;
        boolean result = false;

        public Context(ConsumerContext context) {
            this.consumerContext = context;
        }
    }

    private static class Visitor extends AnalyzedRelationVisitor<Context, AnalyzedRelation> {

        @Override
        public AnalyzedRelation visitQueriedTable(QueriedTable table, Context context) {
            if (table.querySpec().groupBy() == null) {
                return table;
            }

            if (!GroupByConsumer.groupedByClusteredColumnOrPrimaryKeys(
                    table.tableRelation(), table.querySpec().where(), table.querySpec().groupBy())) {
                return table;
            }

            if (table.querySpec().where().hasVersions()) {
                context.consumerContext.validationException(new VersionInvalidException());
                return table;
            }
            context.result = true;
            return optimizedReduceOnCollectorGroupBy(table, table.tableRelation(), context.consumerContext);
        }

        @Override
        public AnalyzedRelation visitInsertFromQuery(InsertFromSubQueryAnalyzedStatement insertFromSubQueryAnalyzedStatement, Context context) {
            InsertFromSubQueryConsumer.planInnerRelation(insertFromSubQueryAnalyzedStatement, context, this);
            return insertFromSubQueryAnalyzedStatement;
        }

        @Override
        protected AnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, Context context) {
            return relation;
        }


        private AnalyzedRelation optimizedReduceOnCollectorGroupBy(QueriedTable table, TableRelation tableRelation, ConsumerContext context) {
            assert GroupByConsumer.groupedByClusteredColumnOrPrimaryKeys(
                    tableRelation, table.querySpec().where(), table.querySpec().groupBy()) : "not grouped by clustered column or primary keys";
            TableInfo tableInfo = tableRelation.tableInfo();
            GroupByConsumer.validateGroupBySymbols(tableRelation, table.querySpec().groupBy());
            List<Symbol> groupBy = table.querySpec().groupBy();

            boolean ignoreSorting = context.rootRelation() != table
                    && table.querySpec().limit() == null
                    && table.querySpec().offset() == TopN.NO_OFFSET;

            ProjectionBuilder projectionBuilder = new ProjectionBuilder(table.querySpec());
            SplitPoints splitPoints = projectionBuilder.getSplitPoints();


            List<Symbol> collectOutputs = new ArrayList<>(
                    groupBy.size() +
                            splitPoints.aggregates().size());
            collectOutputs.addAll(groupBy);
            collectOutputs.addAll(splitPoints.aggregates());

            OrderBy orderBy = table.querySpec().orderBy();
            if (orderBy != null) {
                table.tableRelation().validateOrderBy(orderBy);
            }

            List<Projection> projections = new ArrayList<>();
            GroupProjection groupProjection = projectionBuilder.groupProjection(
                    splitPoints.leaves(),
                    table.querySpec().groupBy(),
                    splitPoints.aggregates(),
                    Aggregation.Step.ITER,
                    Aggregation.Step.FINAL
            );
            groupProjection.setRequiredGranularity(RowGranularity.SHARD);
            projections.add(groupProjection);

            HavingClause havingClause = table.querySpec().having();
            if (havingClause != null) {
                if (havingClause.noMatch()) {
                    return new NoopPlannedAnalyzedRelation(table);
                } else if (havingClause.hasQuery()) {
                    FilterProjection fp = projectionBuilder.filterProjection(
                            collectOutputs,
                            havingClause.query()
                    );
                    fp.requiredGranularity(RowGranularity.SHARD);
                    projections.add(fp);
                }
            }


            boolean outputsMatch = table.querySpec().outputs().size() == collectOutputs.size() &&
                    collectOutputs.containsAll(table.querySpec().outputs());
            boolean collectorTopN = table.querySpec().limit() != null || table.querySpec().offset() > 0 || !outputsMatch;

            if (collectorTopN) {
                projections.add(projectionBuilder.topNProjection(
                        collectOutputs,
                        orderBy,
                        0, 
                        firstNonNull(table.querySpec().limit(), Constants.DEFAULT_SELECT_LIMIT) + table.querySpec().offset(),
                        table.querySpec().outputs()
                ));
            }

            CollectNode collectNode = PlanNodeBuilder.collect(
                    tableInfo,
                    context.plannerContext(),
                    table.querySpec().where(),
                    splitPoints.leaves(),
                    ImmutableList.copyOf(projections)
            );


            List<Projection> handlerProjections = new ArrayList<>();
            MergeNode localMergeNode;
            if (!ignoreSorting && collectorTopN && orderBy != null && orderBy.isSorted()) {


                handlerProjections.add(
                        projectionBuilder.topNProjection(
                                table.querySpec().outputs(),
                                null, 
                                table.querySpec().offset(),
                                firstNonNull(table.querySpec().limit(), Constants.DEFAULT_SELECT_LIMIT),
                                table.querySpec().outputs()
                        )
                );
                localMergeNode = PlanNodeBuilder.sortedLocalMerge(
                        handlerProjections, orderBy, table.querySpec().outputs(), null,
                        collectNode, context.plannerContext());
            } else {
                handlerProjections.add(
                        projectionBuilder.topNProjection(
                                collectorTopN ? table.querySpec().outputs() : collectOutputs,
                                orderBy,
                                table.querySpec().offset(),
                                firstNonNull(table.querySpec().limit(), Constants.DEFAULT_SELECT_LIMIT),
                                table.querySpec().outputs()
                        )
                );

                localMergeNode = PlanNodeBuilder.localMerge(handlerProjections, collectNode,
                        context.plannerContext());
            }
            return new NonDistributedGroupBy(collectNode, localMergeNode);
        }


    }
}

<code block>


package io.crate.planner.consumer;


import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.exceptions.ValidationException;
import io.crate.planner.Planner;
import org.elasticsearch.common.Nullable;

public class ConsumerContext {

    private AnalyzedRelation rootRelation;
    private ValidationException validationException;
    private Planner.Context plannerContext;

    public ConsumerContext(AnalyzedRelation rootRelation, Planner.Context plannerContext) {
        this.rootRelation = rootRelation;
        this.plannerContext = plannerContext;
    }

    public void rootRelation(AnalyzedRelation relation) {
        this.rootRelation = relation;
    }

    public AnalyzedRelation rootRelation() {
        return rootRelation;
    }

    public void validationException(ValidationException validationException){
        this.validationException = validationException;
    }

    @Nullable
    public ValidationException validationException(){
        return validationException;
    }

    public Planner.Context plannerContext() {
        return plannerContext;
    }
}

<code block>


package io.crate.planner.consumer;


import com.google.common.collect.ImmutableList;
import io.crate.analyze.InsertFromSubQueryAnalyzedStatement;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.operation.aggregation.impl.CountAggregation;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.node.dml.InsertFromSubQuery;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.AggregationProjection;
import io.crate.planner.projection.ColumnIndexWriterProjection;
import io.crate.planner.projection.Projection;
import org.elasticsearch.common.settings.ImmutableSettings;


public class InsertFromSubQueryConsumer implements Consumer {

    private final Visitor visitor;

    public InsertFromSubQueryConsumer(){
        visitor = new Visitor();
    }

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        Context ctx = new Context(context);
        context.rootRelation(visitor.process(context.rootRelation(), ctx));
        return ctx.result;
    }

    private static class Context {
        ConsumerContext consumerContext;
        boolean result = false;

        public Context(ConsumerContext context){
            this.consumerContext = context;
        }
    }

    private static class Visitor extends AnalyzedRelationVisitor<Context, AnalyzedRelation> {

        @Override
        public AnalyzedRelation visitInsertFromQuery(InsertFromSubQueryAnalyzedStatement insertFromSubQueryAnalyzedStatement, Context context) {

            ColumnIndexWriterProjection indexWriterProjection = new ColumnIndexWriterProjection(
                    insertFromSubQueryAnalyzedStatement.tableInfo().ident(),
                    null,
                    insertFromSubQueryAnalyzedStatement.tableInfo().primaryKey(),
                    insertFromSubQueryAnalyzedStatement.columns(),
                    insertFromSubQueryAnalyzedStatement.onDuplicateKeyAssignments(),
                    insertFromSubQueryAnalyzedStatement.primaryKeyColumnIndices(),
                    insertFromSubQueryAnalyzedStatement.partitionedByIndices(),
                    insertFromSubQueryAnalyzedStatement.routingColumn(),
                    insertFromSubQueryAnalyzedStatement.routingColumnIndex(),
                    ImmutableSettings.EMPTY,
                    insertFromSubQueryAnalyzedStatement.tableInfo().isPartitioned()
            );

            AnalyzedRelation innerRelation = insertFromSubQueryAnalyzedStatement.subQueryRelation();
            if (innerRelation instanceof PlannedAnalyzedRelation) {
                PlannedAnalyzedRelation analyzedRelation = (PlannedAnalyzedRelation)innerRelation;
                analyzedRelation.addProjection(indexWriterProjection);

                MergeNode mergeNode = null;
                if (analyzedRelation.resultIsDistributed()) {

                    AggregationProjection aggregationProjection = CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION;
                    mergeNode = PlanNodeBuilder.localMerge(
                            ImmutableList.<Projection>of(aggregationProjection),
                            analyzedRelation.resultNode(),
                            context.consumerContext.plannerContext());
                }
                context.result = true;
                return new InsertFromSubQuery(((PlannedAnalyzedRelation) innerRelation).plan(), mergeNode);
            } else {
                return insertFromSubQueryAnalyzedStatement;
            }
        }

        @Override
        protected AnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, Context context) {
            return relation;
        }
    }

    public static <C, R> void planInnerRelation(InsertFromSubQueryAnalyzedStatement insertFromSubQueryAnalyzedStatement,
                                                C context, AnalyzedRelationVisitor<C,R> visitor) {
        if (insertFromSubQueryAnalyzedStatement.subQueryRelation() instanceof PlannedAnalyzedRelation) {

            return;
        }
        R innerRelation = visitor.process(insertFromSubQueryAnalyzedStatement.subQueryRelation(), context);
        if (innerRelation != null && innerRelation instanceof PlannedAnalyzedRelation) {
            insertFromSubQueryAnalyzedStatement.subQueryRelation((PlannedAnalyzedRelation)innerRelation);
        }
    }


}

<code block>


package io.crate.planner.consumer;

import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.exceptions.ValidationException;
import io.crate.planner.Plan;
import io.crate.planner.Planner;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.inject.Singleton;

import javax.annotation.Nullable;
import java.util.ArrayList;
import java.util.List;

@Singleton
public class ConsumingPlanner {

    private final List<Consumer> consumers = new ArrayList<>();

    @Inject
    public ConsumingPlanner() {
        consumers.add(new NonDistributedGroupByConsumer());
        consumers.add(new ReduceOnCollectorGroupByConsumer());
        consumers.add(new DistributedGroupByConsumer());
        consumers.add(new CountConsumer());
        consumers.add(new GlobalAggregateConsumer());
        consumers.add(new ESGetConsumer());
        consumers.add(new QueryThenFetchConsumer());
        consumers.add(new InsertFromSubQueryConsumer());
        consumers.add(new QueryAndFetchConsumer());
    }

    @Nullable
    public Plan plan(AnalyzedRelation rootRelation, Planner.Context plannerContext) {
        ConsumerContext consumerContext = new ConsumerContext(rootRelation, plannerContext);
        for (int i = 0; i < consumers.size(); i++) {
            Consumer consumer = consumers.get(i);
            if (consumer.consume(consumerContext.rootRelation(), consumerContext)) {
                if (consumerContext.rootRelation() instanceof PlannedAnalyzedRelation) {
                    Plan plan = ((PlannedAnalyzedRelation) consumerContext.rootRelation()).plan();
                    assert plan != null;
                    return plan;
                } else {
                    i = 0;
                }
            }
        }
        ValidationException validationException = consumerContext.validationException();
        if (validationException != null) {
            throw validationException;
        }
        return null;
    }
}

<code block>


package io.crate.integrationtests;

import com.google.common.base.Joiner;
import com.google.common.base.Predicate;
import io.crate.TimestampFormat;
import io.crate.action.sql.SQLActionException;
import io.crate.action.sql.SQLBulkResponse;
import io.crate.executor.TaskResult;
import io.crate.testing.TestingHelpers;
import org.apache.commons.collections.map.HashedMap;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.action.admin.cluster.health.ClusterHealthStatus;
import org.elasticsearch.action.admin.indices.exists.indices.IndicesExistsRequest;
import org.elasticsearch.common.collect.MapBuilder;
import org.elasticsearch.common.xcontent.XContentBuilder;
import org.elasticsearch.common.xcontent.XContentFactory;
import org.hamcrest.Matchers;
import org.junit.Rule;
import org.junit.Test;
import org.junit.rules.ExpectedException;

import javax.annotation.Nullable;
import java.security.Timestamp;
import java.util.*;
import java.util.concurrent.TimeUnit;

import static org.hamcrest.Matchers.*;
import static org.hamcrest.core.Is.is;

public class TransportSQLActionTest extends SQLTransportIntegrationTest {

    private Setup setup = new Setup(sqlExecutor);

    @Rule
    public ExpectedException expectedException = ExpectedException.none();


    private <T> List<T> getCol(Object[][] result, int idx) {
        ArrayList<T> res = new ArrayList<>(result.length);
        for (Object[] row : result) {
            res.add((T) row[idx]);
        }
        return res;
    }

    @Test
    public void testSelectKeepsOrder() throws Exception {
        createIndex("test");
        ensureYellow();
        client().prepareIndex("test", "default", "id1").setSource("{}").execute().actionGet();
        refresh();
        execute("select \"_id\" as b, \"_version\" as a from test");
        assertArrayEquals(new String[]{"b", "a"}, response.cols());
        assertEquals(1, response.rowCount());
        assertThat(response.duration(), greaterThanOrEqualTo(0L));
    }

    @Test
    public void testSelectCountStar() throws Exception {
        execute("create table test (\"type\" string) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into test (name) values (?)", new Object[]{"Arthur"});
        execute("insert into test (name) values (?)", new Object[]{"Trillian"});
        refresh();
        execute("select count(*) from test");
        assertEquals(1, response.rowCount());
        assertEquals(2L, response.rows()[0][0]);
    }

    @Test
    public void testSelectZeroLimit() throws Exception {
        execute("create table test (\"type\" string) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into test (name) values (?)", new Object[]{"Arthur"});
        execute("insert into test (name) values (?)", new Object[]{"Trillian"});
        refresh();
        execute("select * from test limit 0");
        assertEquals(0L, response.rowCount());
    }


    @Test
    public void testSelectCountStarWithWhereClause() throws Exception {
        execute("create table test (name string) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into test (name) values (?)", new Object[]{"Arthur"});
        execute("insert into test (name) values (?)", new Object[]{"Trillian"});
        refresh();
        execute("select count(*) from test where name = 'Trillian'");
        assertEquals(1, response.rowCount());
        assertEquals(1L, response.rows()[0][0]);
    }

    @Test
    public void testSelectStar() throws Exception {
        execute("create table test (\"firstName\" string, \"lastName\" string)");
        waitForRelocation(ClusterHealthStatus.GREEN);
        execute("select * from test");
        assertArrayEquals(new String[]{"firstName", "lastName"}, response.cols());
        assertEquals(0, response.rowCount());
    }

    @Test
    public void testSelectStarEmptyMapping() throws Exception {
        prepareCreate("test").execute().actionGet();
        ensureYellow();
        execute("select * from test");
        assertArrayEquals(new String[]{}, response.cols());
        assertEquals(0, response.rowCount());
    }

    @Test
    public void testGroupByOnAnalyzedColumn() throws Exception {
        expectedException.expect(SQLActionException.class);
        expectedException.expectMessage("Cannot GROUP BY 'test1.col1': grouping on analyzed/fulltext columns is not possible");

        execute("create table test1 (col1 string index using fulltext)");
        ensureYellow();
        execute("insert into test1 (col1) values ('abc def, ghi. jkl')");
        refresh();
        execute("select count(col1) from test1 group by col1");
    }


    @Test
    public void testSelectStarWithOther() throws Exception {
        prepareCreate("test")
                .addMapping("default",
                        "firstName", "type=string",
                        "lastName", "type=string")
                .execute().actionGet();
        ensureYellow();
        client().prepareIndex("test", "default", "id1").setRefresh(true)
                .setSource("{\"firstName\":\"Youri\",\"lastName\":\"Zoon\"}")
                .execute().actionGet();
        execute("select \"_version\", *, \"_id\" from test");
        assertArrayEquals(new String[]{"_version", "firstName", "lastName", "_id"},
                response.cols());
        assertEquals(1, response.rowCount());
        assertArrayEquals(new Object[]{1L, "Youri", "Zoon", "id1"}, response.rows()[0]);
    }

    @Test
    public void testSelectWithParams() throws Exception {
        execute("create table test (first_name string, last_name string, age double) with (number_of_replicas = 0)");
        ensureYellow();
        client().prepareIndex("test", "default", "id1").setRefresh(true)
                .setSource("{\"first_name\":\"Youri\",\"last_name\":\"Zoon\", \"age\": 38}")
                .execute().actionGet();

        Object[] args = new Object[]{"id1"};
        execute("select first_name, last_name from test where \"_id\" = $1", args);
        assertArrayEquals(new Object[]{"Youri", "Zoon"}, response.rows()[0]);

        args = new Object[]{"Zoon"};
        execute("select first_name, last_name from test where last_name = $1", args);
        assertArrayEquals(new Object[]{"Youri", "Zoon"}, response.rows()[0]);

        args = new Object[]{38, "Zoon"};
        execute("select first_name, last_name from test where age = $1 and last_name = $2", args);
        assertArrayEquals(new Object[]{"Youri", "Zoon"}, response.rows()[0]);

        args = new Object[]{38, "Zoon"};
        execute("select first_name, last_name from test where age = ? and last_name = ?", args);
        assertArrayEquals(new Object[]{"Youri", "Zoon"}, response.rows()[0]);
    }

    @Test
    public void testSelectStarWithOtherAndAlias() throws Exception {
        prepareCreate("test")
                .addMapping("default",
                        "firstName", "type=string",
                        "lastName", "type=string")
                .execute().actionGet();
        ensureYellow();
        client().prepareIndex("test", "default", "id1").setRefresh(true)
                .setSource("{\"firstName\":\"Youri\",\"lastName\":\"Zoon\"}")
                .execute().actionGet();
        execute("select *, \"_version\", \"_version\" as v from test");
        assertArrayEquals(new String[]{"firstName", "lastName", "_version", "v"},
                response.cols());
        assertEquals(1, response.rowCount());
        assertArrayEquals(new Object[]{"Youri", "Zoon", 1L, 1L}, response.rows()[0]);
    }

    @Test
    public void testFilterByEmptyString() throws Exception {
        prepareCreate("test")
                .addMapping("default",
                        "name", "type=string,index=not_analyzed")
                .execute().actionGet();
        ensureYellow();
        client().prepareIndex("test", "default", "id1").setRefresh(true)
                .setSource("{\"name\":\"\"}")
                .execute().actionGet();
        client().prepareIndex("test", "default", "id2").setRefresh(true)
                .setSource("{\"name\":\"Ruben Lenten\"}")
                .execute().actionGet();

        execute("select name from test where name = ''");
        assertEquals(1, response.rowCount());
        assertEquals("", response.rows()[0][0]);

        execute("select name from test where name != ''");
        assertEquals(1, response.rowCount());
        assertEquals("Ruben Lenten", response.rows()[0][0]);

    }

    @Test
    public void testFilterByNull() throws Exception {
        execute("create table test (name string, o object(ignored))");
        ensureYellow();

        client().prepareIndex("test", "default", "id1").setRefresh(true)
                .setSource("{}")
                .execute().actionGet();
        client().prepareIndex("test", "default", "id2").setRefresh(true)
                .setSource("{\"name\":\"Ruben Lenten\"}")
                .execute().actionGet();
        client().prepareIndex("test", "default", "id3").setRefresh(true)
                .setSource("{\"name\":\"\"}")
                .execute().actionGet();

        execute("select \"_id\" from test where name is null");
        assertEquals(1, response.rowCount());
        assertEquals("id1", response.rows()[0][0]);

        execute("select \"_id\" from test where name is not null order by \"_uid\"");
        assertEquals(2, response.rowCount());
        assertEquals("id2", response.rows()[0][0]);


        execute("select \"_id\" from test where o['invalid'] is null");
        assertEquals(0, response.rowCount());

        execute("select name from test where name is not null and name!=''");
        assertEquals(1, response.rowCount());
        assertEquals("Ruben Lenten", response.rows()[0][0]);

    }

    @Test
    public void testFilterByBoolean() throws Exception {
        prepareCreate("test")
                .addMapping("default",
                        "sunshine", "type=boolean,index=not_analyzed")
                .execute().actionGet();
        ensureYellow();

        execute("insert into test values (?)", new Object[]{true});
        refresh();

        execute("select sunshine from test where sunshine = true");
        assertEquals(1, response.rowCount());
        assertEquals(true, response.rows()[0][0]);

        execute("update test set sunshine=false where sunshine = true");
        assertEquals(1, response.rowCount());
        refresh();

        execute("select sunshine from test where sunshine = ?", new Object[]{false});
        assertEquals(1, response.rowCount());
        assertEquals(false, response.rows()[0][0]);
    }



    @Test
    public void testColsAreCaseSensitive() throws Exception {
        execute("create table test (\"firstname\" string, \"firstName\" string) " +
                "with (number_of_replicas = 0)");
        ensureYellow();
        execute("insert into test (\"firstname\", \"firstName\") values ('LowerCase', 'CamelCase')");
        refresh();

        execute("select FIRSTNAME, \"firstname\", \"firstName\" from test");
        assertArrayEquals(new String[]{"firstname", "firstname", "firstName"}, response.cols());
        assertEquals(1, response.rowCount());
        assertEquals("LowerCase", response.rows()[0][0]);
        assertEquals("LowerCase", response.rows()[0][1]);
        assertEquals("CamelCase", response.rows()[0][2]);
    }


    @Test
    public void testIdSelectWithResult() throws Exception {
        createIndex("test");
        ensureYellow();
        client().prepareIndex("test", "default", "id1").setSource("{}").execute().actionGet();
        refresh();
        execute("select \"_id\" from test");
        assertArrayEquals(new String[]{"_id"}, response.cols());
        assertEquals(1, response.rowCount());
        assertEquals(1, response.rows()[0].length);
        assertEquals("id1", response.rows()[0][0]);
    }

    @Test
    public void testDelete() throws Exception {
        createIndex("test");
        ensureYellow();
        client().prepareIndex("test", "default", "id1").setSource("{}").execute().actionGet();
        refresh();
        execute("delete from test");
        assertEquals(-1, response.rowCount());
        assertThat(response.duration(), greaterThanOrEqualTo(0L));
        execute("select \"_id\" from test");
        assertEquals(0, response.rowCount());
    }

    @Test
    public void testDeleteWithWhere() throws Exception {
        createIndex("test");
        ensureYellow();
        client().prepareIndex("test", "default", "id1").setSource("{}").execute().actionGet();
        client().prepareIndex("test", "default", "id2").setSource("{}").execute().actionGet();
        client().prepareIndex("test", "default", "id3").setSource("{}").execute().actionGet();
        refresh();
        execute("delete from test where \"_id\" = 'id1'");
        assertEquals(1, response.rowCount());
        refresh();
        execute("select \"_id\" from test");
        assertEquals(2, response.rowCount());
    }

    @Test
    public void testSqlRequestWithLimit() throws Exception {
        createIndex("test");
        ensureYellow();
        client().prepareIndex("test", "default", "id1").setSource("{}").execute().actionGet();
        client().prepareIndex("test", "default", "id2").setSource("{}").execute().actionGet();
        refresh();
        execute("select \"_id\" from test limit 1");
        assertEquals(1, response.rowCount());
    }


    @Test
    public void testSqlRequestWithLimitAndOffset() throws Exception {
        execute("create table test (id string primary key) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into test (id) values (?), (?), (?)", new Object[]{"id1", "id2", "id3"});
        refresh();
        execute("select \"id\" from test order by id limit 1 offset 1");
        assertEquals(1, response.rowCount());
        assertThat((String)response.rows()[0][0], is("id2"));
    }


    @Test
    public void testSqlRequestWithFilter() throws Exception {
        createIndex("test");
        ensureYellow();
        client().prepareIndex("test", "default", "id1").setSource("{}").execute().actionGet();
        client().prepareIndex("test", "default", "id2").setSource("{}").execute().actionGet();
        refresh();
        execute("select _id from test where _id='id1'");
        assertEquals(1, response.rowCount());
        assertEquals("id1", response.rows()[0][0]);
    }

    @Test
    public void testSqlRequestWithNotEqual() throws Exception {
        execute("create table test (id string primary key) with (number_of_replicas = 0)");
        ensureYellow();
        execute("insert into test (id) values (?)", new Object[][] {
                new Object[] { "id1" },
                new Object[] { "id2" }
        });
        refresh();
        execute("select id from test where id != 'id1'");
        assertEquals(1, response.rowCount());
        assertEquals("id2", response.rows()[0][0]);
    }


    @Test
    public void testSqlRequestWithOneOrFilter() throws Exception {
        execute("create table test (id string) with (number_of_replicas = 0)");
        ensureYellow();
        execute("insert into test (id) values ('id1'), ('id2'), ('id3')");
        refresh();
        execute("select id from test where id='id1' or id='id3'");
        assertEquals(2, response.rowCount());
        assertThat(this.<String>getCol(response.rows(), 0), containsInAnyOrder("id1", "id3"));
    }

    @Test
    public void testSqlRequestWithOneMultipleOrFilter() throws Exception {
        execute("create table test (id string) with (number_of_replicas = 0)");
        ensureYellow();
        execute("insert into test (id) values ('id1'), ('id2'), ('id3'), ('id4')");
        refresh();
        execute("select id from test where id='id1' or id='id2' or id='id4'");
        assertEquals(3, response.rowCount());
        List<String> col1 = this.getCol(response.rows(), 0);
        assertThat(col1, containsInAnyOrder("id1", "id2", "id4"));
    }

    @Test
    public void testSqlRequestWithDateFilter() throws Exception {
        prepareCreate("test")
                .addMapping("default", XContentFactory.jsonBuilder()
                        .startObject()
                        .startObject("default")
                        .startObject("properties")
                        .startObject("date")
                        .field("type", "date")
                        .endObject()
                        .endObject()
                        .endObject().endObject())
                .execute().actionGet();
        ensureYellow();
        client().prepareIndex("test", "default", "id1")
                .setSource("{\"date\": " +
                        TimestampFormat.parseTimestampString("2013-10-01") + "}")
                .execute().actionGet();
        client().prepareIndex("test", "default", "id2")
                .setSource("{\"date\": " +
                        TimestampFormat.parseTimestampString("2013-10-02") + "}")
                .execute().actionGet();
        refresh();
        execute(
                "select date from test where date = '2013-10-01'");
        assertEquals(1, response.rowCount());
        assertEquals(1380585600000L, response.rows()[0][0]);
    }

    @Test
    public void testSqlRequestWithDateGtFilter() throws Exception {
        prepareCreate("test")
                .addMapping("default",
                        "date", "type=date")
                .execute().actionGet();
        ensureYellow();
        client().prepareIndex("test", "default", "id1")
                .setSource("{\"date\": " +
                        TimestampFormat.parseTimestampString("2013-10-01") + "}")
                .execute().actionGet();
        client().prepareIndex("test", "default", "id2")
                .setSource("{\"date\":" +
                        TimestampFormat.parseTimestampString("2013-10-02") + "}")
                .execute().actionGet();
        refresh();
        execute(
                "select date from test where date > '2013-10-01'");
        assertEquals(1, response.rowCount());
        assertEquals(1380672000000L, response.rows()[0][0]);
    }

    @Test
    public void testSqlRequestWithNumericGtFilter() throws Exception {
        prepareCreate("test")
                .addMapping("default",
                        "i", "type=long")
                .execute().actionGet();
        ensureYellow();
        client().prepareIndex("test", "default", "id1")
                .setSource("{\"i\":10}")
                .execute().actionGet();
        client().prepareIndex("test", "default", "id2")
                .setSource("{\"i\":20}")
                .execute().actionGet();
        refresh();
        execute(
                "select i from test where i > 10");
        assertEquals(1, response.rowCount());
        assertEquals(20L, response.rows()[0][0]);
    }


    @Test
    @SuppressWarnings("unchecked")
    public void testArraySupport() throws Exception {
        execute("create table t1 (id int primary key, strings array(string), integers array(integer)) with (number_of_replicas=0)");
        ensureYellow();

        execute("insert into t1 (id, strings, integers) values (?, ?, ?)",
                new Object[]{
                        1,
                        new String[]{"foo", "bar"},
                        new Integer[]{1, 2, 3}
                }
        );
        refresh();

        execute("select id, strings, integers from t1");
        assertThat(response.rowCount(), is(1L));
        assertThat((Integer) response.rows()[0][0], is(1));
        assertThat(((String) ((Object[]) response.rows()[0][1])[0]), is("foo"));
        assertThat(((String) ((Object[]) response.rows()[0][1])[1]), is("bar"));
        assertThat(((Integer) ((Object[]) response.rows()[0][2])[0]), is(1));
        assertThat(((Integer) ((Object[]) response.rows()[0][2])[1]), is(2));
        assertThat(((Integer) ((Object[]) response.rows()[0][2])[2]), is(3));
    }

    @Test
    @SuppressWarnings("unchecked")
    public void testArrayInsideObject() throws Exception {
        execute("create table t1 (id int primary key, details object as (names array(string))) with (number_of_replicas=0)");
        ensureYellow();

        Map<String, Object> details = new HashMap<>();
        details.put("names", new Object[]{"Arthur", "Trillian"});
        execute("insert into t1 (id, details) values (?, ?)", new Object[]{1, details});
        refresh();

        execute("select details['names'] from t1");
        assertThat(response.rowCount(), is(1L));
        assertThat(((String) ((Object[]) response.rows()[0][0])[0]), is("Arthur"));
        assertThat(((String) ((Object[]) response.rows()[0][0])[1]), is("Trillian"));
    }

    @Test
    public void testArrayInsideObjectArray() throws Exception {
        execute("create table t1 (id int primary key, details array(object as (names array(string)))) with (number_of_replicas=0)");
        ensureYellow();

        Map<String, Object> detail1 = new HashMap<>();
        detail1.put("names", new Object[]{"Arthur", "Trillian"});

        Map<String, Object> detail2 = new HashMap<>();
        detail2.put("names", new Object[]{"Ford", "Slarti"});

        List<Map<String, Object>> details = Arrays.asList(detail1, detail2);

        execute("insert into t1 (id, details) values (?, ?)", new Object[]{1, details});
        refresh();

        expectedException.expect(SQLActionException.class);
        expectedException.expectMessage("cannot query for arrays inside object arrays explicitly");

        execute("select details['names'] from t1");

    }

    @Test
    public void testFullPathRequirement() throws Exception {

        execute("create table t1 (id int primary key, details object as (id int, more_details object as (id int))) with (number_of_replicas=0)");
        ensureYellow();

        Map<String, Object> more_details = new HashMap<>();
        more_details.put("id", 2);

        Map<String, Object> details = new HashMap<>();
        details.put("id", 1);
        details.put("more_details", more_details);

        execute("insert into t1 (id, details) values (2, ?)", new Object[]{details});
        execute("refresh table t1");

        execute("select details from t1 where details['id'] = 2");
        assertThat(response.rowCount(), is(0L));
    }

    @Test
    @SuppressWarnings("unchecked")
    public void testArraySupportWithNullValues() throws Exception {
        execute("create table t1 (id int primary key, strings array(string)) with (number_of_replicas=0)");
        ensureYellow();

        execute("insert into t1 (id, strings) values (?, ?)",
                new Object[]{
                        1,
                        new String[]{"foo", null, "bar"},
                }
        );
        refresh();

        execute("select id, strings from t1");
        assertThat(response.rowCount(), is(1L));
        assertThat((Integer) response.rows()[0][0], is(1));
        assertThat(((String) ((Object[]) response.rows()[0][1])[0]), is("foo"));
        assertThat(((Object[]) response.rows()[0][1])[1], nullValue());
        assertThat(((String) ((Object[]) response.rows()[0][1])[2]), is("bar"));
    }

    @Test
    public void testObjectArrayInsertAndSelect() throws Exception {
        execute("create table t1 (" +
                "  id int primary key, " +
                "  objects array(" +
                "   object as (" +
                "     name string, " +
                "     age int" +
                "   )" +
                "  )" +
                ") with (number_of_replicas=0)");
        ensureYellow();

        Map<String, Object> obj1 = new MapBuilder<String, Object>().put("name", "foo").put("age", 1).map();
        Map<String, Object> obj2 = new MapBuilder<String, Object>().put("name", "bar").put("age", 2).map();

        Object[] args = new Object[]{1, new Object[]{obj1, obj2}};
        execute("insert into t1 (id, objects) values (?, ?)", args);
        refresh();

        execute("select objects from t1");
        assertThat(response.rowCount(), is(1L));

        Object[] objResults = ((Object[]) response.rows()[0][0]);
        Map<String, Object> obj1Result = ((Map) objResults[0]);
        assertThat((String) obj1Result.get("name"), is("foo"));
        assertThat((Integer) obj1Result.get("age"), is(1));

        Map<String, Object> obj2Result = ((Map) objResults[1]);
        assertThat((String) obj2Result.get("name"), is("bar"));
        assertThat((Integer) obj2Result.get("age"), is(2));

        execute("select objects['name'] from t1");
        assertThat(response.rowCount(), is(1L));

        String[] names = Arrays.copyOf(((Object[]) response.rows()[0][0]), 2, String[].class);
        assertThat(names[0], is("foo"));
        assertThat(names[1], is("bar"));

        execute("select objects['name'] from t1 where ? = ANY (objects['name'])", new Object[]{"foo"});
        assertThat(response.rowCount(), is(1L));
    }

    @Test
    public void testGetResponseWithObjectColumn() throws Exception {
        XContentBuilder mapping = XContentFactory.jsonBuilder().startObject()
                .startObject("default")
                .startObject("_meta").field("primary_keys", "id").endObject()
                .startObject("properties")
                .startObject("id")
                .field("type", "string")
                .field("index", "not_analyzed")
                .endObject()
                .startObject("data")
                .field("type", "object")
                .field("index", "not_analyzed")
                .field("dynamic", false)
                .endObject()
                .endObject()
                .endObject()
                .endObject();

        prepareCreate("test")
                .addMapping("default", mapping)
                .execute().actionGet();
        ensureYellow();

        Map<String, Object> data = new HashMap<>();
        data.put("foo", "bar");
        execute("insert into test (id, data) values (?, ?)", new Object[]{"1", data});
        refresh();

        execute("select data from test where id = ?", new Object[]{"1"});
        assertEquals(data, response.rows()[0][0]);
    }

    @Test
    public void testSelectToGetRequestByPlanner() throws Exception {
        this.setup.createTestTableWithPrimaryKey();

        execute("insert into test (pk_col, message) values ('124', 'bar1')");
        assertEquals(1, response.rowCount());
        refresh();
        waitNoPendingTasksOnAll(); 

        execute("select pk_col, message from test where pk_col='124'");
        assertEquals(1, response.rowCount());
        assertEquals("124", response.rows()[0][0]);
        assertThat(response.duration(), greaterThanOrEqualTo(0L));
    }

    @Test
    public void testDeleteToDeleteRequestByPlanner() throws Exception {
        this.setup.createTestTableWithPrimaryKey();

        execute("insert into test (pk_col, message) values ('123', 'bar')");
        assertEquals(1, response.rowCount());
        refresh();

        execute("delete from test where pk_col='123'");
        assertEquals(1, response.rowCount());
        assertThat(response.duration(), greaterThanOrEqualTo(0L));
        refresh();

        execute("select * from test where pk_col='123'");
        assertEquals(0, response.rowCount());
    }

    @Test
    public void testSelectToRoutedRequestByPlanner() throws Exception {
        this.setup.createTestTableWithPrimaryKey();

        execute("insert into test (pk_col, message) values ('1', 'foo')");
        execute("insert into test (pk_col, message) values ('2', 'bar')");
        execute("insert into test (pk_col, message) values ('3', 'baz')");
        refresh();

        execute("SELECT * FROM test WHERE pk_col='1' OR pk_col='2'");
        assertEquals(2, response.rowCount());
        assertThat(response.duration(), greaterThanOrEqualTo(0L));

        execute("SELECT * FROM test WHERE pk_col=? OR pk_col=?", new Object[]{"1", "2"});
        assertEquals(2, response.rowCount());

        awaitBusy(new Predicate<Object>() {
            @Override
            public boolean apply(@Nullable Object input) {
                execute("SELECT * FROM test WHERE (pk_col=? OR pk_col=?) OR pk_col=?", new Object[]{"1", "2", "3"});
                return response.rowCount() == 3
                        && Joiner.on(',').join(Arrays.asList(response.cols())).equals("message,pk_col");
            }
        }, 10, TimeUnit.SECONDS);

    }

    @Test
    public void testSelectToRoutedRequestByPlannerMissingDocuments() throws Exception {
        this.setup.createTestTableWithPrimaryKey();

        execute("insert into test (pk_col, message) values ('1', 'foo')");
        execute("insert into test (pk_col, message) values ('2', 'bar')");
        execute("insert into test (pk_col, message) values ('3', 'baz')");
        refresh();

        execute("SELECT pk_col, message FROM test WHERE pk_col='4' OR pk_col='3'");
        assertEquals(1, response.rowCount());
        assertThat(Arrays.asList(response.rows()[0]), hasItems(new Object[]{"3", "baz"}));
        assertThat(response.duration(), greaterThanOrEqualTo(0L));

        execute("SELECT pk_col, message FROM test WHERE pk_col='4' OR pk_col='99'");
        assertEquals(0, response.rowCount());
    }

    @Test
    public void testSelectToRoutedRequestByPlannerWhereIn() throws Exception {
        this.setup.createTestTableWithPrimaryKey();

        execute("insert into test (pk_col, message) values ('1', 'foo')");
        execute("insert into test (pk_col, message) values ('2', 'bar')");
        execute("insert into test (pk_col, message) values ('3', 'baz')");
        refresh();

        execute("SELECT * FROM test WHERE pk_col IN (?,?,?)", new Object[]{"1", "2", "3"});
        assertEquals(3, response.rowCount());
        assertThat(response.duration(), greaterThanOrEqualTo(0L));
    }

    @Test
    public void testDeleteToRoutedRequestByPlannerWhereIn() throws Exception {
        this.setup.createTestTableWithPrimaryKey();

        execute("insert into test (pk_col, message) values ('1', 'foo')");
        execute("insert into test (pk_col, message) values ('2', 'bar')");
        execute("insert into test (pk_col, message) values ('3', 'baz')");
        refresh();

        execute("DELETE FROM test WHERE pk_col IN (?, ?, ?)", new Object[]{"1", "2", "4"});
        assertThat(response.duration(), greaterThanOrEqualTo(0L));
        refresh();

        execute("SELECT pk_col FROM test");
        assertThat(response.rowCount(), is(1L));
        assertEquals(response.rows()[0][0], "3");

    }


    @Test
    public void testDeleteToRoutedRequestByPlannerWhereOr() throws Exception {
        this.setup.createTestTableWithPrimaryKey();
        execute("insert into test (pk_col, message) values ('1', 'foo'), ('2', 'bar'), ('3', 'baz')");
        refresh();
        execute("DELETE FROM test WHERE pk_col=? or pk_col=? or pk_col=?", new Object[]{"1", "2", "4"});
        assertThat(response.duration(), greaterThanOrEqualTo(0L));
        refresh();
        execute("SELECT pk_col FROM test");
        assertThat(response.rowCount(), is(1L));
        assertEquals(response.rows()[0][0], "3");
    }

    @Test
    public void testUpdateToRoutedRequestByPlannerWhereOr() throws Exception {
        this.setup.createTestTableWithPrimaryKey();
        execute("insert into test (pk_col, message) values ('1', 'foo'), ('2', 'bar'), ('3', 'baz')");
        refresh();
        execute("update test set message='new' WHERE pk_col='1' or pk_col='2' or pk_col='4'");
        assertThat(response.rowCount(), is(2L));
        assertThat(response.duration(), greaterThanOrEqualTo(0L));
        refresh();
        execute("SELECT distinct message FROM test");
        assertThat(response.rowCount(), is(2L));
    }

    @Test
    public void testSelectWithWhereLike() throws Exception {
        this.setup.groupBySetup();

        execute("select name from characters where name like '%ltz'");
        assertEquals(2L, response.rowCount());

        execute("select count(*) from characters where name like 'Jeltz'");
        assertEquals(1L, response.rows()[0][0]);

        execute("select count(*) from characters where race like '%o%'");
        assertEquals(3L, response.rows()[0][0]);

        Map<String, Object> emptyMap = new HashMap<>();
        Map<String, Object> details = new HashMap<>();
        details.put("age", 30);
        details.put("job", "soldier");
        execute("insert into characters (race, gender, name, details) values (?, ?, ?, ?)",
                new Object[]{"Vo*", "male", "Kwaltzz", emptyMap}
        );
        execute("insert into characters (race, gender, name, details) values (?, ?, ?, ?)",
                new Object[]{"Vo?", "male", "Kwaltzzz", emptyMap}
        );
        execute("insert into characters (race, gender, name, details) values (?, ?, ?, ?)",
                new Object[]{"Vo!", "male", "Kwaltzzzz", details}
        );
        execute("insert into characters (race, gender, name, details) values (?, ?, ?, ?)",
                new Object[]{"Vo%", "male", "Kwaltzzzz", details}
        );
        refresh();

        execute("select race from characters where race like 'Vo*'");
        assertEquals(1L, response.rowCount());
        assertEquals("Vo*", response.rows()[0][0]);

        execute("select race from characters where race like ?", new Object[]{"Vo?"});
        assertEquals(1L, response.rowCount());
        assertEquals("Vo?", response.rows()[0][0]);

        execute("select race from characters where race like 'Vo!'");
        assertEquals(1L, response.rowCount());
        assertEquals("Vo!", response.rows()[0][0]);

        execute("select race from characters where race like 'Vo\\%'");
        assertEquals(1L, response.rowCount());
        assertEquals("Vo%", response.rows()[0][0]);

        execute("select race from characters where race like 'Vo_'");
        assertEquals(4L, response.rowCount());

        execute("select race from characters where details['job'] like 'sol%'");
        assertEquals(2L, response.rowCount());
    }

    @Test
    public void testSelectMatch() throws Exception {
        execute("create table quotes (quote string)");
        ensureYellow();
        assertTrue(client().admin().indices().exists(new IndicesExistsRequest("quotes"))
                .actionGet().isExists());

        execute("insert into quotes values (?)", new Object[]{"don't panic"});
        refresh();

        execute("select quote from quotes where match(quote, ?)", new Object[]{"don't panic"});
        assertEquals(1L, response.rowCount());
        assertEquals("don't panic", response.rows()[0][0]);

    }

    @Test
    public void testSelectNotMatch() throws Exception {
        execute("create table quotes (quote string) with (number_of_replicas = 0)");
        ensureYellow();
        assertTrue(client().admin().indices().exists(new IndicesExistsRequest("quotes"))
                .actionGet().isExists());

        execute("insert into quotes values (?), (?)", new Object[]{"don't panic", "hello"});
        refresh();

        execute("select quote from quotes where not match(quote, ?)",
                new Object[]{"don't panic"});
        assertEquals(1L, response.rowCount());
        assertEquals("hello", response.rows()[0][0]);
    }

    @Test
    public void testSelectOrderByScore() throws Exception {
        execute("create table quotes (quote string index off," +
                "index quote_ft using fulltext(quote))");
        ensureYellow();
        execute("insert into quotes values (?)",
                new Object[]{"Would it save you a lot of time if I just gave up and went mad now?"}
        );
        execute("insert into quotes values (?)",
                new Object[]{"Time is an illusion. Lunchtime doubly so"}
        );
        refresh();

        execute("select * from quotes");
        execute("select quote, \"_score\" from quotes where match(quote_ft, ?) " +
                        "order by \"_score\" desc",
                new Object[]{"time", "time"}
        );
        assertEquals(2L, response.rowCount());
        assertEquals("Time is an illusion. Lunchtime doubly so", response.rows()[0][0]);
    }

    @Test
    public void testSelectScoreMatchAll() throws Exception {
        execute("create table quotes (quote string) with (number_of_replicas = 0)");
        ensureYellow();
        assertTrue(client().admin().indices().exists(new IndicesExistsRequest("quotes"))
                .actionGet().isExists());

        execute("insert into quotes values (?), (?)",
                new Object[]{"Would it save you a lot of time if I just gave up and went mad now?",
                        "Time is an illusion. Lunchtime doubly so"}
        );
        refresh();

        execute("select quote, \"_score\" from quotes");
        assertEquals(2L, response.rowCount());
        assertEquals(1.0f, response.rows()[0][1]);
        assertEquals(1.0f, response.rows()[1][1]);
    }

    @Test
    public void testSelectWhereScore() throws Exception {
        execute("create table quotes (quote string, " +
                "index quote_ft using fulltext(quote)) with (number_of_replicas = 0)");
        ensureYellow();
        assertTrue(client().admin().indices().exists(new IndicesExistsRequest("quotes"))
                .actionGet().isExists());

        execute("insert into quotes values (?), (?)",
                new Object[]{"Would it save you a lot of time if I just gave up and went mad now?",
                        "Time is an illusion. Lunchtime doubly so. Take your time."}
        );
        refresh();

        execute("select quote, \"_score\" from quotes where match(quote_ft, 'time') " +
                "and \"_score\" >= 0.98");
        assertEquals(1L, response.rowCount());
        assertThat((Float) response.rows()[0][1], greaterThanOrEqualTo(0.98f));
    }

    @Test
    public void testSelectMatchAnd() throws Exception {
        execute("create table quotes (id int, quote string, " +
                "index quote_fulltext using fulltext(quote) with (analyzer='english')) with (number_of_replicas = 0)");
        ensureYellow();
        assertTrue(client().admin().indices().exists(new IndicesExistsRequest("quotes"))
                .actionGet().isExists());

        execute("insert into quotes (id, quote) values (?, ?), (?, ?)",
                new Object[]{
                        1, "Would it save you a lot of time if I just gave up and went mad now?",
                        2, "Time is an illusion. Lunchtime doubly so"}
        );
        refresh();

        execute("select quote from quotes where match(quote_fulltext, 'time') and id = 1");
        assertEquals(1L, response.rowCount());
    }

    private void nonExistingColumnSetup() {
        execute("create table quotes (" +
                "id integer primary key, " +
                "quote string index off, " +
                "o object(ignored), " +
                "index quote_fulltext using fulltext(quote) with (analyzer='snowball')" +
                ") clustered by (id) into 3 shards with (number_of_replicas = 0)");
        ensureYellow();
        execute("insert into quotes (id, quote) values (1, '\"Nothing particularly exciting," +
                "\" it admitted, \"but they are alternatives.\"')");
        execute("insert into quotes (id, quote) values (2, '\"Have another drink," +
                "\" said Trillian. \"Enjoy yourself.\"')");
        refresh();
    }

    @Test
    public void selectNonExistingColumn() throws Exception {
        nonExistingColumnSetup();
        execute("select o['notExisting'] from quotes");
        assertEquals(2L, response.rowCount());
        assertEquals("o['notExisting']", response.cols()[0]);
        assertNull(response.rows()[0][0]);
        assertNull(response.rows()[1][0]);
    }

    @Test
    public void selectNonExistingAndExistingColumns() throws Exception {
        nonExistingColumnSetup();
        execute("select o['unknown'], id from quotes order by id asc");
        assertEquals(2L, response.rowCount());
        assertEquals("o['unknown']", response.cols()[0]);
        assertEquals("id", response.cols()[1]);
        assertNull(response.rows()[0][0]);
        assertEquals(1, response.rows()[0][1]);
        assertNull(response.rows()[1][0]);
        assertEquals(2, response.rows()[1][1]);
    }

    @Test
    public void selectWhereNonExistingColumn() throws Exception {
        nonExistingColumnSetup();
        execute("select * from quotes where o['something'] > 0");
        assertEquals(0L, response.rowCount());
    }

    @Test
    public void selectWhereDynamicColumnIsNull() throws Exception {
        nonExistingColumnSetup();

        execute("select * from quotes where o['something'] IS NULL");
        assertEquals(0, response.rowCount());
    }

    @Test
    public void selectWhereNonExistingColumnWhereIn() throws Exception {
        nonExistingColumnSetup();
        execute("select * from quotes where o['something'] IN(1,2,3)");
        assertEquals(0L, response.rowCount());
    }

    @Test
    public void selectWhereNonExistingColumnLike() throws Exception {
        nonExistingColumnSetup();
        execute("select * from quotes where o['something'] Like '%bla'");
        assertEquals(0L, response.rowCount());
    }

    @Test
    public void selectWhereNonExistingColumnMatchFunction() throws Exception {
        nonExistingColumnSetup();

        expectedException.expect(SQLActionException.class);
        expectedException.expectMessage("Can only use MATCH on columns of type STRING, not on 'null'");

        execute("select * from quotes where match(o['something'], 'bla')");
    }

    @Test
    public void testSelectCountDistinctZero() throws Exception {
        execute("create table test (col1 int) with (number_of_replicas=0)");
        ensureYellow();

        execute("select count(distinct col1) from test");

        assertEquals(1, response.rowCount());
        assertEquals(0L, response.rows()[0][0]);
    }

    @Test
    public void testRefresh() throws Exception {
        execute("create table test (id int primary key, name string)");
        ensureYellow();
        execute("insert into test (id, name) values (0, 'Trillian'), (1, 'Ford'), (2, 'Zaphod')");
        execute("select count(*) from test");
        assertThat((Long) response.rows()[0][0], lessThan(3L));

        execute("refresh table test");
        assertFalse(response.hasRowCount());
        assertThat(response.rows(), is(TaskResult.EMPTY_OBJS));

        execute("select count(*) from test");
        assertThat((Long) response.rows()[0][0], is(3L));
    }

    @Test
    public void testInsertSelectWithClusteredBy() throws Exception {
        execute("create table quotes (id integer, quote string) clustered by(id) " +
                "with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into quotes (id, quote) values(?, ?)",
                new Object[]{1, "I'd far rather be happy than right any day."});
        assertEquals(1L, response.rowCount());
        refresh();

        execute("select \"_id\", id, quote from quotes where id=1");
        assertEquals(1L, response.rowCount());


        assertNotNull(response.rows()[0][0]);
        assertThat(((String) response.rows()[0][0]).length(), greaterThan(0));
    }

    @Test
    public void testInsertSelectWithAutoGeneratedId() throws Exception {
        execute("create table quotes (id integer, quote string)" +
                "with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into quotes (id, quote) values(?, ?)",
                new Object[]{1, "I'd far rather be happy than right any day."});
        assertEquals(1L, response.rowCount());
        refresh();

        execute("select \"_id\", id, quote from quotes where id=1");
        assertEquals(1L, response.rowCount());


        assertNotNull(response.rows()[0][0]);
        assertThat(((String) response.rows()[0][0]).length(), greaterThan(0));
    }

    @Test
    public void testInsertSelectWithPrimaryKey() throws Exception {
        execute("create table quotes (id integer primary key, quote string)" +
                "with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into quotes (id, quote) values(?, ?)",
                new Object[]{1, "I'd far rather be happy than right any day."});
        assertEquals(1L, response.rowCount());
        refresh();

        execute("select \"_id\", id, quote from quotes where id=1");
        assertEquals(1L, response.rowCount());



        String _id = (String) response.rows()[0][0];
        Integer id = (Integer) response.rows()[0][1];
        assertEquals(id.toString(), _id);
    }

    @Test
    public void testInsertSelectWithMultiplePrimaryKey() throws Exception {
        execute("create table quotes (id integer primary key, author string primary key, " +
                "quote string) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into quotes (id, author, quote) values(?, ?, ?)",
                new Object[]{1, "Ford", "I'd far rather be happy than right any day."});
        assertEquals(1L, response.rowCount());
        refresh();

        execute("select \"_id\", id from quotes where id=1 and author='Ford'");
        assertEquals(1L, response.rowCount());
        assertThat((String) response.rows()[0][0], is("AgExBEZvcmQ="));
        assertThat((Integer) response.rows()[0][1], is(1));
    }

    @Test
    public void testInsertSelectWithMultiplePrimaryKeyAndClusteredBy() throws Exception {
        execute("create table quotes (id integer primary key, author string primary key, " +
                "quote string) clustered by(author) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into quotes (id, author, quote) values(?, ?, ?)",
                new Object[]{1, "Ford", "I'd far rather be happy than right any day."});
        assertEquals(1L, response.rowCount());
        refresh();

        execute("select \"_id\", id from quotes where id=1 and author='Ford'");
        assertEquals(1L, response.rowCount());
        assertThat((String) response.rows()[0][0], is("AgRGb3JkATE="));
        assertThat((Integer) response.rows()[0][1], is(1));
    }

    @Test
    public void testInsertSelectWithMultiplePrimaryOnePkSame() throws Exception {
        execute("create table quotes (id integer primary key, author string primary key, " +
                "quote string) clustered by(author) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into quotes (id, author, quote) values (?, ?, ?), (?, ?, ?)",
                new Object[]{1, "Ford", "I'd far rather be happy than right any day.",
                        1, "Douglas", "Don't panic"}
        );
        assertEquals(2L, response.rowCount());
        refresh();

        execute("select \"_id\", id from quotes where id=1 order by author");
        assertEquals(2L, response.rowCount());
        assertThat((String) response.rows()[0][0], is("AgdEb3VnbGFzATE="));
        assertThat((Integer) response.rows()[0][1], is(1));
        assertThat((String) response.rows()[1][0], is("AgRGb3JkATE="));
        assertThat((Integer) response.rows()[1][1], is(1));
    }

    @Test
    public void testDeleteByIdWithMultiplePrimaryKey() throws Exception {
        execute("create table quotes (id integer primary key, author string primary key, " +
                "quote string) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into quotes (id, author, quote) values (?, ?, ?), (?, ?, ?)",
                new Object[]{1, "Ford", "I'd far rather be happy than right any day.",
                        1, "Douglas", "Don't panic"}
        );
        assertEquals(2L, response.rowCount());
        refresh();

        execute("delete from quotes where id=1 and author='Ford'");
        assertEquals(1L, response.rowCount());
        refresh();

        execute("select quote from quotes where id=1");
        assertEquals(1L, response.rowCount());
    }

    @Test
    public void testDeleteByQueryWithMultiplePrimaryKey() throws Exception {
        execute("create table quotes (id integer primary key, author string primary key, " +
                "quote string) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into quotes (id, author, quote) values (?, ?, ?), (?, ?, ?)",
                new Object[]{1, "Ford", "I'd far rather be happy than right any day.",
                        1, "Douglas", "Don't panic"}
        );
        assertEquals(2L, response.rowCount());
        refresh();

        execute("delete from quotes where id=1");

        assertEquals(-1L, response.rowCount());
        refresh();

        execute("select quote from quotes where id=1");
        assertEquals(0L, response.rowCount());
    }

    @Test
    public void testSelectWhereBoolean() {
        execute("create table a (v boolean)");
        ensureYellow();

        execute("insert into a values (true)");
        execute("insert into a values (true)");
        execute("insert into a values (true)");
        execute("insert into a values (false)");
        execute("insert into a values (false)");
        refresh();

        execute("select v from a where v");
        assertEquals(3L, response.rowCount());

        execute("select v from a where not v");
        assertEquals(2L, response.rowCount());

        execute("select v from a where v or not v");
        assertEquals(5L, response.rowCount());

        execute("select v from a where v and not v");
        assertEquals(0L, response.rowCount());
    }

    @Test
    public void testSelectWhereBooleanPK() {
        execute("create table b (v boolean primary key) clustered by (v)");
        ensureYellow();

        execute("insert into b values (true)");
        execute("insert into b values (false)");
        refresh();

        execute("select v from b where v");
        assertEquals(1L, response.rowCount());

        execute("select v from b where not v");
        assertEquals(1L, response.rowCount());

        execute("select v from b where v or not v");
        assertEquals(2L, response.rowCount());

        execute("select v from b where v and not v");
        assertEquals(0L, response.rowCount());
    }



    @Test
    public void testBulkOperations() throws Exception {
        execute("create table test (id integer primary key, name string) with (number_of_replicas = 0)");
        ensureYellow();
        SQLBulkResponse bulkResp = execute("insert into test (id, name) values (?, ?), (?, ?)",
                new Object[][] {
                        {1, "Earth", 2, "Saturn"},    
                        {3, "Moon", 4, "Mars"}        
                });
        assertThat(bulkResp.results().length, is(4));
        for (SQLBulkResponse.Result result : bulkResp.results()) {
            assertThat(result.rowCount(), is(1L));
        }
        refresh();

        bulkResp = execute("insert into test (id, name) values (?, ?), (?, ?)",
            new Object[][] {
                {1, "Earth", 2, "Saturn"},    
                {3, "Moon", 4, "Mars"}        
            });
        assertThat(bulkResp.results().length, is(4));
        for (SQLBulkResponse.Result result : bulkResp.results()) {
            assertThat(result.rowCount(), is(-2L));
        }

        execute("select name from test order by id asc");
        assertEquals("Earth\nSaturn\nMoon\nMars\n", TestingHelpers.printedTable(response.rows()));


        bulkResp = execute("update test set name = concat(name, '-updated') where id = ?", new Object[][]{
                new Object[]{2},
                new Object[]{3},
                new Object[]{4},
        });
        assertThat(bulkResp.results().length, is(3));
        for (SQLBulkResponse.Result result : bulkResp.results()) {
            assertThat(result.rowCount(), is(1L));
        }
        refresh();

        execute("select count(*) from test where name like '%-updated'");
        assertThat((Long) response.rows()[0][0], is(3L));


        bulkResp = execute("delete from test where id = ?", new Object[][] {
                new Object[] { 1 },
                new Object[] { 3 }
        });
        assertThat(bulkResp.results().length, is(2));
        for (SQLBulkResponse.Result result : bulkResp.results()) {
            assertThat(result.rowCount(), is(1L));
        }
        refresh();

        execute("select count(*) from test");
        assertThat((Long) response.rows()[0][0], is(2L));


        bulkResp = execute("delete from test where name = ?", new Object[][] {
                new Object[] { "Saturn-updated" },
                new Object[] { "Mars-updated" }
        });
        assertThat(bulkResp.results().length, is(2));
        for (SQLBulkResponse.Result result : bulkResp.results()) {
            assertThat(result.rowCount(), is(-1L));
        }
        refresh();

        execute("select count(*) from test");
        assertThat((Long) response.rows()[0][0], is(0L));

    }

    @Test
    public void testSelectFormatFunction() throws Exception {
        this.setup.setUpLocations();
        ensureYellow();
        refresh();

        execute("select format('%s is a %s', name, kind) as sentence from locations order by name");
        assertThat(response.rowCount(), is(13L));
        assertArrayEquals(response.cols(), new String[]{"sentence"});
        assertThat(response.rows()[0].length, is(1));
        assertThat((String) response.rows()[0][0], is(" is a Planet"));
        assertThat((String) response.rows()[1][0], is("Aldebaran is a Star System"));
        assertThat((String) response.rows()[2][0], is("Algol is a Star System"));


    }

    @Test
    public void testAnyArray() throws Exception {
        this.setup.setUpArrayTables();

        execute("select count(*) from any_table where 'Berlin' = ANY (names)");
        assertThat((Long) response.rows()[0][0], is(2L));

        execute("select id, names from any_table where 'Berlin' = ANY (names) order by id");
        assertThat(response.rowCount(), is(2L));
        assertThat((Integer) response.rows()[0][0], is(1));
        assertThat((Integer) response.rows()[1][0], is(3));

        execute("select id from any_table where 'Berlin' != ANY (names) order by id");
        assertThat(response.rowCount(), is(3L));
        assertThat((Integer) response.rows()[0][0], is(1));
        assertThat((Integer) response.rows()[1][0], is(2));
        assertThat((Integer) response.rows()[2][0], is(3));

        execute("select count(id) from any_table where 0.0 < ANY (temps)");
        assertThat((Long) response.rows()[0][0], is(2L));

        execute("select id, names from any_table where 0.0 < ANY (temps) order by id");
        assertThat(response.rowCount(), is(2L));
        assertThat((Integer) response.rows()[0][0], is(2));
        assertThat((Integer) response.rows()[1][0], is(3));

        execute("select count(*) from any_table where 0.0 > ANY (temps)");
        assertThat((Long) response.rows()[0][0], is(2L));

        execute("select id, names from any_table where 0.0 > ANY (temps) order by id");
        assertThat(response.rowCount(), is(2L));
        assertThat((Integer) response.rows()[0][0], is(2));
        assertThat((Integer) response.rows()[1][0], is(3));

        execute("select id, names from any_table where 'Ber%' LIKE ANY (names) order by id");
        assertThat(response.rowCount(), is(2L));
        assertThat((Integer) response.rows()[0][0], is(1));
        assertThat((Integer) response.rows()[1][0], is(3));

    }

    @Test
    public void testNotAnyArray() throws Exception {
        this.setup.setUpArrayTables();

        execute("select id from any_table where NOT 'Hangelsberg' = ANY (names) order by id");
        assertThat(response.rowCount(), is(3L));
        assertThat((Integer) response.rows()[0][0], is(1));
        assertThat((Integer) response.rows()[1][0], is(2));
        assertThat((Integer) response.rows()[2][0], is(4)); 

        execute("select id from any_table where 'Hangelsberg' != ANY (names) order by id");
        assertThat(response.rowCount(), is(3L));
        assertThat((Integer) response.rows()[0][0], is(1));
        assertThat((Integer) response.rows()[1][0], is(2));
        assertThat((Integer) response.rows()[2][0], is(3));
    }

    @Test
    public void testAnyLike() throws Exception {
        this.setup.setUpArrayTables();

        execute("select id from any_table where 'kuh%' LIKE ANY (tags) order by id");
        assertThat(response.rowCount(), is(2L));
        assertThat((Integer) response.rows()[0][0], is(3));
        assertThat((Integer) response.rows()[1][0], is(4));

        execute("select id from any_table where 'kuh%' NOT LIKE ANY (tags) order by id");
        assertThat(response.rowCount(), is(3L));
        assertThat((Integer) response.rows()[0][0], is(1));
        assertThat((Integer) response.rows()[1][0], is(2));
        assertThat((Integer) response.rows()[2][0], is(3));

    }

    @Test
    public void testInsertAndSelectIpType() throws Exception {
        execute("create table ip_table (fqdn string, addr ip) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into ip_table (fqdn, addr) values ('localhost', '127.0.0.1'), ('crate.io', '23.235.33.143')");
        execute("refresh table ip_table");

        execute("select addr from ip_table where addr = '23.235.33.143'");
        assertThat(response.rowCount(), is(1L));
        assertThat((String) response.rows()[0][0], is("23.235.33.143"));

        execute("select addr from ip_table where addr > '127.0.0.1'");
        assertThat(response.rowCount(), is(0L));
        execute("select addr from ip_table where addr > 2130706433"); 
        assertThat(response.rowCount(), is(0L));

        execute("select addr from ip_table where addr < '127.0.0.1'");
        assertThat(response.rowCount(), is(1L));
        assertThat((String) response.rows()[0][0], is("23.235.33.143"));
        execute("select addr from ip_table where addr < 2130706433"); 
        assertThat(response.rowCount(), is(1L));
        assertThat((String) response.rows()[0][0], is("23.235.33.143"));

        execute("select addr from ip_table where addr <= '127.0.0.1'");
        assertThat(response.rowCount(), is(2L));

        execute("select addr from ip_table where addr >= '23.235.33.143'");
        assertThat(response.rowCount(), is(2L));

        execute("select addr from ip_table where addr IS NULL");
        assertThat(response.rowCount(), is(0L));

        execute("select addr from ip_table where addr IS NOT NULL");
        assertThat(response.rowCount(), is(2L));

    }

    @Test
    public void testGroupByOnIpType() throws Exception {
        execute("create table t (i ip) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into t (i) values ('192.168.1.2'), ('192.168.1.2'), ('192.168.1.3')");
        execute("refresh table t");
        execute("select i, count(*) from t group by 1 order by count(*) desc");

        assertThat(response.rowCount(), is(2L));
        assertThat((String) response.rows()[0][0], is("192.168.1.2"));
        assertThat((Long) response.rows()[0][1], is(2L));
        assertThat((String) response.rows()[1][0], is("192.168.1.3"));
        assertThat((Long) response.rows()[1][1], is(1L));
    }

    @Test
    public void testDeleteOnIpType() throws Exception {
        execute("create table ip_table (fqdn string, addr ip) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into ip_table (fqdn, addr) values ('localhost', '127.0.0.1'), ('crate.io', '23.235.33.143')");
        execute("refresh table ip_table");
        execute("delete from ip_table where addr = '127.0.0.1'");
        assertThat(response.rowCount(), is(-1L));
        execute("select addr from ip_table");
        assertThat(response.rowCount(), is(1L));
        assertThat((String) response.rows()[0][0], is("23.235.33.143"));
    }

    @Test
    public void testInsertAndSelectGeoType() throws Exception {
        execute("create table geo_point_table (id int primary key, p geo_point) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into geo_point_table (id, p) values (?, ?)", new Object[]{1, new Double[]{47.22, 12.09}});
        execute("insert into geo_point_table (id, p) values (?, ?)", new Object[]{2, new Double[]{57.22, 7.12}});
        refresh();

        execute("select p from geo_point_table order by id desc");

        assertThat(response.rowCount(), is(2L));
        assertThat(((Double[]) response.rows()[0][0]), Matchers.arrayContaining(57.22, 7.12));
        assertThat(((Double[]) response.rows()[1][0]), Matchers.arrayContaining(47.22, 12.09));
    }

    @Test
    public void testGeoTypeQueries() throws Exception {

        execute("create table t (id int primary key, i int, p geo_point) " +
                "clustered into 1 shards " +
                "with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into t (id, i, p) values (1, 1, 'POINT (10 20)')");
        execute("insert into t (id, i, p) values (2, 1, 'POINT (11 21)')");
        refresh();


        execute("select distance(p, 'POINT (11 21)') from t order by 1");
        assertThat(response.rowCount(), is(2L));

        Double result1 = (Double) response.rows()[0][0];
        Double result2 = (Double) response.rows()[1][0];

        assertThat(result1, is(0.0d));
        assertThat(result2, is(152462.70754934277));

        String stmtOrderBy = "SELECT id " +
                "FROM t " +
                "ORDER BY distance(p, 'POINT(30.0 30.0)')";
        execute(stmtOrderBy);
        assertThat(response.rowCount(), is(2L));
        String expectedOrderBy =
                "2\n" +
                        "1\n";
        assertEquals(expectedOrderBy, TestingHelpers.printedTable(response.rows()));


        String stmtAggregate = "SELECT i, max(distance(p, 'POINT(30.0 30.0)')) " +
                "FROM t " +
                "GROUP BY i";
        execute(stmtAggregate);
        assertThat(response.rowCount(), is(1L));
        String expectedAggregate = "1| 2297790.338709135\n";
        assertEquals(expectedAggregate, TestingHelpers.printedTable(response.rows()));


        execute("select p from t where distance(p, 'POINT (11 21)') > 0.0");
        Double[] row = Arrays.copyOf((Object[])response.rows()[0][0], 2, Double[].class);
        assertThat(row[0], is(10.0d));
        assertThat(row[1], is(20.0d));

        execute("select p from t where distance(p, 'POINT (11 21)') < 10.0");
        row = Arrays.copyOf((Object[])response.rows()[0][0], 2, Double[].class);
        assertThat(row[0], is(11.0d));
        assertThat(row[1], is(21.0d));

        execute("select p from t where distance(p, 'POINT (11 21)') < 10.0 or distance(p, 'POINT (11 21)') > 10.0");
        assertThat(response.rowCount(), is(2L));

        execute("select p from t where distance(p, 'POINT (10 20)') = 0");
        assertThat(response.rowCount(), is(1L));
    }

    @Test
    public void testWithinQuery() throws Exception {
        execute("create table t (id int primary key, p geo_point) " +
                "clustered into 1 shards " +
                "with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into t (id, p) values (1, 'POINT (10 10)')");
        refresh();

        execute("select within(p, 'POLYGON (( 5 5, 30 5, 30 30, 5 30, 5 5 ))') from t");
        assertThat((Boolean) response.rows()[0][0], is(true));

        execute("select * from t where within(p, 'POLYGON (( 5 5, 30 5, 30 30, 5 30, 5 5 ))')");
        assertThat(response.rowCount(), is(1L));
        execute("select * from t where within(p, 'POLYGON (( 5 5, 30 5, 30 30, 5 35, 5 5 ))') = false");
        assertThat(response.rowCount(), is(0L));
    }

    @Test
    public void testTwoSubStrOnSameColumn() throws Exception {
        this.setup.groupBySetup();
        execute("select name, substr(name, 4, 4), substr(name, 3, 5) from sys.nodes order by name");
        assertThat((String) response.rows()[0][0], is("node_s0"));
        assertThat((String) response.rows()[0][1], is("e_s0"));
        assertThat((String) response.rows()[0][2], is("de_s0"));
        assertThat((String) response.rows()[1][0], is("node_s1"));
        assertThat((String) response.rows()[1][1], is("e_s1"));
        assertThat((String) response.rows()[1][2], is("de_s1"));

    }


    @Test
    public void testSelectArithMetricOperatorInOrderBy() throws Exception {
        execute("create table t (i integer, l long, d double) clustered into 3 shards with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into t (i, l, d) values (1, 2, 90.5), (2, 5, 90.5), (193384, 31234594433, 99.0), (10, 21, 99.0), (-1, 4, 99.0)");
        refresh();

        execute("select i, i%3 from t order by i%3, l");
        assertThat(response.rowCount(), is(5L));
        assertThat(TestingHelpers.printedTable(response.rows()), is(
                "-1| -1\n" +
                        "1| 1\n" +
                        "10| 1\n" +
                        "193384| 1\n" +
                        "2| 2\n"));
    }

    @Test
    public void testSelectFailingSearchScript() throws Exception {
        expectedException.expect(SQLActionException.class);
        expectedException.expectMessage("log(x, b): given arguments would result in: 'NaN'");

        execute("create table t (i integer, l long, d double) clustered into 1 shards with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into t (i, l, d) values (1, 2, 90.5)");
        refresh();

        execute("select log(d, l) from t where log(d, -1) >= 0");
    }

    @Test
    public void testSelectGroupByFailingSearchScript() throws Exception {
        expectedException.expect(SQLActionException.class);
        expectedException.expectMessage("log(x, b): given arguments would result in: 'NaN'");

        execute("create table t (i integer, l long, d double) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into t (i, l, d) values (1, 2, 90.5), (0, 4, 100)");
        execute("refresh table t");

        execute("select log(d, l) from t where log(d, -1) >= 0 group by log(d, l)");
    }

    @Test
    public void testNumericScriptOnAllTypes() throws Exception {

        execute("create table t (b byte, s short, i integer, l long, f float, d double, t timestamp) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into t (b, s, i, l, f, d, t) values (1, 2, 3, 4, 5.7, 6.3, '2014-07-30')");
        refresh();

        String[] functionCalls = new String[]{
                "abs(%s)",
                "ceil(%s)",
                "floor(%s)",
                "ln(%s)",
                "log(%s)",
                "log(%s, 2)",
                "random()",
                "round(%s)",
                "sqrt(%s)"
        };

        for (String functionCall : functionCalls) {
            String byteCall = String.format(Locale.ENGLISH, functionCall, "b");
            execute(String.format(Locale.ENGLISH, "select %s, b from t where %s < 2", byteCall, byteCall));

            String shortCall = String.format(Locale.ENGLISH, functionCall, "s");
            execute(String.format(Locale.ENGLISH, "select %s, s from t where %s < 2", shortCall, shortCall));

            String intCall = String.format(Locale.ENGLISH, functionCall, "i");
            execute(String.format(Locale.ENGLISH, "select %s, i from t where %s < 2", intCall, intCall));

            String longCall = String.format(Locale.ENGLISH, functionCall, "l");
            execute(String.format(Locale.ENGLISH, "select %s, l from t where %s < 2", longCall, longCall));

            String floatCall = String.format(Locale.ENGLISH, functionCall, "f");
            execute(String.format(Locale.ENGLISH, "select %s, f from t where %s < 2", floatCall, floatCall));

            String doubleCall = String.format(Locale.ENGLISH, functionCall, "d");
            execute(String.format(Locale.ENGLISH, "select %s, d from t where %s < 2", doubleCall, doubleCall));
        }
    }

    @Test
    public void testMatchNotOnSubColumn() throws Exception {
        execute("create table matchbox (" +
                "  s string index using fulltext with (analyzer='german')," +
                "  o object as (" +
                "    s string index using fulltext with (analyzer='german')," +
                "    m string index using fulltext with (analyzer='german')" +
                "  )," +
                "  o_ignored object(ignored)" +
                ") with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into matchbox (s, o) values ('Arthur Dent', {s='Zaphod Beeblebroox', m='Ford Prefect'})");
        refresh();
        execute("select * from matchbox where match(s, 'Arthur')");
        assertThat(response.rowCount(), is(1L));

        execute("select * from matchbox where match(o['s'], 'Arthur')");
        assertThat(response.rowCount(), is(0L));

        execute("select * from matchbox where match(o['s'], 'Zaphod')");
        assertThat(response.rowCount(), is(1L));

        execute("select * from matchbox where match(s, 'Zaphod')");
        assertThat(response.rowCount(), is(0L));

        execute("select * from matchbox where match(o['m'], 'Ford')");
        assertThat(response.rowCount(), is(1L));

        expectedException.expect(SQLActionException.class);
        expectedException.expectMessage("Can only use MATCH on columns of type STRING, not on 'null'");

        execute("select * from matchbox where match(o_ignored['a'], 'Ford')");
        assertThat(response.rowCount(), is(0L));
    }

    @Test
    public void testSelectWhereMultiColumnMatchDifferentTypesDifferentScore() throws Exception {
        this.setup.setUpLocations();
        refresh();
        execute("select name, description, kind, _score from locations " +
                "where match((kind, name_description_ft 0.5), 'Planet earth') using most_fields with (analyzer='english') order by _score desc");
        assertThat(response.rowCount(), is(5L));
        assertThat(TestingHelpers.printedTable(response.rows()),
                is("Alpha Centauri| 4.1 light-years northwest of earth| Star System| 0.049483635\n" +
                        "| This Planet doesn't really exist| Planet| 0.04724514\nAllosimanius Syneca| Allosimanius Syneca is a planet noted for ice, snow, mind-hurtling beauty and stunning cold.| Planet| 0.021473126\n" +
                        "Bartledan| An Earthlike planet on which Arthur Dent lived for a short time, Bartledan is inhabited by Bartledanians, a race that appears human but only physically.| Planet| 0.018788986\n" +
                        "Galactic Sector QQ7 Active J Gamma| Galactic Sector QQ7 Active J Gamma contains the Sun Zarss, the planet Preliumtarn of the famed Sevorbeupstry and Quentulus Quazgar Mountains.| Galaxy| 0.017716927\n"));

        execute("select name, description, kind, _score from locations " +
                "where match((kind, name_description_ft 0.5), 'Planet earth') using cross_fields order by _score desc");
        assertThat(response.rowCount(), is(5L));
        assertThat(TestingHelpers.printedTable(response.rows()),
                is("Alpha Centauri| 4.1 light-years northwest of earth| Star System| 0.06658964\n" +
                        "| This Planet doesn't really exist| Planet| 0.06235056\n" +
                        "Allosimanius Syneca| Allosimanius Syneca is a planet noted for ice, snow, mind-hurtling beauty and stunning cold.| Planet| 0.02889618\n" +
                        "Bartledan| An Earthlike planet on which Arthur Dent lived for a short time, Bartledan is inhabited by Bartledanians, a race that appears human but only physically.| Planet| 0.025284158\n" +
                        "Galactic Sector QQ7 Active J Gamma| Galactic Sector QQ7 Active J Gamma contains the Sun Zarss, the planet Preliumtarn of the famed Sevorbeupstry and Quentulus Quazgar Mountains.| Galaxy| 0.02338146\n"));
    }

    @Test
    public void testSimpleMatchWithBoost() throws Exception {
        execute("create table characters ( " +
                "  id int primary key, " +
                "  name string, " +
                "  quote string, " +
                "  INDEX name_ft using fulltext(name) with (analyzer = 'english'), " +
                "  INDEX quote_ft using fulltext(quote) with (analyzer = 'english') " +
                ") clustered into 5 shards ");
        ensureYellow();
        execute("insert into characters (id, name, quote) values (?, ?, ?)", new Object[][]{
                new Object[]{1, "Arthur", "It's terribly small, tiny little country."},
                new Object[]{2, "Trillian", " No, it's a country. Off the coast of Africa."},
                new Object[]{3, "Marvin", " It won't work, I have an exceptionally large mind." }
        });
        refresh();
        execute("select characters.name AS characters_name, _score " +
                "from characters " +
                "where match(characters.quote_ft 1.0, 'country') order by _score desc");
        assertThat(response.rows().length, is(2));
        assertThat((String) response.rows()[0][0], is("Trillian"));
        assertThat((float) response.rows()[0][1], is(0.15342641f));
        assertThat((String) response.rows()[1][0], is("Arthur"));
        assertThat((float) response.rows()[1][1], is(0.13424811f));
    }

    @Test
    public void testMatchTypes() throws Exception {
        this.setup.setUpLocations();
        refresh();
        execute("select name, _score from locations where match((kind 0.8, name_description_ft 0.6), 'planet earth') using best_fields order by _score desc");
        assertThat(TestingHelpers.printedTable(response.rows()),
                is("Alpha Centauri| 0.22184466\n| 0.21719791\nAllosimanius Syneca| 0.09626817\nBartledan| 0.08423465\nGalactic Sector QQ7 Active J Gamma| 0.08144922\n"));

        execute("select name, _score from locations where match((kind 0.6, name_description_ft 0.8), 'planet earth') using most_fields order by _score desc");
        assertThat(TestingHelpers.printedTable(response.rows()),
                is("Alpha Centauri| 0.12094267\n| 0.1035407\nAllosimanius Syneca| 0.05248235\nBartledan| 0.045922056\nGalactic Sector QQ7 Active J Gamma| 0.038827762\n"));

        execute("select name, _score from locations where match((kind 0.4, name_description_ft 1.0), 'planet earth') using cross_fields order by _score desc");
        assertThat(TestingHelpers.printedTable(response.rows()),
                is("Alpha Centauri| 0.14147125\n| 0.116184436\nAllosimanius Syneca| 0.061390605\nBartledan| 0.05371678\nGalactic Sector QQ7 Active J Gamma| 0.043569162\n"));

        execute("select name, _score from locations where match((kind 1.0, name_description_ft 0.4), 'Alpha Centauri') using phrase");
        assertThat(TestingHelpers.printedTable(response.rows()),
                is("Alpha Centauri| 0.94653714\n"));

        execute("select name, _score from locations where match(name_description_ft, 'Alpha Centauri') using phrase_prefix");
        assertThat(TestingHelpers.printedTable(response.rows()),
                is("Alpha Centauri| 1.5739591\n"));
    }

    @Test
    public void testMatchOptions() throws Exception {
        this.setup.setUpLocations();
        refresh();

        execute("select name, _score from locations where match((kind, name_description_ft), 'galaxy') " +
                "using best_fields with (analyzer='english') order by _score desc");
        assertThat(TestingHelpers.printedTable(response.rows()),
                is("End of the Galaxy| 0.7260999\nAltair| 0.2895972\nNorth West Ripple| 0.25339755\nOuter Eastern Rim| 0.2246257\n"));

        execute("select name, _score from locations where match((kind, name_description_ft), 'galaxy') " +
                "using best_fields with (fuzziness=0.5) order by _score desc");
        assertThat(TestingHelpers.printedTable(response.rows()),
                is("Outer Eastern Rim| 1.4109559\nEnd of the Galaxy| 1.4109559\nNorth West Ripple| 1.2808706\nGalactic Sector QQ7 Active J Gamma| 1.2808706\nAltair| 0.3842612\nAlgol| 0.25617412\n"));

        execute("select name, _score from locations where match((kind, name_description_ft), 'gala') " +
                "using best_fields with (operator='or', minimum_should_match=2) order by _score desc");
        assertThat(TestingHelpers.printedTable(response.rows()),
                is(""));

        execute("select name, _score from locations where match((kind, name_description_ft), 'gala') " +
                "using phrase_prefix with (slop=1) order by _score desc");
        assertThat(TestingHelpers.printedTable(response.rows()),
                is("End of the Galaxy| 0.75176084\nOuter Eastern Rim| 0.5898516\nGalactic Sector QQ7 Active J Gamma| 0.34636837\nAlgol| 0.32655922\nAltair| 0.32655922\nNorth West Ripple| 0.2857393\n"));

        execute("select name, _score from locations where match((kind, name_description_ft), 'galaxy') " +
                "using phrase with (tie_breaker=2.0) order by _score desc");
        assertThat(TestingHelpers.printedTable(response.rows()),
                is("End of the Galaxy| 0.4618873\nAltair| 0.18054473\nNorth West Ripple| 0.15797664\nOuter Eastern Rim| 0.1428891\n"));

        execute("select name, _score from locations where match((kind, name_description_ft), 'galaxy') " +
                "using best_fields with (zero_terms_query='all') order by _score desc");
        assertThat(TestingHelpers.printedTable(response.rows()),
                is("End of the Galaxy| 0.7260999\nAltair| 0.2895972\nNorth West Ripple| 0.25339755\nOuter Eastern Rim| 0.2246257\n"));
    }

    @Test
    public void testWhereColumnEqColumnAndFunctionEqFunction() throws Exception {
        this.setup.setUpLocations();
        ensureYellow();
        refresh();

        execute("select name from locations where name = name");
        assertThat(response.rowCount(), is(13L));

        execute("select name from locations where substr(name, 1, 1) = substr(name, 1, 1)");
        assertThat(response.rowCount(), is(13L));
    }

    @Test
    public void testNewColumn() throws Exception {
        execute("create table t (name string) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into t (name, score) values ('Ford', 1.2)");
    }

    @Test
    public void testESGetSourceColumns() throws Exception {
        this.setup.setUpLocations();
        ensureYellow();
        refresh();

        execute("select _id, _version from locations where id=2");
        assertNotNull(response.rows()[0][0]);
        assertNotNull(response.rows()[0][1]);

        execute("select _id, name from locations where id=2");
        assertNotNull(response.rows()[0][0]);
        assertNotNull(response.rows()[0][1]);

        execute("select _id, _doc from locations where id=2");
        assertNotNull(response.rows()[0][0]);
        assertNotNull(response.rows()[0][1]);

        execute("select _doc, id from locations where id in (2,3) order by id");
        assertEquals(TestingHelpers.printedTable(response.rows()), "{date=308534400000, race=null, kind=Galaxy, " +
                "name=Outer Eastern Rim, description=The Outer Eastern Rim of the Galaxy where the Guide has " +
                "supplanted the Encyclopedia Galactica among its more relaxed civilisations., id=2, position=2}| 2\n" +
                "{date=1367366400000, race=null, kind=Galaxy, name=Galactic Sector QQ7 Active J Gamma, "+
                "description=Galactic Sector QQ7 Active J Gamma contains the Sun Zarss, the planet Preliumtarn of " +
                "the famed Sevorbeupstry and Quentulus Quazgar Mountains., id=3, position=4}| 3\n");

        execute("select name, kind from locations where id in (2,3) order by id");
        assertEquals(TestingHelpers.printedTable(response.rows()), "Outer Eastern Rim| Galaxy\n" +
                "Galactic Sector QQ7 Active J Gamma| Galaxy\n");

        execute("select name, kind, _id from locations where id in (2,3) order by id");
        assertEquals(TestingHelpers.printedTable(response.rows()), "Outer Eastern Rim| Galaxy| 2\n" +
                "Galactic Sector QQ7 Active J Gamma| Galaxy| 3\n");

        execute("select _raw, id from locations where id in (2,3) order by id");
        assertEquals(TestingHelpers.printedTable(response.rows()), "{\"id\":\"2\",\"name\":\"Outer Eastern Rim\"," +
                "\"date\":308534400000,\"kind\":\"Galaxy\",\"position\":2,\"description\":\"The Outer Eastern Rim " +
                "of the Galaxy where the Guide has supplanted the Encyclopedia Galactica among its more relaxed " +
                "civilisations.\",\"race\":null}| 2\n" +
                "{\"id\":\"3\",\"name\":\"Galactic Sector QQ7 Active J Gamma\",\"date\":1367366400000," +
                "\"kind\":\"Galaxy\",\"position\":4,\"description\":\"Galactic Sector QQ7 Active J Gamma contains " +
                "the Sun Zarss, the planet Preliumtarn of the famed Sevorbeupstry and Quentulus Quazgar Mountains." +
                "\",\"race\":null}| 3\n");
    }
}



<code block>


package io.crate.integrationtests;

import com.carrotsearch.hppc.LongArrayList;
import com.google.common.base.Predicates;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.Iterables;
import com.google.common.util.concurrent.FutureCallback;
import com.google.common.util.concurrent.Futures;
import com.google.common.util.concurrent.ListenableFuture;
import io.crate.action.job.ContextPreparer;
import io.crate.analyze.Analysis;
import io.crate.analyze.Analyzer;
import io.crate.analyze.ParameterContext;
import io.crate.analyze.WhereClause;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.core.collections.Bucket;
import io.crate.core.collections.Row;
import io.crate.executor.Job;
import io.crate.executor.TaskResult;
import io.crate.executor.transport.NodeFetchRequest;
import io.crate.executor.transport.NodeFetchResponse;
import io.crate.executor.transport.TransportExecutor;
import io.crate.executor.transport.TransportFetchNodeAction;
import io.crate.jobs.JobContextService;
import io.crate.jobs.JobExecutionContext;
import io.crate.metadata.ColumnIdent;
import io.crate.metadata.Functions;
import io.crate.metadata.ReferenceInfo;
import io.crate.metadata.doc.DocSchemaInfo;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.fetch.RowInputSymbolVisitor;
import io.crate.planner.Plan;
import io.crate.planner.Planner;
import io.crate.planner.RowGranularity;
import io.crate.planner.consumer.ConsumerContext;
import io.crate.planner.consumer.QueryThenFetchConsumer;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.dql.QueryThenFetch;
import io.crate.planner.projection.FetchProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.sql.parser.SqlParser;
import io.crate.types.DataType;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.action.ActionListener;
import org.elasticsearch.test.ElasticsearchIntegrationTest;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;

import java.util.*;
import java.util.concurrent.CountDownLatch;

import static org.hamcrest.Matchers.*;
import static org.hamcrest.core.Is.is;

@ElasticsearchIntegrationTest.ClusterScope(numDataNodes = 2)
public class FetchOperationIntegrationTest extends SQLTransportIntegrationTest {

    Setup setup = new Setup(sqlExecutor);
    TransportExecutor executor;
    DocSchemaInfo docSchemaInfo;

    @Before
    public void transportSetUp() {
        executor = internalCluster().getInstance(TransportExecutor.class);
        docSchemaInfo = internalCluster().getInstance(DocSchemaInfo.class);
    }

    @After
    public void transportTearDown() {
        executor = null;
        docSchemaInfo = null;
    }

    private void setUpCharacters() {
        sqlExecutor.exec("create table characters (id int primary key, name string) " +
                "clustered into 2 shards with(number_of_replicas=0)");
        sqlExecutor.ensureYellowOrGreen();
        sqlExecutor.exec("insert into characters (id, name) values (?, ?)",
                new Object[][]{
                        new Object[]{1, "Arthur"},
                        new Object[]{2, "Ford"},
                }
        );
        sqlExecutor.refresh("characters");
    }

    private Plan analyzeAndPlan(String stmt) {
        Analysis analysis = analyze(stmt);
        Planner planner = internalCluster().getInstance(Planner.class);
        return planner.plan(analysis);
    }

    private Analysis analyze(String stmt) {
        Analyzer analyzer = internalCluster().getInstance(Analyzer.class);
        return analyzer.analyze(
                SqlParser.createStatement(stmt),
                new ParameterContext(new Object[0], new Object[0][], null)
        );
    }

    private CollectNode createCollectNode(Planner.Context plannerContext, boolean keepContextForFetcher) {
        TableInfo tableInfo = docSchemaInfo.getTableInfo("characters");

        ReferenceInfo docIdRefInfo = tableInfo.getReferenceInfo(new ColumnIdent("_docid"));
        Symbol docIdRef = new Reference(docIdRefInfo);
        List<Symbol> toCollect = ImmutableList.of(docIdRef);

        List<DataType> outputTypes = new ArrayList<>(toCollect.size());
        for (Symbol symbol : toCollect) {
            outputTypes.add(symbol.valueType());
        }
        CollectNode collectNode = new CollectNode(
                plannerContext.nextExecutionNodeId(),
                "collect",
                tableInfo.getRouting(WhereClause.MATCH_ALL, null));
        collectNode.toCollect(toCollect);
        collectNode.outputTypes(outputTypes);
        collectNode.maxRowGranularity(RowGranularity.DOC);
        collectNode.keepContextForFetcher(keepContextForFetcher);
        collectNode.jobId(UUID.randomUUID());
        plannerContext.allocateJobSearchContextIds(collectNode.routing());

        return collectNode;
    }

    private List<Bucket> getBuckets(CollectNode collectNode) throws InterruptedException, java.util.concurrent.ExecutionException {
        List<Bucket> results = new ArrayList<>();
        for (String nodeName : internalCluster().getNodeNames()) {
            ContextPreparer contextPreparer = internalCluster().getInstance(ContextPreparer.class, nodeName);
            JobContextService contextService = internalCluster().getInstance(JobContextService.class, nodeName);

            JobExecutionContext.Builder builder = contextService.newBuilder(collectNode.jobId());
            ListenableFuture<Bucket> future = contextPreparer.prepare(collectNode.jobId(), collectNode, builder);
            assert future != null;

            JobExecutionContext context = contextService.createContext(builder);
            context.start();
            results.add(future.get());
        }
        return results;
    }

    @Test
    public void testCollectDocId() throws Exception {
        setUpCharacters();
        Planner.Context plannerContext = new Planner.Context(clusterService());
        CollectNode collectNode = createCollectNode(plannerContext, false);

        List<Bucket> results = getBuckets(collectNode);

        assertThat(results.size(), is(2));
        int seenJobSearchContextId = -1;
        for (Bucket rows : results) {
            assertThat(rows.size(), is(1));
            Object docIdCol = rows.iterator().next().get(0);
            assertNotNull(docIdCol);
            assertThat(docIdCol, instanceOf(Long.class));
            long docId = (long)docIdCol;

            int jobSearchContextId = (int)(docId >> 32);
            int doc = (int)docId;
            assertThat(doc, is(0));
            assertThat(jobSearchContextId, greaterThan(-1));
            if (seenJobSearchContextId == -1) {
                assertThat(jobSearchContextId, anyOf(is(0), is(1)));
                seenJobSearchContextId = jobSearchContextId;
            } else {
                assertThat(jobSearchContextId, is(seenJobSearchContextId == 0 ? 1 : 0));
            }
        }
    }

    @Test
    public void testFetchAction() throws Exception {
        setUpCharacters();

        Analysis analysis = analyze("select id, name from characters");
        QueryThenFetchConsumer queryThenFetchConsumer = internalCluster().getInstance(QueryThenFetchConsumer.class);
        Planner.Context plannerContext = new Planner.Context(clusterService());
        ConsumerContext consumerContext = new ConsumerContext(analysis.rootRelation(), plannerContext);
        queryThenFetchConsumer.consume(analysis.rootRelation(), consumerContext);

        QueryThenFetch plan = ((QueryThenFetch) ((PlannedAnalyzedRelation) consumerContext.rootRelation()).plan());
        UUID jobId = UUID.randomUUID();
        plan.collectNode().jobId(jobId);

        List<Bucket> results = getBuckets(plan.collectNode());


        TransportFetchNodeAction transportFetchNodeAction = internalCluster().getInstance(TransportFetchNodeAction.class);


        Map<String, LongArrayList> jobSearchContextDocIds = new HashMap<>();
        for (Bucket rows : results) {
            long docId = (long)rows.iterator().next().get(0);

            int jobSearchContextId = (int)(docId >> 32);
            String nodeId = plannerContext.nodeId(jobSearchContextId);
            LongArrayList docIdsPerNode = jobSearchContextDocIds.get(nodeId);
            if (docIdsPerNode == null) {
                docIdsPerNode = new LongArrayList();
                jobSearchContextDocIds.put(nodeId, docIdsPerNode);
            }
            docIdsPerNode.add(docId);
        }

        Iterable<Projection> projections = Iterables.filter(plan.mergeNode().projections(), Predicates.instanceOf(FetchProjection.class));
        FetchProjection fetchProjection = (FetchProjection )Iterables.getOnlyElement(projections);
        RowInputSymbolVisitor rowInputSymbolVisitor = new RowInputSymbolVisitor(internalCluster().getInstance(Functions.class));
        RowInputSymbolVisitor.Context context = rowInputSymbolVisitor.extractImplementations(fetchProjection.outputSymbols());

        final CountDownLatch latch = new CountDownLatch(jobSearchContextDocIds.size());
        final List<Row> rows = new ArrayList<>();
        for (Map.Entry<String, LongArrayList> nodeEntry : jobSearchContextDocIds.entrySet()) {
            NodeFetchRequest nodeFetchRequest = new NodeFetchRequest();
            nodeFetchRequest.jobId(plan.collectNode().jobId());
            nodeFetchRequest.executionNodeId(plan.collectNode().executionNodeId());
            nodeFetchRequest.toFetchReferences(context.references());
            nodeFetchRequest.closeContext(true);
            nodeFetchRequest.jobSearchContextDocIds(nodeEntry.getValue());

            transportFetchNodeAction.execute(nodeEntry.getKey(), nodeFetchRequest, new ActionListener<NodeFetchResponse>() {
                @Override
                public void onResponse(NodeFetchResponse nodeFetchResponse) {
                    for (Row row : nodeFetchResponse.rows()) {
                        rows.add(row);
                    }
                    latch.countDown();
                }

                @Override
                public void onFailure(Throwable e) {
                    latch.countDown();
                    fail(e.getMessage());
                }
            });
        }
        latch.await();

        assertThat(rows.size(), is(2));
        for (Row row : rows) {
            assertThat((Integer) row.get(0), anyOf(is(1), is(2)));
            assertThat((BytesRef) row.get(1), anyOf(is(new BytesRef("Arthur")), is(new BytesRef("Ford"))));
        }
    }

    @Test
    public void testFetchProjection() throws Exception {
        setUpCharacters();

        Plan plan = analyzeAndPlan("select id, name, substr(name, 2) from characters order by id");
        assertThat(plan, instanceOf(QueryThenFetch.class));
        QueryThenFetch qtf = (QueryThenFetch) plan;

        assertThat(qtf.collectNode().keepContextForFetcher(), is(true));
        assertThat(((FetchProjection) qtf.mergeNode().projections().get(1)).jobSearchContextIdToNode(), notNullValue());
        assertThat(((FetchProjection) qtf.mergeNode().projections().get(1)).jobSearchContextIdToShard(), notNullValue());

        Job job = executor.newJob(plan);
        ListenableFuture<List<TaskResult>> results = Futures.allAsList(executor.execute(job));

        final List<Object[]> resultingRows = new ArrayList<>();
        final CountDownLatch latch = new CountDownLatch(1);
        Futures.addCallback(results, new FutureCallback<List<TaskResult>>() {
            @Override
            public void onSuccess(List<TaskResult> resultList) {
                for (Row row : resultList.get(0).rows()) {
                    resultingRows.add(row.materialize());
                }
                latch.countDown();
            }

            @Override
            public void onFailure(Throwable t) {
                latch.countDown();
                fail(t.getMessage());
            }
        });

        latch.await();
        assertThat(resultingRows.size(), is(2));
        assertThat(resultingRows.get(0).length, is(3));
        assertThat((Integer) resultingRows.get(0)[0], is(1));
        assertThat((BytesRef) resultingRows.get(0)[1], is(new BytesRef("Arthur")));
        assertThat((BytesRef) resultingRows.get(0)[2], is(new BytesRef("rthur")));
        assertThat((Integer) resultingRows.get(1)[0], is(2));
        assertThat((BytesRef) resultingRows.get(1)[1], is(new BytesRef("Ford")));
        assertThat((BytesRef) resultingRows.get(1)[2], is(new BytesRef("ord")));
    }

    @Test
    public void testFetchProjectionWithBulkSize() throws Exception {

        setup.setUpLocations();
        sqlExecutor.refresh("locations");
        int bulkSize = 2;

        Plan plan = analyzeAndPlan("select position, name from locations order by position");
        assertThat(plan, instanceOf(QueryThenFetch.class));

        rewriteFetchProjectionToBulkSize(bulkSize, ((QueryThenFetch) plan).mergeNode());

        Job job = executor.newJob(plan);
        ListenableFuture<List<TaskResult>> results = Futures.allAsList(executor.execute(job));

        final List<Object[]> resultingRows = new ArrayList<>();
        final CountDownLatch latch = new CountDownLatch(1);
        Futures.addCallback(results, new FutureCallback<List<TaskResult>>() {
            @Override
            public void onSuccess(List<TaskResult> resultList) {
                for (Row row : resultList.get(0).rows()) {
                    resultingRows.add(row.materialize());
                }
                latch.countDown();
            }

            @Override
            public void onFailure(Throwable t) {
                latch.countDown();
                fail(t.getMessage());
            }
        });
        latch.await();

        assertThat(resultingRows.size(), is(13));
        assertThat(resultingRows.get(0).length, is(2));
        assertThat((Integer) resultingRows.get(0)[0], is(1));
        assertThat((Integer) resultingRows.get(12)[0], is(6));
    }

    private void rewriteFetchProjectionToBulkSize(int bulkSize, MergeNode mergeNode) {
        List<Projection> newProjections = new ArrayList<>(mergeNode.projections().size());
        for (Projection projection : mergeNode.projections()) {
            if (projection instanceof FetchProjection) {
                FetchProjection fetchProjection = (FetchProjection) projection;
                newProjections.add(new FetchProjection(
                        fetchProjection.executionNodeId(),
                        fetchProjection.docIdSymbol(),
                        fetchProjection.inputSymbols(),
                        fetchProjection.outputSymbols(),
                        fetchProjection.partitionedBy(),
                        fetchProjection.executionNodes(),
                        bulkSize,
                        fetchProjection.closeContexts(),
                        fetchProjection.jobSearchContextIdToNode(),
                        fetchProjection.jobSearchContextIdToShard()));
            } else {
                newProjections.add(projection);
            }
        }
        mergeNode.projections(newProjections);
    }
}
<code block>


package io.crate.planner;

import com.carrotsearch.hppc.IntObjectOpenHashMap;
import com.carrotsearch.hppc.procedures.ObjectProcedure;
import com.google.common.base.Preconditions;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.Lists;
import io.crate.analyze.*;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.analyze.relations.TableRelation;
import io.crate.analyze.where.DocKeys;
import io.crate.core.collections.TreeMapBuilder;
import io.crate.exceptions.UnhandledServerException;
import io.crate.metadata.*;
import io.crate.metadata.doc.DocSysColumns;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.aggregation.impl.CountAggregation;
import io.crate.planner.consumer.ConsumerContext;
import io.crate.planner.consumer.ConsumingPlanner;
import io.crate.planner.consumer.UpdateConsumer;
import io.crate.planner.node.ddl.*;
import io.crate.planner.node.dml.ESDeleteByQueryNode;
import io.crate.planner.node.dml.ESDeleteNode;
import io.crate.planner.node.dml.SymbolBasedUpsertByIdNode;
import io.crate.planner.node.dml.Upsert;
import io.crate.planner.node.dql.CollectAndMerge;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.FileUriCollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.management.KillPlan;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.SourceIndexWriterProjection;
import io.crate.planner.projection.WriterProjection;
import io.crate.planner.symbol.InputColumn;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.cluster.node.DiscoveryNodes;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.inject.Singleton;
import org.elasticsearch.common.settings.ImmutableSettings;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.index.shard.ShardId;

import javax.annotation.Nullable;
import java.io.IOException;
import java.util.*;
import java.util.concurrent.atomic.AtomicInteger;

import static com.google.common.base.MoreObjects.firstNonNull;

@Singleton
public class Planner extends AnalyzedStatementVisitor<Planner.Context, Plan> {

    private final ConsumingPlanner consumingPlanner;
    private final ClusterService clusterService;
    private UpdateConsumer updateConsumer;

    public static class Context {

        private final IntObjectOpenHashMap<ShardId> jobSearchContextIdToShard = new IntObjectOpenHashMap<>();
        private final IntObjectOpenHashMap<String> jobSearchContextIdToNode = new IntObjectOpenHashMap<>();
        private final ClusterService clusterService;
        private int jobSearchContextIdBaseSeq = 0;
        private int executionNodeId = 0;

        public Context(ClusterService clusterService) {
            this.clusterService = clusterService;
        }

        public ClusterService clusterService() {
            return clusterService;
        }


        public void allocateJobSearchContextIds(Routing routing) {
            if (routing.jobSearchContextIdBase() > -1 || routing.hasLocations() == false
                    || routing.numShards() == 0) {
                return;
            }
            int jobSearchContextId = jobSearchContextIdBaseSeq;
            jobSearchContextIdBaseSeq += routing.numShards();
            routing.jobSearchContextIdBase(jobSearchContextId);
            for (Map.Entry<String, Map<String, List<Integer>>> nodeEntry : routing.locations().entrySet()) {
                String nodeId = nodeEntry.getKey();
                Map<String, List<Integer>> nodeRouting = nodeEntry.getValue();
                if (nodeRouting != null) {
                    for (Map.Entry<String, List<Integer>> entry : nodeRouting.entrySet()) {
                        for (Integer shardId : entry.getValue()) {
                            jobSearchContextIdToShard.put(jobSearchContextId, new ShardId(entry.getKey(), shardId));
                            jobSearchContextIdToNode.put(jobSearchContextId, nodeId);
                            jobSearchContextId++;
                        }
                    }
                }
            }
        }

        @Nullable
        public ShardId shardId(int jobSearchContextId) {
            return jobSearchContextIdToShard.get(jobSearchContextId);
        }

        public IntObjectOpenHashMap<ShardId> jobSearchContextIdToShard() {
            return jobSearchContextIdToShard;
        }

        @Nullable
        public String nodeId(int jobSearchContextId) {
            return jobSearchContextIdToNode.get(jobSearchContextId);
        }

        public IntObjectOpenHashMap<String> jobSearchContextIdToNode() {
            return jobSearchContextIdToNode;
        }

        public int nextExecutionNodeId() {
            return executionNodeId++;
        }
    }

    @Inject
    public Planner(ClusterService clusterService, ConsumingPlanner consumingPlanner, UpdateConsumer updateConsumer) {
        this.clusterService = clusterService;
        this.updateConsumer = updateConsumer;
        this.consumingPlanner = consumingPlanner;
    }


    public Plan plan(Analysis analysis) {
        AnalyzedStatement analyzedStatement = analysis.analyzedStatement();
        return process(analyzedStatement, new Context(clusterService));
    }

    @Override
    protected Plan visitAnalyzedStatement(AnalyzedStatement analyzedStatement, Context context) {
        throw new UnsupportedOperationException(String.format("AnalyzedStatement \"%s\" not supported.", analyzedStatement));
    }

    @Override
    protected Plan visitSelectStatement(SelectAnalyzedStatement statement, Context context) {
        return consumingPlanner.plan(statement.relation(), context);
    }

    @Override
    protected Plan visitInsertFromValuesStatement(InsertFromValuesAnalyzedStatement statement, Context context) {
        Preconditions.checkState(!statement.sourceMaps().isEmpty(), "no values given");
        return processInsertStatement(statement, context);
    }

    @Override
    protected Plan visitInsertFromSubQueryStatement(InsertFromSubQueryAnalyzedStatement statement, Context context) {
        return consumingPlanner.plan(statement, context);
    }

    @Override
    protected Plan visitUpdateStatement(UpdateAnalyzedStatement statement, Context context) {
        ConsumerContext consumerContext = new ConsumerContext(statement, context);
        PlannedAnalyzedRelation plannedAnalyzedRelation = updateConsumer.consume(statement, consumerContext);
        if (plannedAnalyzedRelation == null) {
            throw new IllegalArgumentException("Couldn't plan Update statement");
        }
        return plannedAnalyzedRelation.plan();
    }

    @Override
    protected Plan visitDeleteStatement(DeleteAnalyzedStatement analyzedStatement, Context context) {
        IterablePlan plan = new IterablePlan();
        TableRelation tableRelation = analyzedStatement.analyzedRelation();
        List<WhereClause> whereClauses = new ArrayList<>(analyzedStatement.whereClauses().size());
        List<DocKeys.DocKey> docKeys = new ArrayList<>(analyzedStatement.whereClauses().size());
        for (WhereClause whereClause : analyzedStatement.whereClauses()) {
            if (whereClause.noMatch()) {
                continue;
            }
            if (whereClause.docKeys().isPresent() && whereClause.docKeys().get().size() == 1) {
                docKeys.add(whereClause.docKeys().get().getOnlyKey());
            } else if (!whereClause.noMatch()) {
                whereClauses.add(whereClause);
            }
        }
        if (!docKeys.isEmpty()) {
            plan.add(new ESDeleteNode(context.nextExecutionNodeId(), tableRelation.tableInfo(), docKeys));
        } else if (!whereClauses.isEmpty()) {
            createESDeleteByQueryNode(tableRelation.tableInfo(), whereClauses, plan, context);
        }

        if (plan.isEmpty()) {
            return NoopPlan.INSTANCE;
        }
        return plan;
    }

    @Override
    protected Plan visitCopyStatement(final CopyAnalyzedStatement analysis, Context context) {
        switch (analysis.mode()) {
            case FROM:
                return copyFromPlan(analysis, context);
            case TO:
                return copyToPlan(analysis, context);
            default:
                throw new UnsupportedOperationException("mode not supported: " + analysis.mode());
        }
    }

    private Plan copyToPlan(CopyAnalyzedStatement analysis, Context context) {
        TableInfo tableInfo = analysis.table();
        WriterProjection projection = new WriterProjection();
        projection.uri(analysis.uri());
        projection.isDirectoryUri(analysis.directoryUri());
        projection.settings(analysis.settings());

        List<Symbol> outputs;
        if (analysis.selectedColumns() != null && !analysis.selectedColumns().isEmpty()) {
            outputs = new ArrayList<>(analysis.selectedColumns().size());
            List<Symbol> columnSymbols = new ArrayList<>(analysis.selectedColumns().size());
            for (int i = 0; i < analysis.selectedColumns().size(); i++) {
                outputs.add(DocReferenceConverter.convertIfPossible(analysis.selectedColumns().get(i), analysis.table()));
                columnSymbols.add(new InputColumn(i, null));
            }
            projection.inputs(columnSymbols);
        } else {
            Reference sourceRef;
            if (analysis.table().isPartitioned() && analysis.partitionIdent() == null) {

                sourceRef = new Reference(analysis.table().getReferenceInfo(DocSysColumns.DOC));
                Map<ColumnIdent, Symbol> overwrites = new HashMap<>();
                for (ReferenceInfo referenceInfo : analysis.table().partitionedByColumns()) {
                    overwrites.put(referenceInfo.ident().columnIdent(), new Reference(referenceInfo));
                }
                projection.overwrites(overwrites);
            } else {
                sourceRef = new Reference(analysis.table().getReferenceInfo(DocSysColumns.RAW));
            }
            outputs = ImmutableList.<Symbol>of(sourceRef);
        }
        CollectNode collectNode = PlanNodeBuilder.collect(
                tableInfo,
                context,
                WhereClause.MATCH_ALL,
                outputs,
                ImmutableList.<Projection>of(projection),
                analysis.partitionIdent()
        );

        MergeNode mergeNode = PlanNodeBuilder.localMerge(
                ImmutableList.<Projection>of(CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION), collectNode, context);
        return new CollectAndMerge(collectNode, mergeNode);
    }

    private Plan copyFromPlan(CopyAnalyzedStatement analysis, Context context) {


        TableInfo table = analysis.table();
        int clusteredByPrimaryKeyIdx = table.primaryKey().indexOf(analysis.table().clusteredBy());
        List<String> partitionedByNames;
        String partitionIdent = null;

        List<BytesRef> partitionValues;
        if (analysis.partitionIdent() == null) {

            if (table.isPartitioned()) {
                partitionedByNames = Lists.newArrayList(
                        Lists.transform(table.partitionedBy(), ColumnIdent.GET_FQN_NAME_FUNCTION));
            } else {
                partitionedByNames = Collections.emptyList();
            }
            partitionValues = ImmutableList.of();
        } else {
            assert table.isPartitioned() : "table must be partitioned if partitionIdent is set";

            PartitionName partitionName = PartitionName.fromPartitionIdent(table.ident().schema(), table.ident().name(), analysis.partitionIdent());
            partitionValues = partitionName.values();

            partitionIdent = partitionName.ident();
            partitionedByNames = Collections.emptyList();
        }

        SourceIndexWriterProjection sourceIndexWriterProjection = new SourceIndexWriterProjection(
                table.ident(),
                partitionIdent,
                new Reference(table.getReferenceInfo(DocSysColumns.RAW)),
                table.primaryKey(),
                table.partitionedBy(),
                partitionValues,
                table.clusteredBy(),
                clusteredByPrimaryKeyIdx,
                analysis.settings(),
                null,
                partitionedByNames.size() > 0 ? partitionedByNames.toArray(new String[partitionedByNames.size()]) : null,
                table.isPartitioned() 
        );
        List<Projection> projections = Arrays.<Projection>asList(sourceIndexWriterProjection);
        partitionedByNames.removeAll(Lists.transform(table.primaryKey(), ColumnIdent.GET_FQN_NAME_FUNCTION));
        int referencesSize = table.primaryKey().size() + partitionedByNames.size() + 1;
        referencesSize = clusteredByPrimaryKeyIdx == -1 ? referencesSize + 1 : referencesSize;

        List<Symbol> toCollect = new ArrayList<>(referencesSize);

        for (ColumnIdent primaryKey : table.primaryKey()) {
            toCollect.add(new Reference(table.getReferenceInfo(primaryKey)));
        }


        for (String partitionedColumn : partitionedByNames) {
            toCollect.add(
                    new Reference(table.getReferenceInfo(ColumnIdent.fromPath(partitionedColumn)))
            );
        }

        if (clusteredByPrimaryKeyIdx == -1) {
            toCollect.add(
                    new Reference(table.getReferenceInfo(table.clusteredBy())));
        }

        if (table.isPartitioned() && analysis.partitionIdent() == null) {
            toCollect.add(new Reference(table.getReferenceInfo(DocSysColumns.DOC)));
        } else {
            toCollect.add(new Reference(table.getReferenceInfo(DocSysColumns.RAW)));
        }

        DiscoveryNodes allNodes = clusterService.state().nodes();
        FileUriCollectNode collectNode = new FileUriCollectNode(
                context.nextExecutionNodeId(),
                "copyFrom",
                generateRouting(allNodes, analysis.settings().getAsInt("num_readers", allNodes.getSize())),
                analysis.uri(),
                toCollect,
                projections,
                analysis.settings().get("compression", null),
                analysis.settings().getAsBoolean("shared", null)
        );
        PlanNodeBuilder.setOutputTypes(collectNode);

        return new CollectAndMerge(collectNode, PlanNodeBuilder.localMerge(
                ImmutableList.<Projection>of(CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION), collectNode, context));
    }

    private Routing generateRouting(DiscoveryNodes allNodes, int maxNodes) {
        final AtomicInteger counter = new AtomicInteger(maxNodes);
        final Map<String, Map<String, List<Integer>>> locations = new TreeMap<>();
        allNodes.dataNodes().keys().forEach(new ObjectProcedure<String>() {
            @Override
            public void apply(String value) {
                if (counter.getAndDecrement() > 0) {
                    locations.put(value, TreeMapBuilder.<String, List<Integer>>newMapBuilder().map());
                }
            }
        });
        return new Routing(locations);
    }

    @Override
    protected Plan visitDDLAnalyzedStatement(AbstractDDLAnalyzedStatement statement, Context context) {
        return new IterablePlan(new GenericDDLNode(statement));
    }

    @Override
    public Plan visitDropBlobTableStatement(DropBlobTableAnalyzedStatement analysis, Context context) {
        if (analysis.noop()) {
            return NoopPlan.INSTANCE;
        }
        return visitDDLAnalyzedStatement(analysis, context);
    }

    @Override
    protected Plan visitDropTableStatement(DropTableAnalyzedStatement analysis, Context context) {
        if (analysis.noop()) {
            return NoopPlan.INSTANCE;
        }
        return new IterablePlan(new DropTableNode(analysis.table()));
    }

    @Override
    protected Plan visitCreateTableStatement(CreateTableAnalyzedStatement analysis, Context context) {
        if (analysis.noOp()) {
            return NoopPlan.INSTANCE;
        }
        TableIdent tableIdent = analysis.tableIdent();

        CreateTableNode createTableNode;
        if (analysis.isPartitioned()) {
            createTableNode = CreateTableNode.createPartitionedTableNode(
                    tableIdent,
                    analysis.ifNotExists(),
                    analysis.tableParameter().settings().getByPrefix("index."),
                    analysis.mapping(),
                    analysis.templateName(),
                    analysis.templatePrefix()
            );
        } else {
            createTableNode = CreateTableNode.createTableNode(
                    tableIdent,
                    analysis.ifNotExists(),
                    analysis.tableParameter().settings(),
                    analysis.mapping()
            );
        }
        return new IterablePlan(createTableNode);
    }

    @Override
    protected Plan visitCreateAnalyzerStatement(CreateAnalyzerAnalyzedStatement analysis, Context context) {
        Settings analyzerSettings;
        try {
            analyzerSettings = analysis.buildSettings();
        } catch (IOException ioe) {
            throw new UnhandledServerException("Could not build analyzer Settings", ioe);
        }

        ESClusterUpdateSettingsNode node = new ESClusterUpdateSettingsNode(analyzerSettings);
        return new IterablePlan(node);
    }

    @Override
    public Plan visitSetStatement(SetAnalyzedStatement analysis, Context context) {
        ESClusterUpdateSettingsNode node = null;
        if (analysis.isReset()) {

            if (analysis.settingsToRemove() != null) {
                node = new ESClusterUpdateSettingsNode(analysis.settingsToRemove(), analysis.settingsToRemove());
            }
        } else {
            if (analysis.settings() != null) {
                if (analysis.isPersistent()) {
                    node = new ESClusterUpdateSettingsNode(analysis.settings());
                } else {
                    node = new ESClusterUpdateSettingsNode(ImmutableSettings.EMPTY, analysis.settings());
                }
            }
        }
        return node != null ? new IterablePlan(node) : NoopPlan.INSTANCE;
    }

    @Override
    public Plan visitKillAnalyzedStatement(KillAnalyzedStatement analysis, Context context) {
        return KillPlan.INSTANCE;
    }

    private void createESDeleteByQueryNode(TableInfo tableInfo,
                                           List<WhereClause> whereClauses,
                                           IterablePlan plan,
                                           Context context) {

        List<String[]> indicesList = new ArrayList<>(whereClauses.size());
        for (WhereClause whereClause : whereClauses) {
            String[] indices = indices(tableInfo, whereClauses.get(0));
            if (indices.length > 0) {
                if (!whereClause.hasQuery() && tableInfo.isPartitioned()) {
                    plan.add(new ESDeletePartitionNode(indices));
                } else {
                    indicesList.add(indices);
                }
            }
        }



        if (!indicesList.isEmpty()) {
            plan.add(new ESDeleteByQueryNode(context.nextExecutionNodeId(), indicesList, whereClauses));
        }
    }

    private Upsert processInsertStatement(InsertFromValuesAnalyzedStatement analysis, Context context) {
        String[] onDuplicateKeyAssignmentsColumns = null;
        if (analysis.onDuplicateKeyAssignmentsColumns().size() > 0) {
            onDuplicateKeyAssignmentsColumns = analysis.onDuplicateKeyAssignmentsColumns().get(0);
        }
        SymbolBasedUpsertByIdNode upsertByIdNode = new SymbolBasedUpsertByIdNode(
                context.nextExecutionNodeId(),
                analysis.tableInfo().isPartitioned(),
                analysis.isBulkRequest(),
                onDuplicateKeyAssignmentsColumns,
                analysis.columns().toArray(new Reference[analysis.columns().size()])
        );
        if (analysis.tableInfo().isPartitioned()) {
            List<String> partitions = analysis.generatePartitions();
            String[] indices = partitions.toArray(new String[partitions.size()]);
            for (int i = 0; i < indices.length; i++) {
                Symbol[] onDuplicateKeyAssignments = null;
                if (analysis.onDuplicateKeyAssignmentsColumns().size() > i) {
                    onDuplicateKeyAssignments = analysis.onDuplicateKeyAssignments().get(i);
                }
                upsertByIdNode.add(
                        indices[i],
                        analysis.ids().get(i),
                        analysis.routingValues().get(i),
                        onDuplicateKeyAssignments,
                        null,
                        analysis.sourceMaps().get(i));
            }
        } else {
            for (int i = 0; i < analysis.ids().size(); i++) {
                Symbol[] onDuplicateKeyAssignments = null;
                if (analysis.onDuplicateKeyAssignments().size() > i) {
                    onDuplicateKeyAssignments = analysis.onDuplicateKeyAssignments().get(i);
                }
                upsertByIdNode.add(
                        analysis.tableInfo().ident().esName(),
                        analysis.ids().get(i),
                        analysis.routingValues().get(i),
                        onDuplicateKeyAssignments,
                        null,
                        analysis.sourceMaps().get(i));
            }
        }

        return new Upsert(ImmutableList.<Plan>of(new IterablePlan(upsertByIdNode)));
    }

    static List<DataType> extractDataTypes(List<Projection> projections, @Nullable List<DataType> inputTypes) {
        if (projections.size() == 0) {
            return inputTypes;
        }
        int projectionIdx = projections.size() - 1;
        Projection lastProjection = projections.get(projectionIdx);
        List<DataType> types = new ArrayList<>(lastProjection.outputs().size());
        List<DataType> dataTypes = firstNonNull(inputTypes, ImmutableList.<DataType>of());

        for (int c = 0; c < lastProjection.outputs().size(); c++) {
            types.add(resolveType(projections, projectionIdx, c, dataTypes));
        }
        return types;
    }

    private static DataType resolveType(List<Projection> projections, int projectionIdx, int columnIdx, List<DataType> inputTypes) {
        Projection projection = projections.get(projectionIdx);
        Symbol symbol = projection.outputs().get(columnIdx);
        DataType type = symbol.valueType();
        if (type == null || (type.equals(DataTypes.UNDEFINED) && symbol instanceof InputColumn)) {
            if (projectionIdx > 0) {
                if (symbol instanceof InputColumn) {
                    columnIdx = ((InputColumn) symbol).index();
                }
                return resolveType(projections, projectionIdx - 1, columnIdx, inputTypes);
            } else {
                assert symbol instanceof InputColumn; 
                return inputTypes.get(((InputColumn) symbol).index());
            }
        }

        return type;
    }



    public static String[] indices(TableInfo tableInfo, WhereClause whereClause) {
        String[] indices;

        if (whereClause.noMatch()) {
            indices = org.elasticsearch.common.Strings.EMPTY_ARRAY;
        } else if (!tableInfo.isPartitioned()) {

            indices = new String[]{tableInfo.ident().esName()};
        } else if (whereClause.partitions().isEmpty()) {
            if (whereClause.noMatch()) {
                return new String[0];
            }


            indices = new String[tableInfo.partitions().size()];
            int i = 0;
            for (PartitionName partitionName: tableInfo.partitions()) {
                indices[i] = partitionName.stringValue();
                i++;
            }
        } else {
            indices = whereClause.partitions().toArray(new String[whereClause.partitions().size()]);
        }
        return indices;
    }
}


<code block>


package io.crate.planner.consumer;

import com.google.common.collect.ImmutableList;
import io.crate.analyze.UpdateAnalyzedStatement;
import io.crate.analyze.VersionRewriter;
import io.crate.analyze.WhereClause;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.analyze.relations.TableRelation;
import io.crate.analyze.where.DocKeys;
import io.crate.metadata.PartitionName;
import io.crate.metadata.ReferenceIdent;
import io.crate.metadata.ReferenceInfo;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.aggregation.impl.CountAggregation;
import io.crate.planner.*;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dml.SymbolBasedUpsertByIdNode;
import io.crate.planner.node.dml.Upsert;
import io.crate.planner.node.dql.CollectAndMerge;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.UpdateProjection;
import io.crate.planner.symbol.InputColumn;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.planner.symbol.ValueSymbolVisitor;
import io.crate.types.DataTypes;
import org.elasticsearch.cluster.routing.operation.plain.Preference;
import org.elasticsearch.common.collect.Tuple;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.inject.Singleton;

import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;
import java.util.Map;

@Singleton
public class UpdateConsumer implements Consumer {

    private final Visitor visitor;

    @Inject
    public UpdateConsumer() {
        visitor = new Visitor();
    }

    @Override
    public PlannedAnalyzedRelation consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        return visitor.process(rootRelation, context);
    }

    class Visitor extends AnalyzedRelationVisitor<ConsumerContext, PlannedAnalyzedRelation> {

        @Override
        public PlannedAnalyzedRelation visitUpdateAnalyzedStatement(UpdateAnalyzedStatement statement, ConsumerContext context) {
            assert statement.sourceRelation() instanceof TableRelation : "sourceRelation of update statement must be a TableRelation";
            TableRelation tableRelation = (TableRelation) statement.sourceRelation();
            TableInfo tableInfo = tableRelation.tableInfo();

            if (tableInfo.schemaInfo().systemSchema() || tableInfo.rowGranularity() != RowGranularity.DOC) {
                return null;
            }

            List<Plan> childNodes = new ArrayList<>(statement.nestedStatements().size());
            SymbolBasedUpsertByIdNode upsertByIdNode = null;
            for (UpdateAnalyzedStatement.NestedAnalyzedStatement nestedAnalysis : statement.nestedStatements()) {
                WhereClause whereClause = nestedAnalysis.whereClause();
                if (whereClause.noMatch()){
                    continue;
                }
                if (whereClause.docKeys().isPresent()) {
                    if (upsertByIdNode == null) {
                        Tuple<String[], Symbol[]> assignments = convertAssignments(nestedAnalysis.assignments());
                        upsertByIdNode = new SymbolBasedUpsertByIdNode(context.plannerContext().nextExecutionNodeId(), false, statement.nestedStatements().size() > 1, assignments.v1(), null);
                        childNodes.add(new IterablePlan(upsertByIdNode));
                    }
                    upsertById(nestedAnalysis, tableInfo, whereClause, upsertByIdNode);
                } else {
                    Plan plan = upsertByQuery(nestedAnalysis, context, tableInfo, whereClause);
                    if (plan != null) {
                        childNodes.add(plan);
                    }
                }
            }
            if (childNodes.size() > 0){
                return new Upsert(childNodes);
            } else {
                return new NoopPlannedAnalyzedRelation(statement);
            }
        }

        private Plan upsertByQuery(UpdateAnalyzedStatement.NestedAnalyzedStatement nestedAnalysis,
                                   ConsumerContext consumerContext,
                                   TableInfo tableInfo,
                                   WhereClause whereClause) {

            Symbol versionSymbol = null;
            if(whereClause.hasVersions()){
                versionSymbol = VersionRewriter.get(whereClause.query());
                whereClause = new WhereClause(whereClause.query(), whereClause.docKeys().orNull(), whereClause.partitions());
            }


            if (!whereClause.noMatch() || !(tableInfo.isPartitioned() && whereClause.partitions().isEmpty())) {

                Reference uidReference = new Reference(
                        new ReferenceInfo(
                                new ReferenceIdent(tableInfo.ident(), "_uid"),
                                RowGranularity.DOC, DataTypes.STRING));

                Tuple<String[], Symbol[]> assignments = convertAssignments(nestedAnalysis.assignments());

                Long version = null;
                if (versionSymbol != null){
                    version = ValueSymbolVisitor.LONG.process(versionSymbol);
                }

                UpdateProjection updateProjection = new UpdateProjection(
                        new InputColumn(0, DataTypes.STRING),
                        assignments.v1(),
                        assignments.v2(),
                        version);

                CollectNode collectNode = PlanNodeBuilder.collect(
                        tableInfo,
                        consumerContext.plannerContext(),
                        whereClause,
                        ImmutableList.<Symbol>of(uidReference),
                        ImmutableList.<Projection>of(updateProjection),
                        null,
                        Preference.PRIMARY.type()
                );
                MergeNode mergeNode = PlanNodeBuilder.localMerge(
                        ImmutableList.<Projection>of(CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION), collectNode,
                        consumerContext.plannerContext());
                return new CollectAndMerge(collectNode, mergeNode);
            } else {
                return null;
            }
        }

        private void upsertById(UpdateAnalyzedStatement.NestedAnalyzedStatement nestedAnalysis,
                                             TableInfo tableInfo,
                                             WhereClause whereClause,
                                             SymbolBasedUpsertByIdNode upsertByIdNode) {
            String[] indices = Planner.indices(tableInfo, whereClause);
            assert tableInfo.isPartitioned() || indices.length == 1;

            Tuple<String[], Symbol[]> assignments = convertAssignments(nestedAnalysis.assignments());


            for (DocKeys.DocKey key : whereClause.docKeys().get()) {
                String index;
                if (key.partitionValues().isPresent()) {
                    index = new PartitionName(tableInfo.ident(), key.partitionValues().get()).stringValue();
                } else {
                    index = indices[0];
                }
                upsertByIdNode.add(
                        index,
                        key.id(),
                        key.routing(),
                        assignments.v2(),
                        key.version().orNull());
            }
        }


        private Tuple<String[], Symbol[]> convertAssignments(Map<Reference, Symbol> assignments) {
            String[] assignmentColumns = new String[assignments.size()];
            Symbol[] assignmentSymbols = new Symbol[assignments.size()];
            Iterator<Reference> it = assignments.keySet().iterator();
            int i = 0;
            while(it.hasNext()) {
                Reference key = it.next();
                assignmentColumns[i] = key.ident().columnIdent().fqn();
                assignmentSymbols[i] = assignments.get(key);
                i++;
            }
            return new Tuple<>(assignmentColumns, assignmentSymbols);
        }

        @Override
        protected PlannedAnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, ConsumerContext context) {
            return null;
        }
    }
}

<code block>

package io.crate.planner.consumer;

import com.google.common.collect.ImmutableList;
import io.crate.analyze.*;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.Routing;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.GroupByConsumer;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.dql.NonDistributedGroupBy;
import io.crate.planner.projection.GroupProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.builder.ProjectionBuilder;
import io.crate.planner.projection.builder.SplitPoints;
import io.crate.planner.symbol.Aggregation;
import io.crate.planner.symbol.Symbol;

import java.util.ArrayList;
import java.util.List;

public class NonDistributedGroupByConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();

    @Override
    public PlannedAnalyzedRelation consume(AnalyzedRelation relation, ConsumerContext context) {
        return VISITOR.process(relation, context);
    }

    private static class Visitor extends AnalyzedRelationVisitor<ConsumerContext, PlannedAnalyzedRelation> {

        @Override
        public PlannedAnalyzedRelation visitQueriedTable(QueriedTable table, ConsumerContext context) {
            if (table.querySpec().groupBy() == null) {
                return null;
            }
            TableInfo tableInfo = table.tableRelation().tableInfo();

            if (table.querySpec().where().hasVersions()) {
                context.validationException(new VersionInvalidException());
                return null;
            }

            Routing routing = tableInfo.getRouting(table.querySpec().where(), null);

            if (GroupByConsumer.requiresDistribution(tableInfo, routing) && !(tableInfo.schemaInfo().systemSchema())) {
                return null;
            }

            return nonDistributedGroupBy(table, context);
        }

        @Override
        protected PlannedAnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, ConsumerContext context) {
            return null;
        }


        private PlannedAnalyzedRelation nonDistributedGroupBy(QueriedTable table, ConsumerContext context) {
            TableInfo tableInfo = table.tableRelation().tableInfo();

            GroupByConsumer.validateGroupBySymbols(table.tableRelation(), table.querySpec().groupBy());
            List<Symbol> groupBy = table.querySpec().groupBy();

            ProjectionBuilder projectionBuilder = new ProjectionBuilder(table.querySpec());
            SplitPoints splitPoints = projectionBuilder.getSplitPoints();


            GroupProjection groupProjection = projectionBuilder.groupProjection(
                    splitPoints.leaves(),
                    table.querySpec().groupBy(),
                    splitPoints.aggregates(),
                    Aggregation.Step.ITER,
                    Aggregation.Step.PARTIAL);

            CollectNode collectNode = PlanNodeBuilder.collect(
                    tableInfo,
                    context.plannerContext(),
                    table.querySpec().where(),
                    splitPoints.leaves(),
                    ImmutableList.<Projection>of(groupProjection)
            );

            List<Symbol> collectOutputs = new ArrayList<>(
                    groupBy.size() +
                            splitPoints.aggregates().size());
            collectOutputs.addAll(groupBy);
            collectOutputs.addAll(splitPoints.aggregates());


            OrderBy orderBy = table.querySpec().orderBy();
            if (orderBy != null) {
                table.tableRelation().validateOrderBy(orderBy);
            }

            List<Projection> projections = new ArrayList<>();
            projections.add(projectionBuilder.groupProjection(
                    collectOutputs,
                    table.querySpec().groupBy(),
                    splitPoints.aggregates(),
                    Aggregation.Step.PARTIAL,
                    Aggregation.Step.FINAL
            ));

            HavingClause havingClause = table.querySpec().having();
            if (havingClause != null) {
                if (havingClause.noMatch()) {
                    return new NoopPlannedAnalyzedRelation(table);
                } else if (havingClause.hasQuery()){
                    projections.add(projectionBuilder.filterProjection(
                            collectOutputs,
                            havingClause.query()
                    ));
                }
            }


            boolean outputsMatch = table.querySpec().outputs().size() == collectOutputs.size() &&
                                    collectOutputs.containsAll(table.querySpec().outputs());
            if (context.rootRelation() == table || !outputsMatch){
                projections.add(projectionBuilder.topNProjection(
                        collectOutputs,
                        orderBy,
                        table.querySpec().offset(),
                        table.querySpec().limit(),
                        table.querySpec().outputs()
                ));
            }
            MergeNode localMergeNode = PlanNodeBuilder.localMerge(projections, collectNode,
                    context.plannerContext());
            return new NonDistributedGroupBy(collectNode, localMergeNode);
        }
    }

}

<code block>


package io.crate.planner.consumer;

import io.crate.analyze.QueriedTable;
import io.crate.analyze.QuerySpec;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.Routing;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.aggregation.impl.CountAggregation;
import io.crate.planner.Planner;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.CountNode;
import io.crate.planner.node.dql.CountPlan;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.Function;
import io.crate.planner.symbol.Symbol;
import io.crate.types.DataType;
import io.crate.types.DataTypes;

import java.util.Collections;
import java.util.List;

import static com.google.common.base.MoreObjects.firstNonNull;

public class CountConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();

    @Override
    public PlannedAnalyzedRelation consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        return VISITOR.process(rootRelation, context);
    }

    private static class Visitor extends AnalyzedRelationVisitor<ConsumerContext, PlannedAnalyzedRelation> {

        @Override
        public PlannedAnalyzedRelation visitQueriedTable(QueriedTable table, ConsumerContext context) {
            QuerySpec querySpec = table.querySpec();
            if (!querySpec.hasAggregates() || querySpec.groupBy()!=null) {
                return null;
            }
            TableInfo tableInfo = table.tableRelation().tableInfo();
            if (tableInfo.schemaInfo().systemSchema()) {
                return null;
            }
            if (!hasOnlyGlobalCount(querySpec.outputs())) {
                return null;
            }
            if(querySpec.where().hasVersions()){
                context.validationException(new VersionInvalidException());
                return null;
            }

            if (firstNonNull(querySpec.limit(), 1) < 1 ||
                    querySpec.offset() > 0){
                return new NoopPlannedAnalyzedRelation(table);
            }

            Routing routing = tableInfo.getRouting(querySpec.where(), null);
            Planner.Context plannerContext = context.plannerContext();
            CountNode countNode = new CountNode(plannerContext.nextExecutionNodeId(), routing, querySpec.where());
            MergeNode mergeNode = new MergeNode(
                    plannerContext.nextExecutionNodeId(),
                    "count-merge",
                    countNode.executionNodes().size());
            mergeNode.inputTypes(Collections.<DataType>singletonList(DataTypes.LONG));
            mergeNode.projections(Collections.<Projection>singletonList(
                    CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION));
            return new CountPlan(countNode, mergeNode);
        }

        @Override
        protected PlannedAnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, ConsumerContext context) {
            return null;
        }

        private boolean hasOnlyGlobalCount(List<Symbol> symbols) {
            if (symbols.size() != 1) {
                return false;
            }
            Symbol symbol = symbols.get(0);
            if (!(symbol instanceof Function)) {
                return false;
            }
            Function function = (Function) symbol;
            return function.info().equals(CountAggregation.COUNT_STAR_FUNCTION);
        }
    }
}

<code block>


package io.crate.planner.consumer;

import com.google.common.collect.ImmutableList;
import io.crate.Constants;
import io.crate.analyze.*;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.analyze.relations.TableRelation;
import io.crate.exceptions.UnsupportedFeatureException;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.DocReferenceConverter;
import io.crate.metadata.FunctionIdent;
import io.crate.metadata.Functions;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.aggregation.impl.SumAggregation;
import io.crate.operation.predicate.MatchPredicate;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.node.dql.QueryAndFetch;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.AggregationProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.TopNProjection;
import io.crate.planner.symbol.*;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import io.crate.types.LongType;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;

import static com.google.common.base.MoreObjects.firstNonNull;

public class QueryAndFetchConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();

    @Override
    public PlannedAnalyzedRelation consume(AnalyzedRelation relation, ConsumerContext context) {
        return VISITOR.process(relation, context);
    }

    private static class Visitor extends AnalyzedRelationVisitor<ConsumerContext, PlannedAnalyzedRelation> {

        @Override
        public PlannedAnalyzedRelation visitQueriedTable(QueriedTable table, ConsumerContext context) {
            TableRelation tableRelation = table.tableRelation();
            if(table.querySpec().where().hasVersions()){
                context.validationException(new VersionInvalidException());
                return null;
            }
            TableInfo tableInfo = tableRelation.tableInfo();

            if (tableInfo.schemaInfo().systemSchema() && table.querySpec().where().hasQuery()) {
                ensureNoLuceneOnlyPredicates(table.querySpec().where().query());
            }
            if (table.querySpec().hasAggregates()) {
                return GlobalAggregateConsumer.globalAggregates(table, tableRelation, table.querySpec().where(), context);
            } else {
               return normalSelect(table, table.querySpec().where(), tableRelation, context);
            }
        }

        @Override
        protected PlannedAnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, ConsumerContext context) {
            return null;
        }

        private void ensureNoLuceneOnlyPredicates(Symbol query) {
            NoPredicateVisitor noPredicateVisitor = new NoPredicateVisitor();
            noPredicateVisitor.process(query, null);
        }

        private static class NoPredicateVisitor extends SymbolVisitor<Void, Void> {
            @Override
            public Void visitFunction(Function symbol, Void context) {
                if (symbol.info().ident().name().equals(MatchPredicate.NAME)) {
                    throw new UnsupportedFeatureException("Cannot use match predicate on system tables");
                }
                for (Symbol argument : symbol.arguments()) {
                    process(argument, context);
                }
                return null;
            }
        }

        private PlannedAnalyzedRelation normalSelect(QueriedTable table,
                                                     WhereClause whereClause,
                                                     TableRelation tableRelation,
                                                     ConsumerContext context){
            QuerySpec querySpec = table.querySpec();
            TableInfo tableInfo = tableRelation.tableInfo();

            List<Symbol> outputSymbols;
            if (tableInfo.schemaInfo().systemSchema()) {
                outputSymbols = tableRelation.resolve(querySpec.outputs());
            } else {
                outputSymbols = new ArrayList<>(querySpec.outputs().size());
                for (Symbol symbol : querySpec.outputs()) {
                    outputSymbols.add(DocReferenceConverter.convertIfPossible(tableRelation.resolve(symbol), tableInfo));
                }
            }
            CollectNode collectNode;
            MergeNode mergeNode = null;
            OrderBy orderBy = querySpec.orderBy();
            if (context.rootRelation() != table) {

                assert !querySpec.isLimited() : "insert from sub query with limit or order by is not supported. " +
                        "Analyzer should have thrown an exception already.";

                ImmutableList<Projection> projections = ImmutableList.<Projection>of();
                collectNode = PlanNodeBuilder.collect(tableInfo,
                        context.plannerContext(),
                        whereClause, outputSymbols, projections);
            } else if (querySpec.isLimited() || orderBy != null) {

                List<Symbol> toCollect;
                List<Symbol> orderByInputColumns = null;
                if (orderBy != null){
                    List<Symbol> orderBySymbols = tableRelation.resolve(orderBy.orderBySymbols());
                    toCollect = new ArrayList<>(outputSymbols.size() + orderBySymbols.size());
                    toCollect.addAll(outputSymbols);

                    for (Symbol orderBySymbol : orderBySymbols) {
                        if (!toCollect.contains(orderBySymbol)) {
                            toCollect.add(orderBySymbol);
                        }
                    }
                    orderByInputColumns = new ArrayList<>();
                    for (Symbol symbol : orderBySymbols) {
                        orderByInputColumns.add(new InputColumn(toCollect.indexOf(symbol), symbol.valueType()));
                    }
                } else {
                    toCollect = new ArrayList<>(outputSymbols.size());
                    toCollect.addAll(outputSymbols);
                }

                List<Symbol> allOutputs = new ArrayList<>(toCollect.size());        
                for (int i = 0; i < toCollect.size(); i++) {
                    allOutputs.add(new InputColumn(i, toCollect.get(i).valueType()));
                }
                List<Symbol> finalOutputs = new ArrayList<>(outputSymbols.size());  
                for (int i = 0; i < outputSymbols.size(); i++) {
                    finalOutputs.add(new InputColumn(i, outputSymbols.get(i).valueType()));
                }



                TopNProjection tnp;
                int limit = firstNonNull(querySpec.limit(), Constants.DEFAULT_SELECT_LIMIT);
                if (orderBy == null){
                    tnp = new TopNProjection(querySpec.offset() + limit, 0);
                } else {
                    tnp = new TopNProjection(querySpec.offset() + limit, 0,
                            orderByInputColumns,
                            orderBy.reverseFlags(),
                            orderBy.nullsFirst()
                    );
                }
                tnp.outputs(allOutputs);
                collectNode = PlanNodeBuilder.collect(tableInfo,
                        context.plannerContext(),
                        whereClause, toCollect, ImmutableList.<Projection>of(tnp));


                tnp = new TopNProjection(limit, querySpec.offset());
                tnp.outputs(finalOutputs);
                if (orderBy == null) {

                    mergeNode = PlanNodeBuilder.localMerge(ImmutableList.<Projection>of(tnp), collectNode,
                            context.plannerContext());
                } else {


                    mergeNode = PlanNodeBuilder.sortedLocalMerge(
                            ImmutableList.<Projection>of(tnp),
                            orderBy,
                            allOutputs,
                            orderByInputColumns,
                            collectNode,
                            context.plannerContext()
                    );
                }
            } else {
                collectNode = PlanNodeBuilder.collect(tableInfo,
                        context.plannerContext(),
                        whereClause, outputSymbols, ImmutableList.<Projection>of());
                mergeNode = PlanNodeBuilder.localMerge(
                        ImmutableList.<Projection>of(), collectNode,
                        context.plannerContext());
            }
            return new QueryAndFetch(collectNode, mergeNode);
        }
    }
}

<code block>


package io.crate.planner.consumer;

import com.google.common.collect.ImmutableList;
import io.crate.analyze.*;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.analyze.relations.TableRelation;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.FunctionInfo;
import io.crate.metadata.ReferenceInfo;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.GlobalAggregate;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.AggregationProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.TopNProjection;
import io.crate.planner.projection.builder.ProjectionBuilder;
import io.crate.planner.projection.builder.SplitPoints;
import io.crate.planner.symbol.*;

import java.util.ArrayList;
import java.util.Collection;
import java.util.List;

import static com.google.common.base.MoreObjects.firstNonNull;


public class GlobalAggregateConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();
    private static final AggregationOutputValidator AGGREGATION_OUTPUT_VALIDATOR = new AggregationOutputValidator();

    @Override
    public PlannedAnalyzedRelation consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        return VISITOR.process(rootRelation, context);
    }

    private static class Visitor extends AnalyzedRelationVisitor<ConsumerContext, PlannedAnalyzedRelation> {

        @Override
        public PlannedAnalyzedRelation visitQueriedTable(QueriedTable table, ConsumerContext context) {
            if (table.querySpec().groupBy()!=null || !table.querySpec().hasAggregates()) {
                return null;
            }
            if (firstNonNull(table.querySpec().limit(), 1) < 1 || table.querySpec().offset() > 0){
                return new NoopPlannedAnalyzedRelation(table);
            }

            if (table.querySpec().where().hasVersions()){
                context.validationException(new VersionInvalidException());
                return null;
            }
            return globalAggregates(table, table.tableRelation(),  table.querySpec().where(), context);
        }

        @Override
        protected PlannedAnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, ConsumerContext context) {
            return null;
        }
    }

    private static boolean noGroupBy(List<Symbol> groupBy) {
        return groupBy == null || groupBy.isEmpty();
    }

    public static PlannedAnalyzedRelation globalAggregates(QueriedTable table,
                                                           TableRelation tableRelation,
                                                           WhereClause whereClause,
                                                           ConsumerContext context) {
        assert noGroupBy(table.querySpec().groupBy()) : "must not have group by clause for global aggregate queries";
        validateAggregationOutputs(tableRelation, table.querySpec().outputs());


        ProjectionBuilder projectionBuilder = new ProjectionBuilder(table.querySpec());
        SplitPoints splitPoints = projectionBuilder.getSplitPoints();

        AggregationProjection ap = projectionBuilder.aggregationProjection(
                splitPoints.leaves(),
                splitPoints.aggregates(),
                Aggregation.Step.ITER,
                Aggregation.Step.PARTIAL);

        CollectNode collectNode = PlanNodeBuilder.collect(
                tableRelation.tableInfo(),
                context.plannerContext(),
                whereClause,
                splitPoints.leaves(),
                ImmutableList.<Projection>of(ap)
        );


        List<Projection> projections = new ArrayList<>();
        projections.add(projectionBuilder.aggregationProjection(
                splitPoints.aggregates(),
                splitPoints.aggregates(),
                Aggregation.Step.PARTIAL,
                Aggregation.Step.FINAL));

        HavingClause havingClause = table.querySpec().having();
        if(havingClause != null){
            if (havingClause.noMatch()) {
                return new NoopPlannedAnalyzedRelation(table);
            } else if (havingClause.hasQuery()){
                projections.add(projectionBuilder.filterProjection(
                        splitPoints.aggregates(),
                        havingClause.query()
                ));
            }
        }

        TopNProjection topNProjection = projectionBuilder.topNProjection(
                splitPoints.aggregates(),
                null, 0, 1,
                table.querySpec().outputs()
                );
        projections.add(topNProjection);
        MergeNode localMergeNode = PlanNodeBuilder.localMerge(projections, collectNode,
                context.plannerContext());
        return new GlobalAggregate(collectNode, localMergeNode);
    }

    private static void validateAggregationOutputs(TableRelation tableRelation, Collection<? extends Symbol> outputSymbols) {
        OutputValidatorContext context = new OutputValidatorContext(tableRelation);
        for (Symbol outputSymbol : outputSymbols) {
            context.insideAggregation = false;
            AGGREGATION_OUTPUT_VALIDATOR.process(outputSymbol, context);
        }
    }

    private static class OutputValidatorContext {
        private final TableRelation tableRelation;
        private boolean insideAggregation = false;

        public OutputValidatorContext(TableRelation tableRelation) {
            this.tableRelation = tableRelation;
        }
    }

    private static class AggregationOutputValidator extends SymbolVisitor<OutputValidatorContext, Void> {

        @Override
        public Void visitFunction(Function symbol, OutputValidatorContext context) {
            context.insideAggregation = context.insideAggregation || symbol.info().type().equals(FunctionInfo.Type.AGGREGATE);
            for (Symbol argument : symbol.arguments()) {
                process(argument, context);
            }
            context.insideAggregation = false;
            return null;
        }

        @Override
        public Void visitReference(Reference symbol, OutputValidatorContext context) {
            if (context.insideAggregation) {
                ReferenceInfo.IndexType indexType = symbol.info().indexType();
                if (indexType == ReferenceInfo.IndexType.ANALYZED) {
                    throw new IllegalArgumentException(String.format(
                            "Cannot select analyzed column '%s' within grouping or aggregations", SymbolFormatter.format(symbol)));
                } else if (indexType == ReferenceInfo.IndexType.NO) {
                    throw new IllegalArgumentException(String.format(
                            "Cannot select non-indexed column '%s' within grouping or aggregations", SymbolFormatter.format(symbol)));
                }
            }
            return null;
        }

        @Override
        public Void visitField(Field field, OutputValidatorContext context) {
            return process(context.tableRelation.resolveField(field), context);
        }

        @Override
        protected Void visitSymbol(Symbol symbol, OutputValidatorContext context) {
            return null;
        }
    }
}

<code block>


package io.crate.planner.consumer;


import io.crate.analyze.OrderBy;
import io.crate.analyze.QueriedTable;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.ESGetNode;

public class ESGetConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();

    @Override
    public PlannedAnalyzedRelation consume(AnalyzedRelation relation, ConsumerContext context) {
        return VISITOR.process(relation, context);
    }

    private static class Visitor extends AnalyzedRelationVisitor<ConsumerContext, PlannedAnalyzedRelation> {

        @Override
        public PlannedAnalyzedRelation visitQueriedTable(QueriedTable table, ConsumerContext context) {
            if (context.rootRelation() != table) {
                return null;
            }

            if (table.querySpec().hasAggregates() || table.querySpec().groupBy() != null) {
                return null;
            }
            TableInfo tableInfo = table.tableRelation().tableInfo();
            if (tableInfo.isAlias()
                    || tableInfo.schemaInfo().systemSchema()
                    || tableInfo.rowGranularity() != RowGranularity.DOC) {
                return null;
            }

            if (!table.querySpec().where().docKeys().isPresent()) {
                return null;
            }

            if(table.querySpec().where().docKeys().get().withVersions()){
                context.validationException(new VersionInvalidException());
                return null;
            }
            Integer limit = table.querySpec().limit();
            if (limit != null){
                if (limit == 0){
                    return new NoopPlannedAnalyzedRelation(table);
                }
            }

            OrderBy orderBy = table.querySpec().orderBy();
            if (orderBy != null){
                table.tableRelation().validateOrderBy(orderBy);
            }
            return new ESGetNode(context.plannerContext().nextExecutionNodeId(), tableInfo, table.querySpec());
        }

        @Override
        protected PlannedAnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, ConsumerContext context) {
            return null;
        }
    }
}

<code block>


package io.crate.planner.consumer;

import com.google.common.collect.ImmutableList;
import io.crate.Constants;
import io.crate.analyze.OrderBy;
import io.crate.analyze.QueriedTable;
import io.crate.analyze.QuerySpec;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.ColumnIdent;
import io.crate.metadata.DocReferenceConverter;
import io.crate.metadata.ReferenceInfo;
import io.crate.metadata.ScoreReferenceDetector;
import io.crate.metadata.doc.DocSysColumns;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.projectors.FetchProjector;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.dql.QueryThenFetch;
import io.crate.planner.projection.FetchProjection;
import io.crate.planner.projection.MergeProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.TopNProjection;
import io.crate.planner.projection.builder.ProjectionBuilder;
import io.crate.planner.projection.builder.SplitPoints;
import io.crate.planner.symbol.*;
import io.crate.types.DataTypes;

import java.util.ArrayList;
import java.util.Collections;
import java.util.List;

public class QueryThenFetchConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();
    private static final OutputOrderReferenceCollector OUTPUT_ORDER_REFERENCE_COLLECTOR = new OutputOrderReferenceCollector();
    private static final ReferencesCollector REFERENCES_COLLECTOR = new ReferencesCollector();
    private static final ScoreReferenceDetector SCORE_REFERENCE_DETECTOR = new ScoreReferenceDetector();
    private static final ColumnIdent DOC_ID_COLUMN_IDENT = new ColumnIdent(DocSysColumns.DOCID.name());
    private static final InputColumn DEFAULT_DOC_ID_INPUT_COLUMN = new InputColumn(0, DataTypes.STRING);

    @Override
    public PlannedAnalyzedRelation consume(AnalyzedRelation relation, ConsumerContext context) {
        return VISITOR.process(relation, context);
    }

    private static class Visitor extends AnalyzedRelationVisitor<ConsumerContext, PlannedAnalyzedRelation> {

        @Override
        public PlannedAnalyzedRelation visitQueriedTable(QueriedTable table, ConsumerContext context) {
            if (context.rootRelation() != table) {
                return null;
            }
            QuerySpec querySpec = table.querySpec();
            if (querySpec.hasAggregates() || querySpec.groupBy()!=null) {
                return null;
            }
            TableInfo tableInfo = table.tableRelation().tableInfo();
            if (tableInfo.schemaInfo().systemSchema() || tableInfo.rowGranularity() != RowGranularity.DOC) {
                return null;
            }

            if(querySpec.where().hasVersions()){
                context.validationException(new VersionInvalidException());
                return null;
            }

            if (querySpec.where().noMatch()) {
                return new NoopPlannedAnalyzedRelation(table);
            }

            boolean outputsAreAllOrdered = false;
            boolean needFetchProjection = REFERENCES_COLLECTOR.collect(querySpec.outputs()).containsAnyReference();
            List<Projection> collectProjections = new ArrayList<>();
            List<Projection> mergeProjections = new ArrayList<>();
            List<Symbol> collectSymbols = new ArrayList<>();
            List<Symbol> outputSymbols = new ArrayList<>();
            ReferenceInfo docIdRefInfo = tableInfo.getReferenceInfo(DOC_ID_COLUMN_IDENT);

            ProjectionBuilder projectionBuilder = new ProjectionBuilder(querySpec);
            SplitPoints splitPoints = projectionBuilder.getSplitPoints();


            OrderBy orderBy = querySpec.orderBy();
            if (orderBy != null) {
                table.tableRelation().validateOrderBy(orderBy);




                OutputOrderReferenceContext outputOrderContext =
                        OUTPUT_ORDER_REFERENCE_COLLECTOR.collect(splitPoints.leaves());
                outputOrderContext.collectOrderBy = true;
                OUTPUT_ORDER_REFERENCE_COLLECTOR.collect(orderBy.orderBySymbols(), outputOrderContext);
                outputsAreAllOrdered = outputOrderContext.outputsAreAllOrdered();
                if (outputsAreAllOrdered) {
                    collectSymbols = splitPoints.toCollect();
                } else {
                    collectSymbols.addAll(orderBy.orderBySymbols());
                }
            }

            needFetchProjection = needFetchProjection & !outputsAreAllOrdered;

            if (needFetchProjection) {
                collectSymbols.add(0, new Reference(docIdRefInfo));
                for (Symbol symbol : querySpec.outputs()) {

                    if (SCORE_REFERENCE_DETECTOR.detect(symbol) && !collectSymbols.contains(symbol)) {
                        collectSymbols.add(symbol);
                    }
                    outputSymbols.add(DocReferenceConverter.convertIfPossible(symbol, tableInfo));
                }
            } else {

                collectSymbols = splitPoints.toCollect();
            }
            if (orderBy != null) {
                MergeProjection mergeProjection = projectionBuilder.mergeProjection(
                        collectSymbols,
                        orderBy);
                collectProjections.add(mergeProjection);
            }

            Integer limit = querySpec.limit();

            if ( limit == null && context.rootRelation() == table) {
                limit = Constants.DEFAULT_SELECT_LIMIT;
            }

            CollectNode collectNode = PlanNodeBuilder.collect(
                    tableInfo,
                    context.plannerContext(),
                    querySpec.where(),
                    collectSymbols,
                    ImmutableList.<Projection>of(),
                    orderBy,
                    limit == null ? null : limit + querySpec.offset()
            );


            collectNode.keepContextForFetcher(needFetchProjection);
            collectNode.projections(collectProjections);



            TopNProjection topNProjection;
            if (needFetchProjection) {
                topNProjection = projectionBuilder.topNProjection(
                        collectSymbols,
                        null,
                        querySpec.offset(),
                        querySpec.limit(),
                        null);
                mergeProjections.add(topNProjection);



                int bulkSize = FetchProjector.NO_BULK_REQUESTS;
                if (topNProjection.limit() > Constants.DEFAULT_SELECT_LIMIT) {
                    bulkSize = Constants.DEFAULT_SELECT_LIMIT;
                }

                FetchProjection fetchProjection = new FetchProjection(
                        collectNode.executionNodeId(),
                        DEFAULT_DOC_ID_INPUT_COLUMN, collectSymbols, outputSymbols,
                        tableInfo.partitionedByColumns(),
                        collectNode.executionNodes(),
                        bulkSize,
                        querySpec.isLimited(),
                        context.plannerContext().jobSearchContextIdToNode(),
                        context.plannerContext().jobSearchContextIdToShard()
                );
                mergeProjections.add(fetchProjection);
            } else {
                topNProjection = projectionBuilder.topNProjection(
                        collectSymbols,
                        null,
                        querySpec.offset(),
                        querySpec.limit(),
                        querySpec.outputs());
                mergeProjections.add(topNProjection);
            }

            MergeNode localMergeNode;
            if (orderBy != null) {
                localMergeNode = PlanNodeBuilder.sortedLocalMerge(
                        mergeProjections,
                        orderBy,
                        collectSymbols,
                        null,
                        collectNode,
                        context.plannerContext());
            } else {
                localMergeNode = PlanNodeBuilder.localMerge(
                        mergeProjections,
                        collectNode,
                        context.plannerContext());
            }


            if (limit != null && limit + querySpec.offset() > Constants.PAGE_SIZE) {
                collectNode.downstreamNodes(Collections.singletonList(context.plannerContext().clusterService().localNode().id()));
                collectNode.downstreamExecutionNodeId(localMergeNode.executionNodeId());
            }
            return new QueryThenFetch(collectNode, localMergeNode);
        }

        @Override
        protected PlannedAnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, ConsumerContext context) {
            return null;
        }
    }

    static class OutputOrderReferenceContext {

        private List<Reference> outputReferences = new ArrayList<>();
        private List<Reference> orderByReferences = new ArrayList<>();
        public boolean collectOrderBy = false;

        public void addReference(Reference reference) {
            if (collectOrderBy) {
                orderByReferences.add(reference);
            } else {
                outputReferences.add(reference);
            }
        }

        public boolean outputsAreAllOrdered() {
            return orderByReferences.containsAll(outputReferences);
        }

    }

    static class OutputOrderReferenceCollector extends SymbolVisitor<OutputOrderReferenceContext, Void> {

        public OutputOrderReferenceContext collect(List<Symbol> symbols) {
            OutputOrderReferenceContext context = new OutputOrderReferenceContext();
            collect(symbols, context);
            return context;
        }

        public void collect(List<Symbol> symbols, OutputOrderReferenceContext context) {
            for (Symbol symbol : symbols) {
                process(symbol, context);
            }
        }

        @Override
        public Void visitAggregation(Aggregation aggregation, OutputOrderReferenceContext context) {
            for (Symbol symbol : aggregation.inputs()) {
                process(symbol, context);
            }
            return null;
        }

        @Override
        public Void visitReference(Reference symbol, OutputOrderReferenceContext context) {
            context.addReference(symbol);
            return null;
        }

        @Override
        public Void visitDynamicReference(DynamicReference symbol, OutputOrderReferenceContext context) {
            return visitReference(symbol, context);
        }

        @Override
        public Void visitFunction(Function function, OutputOrderReferenceContext context) {
            for (Symbol symbol : function.arguments()) {
                process(symbol, context);
            }
            return null;
        }
    }

    static class ReferencesCollectorContext {
        private List<Reference> outputReferences = new ArrayList<>();

        public void addReference(Reference reference) {
            outputReferences.add(reference);
        }

        public boolean containsAnyReference() {
            return !outputReferences.isEmpty();
        }
    }

    static class ReferencesCollector extends SymbolVisitor<ReferencesCollectorContext, Void> {

        public ReferencesCollectorContext collect(List<Symbol> symbols) {
            ReferencesCollectorContext context = new ReferencesCollectorContext();
            collect(symbols, context);
            return context;
        }

        public void collect(List<Symbol> symbols, ReferencesCollectorContext context) {
            for (Symbol symbol : symbols) {
                process(symbol, context);
            }
        }

        @Override
        public Void visitAggregation(Aggregation aggregation, ReferencesCollectorContext context) {
            for (Symbol symbol : aggregation.inputs()) {
                process(symbol, context);
            }
            return null;
        }

        @Override
        public Void visitReference(Reference symbol, ReferencesCollectorContext context) {
            context.addReference(symbol);
            return null;
        }

        @Override
        public Void visitDynamicReference(DynamicReference symbol, ReferencesCollectorContext context) {
            return visitReference(symbol, context);
        }

        @Override
        public Void visitFunction(Function function, ReferencesCollectorContext context) {
            for (Symbol symbol : function.arguments()) {
                process(symbol, context);
            }
            return null;
        }

    }
}

<code block>


package io.crate.planner.consumer;

import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.PlannedAnalyzedRelation;

public interface Consumer {

    PlannedAnalyzedRelation consume(AnalyzedRelation relation, ConsumerContext context);
}

<code block>


package io.crate.planner.consumer;

import com.google.common.base.MoreObjects;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.Lists;
import com.google.common.collect.Sets;
import io.crate.Constants;
import io.crate.analyze.HavingClause;
import io.crate.analyze.OrderBy;
import io.crate.analyze.QueriedTable;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.Routing;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.DistributedGroupBy;
import io.crate.planner.node.dql.GroupByConsumer;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.GroupProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.TopNProjection;
import io.crate.planner.projection.builder.ProjectionBuilder;
import io.crate.planner.projection.builder.SplitPoints;
import io.crate.planner.symbol.Aggregation;
import io.crate.planner.symbol.Symbol;

import java.util.ArrayList;
import java.util.LinkedList;
import java.util.List;

public class DistributedGroupByConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();

    @Override
    public PlannedAnalyzedRelation consume(AnalyzedRelation relation, ConsumerContext context) {
        return VISITOR.process(relation, context);
    }

    private static class Visitor extends AnalyzedRelationVisitor<ConsumerContext, PlannedAnalyzedRelation> {

        @Override
        public PlannedAnalyzedRelation visitQueriedTable(QueriedTable table, ConsumerContext context) {
            List<Symbol> groupBy = table.querySpec().groupBy();
            if (groupBy == null) {
                return null;
            }

            TableInfo tableInfo = table.tableRelation().tableInfo();
            if(table.querySpec().where().hasVersions()){
                context.validationException(new VersionInvalidException());
                return null;
            }

            Routing routing = tableInfo.getRouting(table.querySpec().where(), null);

            GroupByConsumer.validateGroupBySymbols(table.tableRelation(), table.querySpec().groupBy());

            ProjectionBuilder projectionBuilder = new ProjectionBuilder(table.querySpec());

            SplitPoints splitPoints = projectionBuilder.getSplitPoints();


            GroupProjection groupProjection = projectionBuilder.groupProjection(
                    splitPoints.leaves(),
                    table.querySpec().groupBy(),
                    splitPoints.aggregates(),
                    Aggregation.Step.ITER,
                    Aggregation.Step.PARTIAL);

            CollectNode collectNode = PlanNodeBuilder.distributingCollect(
                    tableInfo,
                    context.plannerContext(),
                    table.querySpec().where(),
                    splitPoints.leaves(),
                    Lists.newArrayList(routing.nodes()),
                    ImmutableList.<Projection>of(groupProjection)
            );



            List<Symbol> collectOutputs = new ArrayList<>(
                    groupBy.size() +
                            splitPoints.aggregates().size());
            collectOutputs.addAll(groupBy);
            collectOutputs.addAll(splitPoints.aggregates());

            List<Projection> reducerProjections = new LinkedList<>();
            reducerProjections.add(projectionBuilder.groupProjection(
                    collectOutputs,
                    table.querySpec().groupBy(),
                    splitPoints.aggregates(),
                    Aggregation.Step.PARTIAL,
                    Aggregation.Step.FINAL)
            );

            OrderBy orderBy = table.querySpec().orderBy();
            if (orderBy != null) {
                table.tableRelation().validateOrderBy(orderBy);
            }

            HavingClause havingClause = table.querySpec().having();
            if (havingClause != null) {
                if (havingClause.noMatch()) {
                    return new NoopPlannedAnalyzedRelation(table);
                } else if (havingClause.hasQuery()) {
                    reducerProjections.add(projectionBuilder.filterProjection(
                            collectOutputs,
                            havingClause.query()
                    ));
                }
            }

            boolean isRootRelation = context.rootRelation() == table;
            if (isRootRelation) {
                reducerProjections.add(projectionBuilder.topNProjection(
                        collectOutputs,
                        orderBy,
                        0,
                        MoreObjects.firstNonNull(table.querySpec().limit(),
                                Constants.DEFAULT_SELECT_LIMIT) + table.querySpec().offset(),
                        table.querySpec().outputs()));
            }
            MergeNode mergeNode = PlanNodeBuilder.distributedMerge(
                    collectNode,
                    context.plannerContext(),
                    reducerProjections
            );


            MergeNode localMergeNode = null;
            String localNodeId = context.plannerContext().clusterService().state().nodes().localNodeId();
            if(isRootRelation) {
                TopNProjection topN = projectionBuilder.topNProjection(
                        table.querySpec().outputs(),
                        orderBy,
                        table.querySpec().offset(),
                        table.querySpec().limit(),
                        null);
                localMergeNode = PlanNodeBuilder.localMerge(ImmutableList.<Projection>of(topN),
                        mergeNode, context.plannerContext());
                localMergeNode.executionNodes(Sets.newHashSet(localNodeId));

                mergeNode.downstreamNodes(localMergeNode.executionNodes());
                mergeNode.downstreamExecutionNodeId(localMergeNode.executionNodeId());
            } else {
                mergeNode.downstreamNodes(Sets.newHashSet(localNodeId));
                mergeNode.downstreamExecutionNodeId(mergeNode.executionNodeId() + 1);
            }

            collectNode.downstreamExecutionNodeId(mergeNode.executionNodeId());
            return new DistributedGroupBy(
                    collectNode,
                    mergeNode,
                    localMergeNode
            );
        }

        @Override
        protected PlannedAnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, ConsumerContext context) {
            return null;
        }
    }
}

<code block>


package io.crate.planner.consumer;

import com.google.common.collect.ImmutableList;
import io.crate.Constants;
import io.crate.analyze.HavingClause;
import io.crate.analyze.OrderBy;
import io.crate.analyze.QueriedTable;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.analyze.relations.TableRelation;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.projectors.TopN;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.GroupByConsumer;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.dql.NonDistributedGroupBy;
import io.crate.planner.projection.FilterProjection;
import io.crate.planner.projection.GroupProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.builder.ProjectionBuilder;
import io.crate.planner.projection.builder.SplitPoints;
import io.crate.planner.symbol.Aggregation;
import io.crate.planner.symbol.Symbol;

import java.util.ArrayList;
import java.util.List;

import static com.google.common.base.MoreObjects.firstNonNull;

public class ReduceOnCollectorGroupByConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();

    @Override
    public PlannedAnalyzedRelation consume(AnalyzedRelation relation, ConsumerContext context) {
        return VISITOR.process(relation, context);
    }

    private static class Visitor extends AnalyzedRelationVisitor<ConsumerContext, PlannedAnalyzedRelation> {

        @Override
        public PlannedAnalyzedRelation visitQueriedTable(QueriedTable table, ConsumerContext context) {
            if (table.querySpec().groupBy() == null) {
                return null;
            }

            if (!GroupByConsumer.groupedByClusteredColumnOrPrimaryKeys(
                    table.tableRelation(), table.querySpec().where(), table.querySpec().groupBy())) {
                return null;
            }

            if (table.querySpec().where().hasVersions()) {
                context.validationException(new VersionInvalidException());
                return null;
            }
            return optimizedReduceOnCollectorGroupBy(table, table.tableRelation(), context);
        }


        @Override
        protected PlannedAnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, ConsumerContext context) {
            return null;
        }


        private PlannedAnalyzedRelation optimizedReduceOnCollectorGroupBy(QueriedTable table, TableRelation tableRelation, ConsumerContext context) {
            assert GroupByConsumer.groupedByClusteredColumnOrPrimaryKeys(
                    tableRelation, table.querySpec().where(), table.querySpec().groupBy()) : "not grouped by clustered column or primary keys";
            TableInfo tableInfo = tableRelation.tableInfo();
            GroupByConsumer.validateGroupBySymbols(tableRelation, table.querySpec().groupBy());
            List<Symbol> groupBy = table.querySpec().groupBy();

            boolean ignoreSorting = context.rootRelation() != table
                    && table.querySpec().limit() == null
                    && table.querySpec().offset() == TopN.NO_OFFSET;

            ProjectionBuilder projectionBuilder = new ProjectionBuilder(table.querySpec());
            SplitPoints splitPoints = projectionBuilder.getSplitPoints();


            List<Symbol> collectOutputs = new ArrayList<>(
                    groupBy.size() +
                            splitPoints.aggregates().size());
            collectOutputs.addAll(groupBy);
            collectOutputs.addAll(splitPoints.aggregates());

            OrderBy orderBy = table.querySpec().orderBy();
            if (orderBy != null) {
                table.tableRelation().validateOrderBy(orderBy);
            }

            List<Projection> projections = new ArrayList<>();
            GroupProjection groupProjection = projectionBuilder.groupProjection(
                    splitPoints.leaves(),
                    table.querySpec().groupBy(),
                    splitPoints.aggregates(),
                    Aggregation.Step.ITER,
                    Aggregation.Step.FINAL
            );
            groupProjection.setRequiredGranularity(RowGranularity.SHARD);
            projections.add(groupProjection);

            HavingClause havingClause = table.querySpec().having();
            if (havingClause != null) {
                if (havingClause.noMatch()) {
                    return new NoopPlannedAnalyzedRelation(table);
                } else if (havingClause.hasQuery()) {
                    FilterProjection fp = projectionBuilder.filterProjection(
                            collectOutputs,
                            havingClause.query()
                    );
                    fp.requiredGranularity(RowGranularity.SHARD);
                    projections.add(fp);
                }
            }


            boolean outputsMatch = table.querySpec().outputs().size() == collectOutputs.size() &&
                    collectOutputs.containsAll(table.querySpec().outputs());
            boolean collectorTopN = table.querySpec().limit() != null || table.querySpec().offset() > 0 || !outputsMatch;

            if (collectorTopN) {
                projections.add(projectionBuilder.topNProjection(
                        collectOutputs,
                        orderBy,
                        0, 
                        firstNonNull(table.querySpec().limit(), Constants.DEFAULT_SELECT_LIMIT) + table.querySpec().offset(),
                        table.querySpec().outputs()
                ));
            }

            CollectNode collectNode = PlanNodeBuilder.collect(
                    tableInfo,
                    context.plannerContext(),
                    table.querySpec().where(),
                    splitPoints.leaves(),
                    ImmutableList.copyOf(projections)
            );


            List<Projection> handlerProjections = new ArrayList<>();
            MergeNode localMergeNode;
            if (!ignoreSorting && collectorTopN && orderBy != null && orderBy.isSorted()) {


                handlerProjections.add(
                        projectionBuilder.topNProjection(
                                table.querySpec().outputs(),
                                null, 
                                table.querySpec().offset(),
                                firstNonNull(table.querySpec().limit(), Constants.DEFAULT_SELECT_LIMIT),
                                table.querySpec().outputs()
                        )
                );
                localMergeNode = PlanNodeBuilder.sortedLocalMerge(
                        handlerProjections, orderBy, table.querySpec().outputs(), null,
                        collectNode, context.plannerContext());
            } else {
                handlerProjections.add(
                        projectionBuilder.topNProjection(
                                collectorTopN ? table.querySpec().outputs() : collectOutputs,
                                orderBy,
                                table.querySpec().offset(),
                                firstNonNull(table.querySpec().limit(), Constants.DEFAULT_SELECT_LIMIT),
                                table.querySpec().outputs()
                        )
                );

                localMergeNode = PlanNodeBuilder.localMerge(handlerProjections, collectNode,
                        context.plannerContext());
            }
            return new NonDistributedGroupBy(collectNode, localMergeNode);
        }


    }
}

<code block>


package io.crate.planner.consumer;


import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.exceptions.ValidationException;
import io.crate.planner.Planner;
import org.elasticsearch.common.Nullable;

public class ConsumerContext {

    private final AnalyzedRelation rootRelation;
    private final Planner.Context plannerContext;

    private ValidationException validationException;

    public ConsumerContext(AnalyzedRelation rootRelation, Planner.Context plannerContext) {
        this.rootRelation = rootRelation;
        this.plannerContext = plannerContext;
    }

    public AnalyzedRelation rootRelation() {
        return rootRelation;
    }

    public void validationException(ValidationException validationException) {
        this.validationException = validationException;
    }

    @Nullable
    public ValidationException validationException() {
        return validationException;
    }

    public Planner.Context plannerContext() {
        return plannerContext;
    }
}

<code block>


package io.crate.planner.consumer;


import com.google.common.collect.ImmutableList;
import io.crate.analyze.InsertFromSubQueryAnalyzedStatement;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.operation.aggregation.impl.CountAggregation;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.node.dml.InsertFromSubQuery;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.AggregationProjection;
import io.crate.planner.projection.ColumnIndexWriterProjection;
import io.crate.planner.projection.Projection;
import org.elasticsearch.common.settings.ImmutableSettings;


public class InsertFromSubQueryConsumer implements Consumer {

    private final Visitor visitor;

    public InsertFromSubQueryConsumer(ConsumingPlanner consumingPlanner) {
        visitor = new Visitor(consumingPlanner);
    }

    @Override
    public PlannedAnalyzedRelation consume(AnalyzedRelation relation, ConsumerContext context) {
        return visitor.process(relation, context);
    }

    private static class Visitor extends AnalyzedRelationVisitor<ConsumerContext, PlannedAnalyzedRelation> {

        private final ConsumingPlanner consumingPlanner;

        public Visitor(ConsumingPlanner consumingPlanner) {
            this.consumingPlanner = consumingPlanner;
        }

        @Override
        public PlannedAnalyzedRelation visitInsertFromQuery(InsertFromSubQueryAnalyzedStatement statement,
                                                            ConsumerContext context) {

            ColumnIndexWriterProjection indexWriterProjection = new ColumnIndexWriterProjection(
                    statement.tableInfo().ident(),
                    null,
                    statement.tableInfo().primaryKey(),
                    statement.columns(),
                    statement.onDuplicateKeyAssignments(),
                    statement.primaryKeyColumnIndices(),
                    statement.partitionedByIndices(),
                    statement.routingColumn(),
                    statement.routingColumnIndex(),
                    ImmutableSettings.EMPTY,
                    statement.tableInfo().isPartitioned()
            );
            PlannedAnalyzedRelation plannedSubQuery = consumingPlanner.plan(statement.subQueryRelation(), context);
            if (plannedSubQuery == null) {
                return null;
            }

            plannedSubQuery.addProjection(indexWriterProjection);

            MergeNode mergeNode = null;
            if (plannedSubQuery.resultIsDistributed()) {

                AggregationProjection aggregationProjection = CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION;
                mergeNode = PlanNodeBuilder.localMerge(
                        ImmutableList.<Projection>of(aggregationProjection),
                        plannedSubQuery.resultNode(),
                        context.plannerContext());
            }
            return new InsertFromSubQuery(plannedSubQuery.plan(), mergeNode);
        }

        @Override
        protected PlannedAnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, ConsumerContext context) {
            return null;
        }
    }
}

<code block>


package io.crate.planner.consumer;

import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.exceptions.ValidationException;
import io.crate.planner.Plan;
import io.crate.planner.Planner;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.inject.Singleton;

import javax.annotation.Nullable;
import java.util.ArrayList;
import java.util.List;

@Singleton
public class ConsumingPlanner {

    private final List<Consumer> consumers = new ArrayList<>();

    @Inject
    public ConsumingPlanner() {
        consumers.add(new NonDistributedGroupByConsumer());
        consumers.add(new ReduceOnCollectorGroupByConsumer());
        consumers.add(new DistributedGroupByConsumer());
        consumers.add(new CountConsumer());
        consumers.add(new GlobalAggregateConsumer());
        consumers.add(new ESGetConsumer());
        consumers.add(new QueryThenFetchConsumer());
        consumers.add(new InsertFromSubQueryConsumer(this));
        consumers.add(new QueryAndFetchConsumer());
    }

    @Nullable
    public Plan plan(AnalyzedRelation rootRelation, Planner.Context plannerContext) {
        ConsumerContext consumerContext = new ConsumerContext(rootRelation, plannerContext);
        PlannedAnalyzedRelation plannedAnalyzedRelation = plan(rootRelation, consumerContext);
        if (plannedAnalyzedRelation != null) {
            return plannedAnalyzedRelation.plan();
        }
        return null;
    }

    @Nullable
    public PlannedAnalyzedRelation plan(AnalyzedRelation relation, ConsumerContext consumerContext) {
        for (Consumer consumer : consumers) {
            PlannedAnalyzedRelation plannedAnalyzedRelation = consumer.consume(relation, consumerContext);
            if (plannedAnalyzedRelation != null) {
                return plannedAnalyzedRelation;
            }
        }
        ValidationException validationException = consumerContext.validationException();
        if (validationException != null) {
            throw validationException;
        }
        return null;
    }
}

<code block>


package io.crate.integrationtests;

import com.google.common.base.Joiner;
import com.google.common.base.Predicate;
import io.crate.TimestampFormat;
import io.crate.action.sql.SQLActionException;
import io.crate.action.sql.SQLBulkResponse;
import io.crate.executor.TaskResult;
import io.crate.testing.TestingHelpers;
import org.elasticsearch.action.admin.cluster.health.ClusterHealthStatus;
import org.elasticsearch.action.admin.indices.exists.indices.IndicesExistsRequest;
import org.elasticsearch.common.collect.MapBuilder;
import org.elasticsearch.common.xcontent.XContentBuilder;
import org.elasticsearch.common.xcontent.XContentFactory;
import org.hamcrest.Matchers;
import org.junit.Rule;
import org.junit.Test;
import org.junit.rules.ExpectedException;

import javax.annotation.Nullable;
import java.util.*;
import java.util.concurrent.TimeUnit;

import static org.hamcrest.Matchers.*;
import static org.hamcrest.core.Is.is;

public class TransportSQLActionTest extends SQLTransportIntegrationTest {

    private Setup setup = new Setup(sqlExecutor);

    @Rule
    public ExpectedException expectedException = ExpectedException.none();


    private <T> List<T> getCol(Object[][] result, int idx) {
        ArrayList<T> res = new ArrayList<>(result.length);
        for (Object[] row : result) {
            res.add((T) row[idx]);
        }
        return res;
    }

    @Test
    public void testSelectKeepsOrder() throws Exception {
        createIndex("test");
        ensureYellow();
        client().prepareIndex("test", "default", "id1").setSource("{}").execute().actionGet();
        refresh();
        execute("select \"_id\" as b, \"_version\" as a from test");
        assertArrayEquals(new String[]{"b", "a"}, response.cols());
        assertEquals(1, response.rowCount());
        assertThat(response.duration(), greaterThanOrEqualTo(0L));
    }

    @Test
    public void testSelectCountStar() throws Exception {
        execute("create table test (\"type\" string) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into test (name) values (?)", new Object[]{"Arthur"});
        execute("insert into test (name) values (?)", new Object[]{"Trillian"});
        refresh();
        execute("select count(*) from test");
        assertEquals(1, response.rowCount());
        assertEquals(2L, response.rows()[0][0]);
    }

    @Test
    public void testSelectZeroLimit() throws Exception {
        execute("create table test (\"type\" string) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into test (name) values (?)", new Object[]{"Arthur"});
        execute("insert into test (name) values (?)", new Object[]{"Trillian"});
        refresh();
        execute("select * from test limit 0");
        assertEquals(0L, response.rowCount());
    }


    @Test
    public void testSelectCountStarWithWhereClause() throws Exception {
        execute("create table test (name string) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into test (name) values (?)", new Object[]{"Arthur"});
        execute("insert into test (name) values (?)", new Object[]{"Trillian"});
        refresh();
        execute("select count(*) from test where name = 'Trillian'");
        assertEquals(1, response.rowCount());
        assertEquals(1L, response.rows()[0][0]);
    }

    @Test
    public void testSelectStar() throws Exception {
        execute("create table test (\"firstName\" string, \"lastName\" string)");
        waitForRelocation(ClusterHealthStatus.GREEN);
        execute("select * from test");
        assertArrayEquals(new String[]{"firstName", "lastName"}, response.cols());
        assertEquals(0, response.rowCount());
    }

    @Test
    public void testSelectStarEmptyMapping() throws Exception {
        prepareCreate("test").execute().actionGet();
        ensureYellow();
        execute("select * from test");
        assertArrayEquals(new String[]{}, response.cols());
        assertEquals(0, response.rowCount());
    }

    @Test
    public void testGroupByOnAnalyzedColumn() throws Exception {
        expectedException.expect(SQLActionException.class);
        expectedException.expectMessage("Cannot GROUP BY 'test1.col1': grouping on analyzed/fulltext columns is not possible");

        execute("create table test1 (col1 string index using fulltext)");
        ensureYellow();
        execute("insert into test1 (col1) values ('abc def, ghi. jkl')");
        refresh();
        execute("select count(col1) from test1 group by col1");
    }


    @Test
    public void testSelectStarWithOther() throws Exception {
        prepareCreate("test")
                .addMapping("default",
                        "firstName", "type=string",
                        "lastName", "type=string")
                .execute().actionGet();
        ensureYellow();
        client().prepareIndex("test", "default", "id1").setRefresh(true)
                .setSource("{\"firstName\":\"Youri\",\"lastName\":\"Zoon\"}")
                .execute().actionGet();
        execute("select \"_version\", *, \"_id\" from test");
        assertArrayEquals(new String[]{"_version", "firstName", "lastName", "_id"},
                response.cols());
        assertEquals(1, response.rowCount());
        assertArrayEquals(new Object[]{1L, "Youri", "Zoon", "id1"}, response.rows()[0]);
    }

    @Test
    public void testSelectWithParams() throws Exception {
        execute("create table test (first_name string, last_name string, age double) with (number_of_replicas = 0)");
        ensureYellow();
        client().prepareIndex("test", "default", "id1").setRefresh(true)
                .setSource("{\"first_name\":\"Youri\",\"last_name\":\"Zoon\", \"age\": 38}")
                .execute().actionGet();

        Object[] args = new Object[]{"id1"};
        execute("select first_name, last_name from test where \"_id\" = $1", args);
        assertArrayEquals(new Object[]{"Youri", "Zoon"}, response.rows()[0]);

        args = new Object[]{"Zoon"};
        execute("select first_name, last_name from test where last_name = $1", args);
        assertArrayEquals(new Object[]{"Youri", "Zoon"}, response.rows()[0]);

        args = new Object[]{38, "Zoon"};
        execute("select first_name, last_name from test where age = $1 and last_name = $2", args);
        assertArrayEquals(new Object[]{"Youri", "Zoon"}, response.rows()[0]);

        args = new Object[]{38, "Zoon"};
        execute("select first_name, last_name from test where age = ? and last_name = ?", args);
        assertArrayEquals(new Object[]{"Youri", "Zoon"}, response.rows()[0]);
    }

    @Test
    public void testSelectStarWithOtherAndAlias() throws Exception {
        prepareCreate("test")
                .addMapping("default",
                        "firstName", "type=string",
                        "lastName", "type=string")
                .execute().actionGet();
        ensureYellow();
        client().prepareIndex("test", "default", "id1").setRefresh(true)
                .setSource("{\"firstName\":\"Youri\",\"lastName\":\"Zoon\"}")
                .execute().actionGet();
        execute("select *, \"_version\", \"_version\" as v from test");
        assertArrayEquals(new String[]{"firstName", "lastName", "_version", "v"},
                response.cols());
        assertEquals(1, response.rowCount());
        assertArrayEquals(new Object[]{"Youri", "Zoon", 1L, 1L}, response.rows()[0]);
    }

    @Test
    public void testFilterByEmptyString() throws Exception {
        prepareCreate("test")
                .addMapping("default",
                        "name", "type=string,index=not_analyzed")
                .execute().actionGet();
        ensureYellow();
        client().prepareIndex("test", "default", "id1").setRefresh(true)
                .setSource("{\"name\":\"\"}")
                .execute().actionGet();
        client().prepareIndex("test", "default", "id2").setRefresh(true)
                .setSource("{\"name\":\"Ruben Lenten\"}")
                .execute().actionGet();

        execute("select name from test where name = ''");
        assertEquals(1, response.rowCount());
        assertEquals("", response.rows()[0][0]);

        execute("select name from test where name != ''");
        assertEquals(1, response.rowCount());
        assertEquals("Ruben Lenten", response.rows()[0][0]);

    }

    @Test
    public void testFilterByNull() throws Exception {
        execute("create table test (name string, o object(ignored))");
        ensureYellow();

        client().prepareIndex("test", "default", "id1").setRefresh(true)
                .setSource("{}")
                .execute().actionGet();
        client().prepareIndex("test", "default", "id2").setRefresh(true)
                .setSource("{\"name\":\"Ruben Lenten\"}")
                .execute().actionGet();
        client().prepareIndex("test", "default", "id3").setRefresh(true)
                .setSource("{\"name\":\"\"}")
                .execute().actionGet();

        execute("select \"_id\" from test where name is null");
        assertEquals(1, response.rowCount());
        assertEquals("id1", response.rows()[0][0]);

        execute("select \"_id\" from test where name is not null order by \"_uid\"");
        assertEquals(2, response.rowCount());
        assertEquals("id2", response.rows()[0][0]);


        execute("select \"_id\" from test where o['invalid'] is null");
        assertEquals(0, response.rowCount());

        execute("select name from test where name is not null and name!=''");
        assertEquals(1, response.rowCount());
        assertEquals("Ruben Lenten", response.rows()[0][0]);

    }

    @Test
    public void testFilterByBoolean() throws Exception {
        prepareCreate("test")
                .addMapping("default",
                        "sunshine", "type=boolean,index=not_analyzed")
                .execute().actionGet();
        ensureYellow();

        execute("insert into test values (?)", new Object[]{true});
        refresh();

        execute("select sunshine from test where sunshine = true");
        assertEquals(1, response.rowCount());
        assertEquals(true, response.rows()[0][0]);

        execute("update test set sunshine=false where sunshine = true");
        assertEquals(1, response.rowCount());
        refresh();

        execute("select sunshine from test where sunshine = ?", new Object[]{false});
        assertEquals(1, response.rowCount());
        assertEquals(false, response.rows()[0][0]);
    }



    @Test
    public void testColsAreCaseSensitive() throws Exception {
        execute("create table test (\"firstname\" string, \"firstName\" string) " +
                "with (number_of_replicas = 0)");
        ensureYellow();
        execute("insert into test (\"firstname\", \"firstName\") values ('LowerCase', 'CamelCase')");
        refresh();

        execute("select FIRSTNAME, \"firstname\", \"firstName\" from test");
        assertArrayEquals(new String[]{"firstname", "firstname", "firstName"}, response.cols());
        assertEquals(1, response.rowCount());
        assertEquals("LowerCase", response.rows()[0][0]);
        assertEquals("LowerCase", response.rows()[0][1]);
        assertEquals("CamelCase", response.rows()[0][2]);
    }


    @Test
    public void testIdSelectWithResult() throws Exception {
        createIndex("test");
        ensureYellow();
        client().prepareIndex("test", "default", "id1").setSource("{}").execute().actionGet();
        refresh();
        execute("select \"_id\" from test");
        assertArrayEquals(new String[]{"_id"}, response.cols());
        assertEquals(1, response.rowCount());
        assertEquals(1, response.rows()[0].length);
        assertEquals("id1", response.rows()[0][0]);
    }

    @Test
    public void testDelete() throws Exception {
        createIndex("test");
        ensureYellow();
        client().prepareIndex("test", "default", "id1").setSource("{}").execute().actionGet();
        refresh();
        execute("delete from test");
        assertEquals(-1, response.rowCount());
        assertThat(response.duration(), greaterThanOrEqualTo(0L));
        execute("select \"_id\" from test");
        assertEquals(0, response.rowCount());
    }

    @Test
    public void testDeleteWithWhere() throws Exception {
        createIndex("test");
        ensureYellow();
        client().prepareIndex("test", "default", "id1").setSource("{}").execute().actionGet();
        client().prepareIndex("test", "default", "id2").setSource("{}").execute().actionGet();
        client().prepareIndex("test", "default", "id3").setSource("{}").execute().actionGet();
        refresh();
        execute("delete from test where \"_id\" = 'id1'");
        assertEquals(1, response.rowCount());
        refresh();
        execute("select \"_id\" from test");
        assertEquals(2, response.rowCount());
    }

    @Test
    public void testSqlRequestWithLimit() throws Exception {
        createIndex("test");
        ensureYellow();
        client().prepareIndex("test", "default", "id1").setSource("{}").execute().actionGet();
        client().prepareIndex("test", "default", "id2").setSource("{}").execute().actionGet();
        refresh();
        execute("select \"_id\" from test limit 1");
        assertEquals(1, response.rowCount());
    }


    @Test
    public void testSqlRequestWithLimitAndOffset() throws Exception {
        execute("create table test (id string primary key) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into test (id) values (?), (?), (?)", new Object[]{"id1", "id2", "id3"});
        refresh();
        execute("select \"id\" from test order by id limit 1 offset 1");
        assertEquals(1, response.rowCount());
        assertThat((String)response.rows()[0][0], is("id2"));
    }


    @Test
    public void testSqlRequestWithFilter() throws Exception {
        createIndex("test");
        ensureYellow();
        client().prepareIndex("test", "default", "id1").setSource("{}").execute().actionGet();
        client().prepareIndex("test", "default", "id2").setSource("{}").execute().actionGet();
        refresh();
        execute("select _id from test where _id='id1'");
        assertEquals(1, response.rowCount());
        assertEquals("id1", response.rows()[0][0]);
    }

    @Test
    public void testSqlRequestWithNotEqual() throws Exception {
        execute("create table test (id string primary key) with (number_of_replicas = 0)");
        ensureYellow();
        execute("insert into test (id) values (?)", new Object[][] {
                new Object[] { "id1" },
                new Object[] { "id2" }
        });
        refresh();
        execute("select id from test where id != 'id1'");
        assertEquals(1, response.rowCount());
        assertEquals("id2", response.rows()[0][0]);
    }


    @Test
    public void testSqlRequestWithOneOrFilter() throws Exception {
        execute("create table test (id string) with (number_of_replicas = 0)");
        ensureYellow();
        execute("insert into test (id) values ('id1'), ('id2'), ('id3')");
        refresh();
        execute("select id from test where id='id1' or id='id3'");
        assertEquals(2, response.rowCount());
        assertThat(this.<String>getCol(response.rows(), 0), containsInAnyOrder("id1", "id3"));
    }

    @Test
    public void testSqlRequestWithOneMultipleOrFilter() throws Exception {
        execute("create table test (id string) with (number_of_replicas = 0)");
        ensureYellow();
        execute("insert into test (id) values ('id1'), ('id2'), ('id3'), ('id4')");
        refresh();
        execute("select id from test where id='id1' or id='id2' or id='id4'");
        assertEquals(3, response.rowCount());
        List<String> col1 = this.getCol(response.rows(), 0);
        assertThat(col1, containsInAnyOrder("id1", "id2", "id4"));
    }

    @Test
    public void testSqlRequestWithDateFilter() throws Exception {
        prepareCreate("test")
                .addMapping("default", XContentFactory.jsonBuilder()
                        .startObject()
                        .startObject("default")
                        .startObject("properties")
                        .startObject("date")
                        .field("type", "date")
                        .endObject()
                        .endObject()
                        .endObject().endObject())
                .execute().actionGet();
        ensureYellow();
        client().prepareIndex("test", "default", "id1")
                .setSource("{\"date\": " +
                        TimestampFormat.parseTimestampString("2013-10-01") + "}")
                .execute().actionGet();
        client().prepareIndex("test", "default", "id2")
                .setSource("{\"date\": " +
                        TimestampFormat.parseTimestampString("2013-10-02") + "}")
                .execute().actionGet();
        refresh();
        execute(
                "select date from test where date = '2013-10-01'");
        assertEquals(1, response.rowCount());
        assertEquals(1380585600000L, response.rows()[0][0]);
    }

    @Test
    public void testSqlRequestWithDateGtFilter() throws Exception {
        prepareCreate("test")
                .addMapping("default",
                        "date", "type=date")
                .execute().actionGet();
        ensureYellow();
        client().prepareIndex("test", "default", "id1")
                .setSource("{\"date\": " +
                        TimestampFormat.parseTimestampString("2013-10-01") + "}")
                .execute().actionGet();
        client().prepareIndex("test", "default", "id2")
                .setSource("{\"date\":" +
                        TimestampFormat.parseTimestampString("2013-10-02") + "}")
                .execute().actionGet();
        refresh();
        execute(
                "select date from test where date > '2013-10-01'");
        assertEquals(1, response.rowCount());
        assertEquals(1380672000000L, response.rows()[0][0]);
    }

    @Test
    public void testSqlRequestWithNumericGtFilter() throws Exception {
        prepareCreate("test")
                .addMapping("default",
                        "i", "type=long")
                .execute().actionGet();
        ensureYellow();
        client().prepareIndex("test", "default", "id1")
                .setSource("{\"i\":10}")
                .execute().actionGet();
        client().prepareIndex("test", "default", "id2")
                .setSource("{\"i\":20}")
                .execute().actionGet();
        refresh();
        execute(
                "select i from test where i > 10");
        assertEquals(1, response.rowCount());
        assertEquals(20L, response.rows()[0][0]);
    }


    @Test
    @SuppressWarnings("unchecked")
    public void testArraySupport() throws Exception {
        execute("create table t1 (id int primary key, strings array(string), integers array(integer)) with (number_of_replicas=0)");
        ensureYellow();

        execute("insert into t1 (id, strings, integers) values (?, ?, ?)",
                new Object[]{
                        1,
                        new String[]{"foo", "bar"},
                        new Integer[]{1, 2, 3}
                }
        );
        refresh();

        execute("select id, strings, integers from t1");
        assertThat(response.rowCount(), is(1L));
        assertThat((Integer) response.rows()[0][0], is(1));
        assertThat(((String) ((Object[]) response.rows()[0][1])[0]), is("foo"));
        assertThat(((String) ((Object[]) response.rows()[0][1])[1]), is("bar"));
        assertThat(((Integer) ((Object[]) response.rows()[0][2])[0]), is(1));
        assertThat(((Integer) ((Object[]) response.rows()[0][2])[1]), is(2));
        assertThat(((Integer) ((Object[]) response.rows()[0][2])[2]), is(3));
    }

    @Test
    @SuppressWarnings("unchecked")
    public void testArrayInsideObject() throws Exception {
        execute("create table t1 (id int primary key, details object as (names array(string))) with (number_of_replicas=0)");
        ensureYellow();

        Map<String, Object> details = new HashMap<>();
        details.put("names", new Object[]{"Arthur", "Trillian"});
        execute("insert into t1 (id, details) values (?, ?)", new Object[]{1, details});
        refresh();

        execute("select details['names'] from t1");
        assertThat(response.rowCount(), is(1L));
        assertThat(((String) ((Object[]) response.rows()[0][0])[0]), is("Arthur"));
        assertThat(((String) ((Object[]) response.rows()[0][0])[1]), is("Trillian"));
    }

    @Test
    public void testArrayInsideObjectArray() throws Exception {
        execute("create table t1 (id int primary key, details array(object as (names array(string)))) with (number_of_replicas=0)");
        ensureYellow();

        Map<String, Object> detail1 = new HashMap<>();
        detail1.put("names", new Object[]{"Arthur", "Trillian"});

        Map<String, Object> detail2 = new HashMap<>();
        detail2.put("names", new Object[]{"Ford", "Slarti"});

        List<Map<String, Object>> details = Arrays.asList(detail1, detail2);

        execute("insert into t1 (id, details) values (?, ?)", new Object[]{1, details});
        refresh();

        expectedException.expect(SQLActionException.class);
        expectedException.expectMessage("cannot query for arrays inside object arrays explicitly");

        execute("select details['names'] from t1");

    }

    @Test
    public void testFullPathRequirement() throws Exception {

        execute("create table t1 (id int primary key, details object as (id int, more_details object as (id int))) with (number_of_replicas=0)");
        ensureYellow();

        Map<String, Object> more_details = new HashMap<>();
        more_details.put("id", 2);

        Map<String, Object> details = new HashMap<>();
        details.put("id", 1);
        details.put("more_details", more_details);

        execute("insert into t1 (id, details) values (2, ?)", new Object[]{details});
        execute("refresh table t1");

        execute("select details from t1 where details['id'] = 2");
        assertThat(response.rowCount(), is(0L));
    }

    @Test
    @SuppressWarnings("unchecked")
    public void testArraySupportWithNullValues() throws Exception {
        execute("create table t1 (id int primary key, strings array(string)) with (number_of_replicas=0)");
        ensureYellow();

        execute("insert into t1 (id, strings) values (?, ?)",
                new Object[]{
                        1,
                        new String[]{"foo", null, "bar"},
                }
        );
        refresh();

        execute("select id, strings from t1");
        assertThat(response.rowCount(), is(1L));
        assertThat((Integer) response.rows()[0][0], is(1));
        assertThat(((String) ((Object[]) response.rows()[0][1])[0]), is("foo"));
        assertThat(((Object[]) response.rows()[0][1])[1], nullValue());
        assertThat(((String) ((Object[]) response.rows()[0][1])[2]), is("bar"));
    }

    @Test
    public void testObjectArrayInsertAndSelect() throws Exception {
        execute("create table t1 (" +
                "  id int primary key, " +
                "  objects array(" +
                "   object as (" +
                "     name string, " +
                "     age int" +
                "   )" +
                "  )" +
                ") with (number_of_replicas=0)");
        ensureYellow();

        Map<String, Object> obj1 = new MapBuilder<String, Object>().put("name", "foo").put("age", 1).map();
        Map<String, Object> obj2 = new MapBuilder<String, Object>().put("name", "bar").put("age", 2).map();

        Object[] args = new Object[]{1, new Object[]{obj1, obj2}};
        execute("insert into t1 (id, objects) values (?, ?)", args);
        refresh();

        execute("select objects from t1");
        assertThat(response.rowCount(), is(1L));

        Object[] objResults = ((Object[]) response.rows()[0][0]);
        Map<String, Object> obj1Result = ((Map) objResults[0]);
        assertThat((String) obj1Result.get("name"), is("foo"));
        assertThat((Integer) obj1Result.get("age"), is(1));

        Map<String, Object> obj2Result = ((Map) objResults[1]);
        assertThat((String) obj2Result.get("name"), is("bar"));
        assertThat((Integer) obj2Result.get("age"), is(2));

        execute("select objects['name'] from t1");
        assertThat(response.rowCount(), is(1L));

        String[] names = Arrays.copyOf(((Object[]) response.rows()[0][0]), 2, String[].class);
        assertThat(names[0], is("foo"));
        assertThat(names[1], is("bar"));

        execute("select objects['name'] from t1 where ? = ANY (objects['name'])", new Object[]{"foo"});
        assertThat(response.rowCount(), is(1L));
    }

    @Test
    public void testGetResponseWithObjectColumn() throws Exception {
        XContentBuilder mapping = XContentFactory.jsonBuilder().startObject()
                .startObject("default")
                .startObject("_meta").field("primary_keys", "id").endObject()
                .startObject("properties")
                .startObject("id")
                .field("type", "string")
                .field("index", "not_analyzed")
                .endObject()
                .startObject("data")
                .field("type", "object")
                .field("index", "not_analyzed")
                .field("dynamic", false)
                .endObject()
                .endObject()
                .endObject()
                .endObject();

        prepareCreate("test")
                .addMapping("default", mapping)
                .execute().actionGet();
        ensureYellow();

        Map<String, Object> data = new HashMap<>();
        data.put("foo", "bar");
        execute("insert into test (id, data) values (?, ?)", new Object[]{"1", data});
        refresh();

        execute("select data from test where id = ?", new Object[]{"1"});
        assertEquals(data, response.rows()[0][0]);
    }

    @Test
    public void testSelectToGetRequestByPlanner() throws Exception {
        this.setup.createTestTableWithPrimaryKey();

        execute("insert into test (pk_col, message) values ('124', 'bar1')");
        assertEquals(1, response.rowCount());
        refresh();
        waitNoPendingTasksOnAll(); 

        execute("select pk_col, message from test where pk_col='124'");
        assertEquals(1, response.rowCount());
        assertEquals("124", response.rows()[0][0]);
        assertThat(response.duration(), greaterThanOrEqualTo(0L));
    }

    @Test
    public void testDeleteToDeleteRequestByPlanner() throws Exception {
        this.setup.createTestTableWithPrimaryKey();

        execute("insert into test (pk_col, message) values ('123', 'bar')");
        assertEquals(1, response.rowCount());
        refresh();

        execute("delete from test where pk_col='123'");
        assertEquals(1, response.rowCount());
        assertThat(response.duration(), greaterThanOrEqualTo(0L));
        refresh();

        execute("select * from test where pk_col='123'");
        assertEquals(0, response.rowCount());
    }

    @Test
    public void testSelectToRoutedRequestByPlanner() throws Exception {
        this.setup.createTestTableWithPrimaryKey();

        execute("insert into test (pk_col, message) values ('1', 'foo')");
        execute("insert into test (pk_col, message) values ('2', 'bar')");
        execute("insert into test (pk_col, message) values ('3', 'baz')");
        refresh();

        execute("SELECT * FROM test WHERE pk_col='1' OR pk_col='2'");
        assertEquals(2, response.rowCount());
        assertThat(response.duration(), greaterThanOrEqualTo(0L));

        execute("SELECT * FROM test WHERE pk_col=? OR pk_col=?", new Object[]{"1", "2"});
        assertEquals(2, response.rowCount());

        awaitBusy(new Predicate<Object>() {
            @Override
            public boolean apply(@Nullable Object input) {
                execute("SELECT * FROM test WHERE (pk_col=? OR pk_col=?) OR pk_col=?", new Object[]{"1", "2", "3"});
                return response.rowCount() == 3
                        && Joiner.on(',').join(Arrays.asList(response.cols())).equals("message,pk_col");
            }
        }, 10, TimeUnit.SECONDS);

    }

    @Test
    public void testSelectToRoutedRequestByPlannerMissingDocuments() throws Exception {
        this.setup.createTestTableWithPrimaryKey();

        execute("insert into test (pk_col, message) values ('1', 'foo')");
        execute("insert into test (pk_col, message) values ('2', 'bar')");
        execute("insert into test (pk_col, message) values ('3', 'baz')");
        refresh();

        execute("SELECT pk_col, message FROM test WHERE pk_col='4' OR pk_col='3'");
        assertEquals(1, response.rowCount());
        assertThat(Arrays.asList(response.rows()[0]), hasItems(new Object[]{"3", "baz"}));
        assertThat(response.duration(), greaterThanOrEqualTo(0L));

        execute("SELECT pk_col, message FROM test WHERE pk_col='4' OR pk_col='99'");
        assertEquals(0, response.rowCount());
    }

    @Test
    public void testSelectToRoutedRequestByPlannerWhereIn() throws Exception {
        this.setup.createTestTableWithPrimaryKey();

        execute("insert into test (pk_col, message) values ('1', 'foo')");
        execute("insert into test (pk_col, message) values ('2', 'bar')");
        execute("insert into test (pk_col, message) values ('3', 'baz')");
        refresh();

        execute("SELECT * FROM test WHERE pk_col IN (?,?,?)", new Object[]{"1", "2", "3"});
        assertEquals(3, response.rowCount());
        assertThat(response.duration(), greaterThanOrEqualTo(0L));
    }

    @Test
    public void testDeleteToRoutedRequestByPlannerWhereIn() throws Exception {
        this.setup.createTestTableWithPrimaryKey();

        execute("insert into test (pk_col, message) values ('1', 'foo')");
        execute("insert into test (pk_col, message) values ('2', 'bar')");
        execute("insert into test (pk_col, message) values ('3', 'baz')");
        refresh();

        execute("DELETE FROM test WHERE pk_col IN (?, ?, ?)", new Object[]{"1", "2", "4"});
        assertThat(response.duration(), greaterThanOrEqualTo(0L));
        refresh();

        execute("SELECT pk_col FROM test");
        assertThat(response.rowCount(), is(1L));
        assertEquals(response.rows()[0][0], "3");

    }


    @Test
    public void testDeleteToRoutedRequestByPlannerWhereOr() throws Exception {
        this.setup.createTestTableWithPrimaryKey();
        execute("insert into test (pk_col, message) values ('1', 'foo'), ('2', 'bar'), ('3', 'baz')");
        refresh();
        execute("DELETE FROM test WHERE pk_col=? or pk_col=? or pk_col=?", new Object[]{"1", "2", "4"});
        assertThat(response.duration(), greaterThanOrEqualTo(0L));
        refresh();
        execute("SELECT pk_col FROM test");
        assertThat(response.rowCount(), is(1L));
        assertEquals(response.rows()[0][0], "3");
    }

    @Test
    public void testUpdateToRoutedRequestByPlannerWhereOr() throws Exception {
        this.setup.createTestTableWithPrimaryKey();
        execute("insert into test (pk_col, message) values ('1', 'foo'), ('2', 'bar'), ('3', 'baz')");
        refresh();
        execute("update test set message='new' WHERE pk_col='1' or pk_col='2' or pk_col='4'");
        assertThat(response.rowCount(), is(2L));
        assertThat(response.duration(), greaterThanOrEqualTo(0L));
        refresh();
        execute("SELECT distinct message FROM test");
        assertThat(response.rowCount(), is(2L));
    }

    @Test
    public void testSelectWithWhereLike() throws Exception {
        this.setup.groupBySetup();

        execute("select name from characters where name like '%ltz'");
        assertEquals(2L, response.rowCount());

        execute("select count(*) from characters where name like 'Jeltz'");
        assertEquals(1L, response.rows()[0][0]);

        execute("select count(*) from characters where race like '%o%'");
        assertEquals(3L, response.rows()[0][0]);

        Map<String, Object> emptyMap = new HashMap<>();
        Map<String, Object> details = new HashMap<>();
        details.put("age", 30);
        details.put("job", "soldier");
        execute("insert into characters (race, gender, name, details) values (?, ?, ?, ?)",
                new Object[]{"Vo*", "male", "Kwaltzz", emptyMap}
        );
        execute("insert into characters (race, gender, name, details) values (?, ?, ?, ?)",
                new Object[]{"Vo?", "male", "Kwaltzzz", emptyMap}
        );
        execute("insert into characters (race, gender, name, details) values (?, ?, ?, ?)",
                new Object[]{"Vo!", "male", "Kwaltzzzz", details}
        );
        execute("insert into characters (race, gender, name, details) values (?, ?, ?, ?)",
                new Object[]{"Vo%", "male", "Kwaltzzzz", details}
        );
        refresh();

        execute("select race from characters where race like 'Vo*'");
        assertEquals(1L, response.rowCount());
        assertEquals("Vo*", response.rows()[0][0]);

        execute("select race from characters where race like ?", new Object[]{"Vo?"});
        assertEquals(1L, response.rowCount());
        assertEquals("Vo?", response.rows()[0][0]);

        execute("select race from characters where race like 'Vo!'");
        assertEquals(1L, response.rowCount());
        assertEquals("Vo!", response.rows()[0][0]);

        execute("select race from characters where race like 'Vo\\%'");
        assertEquals(1L, response.rowCount());
        assertEquals("Vo%", response.rows()[0][0]);

        execute("select race from characters where race like 'Vo_'");
        assertEquals(4L, response.rowCount());

        execute("select race from characters where details['job'] like 'sol%'");
        assertEquals(2L, response.rowCount());
    }

    @Test
    public void testSelectMatch() throws Exception {
        execute("create table quotes (quote string)");
        ensureYellow();
        assertTrue(client().admin().indices().exists(new IndicesExistsRequest("quotes"))
                .actionGet().isExists());

        execute("insert into quotes values (?)", new Object[]{"don't panic"});
        refresh();

        execute("select quote from quotes where match(quote, ?)", new Object[]{"don't panic"});
        assertEquals(1L, response.rowCount());
        assertEquals("don't panic", response.rows()[0][0]);

    }

    @Test
    public void testSelectNotMatch() throws Exception {
        execute("create table quotes (quote string) with (number_of_replicas = 0)");
        ensureYellow();
        assertTrue(client().admin().indices().exists(new IndicesExistsRequest("quotes"))
                .actionGet().isExists());

        execute("insert into quotes values (?), (?)", new Object[]{"don't panic", "hello"});
        refresh();

        execute("select quote from quotes where not match(quote, ?)",
                new Object[]{"don't panic"});
        assertEquals(1L, response.rowCount());
        assertEquals("hello", response.rows()[0][0]);
    }

    @Test
    public void testSelectOrderByScore() throws Exception {
        execute("create table quotes (quote string index off," +
                "index quote_ft using fulltext(quote))");
        ensureYellow();
        execute("insert into quotes values (?)",
                new Object[]{"Would it save you a lot of time if I just gave up and went mad now?"}
        );
        execute("insert into quotes values (?)",
                new Object[]{"Time is an illusion. Lunchtime doubly so"}
        );
        refresh();

        execute("select * from quotes");
        execute("select quote, \"_score\" from quotes where match(quote_ft, ?) " +
                        "order by \"_score\" desc",
                new Object[]{"time", "time"}
        );
        assertEquals(2L, response.rowCount());
        assertEquals("Time is an illusion. Lunchtime doubly so", response.rows()[0][0]);
    }

    @Test
    public void testSelectScoreMatchAll() throws Exception {
        execute("create table quotes (quote string) with (number_of_replicas = 0)");
        ensureYellow();
        assertTrue(client().admin().indices().exists(new IndicesExistsRequest("quotes"))
                .actionGet().isExists());

        execute("insert into quotes values (?), (?)",
                new Object[]{"Would it save you a lot of time if I just gave up and went mad now?",
                        "Time is an illusion. Lunchtime doubly so"}
        );
        refresh();

        execute("select quote, \"_score\" from quotes");
        assertEquals(2L, response.rowCount());
        assertEquals(1.0f, response.rows()[0][1]);
        assertEquals(1.0f, response.rows()[1][1]);
    }

    @Test
    public void testSelectWhereScore() throws Exception {
        execute("create table quotes (quote string, " +
                "index quote_ft using fulltext(quote)) with (number_of_replicas = 0)");
        ensureYellow();
        assertTrue(client().admin().indices().exists(new IndicesExistsRequest("quotes"))
                .actionGet().isExists());

        execute("insert into quotes values (?), (?)",
                new Object[]{"Would it save you a lot of time if I just gave up and went mad now?",
                        "Time is an illusion. Lunchtime doubly so. Take your time."}
        );
        refresh();

        execute("select quote, \"_score\" from quotes where match(quote_ft, 'time') " +
                "and \"_score\" >= 0.98");
        assertEquals(1L, response.rowCount());
        assertThat((Float) response.rows()[0][1], greaterThanOrEqualTo(0.98f));
    }

    @Test
    public void testSelectMatchAnd() throws Exception {
        execute("create table quotes (id int, quote string, " +
                "index quote_fulltext using fulltext(quote) with (analyzer='english')) with (number_of_replicas = 0)");
        ensureYellow();
        assertTrue(client().admin().indices().exists(new IndicesExistsRequest("quotes"))
                .actionGet().isExists());

        execute("insert into quotes (id, quote) values (?, ?), (?, ?)",
                new Object[]{
                        1, "Would it save you a lot of time if I just gave up and went mad now?",
                        2, "Time is an illusion. Lunchtime doubly so"}
        );
        refresh();

        execute("select quote from quotes where match(quote_fulltext, 'time') and id = 1");
        assertEquals(1L, response.rowCount());
    }

    private void nonExistingColumnSetup() {
        execute("create table quotes (" +
                "id integer primary key, " +
                "quote string index off, " +
                "o object(ignored), " +
                "index quote_fulltext using fulltext(quote) with (analyzer='snowball')" +
                ") clustered by (id) into 3 shards with (number_of_replicas = 0)");
        ensureYellow();
        execute("insert into quotes (id, quote) values (1, '\"Nothing particularly exciting," +
                "\" it admitted, \"but they are alternatives.\"')");
        execute("insert into quotes (id, quote) values (2, '\"Have another drink," +
                "\" said Trillian. \"Enjoy yourself.\"')");
        refresh();
    }

    @Test
    public void selectNonExistingColumn() throws Exception {
        nonExistingColumnSetup();
        execute("select o['notExisting'] from quotes");
        assertEquals(2L, response.rowCount());
        assertEquals("o['notExisting']", response.cols()[0]);
        assertNull(response.rows()[0][0]);
        assertNull(response.rows()[1][0]);
    }

    @Test
    public void selectNonExistingAndExistingColumns() throws Exception {
        nonExistingColumnSetup();
        execute("select o['unknown'], id from quotes order by id asc");
        assertEquals(2L, response.rowCount());
        assertEquals("o['unknown']", response.cols()[0]);
        assertEquals("id", response.cols()[1]);
        assertNull(response.rows()[0][0]);
        assertEquals(1, response.rows()[0][1]);
        assertNull(response.rows()[1][0]);
        assertEquals(2, response.rows()[1][1]);
    }

    @Test
    public void selectWhereNonExistingColumn() throws Exception {
        nonExistingColumnSetup();
        execute("select * from quotes where o['something'] > 0");
        assertEquals(0L, response.rowCount());
    }

    @Test
    public void selectWhereDynamicColumnIsNull() throws Exception {
        nonExistingColumnSetup();

        execute("select * from quotes where o['something'] IS NULL");
        assertEquals(0, response.rowCount());
    }

    @Test
    public void selectWhereNonExistingColumnWhereIn() throws Exception {
        nonExistingColumnSetup();
        execute("select * from quotes where o['something'] IN(1,2,3)");
        assertEquals(0L, response.rowCount());
    }

    @Test
    public void selectWhereNonExistingColumnLike() throws Exception {
        nonExistingColumnSetup();
        execute("select * from quotes where o['something'] Like '%bla'");
        assertEquals(0L, response.rowCount());
    }

    @Test
    public void selectWhereNonExistingColumnMatchFunction() throws Exception {
        nonExistingColumnSetup();

        expectedException.expect(SQLActionException.class);
        expectedException.expectMessage("Can only use MATCH on columns of type STRING, not on 'null'");

        execute("select * from quotes where match(o['something'], 'bla')");
    }

    @Test
    public void testSelectCountDistinctZero() throws Exception {
        execute("create table test (col1 int) with (number_of_replicas=0)");
        ensureYellow();

        execute("select count(distinct col1) from test");

        assertEquals(1, response.rowCount());
        assertEquals(0L, response.rows()[0][0]);
    }

    @Test
    public void testRefresh() throws Exception {
        execute("create table test (id int primary key, name string)");
        ensureYellow();
        execute("insert into test (id, name) values (0, 'Trillian'), (1, 'Ford'), (2, 'Zaphod')");
        execute("select count(*) from test");
        assertThat((Long) response.rows()[0][0], lessThan(3L));

        execute("refresh table test");
        assertFalse(response.hasRowCount());
        assertThat(response.rows(), is(TaskResult.EMPTY_OBJS));

        execute("select count(*) from test");
        assertThat((Long) response.rows()[0][0], is(3L));
    }

    @Test
    public void testInsertSelectWithClusteredBy() throws Exception {
        execute("create table quotes (id integer, quote string) clustered by(id) " +
                "with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into quotes (id, quote) values(?, ?)",
                new Object[]{1, "I'd far rather be happy than right any day."});
        assertEquals(1L, response.rowCount());
        refresh();

        execute("select \"_id\", id, quote from quotes where id=1");
        assertEquals(1L, response.rowCount());


        assertNotNull(response.rows()[0][0]);
        assertThat(((String) response.rows()[0][0]).length(), greaterThan(0));
    }

    @Test
    public void testInsertSelectWithAutoGeneratedId() throws Exception {
        execute("create table quotes (id integer, quote string)" +
                "with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into quotes (id, quote) values(?, ?)",
                new Object[]{1, "I'd far rather be happy than right any day."});
        assertEquals(1L, response.rowCount());
        refresh();

        execute("select \"_id\", id, quote from quotes where id=1");
        assertEquals(1L, response.rowCount());


        assertNotNull(response.rows()[0][0]);
        assertThat(((String) response.rows()[0][0]).length(), greaterThan(0));
    }

    @Test
    public void testInsertSelectWithPrimaryKey() throws Exception {
        execute("create table quotes (id integer primary key, quote string)" +
                "with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into quotes (id, quote) values(?, ?)",
                new Object[]{1, "I'd far rather be happy than right any day."});
        assertEquals(1L, response.rowCount());
        refresh();

        execute("select \"_id\", id, quote from quotes where id=1");
        assertEquals(1L, response.rowCount());



        String _id = (String) response.rows()[0][0];
        Integer id = (Integer) response.rows()[0][1];
        assertEquals(id.toString(), _id);
    }

    @Test
    public void testInsertSelectWithMultiplePrimaryKey() throws Exception {
        execute("create table quotes (id integer primary key, author string primary key, " +
                "quote string) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into quotes (id, author, quote) values(?, ?, ?)",
                new Object[]{1, "Ford", "I'd far rather be happy than right any day."});
        assertEquals(1L, response.rowCount());
        refresh();

        execute("select \"_id\", id from quotes where id=1 and author='Ford'");
        assertEquals(1L, response.rowCount());
        assertThat((String) response.rows()[0][0], is("AgExBEZvcmQ="));
        assertThat((Integer) response.rows()[0][1], is(1));
    }

    @Test
    public void testInsertSelectWithMultiplePrimaryKeyAndClusteredBy() throws Exception {
        execute("create table quotes (id integer primary key, author string primary key, " +
                "quote string) clustered by(author) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into quotes (id, author, quote) values(?, ?, ?)",
                new Object[]{1, "Ford", "I'd far rather be happy than right any day."});
        assertEquals(1L, response.rowCount());
        refresh();

        execute("select \"_id\", id from quotes where id=1 and author='Ford'");
        assertEquals(1L, response.rowCount());
        assertThat((String) response.rows()[0][0], is("AgRGb3JkATE="));
        assertThat((Integer) response.rows()[0][1], is(1));
    }

    @Test
    public void testInsertSelectWithMultiplePrimaryOnePkSame() throws Exception {
        execute("create table quotes (id integer primary key, author string primary key, " +
                "quote string) clustered by(author) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into quotes (id, author, quote) values (?, ?, ?), (?, ?, ?)",
                new Object[]{1, "Ford", "I'd far rather be happy than right any day.",
                        1, "Douglas", "Don't panic"}
        );
        assertEquals(2L, response.rowCount());
        refresh();

        execute("select \"_id\", id from quotes where id=1 order by author");
        assertEquals(2L, response.rowCount());
        assertThat((String) response.rows()[0][0], is("AgdEb3VnbGFzATE="));
        assertThat((Integer) response.rows()[0][1], is(1));
        assertThat((String) response.rows()[1][0], is("AgRGb3JkATE="));
        assertThat((Integer) response.rows()[1][1], is(1));
    }

    @Test
    public void testDeleteByIdWithMultiplePrimaryKey() throws Exception {
        execute("create table quotes (id integer primary key, author string primary key, " +
                "quote string) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into quotes (id, author, quote) values (?, ?, ?), (?, ?, ?)",
                new Object[]{1, "Ford", "I'd far rather be happy than right any day.",
                        1, "Douglas", "Don't panic"}
        );
        assertEquals(2L, response.rowCount());
        refresh();

        execute("delete from quotes where id=1 and author='Ford'");
        assertEquals(1L, response.rowCount());
        refresh();

        execute("select quote from quotes where id=1");
        assertEquals(1L, response.rowCount());
    }

    @Test
    public void testDeleteByQueryWithMultiplePrimaryKey() throws Exception {
        execute("create table quotes (id integer primary key, author string primary key, " +
                "quote string) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into quotes (id, author, quote) values (?, ?, ?), (?, ?, ?)",
                new Object[]{1, "Ford", "I'd far rather be happy than right any day.",
                        1, "Douglas", "Don't panic"}
        );
        assertEquals(2L, response.rowCount());
        refresh();

        execute("delete from quotes where id=1");

        assertEquals(-1L, response.rowCount());
        refresh();

        execute("select quote from quotes where id=1");
        assertEquals(0L, response.rowCount());
    }

    @Test
    public void testSelectWhereBoolean() {
        execute("create table a (v boolean)");
        ensureYellow();

        execute("insert into a values (true)");
        execute("insert into a values (true)");
        execute("insert into a values (true)");
        execute("insert into a values (false)");
        execute("insert into a values (false)");
        refresh();

        execute("select v from a where v");
        assertEquals(3L, response.rowCount());

        execute("select v from a where not v");
        assertEquals(2L, response.rowCount());

        execute("select v from a where v or not v");
        assertEquals(5L, response.rowCount());

        execute("select v from a where v and not v");
        assertEquals(0L, response.rowCount());
    }

    @Test
    public void testSelectWhereBooleanPK() {
        execute("create table b (v boolean primary key) clustered by (v)");
        ensureYellow();

        execute("insert into b values (true)");
        execute("insert into b values (false)");
        refresh();

        execute("select v from b where v");
        assertEquals(1L, response.rowCount());

        execute("select v from b where not v");
        assertEquals(1L, response.rowCount());

        execute("select v from b where v or not v");
        assertEquals(2L, response.rowCount());

        execute("select v from b where v and not v");
        assertEquals(0L, response.rowCount());
    }



    @Test
    public void testBulkOperations() throws Exception {
        execute("create table test (id integer primary key, name string) with (number_of_replicas = 0)");
        ensureYellow();
        SQLBulkResponse bulkResp = execute("insert into test (id, name) values (?, ?), (?, ?)",
                new Object[][] {
                        {1, "Earth", 2, "Saturn"},    
                        {3, "Moon", 4, "Mars"}        
                });
        assertThat(bulkResp.results().length, is(4));
        for (SQLBulkResponse.Result result : bulkResp.results()) {
            assertThat(result.rowCount(), is(1L));
        }
        refresh();

        bulkResp = execute("insert into test (id, name) values (?, ?), (?, ?)",
            new Object[][] {
                {1, "Earth", 2, "Saturn"},    
                {3, "Moon", 4, "Mars"}        
            });
        assertThat(bulkResp.results().length, is(4));
        for (SQLBulkResponse.Result result : bulkResp.results()) {
            assertThat(result.rowCount(), is(-2L));
        }

        execute("select name from test order by id asc");
        assertEquals("Earth\nSaturn\nMoon\nMars\n", TestingHelpers.printedTable(response.rows()));


        bulkResp = execute("update test set name = concat(name, '-updated') where id = ?", new Object[][]{
                new Object[]{2},
                new Object[]{3},
                new Object[]{4},
        });
        assertThat(bulkResp.results().length, is(3));
        for (SQLBulkResponse.Result result : bulkResp.results()) {
            assertThat(result.rowCount(), is(1L));
        }
        refresh();

        execute("select count(*) from test where name like '%-updated'");
        assertThat((Long) response.rows()[0][0], is(3L));


        bulkResp = execute("delete from test where id = ?", new Object[][] {
                new Object[] { 1 },
                new Object[] { 3 }
        });
        assertThat(bulkResp.results().length, is(2));
        for (SQLBulkResponse.Result result : bulkResp.results()) {
            assertThat(result.rowCount(), is(1L));
        }
        refresh();

        execute("select count(*) from test");
        assertThat((Long) response.rows()[0][0], is(2L));


        bulkResp = execute("delete from test where name = ?", new Object[][] {
                new Object[] { "Saturn-updated" },
                new Object[] { "Mars-updated" }
        });
        assertThat(bulkResp.results().length, is(2));
        for (SQLBulkResponse.Result result : bulkResp.results()) {
            assertThat(result.rowCount(), is(-1L));
        }
        refresh();

        execute("select count(*) from test");
        assertThat((Long) response.rows()[0][0], is(0L));

    }

    @Test
    public void testSelectFormatFunction() throws Exception {
        this.setup.setUpLocations();
        ensureYellow();
        refresh();

        execute("select format('%s is a %s', name, kind) as sentence from locations order by name");
        assertThat(response.rowCount(), is(13L));
        assertArrayEquals(response.cols(), new String[]{"sentence"});
        assertThat(response.rows()[0].length, is(1));
        assertThat((String) response.rows()[0][0], is(" is a Planet"));
        assertThat((String) response.rows()[1][0], is("Aldebaran is a Star System"));
        assertThat((String) response.rows()[2][0], is("Algol is a Star System"));


    }

    @Test
    public void testAnyArray() throws Exception {
        this.setup.setUpArrayTables();

        execute("select count(*) from any_table where 'Berlin' = ANY (names)");
        assertThat((Long) response.rows()[0][0], is(2L));

        execute("select id, names from any_table where 'Berlin' = ANY (names) order by id");
        assertThat(response.rowCount(), is(2L));
        assertThat((Integer) response.rows()[0][0], is(1));
        assertThat((Integer) response.rows()[1][0], is(3));

        execute("select id from any_table where 'Berlin' != ANY (names) order by id");
        assertThat(response.rowCount(), is(3L));
        assertThat((Integer) response.rows()[0][0], is(1));
        assertThat((Integer) response.rows()[1][0], is(2));
        assertThat((Integer) response.rows()[2][0], is(3));

        execute("select count(id) from any_table where 0.0 < ANY (temps)");
        assertThat((Long) response.rows()[0][0], is(2L));

        execute("select id, names from any_table where 0.0 < ANY (temps) order by id");
        assertThat(response.rowCount(), is(2L));
        assertThat((Integer) response.rows()[0][0], is(2));
        assertThat((Integer) response.rows()[1][0], is(3));

        execute("select count(*) from any_table where 0.0 > ANY (temps)");
        assertThat((Long) response.rows()[0][0], is(2L));

        execute("select id, names from any_table where 0.0 > ANY (temps) order by id");
        assertThat(response.rowCount(), is(2L));
        assertThat((Integer) response.rows()[0][0], is(2));
        assertThat((Integer) response.rows()[1][0], is(3));

        execute("select id, names from any_table where 'Ber%' LIKE ANY (names) order by id");
        assertThat(response.rowCount(), is(2L));
        assertThat((Integer) response.rows()[0][0], is(1));
        assertThat((Integer) response.rows()[1][0], is(3));

    }

    @Test
    public void testNotAnyArray() throws Exception {
        this.setup.setUpArrayTables();

        execute("select id from any_table where NOT 'Hangelsberg' = ANY (names) order by id");
        assertThat(response.rowCount(), is(3L));
        assertThat((Integer) response.rows()[0][0], is(1));
        assertThat((Integer) response.rows()[1][0], is(2));
        assertThat((Integer) response.rows()[2][0], is(4)); 

        execute("select id from any_table where 'Hangelsberg' != ANY (names) order by id");
        assertThat(response.rowCount(), is(3L));
        assertThat((Integer) response.rows()[0][0], is(1));
        assertThat((Integer) response.rows()[1][0], is(2));
        assertThat((Integer) response.rows()[2][0], is(3));
    }

    @Test
    public void testAnyLike() throws Exception {
        this.setup.setUpArrayTables();

        execute("select id from any_table where 'kuh%' LIKE ANY (tags) order by id");
        assertThat(response.rowCount(), is(2L));
        assertThat((Integer) response.rows()[0][0], is(3));
        assertThat((Integer) response.rows()[1][0], is(4));

        execute("select id from any_table where 'kuh%' NOT LIKE ANY (tags) order by id");
        assertThat(response.rowCount(), is(3L));
        assertThat((Integer) response.rows()[0][0], is(1));
        assertThat((Integer) response.rows()[1][0], is(2));
        assertThat((Integer) response.rows()[2][0], is(3));

    }

    @Test
    public void testInsertAndSelectIpType() throws Exception {
        execute("create table ip_table (fqdn string, addr ip) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into ip_table (fqdn, addr) values ('localhost', '127.0.0.1'), ('crate.io', '23.235.33.143')");
        execute("refresh table ip_table");

        execute("select addr from ip_table where addr = '23.235.33.143'");
        assertThat(response.rowCount(), is(1L));
        assertThat((String) response.rows()[0][0], is("23.235.33.143"));

        execute("select addr from ip_table where addr > '127.0.0.1'");
        assertThat(response.rowCount(), is(0L));
        execute("select addr from ip_table where addr > 2130706433"); 
        assertThat(response.rowCount(), is(0L));

        execute("select addr from ip_table where addr < '127.0.0.1'");
        assertThat(response.rowCount(), is(1L));
        assertThat((String) response.rows()[0][0], is("23.235.33.143"));
        execute("select addr from ip_table where addr < 2130706433"); 
        assertThat(response.rowCount(), is(1L));
        assertThat((String) response.rows()[0][0], is("23.235.33.143"));

        execute("select addr from ip_table where addr <= '127.0.0.1'");
        assertThat(response.rowCount(), is(2L));

        execute("select addr from ip_table where addr >= '23.235.33.143'");
        assertThat(response.rowCount(), is(2L));

        execute("select addr from ip_table where addr IS NULL");
        assertThat(response.rowCount(), is(0L));

        execute("select addr from ip_table where addr IS NOT NULL");
        assertThat(response.rowCount(), is(2L));

    }

    @Test
    public void testGroupByOnIpType() throws Exception {
        execute("create table t (i ip) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into t (i) values ('192.168.1.2'), ('192.168.1.2'), ('192.168.1.3')");
        execute("refresh table t");
        execute("select i, count(*) from t group by 1 order by count(*) desc");

        assertThat(response.rowCount(), is(2L));
        assertThat((String) response.rows()[0][0], is("192.168.1.2"));
        assertThat((Long) response.rows()[0][1], is(2L));
        assertThat((String) response.rows()[1][0], is("192.168.1.3"));
        assertThat((Long) response.rows()[1][1], is(1L));
    }

    @Test
    public void testDeleteOnIpType() throws Exception {
        execute("create table ip_table (fqdn string, addr ip) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into ip_table (fqdn, addr) values ('localhost', '127.0.0.1'), ('crate.io', '23.235.33.143')");
        execute("refresh table ip_table");
        execute("delete from ip_table where addr = '127.0.0.1'");
        assertThat(response.rowCount(), is(-1L));
        execute("select addr from ip_table");
        assertThat(response.rowCount(), is(1L));
        assertThat((String) response.rows()[0][0], is("23.235.33.143"));
    }

    @Test
    public void testInsertAndSelectGeoType() throws Exception {
        execute("create table geo_point_table (id int primary key, p geo_point) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into geo_point_table (id, p) values (?, ?)", new Object[]{1, new Double[]{47.22, 12.09}});
        execute("insert into geo_point_table (id, p) values (?, ?)", new Object[]{2, new Double[]{57.22, 7.12}});
        refresh();

        execute("select p from geo_point_table order by id desc");

        assertThat(response.rowCount(), is(2L));
        assertThat(((Double[]) response.rows()[0][0]), Matchers.arrayContaining(57.22, 7.12));
        assertThat(((Double[]) response.rows()[1][0]), Matchers.arrayContaining(47.22, 12.09));
    }

    @Test
    public void testGeoTypeQueries() throws Exception {

        execute("create table t (id int primary key, i int, p geo_point) " +
                "clustered into 1 shards " +
                "with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into t (id, i, p) values (1, 1, 'POINT (10 20)')");
        execute("insert into t (id, i, p) values (2, 1, 'POINT (11 21)')");
        refresh();


        execute("select distance(p, 'POINT (11 21)') from t order by 1");
        assertThat(response.rowCount(), is(2L));

        Double result1 = (Double) response.rows()[0][0];
        Double result2 = (Double) response.rows()[1][0];

        assertThat(result1, is(0.0d));
        assertThat(result2, is(152462.70754934277));

        String stmtOrderBy = "SELECT id " +
                "FROM t " +
                "ORDER BY distance(p, 'POINT(30.0 30.0)')";
        execute(stmtOrderBy);
        assertThat(response.rowCount(), is(2L));
        String expectedOrderBy =
                "2\n" +
                        "1\n";
        assertEquals(expectedOrderBy, TestingHelpers.printedTable(response.rows()));


        String stmtAggregate = "SELECT i, max(distance(p, 'POINT(30.0 30.0)')) " +
                "FROM t " +
                "GROUP BY i";
        execute(stmtAggregate);
        assertThat(response.rowCount(), is(1L));
        String expectedAggregate = "1| 2297790.338709135\n";
        assertEquals(expectedAggregate, TestingHelpers.printedTable(response.rows()));


        execute("select p from t where distance(p, 'POINT (11 21)') > 0.0");
        Double[] row = Arrays.copyOf((Object[])response.rows()[0][0], 2, Double[].class);
        assertThat(row[0], is(10.0d));
        assertThat(row[1], is(20.0d));

        execute("select p from t where distance(p, 'POINT (11 21)') < 10.0");
        row = Arrays.copyOf((Object[])response.rows()[0][0], 2, Double[].class);
        assertThat(row[0], is(11.0d));
        assertThat(row[1], is(21.0d));

        execute("select p from t where distance(p, 'POINT (11 21)') < 10.0 or distance(p, 'POINT (11 21)') > 10.0");
        assertThat(response.rowCount(), is(2L));

        execute("select p from t where distance(p, 'POINT (10 20)') = 0");
        assertThat(response.rowCount(), is(1L));
    }

    @Test
    public void testWithinQuery() throws Exception {
        execute("create table t (id int primary key, p geo_point) " +
                "clustered into 1 shards " +
                "with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into t (id, p) values (1, 'POINT (10 10)')");
        refresh();

        execute("select within(p, 'POLYGON (( 5 5, 30 5, 30 30, 5 30, 5 5 ))') from t");
        assertThat((Boolean) response.rows()[0][0], is(true));

        execute("select * from t where within(p, 'POLYGON (( 5 5, 30 5, 30 30, 5 30, 5 5 ))')");
        assertThat(response.rowCount(), is(1L));
        execute("select * from t where within(p, 'POLYGON (( 5 5, 30 5, 30 30, 5 35, 5 5 ))') = false");
        assertThat(response.rowCount(), is(0L));
    }

    @Test
    public void testTwoSubStrOnSameColumn() throws Exception {
        this.setup.groupBySetup();
        execute("select name, substr(name, 4, 4), substr(name, 3, 5) from sys.nodes order by name");
        assertThat((String) response.rows()[0][0], is("node_s0"));
        assertThat((String) response.rows()[0][1], is("e_s0"));
        assertThat((String) response.rows()[0][2], is("de_s0"));
        assertThat((String) response.rows()[1][0], is("node_s1"));
        assertThat((String) response.rows()[1][1], is("e_s1"));
        assertThat((String) response.rows()[1][2], is("de_s1"));

    }


    @Test
    public void testSelectArithMetricOperatorInOrderBy() throws Exception {
        execute("create table t (i integer, l long, d double) clustered into 3 shards with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into t (i, l, d) values (1, 2, 90.5), (2, 5, 90.5), (193384, 31234594433, 99.0), (10, 21, 99.0), (-1, 4, 99.0)");
        refresh();

        execute("select i, i%3 from t order by i%3, l");
        assertThat(response.rowCount(), is(5L));
        assertThat(TestingHelpers.printedTable(response.rows()), is(
                "-1| -1\n" +
                        "1| 1\n" +
                        "10| 1\n" +
                        "193384| 1\n" +
                        "2| 2\n"));
    }

    @Test
    public void testSelectFailingSearchScript() throws Exception {
        expectedException.expect(SQLActionException.class);
        expectedException.expectMessage("log(x, b): given arguments would result in: 'NaN'");

        execute("create table t (i integer, l long, d double) clustered into 1 shards with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into t (i, l, d) values (1, 2, 90.5)");
        refresh();

        execute("select log(d, l) from t where log(d, -1) >= 0");
    }

    @Test
    public void testSelectGroupByFailingSearchScript() throws Exception {
        expectedException.expect(SQLActionException.class);
        expectedException.expectMessage("log(x, b): given arguments would result in: 'NaN'");

        execute("create table t (i integer, l long, d double) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into t (i, l, d) values (1, 2, 90.5), (0, 4, 100)");
        execute("refresh table t");

        execute("select log(d, l) from t where log(d, -1) >= 0 group by log(d, l)");
    }

    @Test
    public void testNumericScriptOnAllTypes() throws Exception {

        execute("create table t (b byte, s short, i integer, l long, f float, d double, t timestamp) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into t (b, s, i, l, f, d, t) values (1, 2, 3, 4, 5.7, 6.3, '2014-07-30')");
        refresh();

        String[] functionCalls = new String[]{
                "abs(%s)",
                "ceil(%s)",
                "floor(%s)",
                "ln(%s)",
                "log(%s)",
                "log(%s, 2)",
                "random()",
                "round(%s)",
                "sqrt(%s)"
        };

        for (String functionCall : functionCalls) {
            String byteCall = String.format(Locale.ENGLISH, functionCall, "b");
            execute(String.format(Locale.ENGLISH, "select %s, b from t where %s < 2", byteCall, byteCall));

            String shortCall = String.format(Locale.ENGLISH, functionCall, "s");
            execute(String.format(Locale.ENGLISH, "select %s, s from t where %s < 2", shortCall, shortCall));

            String intCall = String.format(Locale.ENGLISH, functionCall, "i");
            execute(String.format(Locale.ENGLISH, "select %s, i from t where %s < 2", intCall, intCall));

            String longCall = String.format(Locale.ENGLISH, functionCall, "l");
            execute(String.format(Locale.ENGLISH, "select %s, l from t where %s < 2", longCall, longCall));

            String floatCall = String.format(Locale.ENGLISH, functionCall, "f");
            execute(String.format(Locale.ENGLISH, "select %s, f from t where %s < 2", floatCall, floatCall));

            String doubleCall = String.format(Locale.ENGLISH, functionCall, "d");
            execute(String.format(Locale.ENGLISH, "select %s, d from t where %s < 2", doubleCall, doubleCall));
        }
    }

    @Test
    public void testMatchNotOnSubColumn() throws Exception {
        execute("create table matchbox (" +
                "  s string index using fulltext with (analyzer='german')," +
                "  o object as (" +
                "    s string index using fulltext with (analyzer='german')," +
                "    m string index using fulltext with (analyzer='german')" +
                "  )," +
                "  o_ignored object(ignored)" +
                ") with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into matchbox (s, o) values ('Arthur Dent', {s='Zaphod Beeblebroox', m='Ford Prefect'})");
        refresh();
        execute("select * from matchbox where match(s, 'Arthur')");
        assertThat(response.rowCount(), is(1L));

        execute("select * from matchbox where match(o['s'], 'Arthur')");
        assertThat(response.rowCount(), is(0L));

        execute("select * from matchbox where match(o['s'], 'Zaphod')");
        assertThat(response.rowCount(), is(1L));

        execute("select * from matchbox where match(s, 'Zaphod')");
        assertThat(response.rowCount(), is(0L));

        execute("select * from matchbox where match(o['m'], 'Ford')");
        assertThat(response.rowCount(), is(1L));

        expectedException.expect(SQLActionException.class);
        expectedException.expectMessage("Can only use MATCH on columns of type STRING, not on 'null'");

        execute("select * from matchbox where match(o_ignored['a'], 'Ford')");
        assertThat(response.rowCount(), is(0L));
    }

    @Test
    public void testSelectWhereMultiColumnMatchDifferentTypesDifferentScore() throws Exception {
        this.setup.setUpLocations();
        refresh();
        execute("select name, description, kind, _score from locations " +
                "where match((kind, name_description_ft 0.5), 'Planet earth') using most_fields with (analyzer='english') order by _score desc");
        assertThat(response.rowCount(), is(5L));
        assertThat(TestingHelpers.printedTable(response.rows()),
                is("Alpha Centauri| 4.1 light-years northwest of earth| Star System| 0.049483635\n" +
                        "| This Planet doesn't really exist| Planet| 0.04724514\nAllosimanius Syneca| Allosimanius Syneca is a planet noted for ice, snow, mind-hurtling beauty and stunning cold.| Planet| 0.021473126\n" +
                        "Bartledan| An Earthlike planet on which Arthur Dent lived for a short time, Bartledan is inhabited by Bartledanians, a race that appears human but only physically.| Planet| 0.018788986\n" +
                        "Galactic Sector QQ7 Active J Gamma| Galactic Sector QQ7 Active J Gamma contains the Sun Zarss, the planet Preliumtarn of the famed Sevorbeupstry and Quentulus Quazgar Mountains.| Galaxy| 0.017716927\n"));

        execute("select name, description, kind, _score from locations " +
                "where match((kind, name_description_ft 0.5), 'Planet earth') using cross_fields order by _score desc");
        assertThat(response.rowCount(), is(5L));
        assertThat(TestingHelpers.printedTable(response.rows()),
                is("Alpha Centauri| 4.1 light-years northwest of earth| Star System| 0.06658964\n" +
                        "| This Planet doesn't really exist| Planet| 0.06235056\n" +
                        "Allosimanius Syneca| Allosimanius Syneca is a planet noted for ice, snow, mind-hurtling beauty and stunning cold.| Planet| 0.02889618\n" +
                        "Bartledan| An Earthlike planet on which Arthur Dent lived for a short time, Bartledan is inhabited by Bartledanians, a race that appears human but only physically.| Planet| 0.025284158\n" +
                        "Galactic Sector QQ7 Active J Gamma| Galactic Sector QQ7 Active J Gamma contains the Sun Zarss, the planet Preliumtarn of the famed Sevorbeupstry and Quentulus Quazgar Mountains.| Galaxy| 0.02338146\n"));
    }

    @Test
    public void testSimpleMatchWithBoost() throws Exception {
        execute("create table characters ( " +
                "  id int primary key, " +
                "  name string, " +
                "  quote string, " +
                "  INDEX name_ft using fulltext(name) with (analyzer = 'english'), " +
                "  INDEX quote_ft using fulltext(quote) with (analyzer = 'english') " +
                ") clustered into 5 shards ");
        ensureYellow();
        execute("insert into characters (id, name, quote) values (?, ?, ?)", new Object[][]{
                new Object[]{1, "Arthur", "It's terribly small, tiny little country."},
                new Object[]{2, "Trillian", " No, it's a country. Off the coast of Africa."},
                new Object[]{3, "Marvin", " It won't work, I have an exceptionally large mind." }
        });
        refresh();
        execute("select characters.name AS characters_name, _score " +
                "from characters " +
                "where match(characters.quote_ft 1.0, 'country') order by _score desc");
        assertThat(response.rows().length, is(2));
        assertThat((String) response.rows()[0][0], is("Trillian"));
        assertThat((float) response.rows()[0][1], is(0.15342641f));
        assertThat((String) response.rows()[1][0], is("Arthur"));
        assertThat((float) response.rows()[1][1], is(0.13424811f));
    }

    @Test
    public void testMatchTypes() throws Exception {
        this.setup.setUpLocations();
        refresh();
        execute("select name, _score from locations where match((kind 0.8, name_description_ft 0.6), 'planet earth') using best_fields order by _score desc");
        assertThat(TestingHelpers.printedTable(response.rows()),
                is("Alpha Centauri| 0.22184466\n| 0.21719791\nAllosimanius Syneca| 0.09626817\nBartledan| 0.08423465\nGalactic Sector QQ7 Active J Gamma| 0.08144922\n"));

        execute("select name, _score from locations where match((kind 0.6, name_description_ft 0.8), 'planet earth') using most_fields order by _score desc");
        assertThat(TestingHelpers.printedTable(response.rows()),
                is("Alpha Centauri| 0.12094267\n| 0.1035407\nAllosimanius Syneca| 0.05248235\nBartledan| 0.045922056\nGalactic Sector QQ7 Active J Gamma| 0.038827762\n"));

        execute("select name, _score from locations where match((kind 0.4, name_description_ft 1.0), 'planet earth') using cross_fields order by _score desc");
        assertThat(TestingHelpers.printedTable(response.rows()),
                is("Alpha Centauri| 0.14147125\n| 0.116184436\nAllosimanius Syneca| 0.061390605\nBartledan| 0.05371678\nGalactic Sector QQ7 Active J Gamma| 0.043569162\n"));

        execute("select name, _score from locations where match((kind 1.0, name_description_ft 0.4), 'Alpha Centauri') using phrase");
        assertThat(TestingHelpers.printedTable(response.rows()),
                is("Alpha Centauri| 0.94653714\n"));

        execute("select name, _score from locations where match(name_description_ft, 'Alpha Centauri') using phrase_prefix");
        assertThat(TestingHelpers.printedTable(response.rows()),
                is("Alpha Centauri| 1.5739591\n"));
    }

    @Test
    public void testMatchOptions() throws Exception {
        this.setup.setUpLocations();
        refresh();

        execute("select name, _score from locations where match((kind, name_description_ft), 'galaxy') " +
                "using best_fields with (analyzer='english') order by _score desc");
        assertThat(TestingHelpers.printedTable(response.rows()),
                is("End of the Galaxy| 0.7260999\nAltair| 0.2895972\nNorth West Ripple| 0.25339755\nOuter Eastern Rim| 0.2246257\n"));

        execute("select name, _score from locations where match((kind, name_description_ft), 'galaxy') " +
                "using best_fields with (fuzziness=0.5) order by _score desc");
        assertThat(TestingHelpers.printedTable(response.rows()),
                is("Outer Eastern Rim| 1.4109559\nEnd of the Galaxy| 1.4109559\nNorth West Ripple| 1.2808706\nGalactic Sector QQ7 Active J Gamma| 1.2808706\nAltair| 0.3842612\nAlgol| 0.25617412\n"));

        execute("select name, _score from locations where match((kind, name_description_ft), 'gala') " +
                "using best_fields with (operator='or', minimum_should_match=2) order by _score desc");
        assertThat(TestingHelpers.printedTable(response.rows()),
                is(""));

        execute("select name, _score from locations where match((kind, name_description_ft), 'gala') " +
                "using phrase_prefix with (slop=1) order by _score desc");
        assertThat(TestingHelpers.printedTable(response.rows()),
                is("End of the Galaxy| 0.75176084\nOuter Eastern Rim| 0.5898516\nGalactic Sector QQ7 Active J Gamma| 0.34636837\nAlgol| 0.32655922\nAltair| 0.32655922\nNorth West Ripple| 0.2857393\n"));

        execute("select name, _score from locations where match((kind, name_description_ft), 'galaxy') " +
                "using phrase with (tie_breaker=2.0) order by _score desc");
        assertThat(TestingHelpers.printedTable(response.rows()),
                is("End of the Galaxy| 0.4618873\nAltair| 0.18054473\nNorth West Ripple| 0.15797664\nOuter Eastern Rim| 0.1428891\n"));

        execute("select name, _score from locations where match((kind, name_description_ft), 'galaxy') " +
                "using best_fields with (zero_terms_query='all') order by _score desc");
        assertThat(TestingHelpers.printedTable(response.rows()),
                is("End of the Galaxy| 0.7260999\nAltair| 0.2895972\nNorth West Ripple| 0.25339755\nOuter Eastern Rim| 0.2246257\n"));
    }

    @Test
    public void testWhereColumnEqColumnAndFunctionEqFunction() throws Exception {
        this.setup.setUpLocations();
        ensureYellow();
        refresh();

        execute("select name from locations where name = name");
        assertThat(response.rowCount(), is(13L));

        execute("select name from locations where substr(name, 1, 1) = substr(name, 1, 1)");
        assertThat(response.rowCount(), is(13L));
    }

    @Test
    public void testNewColumn() throws Exception {
        execute("create table t (name string) with (number_of_replicas=0)");
        ensureYellow();
        execute("insert into t (name, score) values ('Ford', 1.2)");
    }

    @Test
    public void testESGetSourceColumns() throws Exception {
        this.setup.setUpLocations();
        ensureYellow();
        refresh();

        execute("select _id, _version from locations where id=2");
        assertNotNull(response.rows()[0][0]);
        assertNotNull(response.rows()[0][1]);

        execute("select _id, name from locations where id=2");
        assertNotNull(response.rows()[0][0]);
        assertNotNull(response.rows()[0][1]);

        execute("select _id, _doc from locations where id=2");
        assertNotNull(response.rows()[0][0]);
        assertNotNull(response.rows()[0][1]);

        execute("select _doc, id from locations where id in (2,3) order by id");
        assertEquals(TestingHelpers.printedTable(response.rows()), "{date=308534400000, race=null, kind=Galaxy, " +
                "name=Outer Eastern Rim, description=The Outer Eastern Rim of the Galaxy where the Guide has " +
                "supplanted the Encyclopedia Galactica among its more relaxed civilisations., id=2, position=2}| 2\n" +
                "{date=1367366400000, race=null, kind=Galaxy, name=Galactic Sector QQ7 Active J Gamma, "+
                "description=Galactic Sector QQ7 Active J Gamma contains the Sun Zarss, the planet Preliumtarn of " +
                "the famed Sevorbeupstry and Quentulus Quazgar Mountains., id=3, position=4}| 3\n");

        execute("select name, kind from locations where id in (2,3) order by id");
        assertEquals(TestingHelpers.printedTable(response.rows()), "Outer Eastern Rim| Galaxy\n" +
                "Galactic Sector QQ7 Active J Gamma| Galaxy\n");

        execute("select name, kind, _id from locations where id in (2,3) order by id");
        assertEquals(TestingHelpers.printedTable(response.rows()), "Outer Eastern Rim| Galaxy| 2\n" +
                "Galactic Sector QQ7 Active J Gamma| Galaxy| 3\n");

        execute("select _raw, id from locations where id in (2,3) order by id");
        assertEquals(TestingHelpers.printedTable(response.rows()), "{\"id\":\"2\",\"name\":\"Outer Eastern Rim\"," +
                "\"date\":308534400000,\"kind\":\"Galaxy\",\"position\":2,\"description\":\"The Outer Eastern Rim " +
                "of the Galaxy where the Guide has supplanted the Encyclopedia Galactica among its more relaxed " +
                "civilisations.\",\"race\":null}| 2\n" +
                "{\"id\":\"3\",\"name\":\"Galactic Sector QQ7 Active J Gamma\",\"date\":1367366400000," +
                "\"kind\":\"Galaxy\",\"position\":4,\"description\":\"Galactic Sector QQ7 Active J Gamma contains " +
                "the Sun Zarss, the planet Preliumtarn of the famed Sevorbeupstry and Quentulus Quazgar Mountains." +
                "\",\"race\":null}| 3\n");
    }
}



<code block>


package io.crate.integrationtests;

import com.carrotsearch.hppc.LongArrayList;
import com.google.common.base.Predicates;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.Iterables;
import com.google.common.util.concurrent.FutureCallback;
import com.google.common.util.concurrent.Futures;
import com.google.common.util.concurrent.ListenableFuture;
import io.crate.action.job.ContextPreparer;
import io.crate.analyze.Analysis;
import io.crate.analyze.Analyzer;
import io.crate.analyze.ParameterContext;
import io.crate.analyze.WhereClause;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.core.collections.Bucket;
import io.crate.core.collections.Row;
import io.crate.executor.Job;
import io.crate.executor.TaskResult;
import io.crate.executor.transport.NodeFetchRequest;
import io.crate.executor.transport.NodeFetchResponse;
import io.crate.executor.transport.TransportExecutor;
import io.crate.executor.transport.TransportFetchNodeAction;
import io.crate.jobs.JobContextService;
import io.crate.jobs.JobExecutionContext;
import io.crate.metadata.ColumnIdent;
import io.crate.metadata.Functions;
import io.crate.metadata.ReferenceInfo;
import io.crate.metadata.doc.DocSchemaInfo;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.fetch.RowInputSymbolVisitor;
import io.crate.planner.Plan;
import io.crate.planner.Planner;
import io.crate.planner.RowGranularity;
import io.crate.planner.consumer.ConsumerContext;
import io.crate.planner.consumer.QueryThenFetchConsumer;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.dql.QueryThenFetch;
import io.crate.planner.projection.FetchProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.sql.parser.SqlParser;
import io.crate.types.DataType;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.action.ActionListener;
import org.elasticsearch.test.ElasticsearchIntegrationTest;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;

import java.util.*;
import java.util.concurrent.CountDownLatch;

import static org.hamcrest.Matchers.*;
import static org.hamcrest.core.Is.is;

@ElasticsearchIntegrationTest.ClusterScope(numDataNodes = 2)
public class FetchOperationIntegrationTest extends SQLTransportIntegrationTest {

    Setup setup = new Setup(sqlExecutor);
    TransportExecutor executor;
    DocSchemaInfo docSchemaInfo;

    @Before
    public void transportSetUp() {
        executor = internalCluster().getInstance(TransportExecutor.class);
        docSchemaInfo = internalCluster().getInstance(DocSchemaInfo.class);
    }

    @After
    public void transportTearDown() {
        executor = null;
        docSchemaInfo = null;
    }

    private void setUpCharacters() {
        sqlExecutor.exec("create table characters (id int primary key, name string) " +
                "clustered into 2 shards with(number_of_replicas=0)");
        sqlExecutor.ensureYellowOrGreen();
        sqlExecutor.exec("insert into characters (id, name) values (?, ?)",
                new Object[][]{
                        new Object[]{1, "Arthur"},
                        new Object[]{2, "Ford"},
                }
        );
        sqlExecutor.refresh("characters");
    }

    private Plan analyzeAndPlan(String stmt) {
        Analysis analysis = analyze(stmt);
        Planner planner = internalCluster().getInstance(Planner.class);
        return planner.plan(analysis);
    }

    private Analysis analyze(String stmt) {
        Analyzer analyzer = internalCluster().getInstance(Analyzer.class);
        return analyzer.analyze(
                SqlParser.createStatement(stmt),
                new ParameterContext(new Object[0], new Object[0][], null)
        );
    }

    private CollectNode createCollectNode(Planner.Context plannerContext, boolean keepContextForFetcher) {
        TableInfo tableInfo = docSchemaInfo.getTableInfo("characters");

        ReferenceInfo docIdRefInfo = tableInfo.getReferenceInfo(new ColumnIdent("_docid"));
        Symbol docIdRef = new Reference(docIdRefInfo);
        List<Symbol> toCollect = ImmutableList.of(docIdRef);

        List<DataType> outputTypes = new ArrayList<>(toCollect.size());
        for (Symbol symbol : toCollect) {
            outputTypes.add(symbol.valueType());
        }
        CollectNode collectNode = new CollectNode(
                plannerContext.nextExecutionNodeId(),
                "collect",
                tableInfo.getRouting(WhereClause.MATCH_ALL, null));
        collectNode.toCollect(toCollect);
        collectNode.outputTypes(outputTypes);
        collectNode.maxRowGranularity(RowGranularity.DOC);
        collectNode.keepContextForFetcher(keepContextForFetcher);
        collectNode.jobId(UUID.randomUUID());
        plannerContext.allocateJobSearchContextIds(collectNode.routing());

        return collectNode;
    }

    private List<Bucket> getBuckets(CollectNode collectNode) throws InterruptedException, java.util.concurrent.ExecutionException {
        List<Bucket> results = new ArrayList<>();
        for (String nodeName : internalCluster().getNodeNames()) {
            ContextPreparer contextPreparer = internalCluster().getInstance(ContextPreparer.class, nodeName);
            JobContextService contextService = internalCluster().getInstance(JobContextService.class, nodeName);

            JobExecutionContext.Builder builder = contextService.newBuilder(collectNode.jobId());
            ListenableFuture<Bucket> future = contextPreparer.prepare(collectNode.jobId(), collectNode, builder);
            assert future != null;

            JobExecutionContext context = contextService.createContext(builder);
            context.start();
            results.add(future.get());
        }
        return results;
    }

    @Test
    public void testCollectDocId() throws Exception {
        setUpCharacters();
        Planner.Context plannerContext = new Planner.Context(clusterService());
        CollectNode collectNode = createCollectNode(plannerContext, false);

        List<Bucket> results = getBuckets(collectNode);

        assertThat(results.size(), is(2));
        int seenJobSearchContextId = -1;
        for (Bucket rows : results) {
            assertThat(rows.size(), is(1));
            Object docIdCol = rows.iterator().next().get(0);
            assertNotNull(docIdCol);
            assertThat(docIdCol, instanceOf(Long.class));
            long docId = (long)docIdCol;

            int jobSearchContextId = (int)(docId >> 32);
            int doc = (int)docId;
            assertThat(doc, is(0));
            assertThat(jobSearchContextId, greaterThan(-1));
            if (seenJobSearchContextId == -1) {
                assertThat(jobSearchContextId, anyOf(is(0), is(1)));
                seenJobSearchContextId = jobSearchContextId;
            } else {
                assertThat(jobSearchContextId, is(seenJobSearchContextId == 0 ? 1 : 0));
            }
        }
    }

    @Test
    public void testFetchAction() throws Exception {
        setUpCharacters();

        Analysis analysis = analyze("select id, name from characters");
        QueryThenFetchConsumer queryThenFetchConsumer = internalCluster().getInstance(QueryThenFetchConsumer.class);
        Planner.Context plannerContext = new Planner.Context(clusterService());
        ConsumerContext consumerContext = new ConsumerContext(analysis.rootRelation(), plannerContext);
        QueryThenFetch plan = (QueryThenFetch) queryThenFetchConsumer.consume(analysis.rootRelation(), consumerContext).plan();

        UUID jobId = UUID.randomUUID();
        plan.collectNode().jobId(jobId);

        List<Bucket> results = getBuckets(plan.collectNode());


        TransportFetchNodeAction transportFetchNodeAction = internalCluster().getInstance(TransportFetchNodeAction.class);


        Map<String, LongArrayList> jobSearchContextDocIds = new HashMap<>();
        for (Bucket rows : results) {
            long docId = (long)rows.iterator().next().get(0);

            int jobSearchContextId = (int)(docId >> 32);
            String nodeId = plannerContext.nodeId(jobSearchContextId);
            LongArrayList docIdsPerNode = jobSearchContextDocIds.get(nodeId);
            if (docIdsPerNode == null) {
                docIdsPerNode = new LongArrayList();
                jobSearchContextDocIds.put(nodeId, docIdsPerNode);
            }
            docIdsPerNode.add(docId);
        }

        Iterable<Projection> projections = Iterables.filter(plan.mergeNode().projections(), Predicates.instanceOf(FetchProjection.class));
        FetchProjection fetchProjection = (FetchProjection )Iterables.getOnlyElement(projections);
        RowInputSymbolVisitor rowInputSymbolVisitor = new RowInputSymbolVisitor(internalCluster().getInstance(Functions.class));
        RowInputSymbolVisitor.Context context = rowInputSymbolVisitor.extractImplementations(fetchProjection.outputSymbols());

        final CountDownLatch latch = new CountDownLatch(jobSearchContextDocIds.size());
        final List<Row> rows = new ArrayList<>();
        for (Map.Entry<String, LongArrayList> nodeEntry : jobSearchContextDocIds.entrySet()) {
            NodeFetchRequest nodeFetchRequest = new NodeFetchRequest();
            nodeFetchRequest.jobId(plan.collectNode().jobId());
            nodeFetchRequest.executionNodeId(plan.collectNode().executionNodeId());
            nodeFetchRequest.toFetchReferences(context.references());
            nodeFetchRequest.closeContext(true);
            nodeFetchRequest.jobSearchContextDocIds(nodeEntry.getValue());

            transportFetchNodeAction.execute(nodeEntry.getKey(), nodeFetchRequest, new ActionListener<NodeFetchResponse>() {
                @Override
                public void onResponse(NodeFetchResponse nodeFetchResponse) {
                    for (Row row : nodeFetchResponse.rows()) {
                        rows.add(row);
                    }
                    latch.countDown();
                }

                @Override
                public void onFailure(Throwable e) {
                    latch.countDown();
                    fail(e.getMessage());
                }
            });
        }
        latch.await();

        assertThat(rows.size(), is(2));
        for (Row row : rows) {
            assertThat((Integer) row.get(0), anyOf(is(1), is(2)));
            assertThat((BytesRef) row.get(1), anyOf(is(new BytesRef("Arthur")), is(new BytesRef("Ford"))));
        }
    }

    @Test
    public void testFetchProjection() throws Exception {
        setUpCharacters();

        Plan plan = analyzeAndPlan("select id, name, substr(name, 2) from characters order by id");
        assertThat(plan, instanceOf(QueryThenFetch.class));
        QueryThenFetch qtf = (QueryThenFetch) plan;

        assertThat(qtf.collectNode().keepContextForFetcher(), is(true));
        assertThat(((FetchProjection) qtf.mergeNode().projections().get(1)).jobSearchContextIdToNode(), notNullValue());
        assertThat(((FetchProjection) qtf.mergeNode().projections().get(1)).jobSearchContextIdToShard(), notNullValue());

        Job job = executor.newJob(plan);
        ListenableFuture<List<TaskResult>> results = Futures.allAsList(executor.execute(job));

        final List<Object[]> resultingRows = new ArrayList<>();
        final CountDownLatch latch = new CountDownLatch(1);
        Futures.addCallback(results, new FutureCallback<List<TaskResult>>() {
            @Override
            public void onSuccess(List<TaskResult> resultList) {
                for (Row row : resultList.get(0).rows()) {
                    resultingRows.add(row.materialize());
                }
                latch.countDown();
            }

            @Override
            public void onFailure(Throwable t) {
                latch.countDown();
                fail(t.getMessage());
            }
        });

        latch.await();
        assertThat(resultingRows.size(), is(2));
        assertThat(resultingRows.get(0).length, is(3));
        assertThat((Integer) resultingRows.get(0)[0], is(1));
        assertThat((BytesRef) resultingRows.get(0)[1], is(new BytesRef("Arthur")));
        assertThat((BytesRef) resultingRows.get(0)[2], is(new BytesRef("rthur")));
        assertThat((Integer) resultingRows.get(1)[0], is(2));
        assertThat((BytesRef) resultingRows.get(1)[1], is(new BytesRef("Ford")));
        assertThat((BytesRef) resultingRows.get(1)[2], is(new BytesRef("ord")));
    }

    @Test
    public void testFetchProjectionWithBulkSize() throws Exception {

        setup.setUpLocations();
        sqlExecutor.refresh("locations");
        int bulkSize = 2;

        Plan plan = analyzeAndPlan("select position, name from locations order by position");
        assertThat(plan, instanceOf(QueryThenFetch.class));

        rewriteFetchProjectionToBulkSize(bulkSize, ((QueryThenFetch) plan).mergeNode());

        Job job = executor.newJob(plan);
        ListenableFuture<List<TaskResult>> results = Futures.allAsList(executor.execute(job));

        final List<Object[]> resultingRows = new ArrayList<>();
        final CountDownLatch latch = new CountDownLatch(1);
        Futures.addCallback(results, new FutureCallback<List<TaskResult>>() {
            @Override
            public void onSuccess(List<TaskResult> resultList) {
                for (Row row : resultList.get(0).rows()) {
                    resultingRows.add(row.materialize());
                }
                latch.countDown();
            }

            @Override
            public void onFailure(Throwable t) {
                latch.countDown();
                fail(t.getMessage());
            }
        });
        latch.await();

        assertThat(resultingRows.size(), is(13));
        assertThat(resultingRows.get(0).length, is(2));
        assertThat((Integer) resultingRows.get(0)[0], is(1));
        assertThat((Integer) resultingRows.get(12)[0], is(6));
    }

    private void rewriteFetchProjectionToBulkSize(int bulkSize, MergeNode mergeNode) {
        List<Projection> newProjections = new ArrayList<>(mergeNode.projections().size());
        for (Projection projection : mergeNode.projections()) {
            if (projection instanceof FetchProjection) {
                FetchProjection fetchProjection = (FetchProjection) projection;
                newProjections.add(new FetchProjection(
                        fetchProjection.executionNodeId(),
                        fetchProjection.docIdSymbol(),
                        fetchProjection.inputSymbols(),
                        fetchProjection.outputSymbols(),
                        fetchProjection.partitionedBy(),
                        fetchProjection.executionNodes(),
                        bulkSize,
                        fetchProjection.closeContexts(),
                        fetchProjection.jobSearchContextIdToNode(),
                        fetchProjection.jobSearchContextIdToShard()));
            } else {
                newProjections.add(projection);
            }
        }
        mergeNode.projections(newProjections);
    }
}