package org.deeplearning4j.translate;

import org.deeplearning4j.caffe.Caffe.*;
import com.google.protobuf.CodedInputStream;

import java.io.BufferedInputStream;
import java.io.FileInputStream;
import java.io.IOException;
import java.io.InputStream;


public class CaffeModelToJavaClass {

    
    public static NetParameter readCaffeModel(InputStream is, int sizeLimitMb) throws IOException {
        CodedInputStream codeStream = CodedInputStream.newInstance(is);
        
        int oldLimit = codeStream.setSizeLimit(sizeLimitMb * 1024 * 1024);
        return NetParameter.parseFrom(codeStream);
    }

    
    public static NetParameter readCaffeModel(String caffeModelPath, int sizeLimitMb) throws IOException {
        InputStream is = new BufferedInputStream(new FileInputStream(caffeModelPath));
        return readCaffeModel(is,sizeLimitMb);
    }
}

<code block>
package org.deeplearning4j.translate;

import org.deeplearning4j.caffe.Caffe.NetParameter;
import org.springframework.core.io.ClassPathResource;
import org.junit.Test;
import static org.junit.Assert.*;


public class CaffeTest {

    @Test
    public void testCaffeModelToJavaClass() throws Exception {
        
        String imagenetCaffeModelPath = new ClassPathResource("nin_imagenet_conv.caffemodel").getURL().getFile();

        NetParameter net = CaffeModelToJavaClass.readCaffeModel(imagenetCaffeModelPath, 1000);
        assertEquals(net.getName(), "CaffeNet");
        assertEquals(net.getLayersCount(), 31);
        assertEquals(net.getLayers(0).getName(), "data");
        assertEquals(net.getLayers(30).getName(), "loss");
        assertEquals(net.getLayers(15).getBlobs(0).getData(0), -0.008252043f, 1e-1);

    }

}

<code block>
package org.deeplearning4j.translate;

import org.deeplearning4j.caffe.Caffe.*;
import com.google.protobuf.CodedInputStream;
import org.springframework.core.io.ClassPathResource;

import java.io.FileInputStream;
import java.io.IOException;
import java.io.InputStream;


public class CaffeModelToJavaClass {

    public static NetParameter readCaffeModel(String caffeModelPath, int sizeLimitMb) throws IOException {
        InputStream is = new FileInputStream(caffeModelPath);
        CodedInputStream codeStream = CodedInputStream.newInstance(is);
        
        int oldLimit = codeStream.setSizeLimit(sizeLimitMb * 1024 * 1024);
        return NetParameter.parseFrom(codeStream);
    }
}

<code block>
package org.deeplearning4j.translate;

import org.deeplearning4j.caffe.Caffe.NetParameter;
import org.springframework.core.io.ClassPathResource;
import org.junit.Test;
import static org.junit.Assert.*;

import java.io.IOException;
import java.nio.file.Paths;


public class CaffeTest {

    @Test
    public void testCaffeModelToJavaClass() throws Exception {
        
        String imagenetCaffeModelPath = new ClassPathResource("nin_imagenet_conv.caffemodel").getURL().getFile();

        NetParameter net = CaffeModelToJavaClass.readCaffeModel(imagenetCaffeModelPath, 1000);
        assertEquals(net.getName(), "CaffeNet");
        assertEquals(net.getLayersCount(), 31);
        assertEquals(net.getLayers(0).getName(), "data");
        assertEquals(net.getLayers(30).getName(), "loss");
        assertEquals(net.getLayers(15).getBlobs(0).getData(0), -0.008252043f, 1e-1);

    }

}

<code block>


package org.deeplearning4j.optimize.solvers;

import org.deeplearning4j.berkeley.Pair;
import org.deeplearning4j.exception.InvalidStepException;
import org.deeplearning4j.nn.api.Layer;
import org.deeplearning4j.nn.api.Model;
import org.deeplearning4j.nn.api.Updater;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.gradient.DefaultGradient;
import org.deeplearning4j.nn.gradient.Gradient;
import org.deeplearning4j.nn.updater.UpdaterCreator;
import org.deeplearning4j.optimize.api.ConvexOptimizer;
import org.deeplearning4j.optimize.api.IterationListener;
import org.deeplearning4j.optimize.api.StepFunction;
import org.deeplearning4j.optimize.api.TerminationCondition;
import org.deeplearning4j.optimize.terminations.EpsTermination;
import org.deeplearning4j.optimize.terminations.ZeroDirection;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.learning.GradientUpdater;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.*;
import java.util.concurrent.ConcurrentHashMap;


public abstract class BaseOptimizer implements ConvexOptimizer {

    protected NeuralNetConfiguration conf;
    protected int iteration = 0;
    protected static final Logger log = LoggerFactory.getLogger(BaseOptimizer.class);
    protected StepFunction stepFunction;
    protected Collection<IterationListener> iterationListeners = new ArrayList<>();
    protected Collection<TerminationCondition> terminationConditions = new ArrayList<>();
    protected Model model;
    protected BackTrackLineSearch lineMaximizer;
    protected Updater updater;
    protected double step;
    private int batchSize = 10;
    protected double score,oldScore;
    protected double stepMax = Double.MAX_VALUE;
    public final static String GRADIENT_KEY = "g";
    public final static String SCORE_KEY = "score";
    public final static String PARAMS_KEY = "params";
    public final static String SEARCH_DIR = "searchDirection";
    protected Map<String,Object> searchState = new ConcurrentHashMap<>();

    
    public BaseOptimizer(NeuralNetConfiguration conf,StepFunction stepFunction,Collection<IterationListener> iterationListeners,Model model) {
        this(conf,stepFunction,iterationListeners, Arrays.asList(new ZeroDirection(),new EpsTermination()),model);
    }


    
    public BaseOptimizer(NeuralNetConfiguration conf,StepFunction stepFunction,Collection<IterationListener> iterationListeners,Collection<TerminationCondition> terminationConditions,Model model) {
        this.conf = conf;
        this.stepFunction = stepFunction;
        this.iterationListeners = iterationListeners != null ? iterationListeners : new ArrayList<IterationListener>();
        this.terminationConditions = terminationConditions;
        this.model = model;
        lineMaximizer = new BackTrackLineSearch(model,stepFunction,this);
        lineMaximizer.setStepMax(stepMax);
        lineMaximizer.setMaxIterations(conf.getNumLineSearchIterations());

    }


    @Override
    public double score() {
        model.setScore();
        return model.score();
    }


    @Override
    public Pair<Gradient,Double> gradientAndScore() {
        model.setScore();
        Pair<Gradient,Double> pair = model.gradientAndScore();
        for(String paramType : pair.getFirst().gradientForVariable().keySet()) {
            INDArray gradient = pair.getFirst().getGradientFor(paramType);
            updateGradientAccordingToParams(gradient, model, model.batchSize(), paramType,iteration);
        }
        return pair;
    }


    
    @Override
    public  boolean optimize() {
        
        model.validateInput();
        Pair<Gradient,Double> pair = gradientAndScore();
        score = pair.getSecond();
        setupSearchState(pair);
        INDArray gradient = (INDArray) searchState.get(GRADIENT_KEY);
        INDArray searchDirection, parameters;

        
        

        
        preProcessLine(gradient);

        for(int i = 0; i < conf.getNumIterations(); i++) {
            
            try {
                gradient = (INDArray) searchState.get(GRADIENT_KEY);
                searchDirection = (INDArray) searchState.get(SEARCH_DIR);
                parameters = (INDArray) searchState.get(PARAMS_KEY);
                step = lineMaximizer.optimize(parameters, gradient, searchDirection);
            } catch (InvalidStepException e) {
                log.warn("Invalid step...continuing another iteration: {}",e.getMessage());
            }

            
            oldScore = score;
            pair = gradientAndScore();
            setupSearchState(pair);
            score = pair.getSecond();

            
            for(IterationListener listener : iterationListeners)
                listener.iterationDone(model,i);


            
            for(TerminationCondition condition : terminationConditions){
                if(condition.terminate(score,oldScore,new Object[]{gradient})){
                	log.debug("Hit termination condition: score={}, oldScore={}, condition={}",score,oldScore,condition);
                    return true;
                }
            }

            
            postStep();
            this.iteration++;

            
            for(TerminationCondition condition : terminationConditions)
                if(condition.terminate(score,oldScore,new Object[]{gradient}))
                    return true;


        }

        return true;
    }

    protected  void postFirstStep(INDArray gradient) {
        
    }

    @Override
    public int batchSize() {
        return batchSize;
    }

    @Override
    public void setBatchSize(int batchSize) {
        this.batchSize = batchSize;
    }


    
    @Override
    public  void preProcessLine(INDArray line) {
        
    }
    
    @Override
    public  void postStep() {
        
    }


    @Override
    public void updateGradientAccordingToParams(INDArray gradient, Model model, int batchSize, String paramType, int iteration) {
        if(updater == null)
            updater = UpdaterCreator.getUpdater(model.conf());
        Layer layer = (Layer) model;
        Gradient g = new DefaultGradient();
        g.setGradientFor(paramType,gradient);
        updater.update(layer,g,iteration);

    }

    
    @Override
    public  void setupSearchState(Pair<Gradient, Double> pair) {
        INDArray gradient = pair.getFirst().gradient(conf.variables());
        INDArray params = model.params();
        searchState.put(GRADIENT_KEY,gradient);
        searchState.put(SCORE_KEY,pair.getSecond());
        searchState.put(PARAMS_KEY,params);

    }




}

<code block>
package org.deeplearning4j.optimize.solver;

import static org.junit.Assert.*;

import java.util.Arrays;
import java.util.Collection;
import java.util.Map;

import org.deeplearning4j.berkeley.Pair;
import org.deeplearning4j.datasets.iterator.DataSetIterator;
import org.deeplearning4j.datasets.iterator.impl.IrisDataSetIterator;
import org.deeplearning4j.nn.api.Layer;
import org.deeplearning4j.nn.api.Model;
import org.deeplearning4j.nn.api.OptimizationAlgorithm;
import org.deeplearning4j.nn.conf.MultiLayerConfiguration;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.conf.distribution.NormalDistribution;
import org.deeplearning4j.nn.conf.layers.OutputLayer;
import org.deeplearning4j.nn.conf.layers.RBM;
import org.deeplearning4j.nn.conf.override.ConfOverride;
import org.deeplearning4j.nn.gradient.DefaultGradient;
import org.deeplearning4j.nn.gradient.Gradient;
import org.deeplearning4j.nn.multilayer.MultiLayerNetwork;
import org.deeplearning4j.nn.weights.WeightInit;
import org.deeplearning4j.optimize.api.ConvexOptimizer;
import org.deeplearning4j.optimize.api.IterationListener;
import org.deeplearning4j.optimize.solvers.ConjugateGradient;
import org.deeplearning4j.optimize.solvers.LineGradientDescent;
import org.deeplearning4j.optimize.solvers.StochasticGradientDescent;
import org.deeplearning4j.optimize.solvers.LBFGS;
import org.deeplearning4j.optimize.stepfunctions.DefaultStepFunction;
import org.junit.Test;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.api.rng.DefaultRandom;
import org.nd4j.linalg.api.rng.Random;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.lossfunctions.LossFunctions.LossFunction;

public class TestOptimizers {
	
	@Test
	public void testOptimizersBasicMLPBackprop(){
		
		
		DataSetIterator iter = new IrisDataSetIterator(5,50);
		
		for( OptimizationAlgorithm oa : OptimizationAlgorithm.values() ){
			MultiLayerNetwork network = new MultiLayerNetwork(getMLPConfigIris(oa));
			network.init();
			
			iter.reset();
			network.fit(iter);
		}
	}
	
	private static MultiLayerConfiguration getMLPConfigIris( OptimizationAlgorithm oa ){
		MultiLayerConfiguration c = new NeuralNetConfiguration.Builder()
		.nIn(4).nOut(3)
		.weightInit(WeightInit.DISTRIBUTION)
		.dist(new NormalDistribution(0, 0.1))

		.activationFunction("sigmoid")
		.lossFunction(LossFunction.MCXENT)
		.optimizationAlgo(oa)
		.iterations(1)
		.batchSize(5)
		.constrainGradientToUnitNorm(false)
		.corruptionLevel(0.0)
		.layer(new RBM())
		.learningRate(0.1).useAdaGrad(false)
		.regularization(true)
		.l2(0.01)
		.applySparsity(false).sparsity(0.0)
		.seed(12345L)
		.list(4).hiddenLayerSizes(8,10,5)
		.backward(true).pretrain(false)
		.useDropConnect(false)

		.override(3, new ConfOverride() {
			@Override
			public void overrideLayer(int i, NeuralNetConfiguration.Builder builder) {
				builder.activationFunction("softmax");
				builder.layer(new OutputLayer());
				builder.weightInit(WeightInit.DISTRIBUTION);
				builder.dist(new NormalDistribution(0, 0.1));
			}
		}).build();

		return c;
	}
	
	@Test
	public void testSphereFnOptStochGradDescent(){
		testSphereFnOptHelper(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT,-1,2);
		testSphereFnOptHelper(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT,-1,10);
		testSphereFnOptHelper(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT,-1,100);
	}
	
	@Test
	public void testSphereFnOptLineGradDescent(){
		
		int[] numLineSearchIter = {1,2,5,10};
		for( int n : numLineSearchIter )
			testSphereFnOptHelper(OptimizationAlgorithm.LINE_GRADIENT_DESCENT,n,2);
		
		for( int n : numLineSearchIter )
			testSphereFnOptHelper(OptimizationAlgorithm.LINE_GRADIENT_DESCENT,n,10);

		for( int n : numLineSearchIter )
			testSphereFnOptHelper(OptimizationAlgorithm.LINE_GRADIENT_DESCENT,n,100);
	}
	
	@Test
	public void testSphereFnOptCG(){
		
		int[] numLineSearchIter = {1,2,5,10};
		for( int n : numLineSearchIter )
			testSphereFnOptHelper(OptimizationAlgorithm.CONJUGATE_GRADIENT,n,2);
		
		for( int n : numLineSearchIter )
			testSphereFnOptHelper(OptimizationAlgorithm.CONJUGATE_GRADIENT,n,10);
		
		for( int n : numLineSearchIter )
			testSphereFnOptHelper(OptimizationAlgorithm.CONJUGATE_GRADIENT,n,100);
	}
	
	@Test
	public void testSphereFnOptLBFGS(){
		
		int[] numLineSearchIter = {1,2,5,10};
		for( int n : numLineSearchIter )
			testSphereFnOptHelper(OptimizationAlgorithm.LBFGS,n,2);
		
		for( int n : numLineSearchIter )
			testSphereFnOptHelper(OptimizationAlgorithm.LBFGS,n,10);
		
		for( int n : numLineSearchIter )
			testSphereFnOptHelper(OptimizationAlgorithm.LBFGS,n,100);
	}
	
	
	private static final boolean PRINT_OPT_RESULTS = true;
	public void testSphereFnOptHelper( OptimizationAlgorithm oa, int numLineSearchIter, int nDimensions ){
		
		if( PRINT_OPT_RESULTS ) System.out.println("---------\n Alg=" + oa
				+ ", nIter=" + numLineSearchIter + ", nDimensions=" + nDimensions );
		
		NeuralNetConfiguration conf = new NeuralNetConfiguration.Builder()
		.numLineSearchIterations(numLineSearchIter)
		.iterations(1000)
		.learningRate(0.01)
		.layer(new RBM()).batchSize(1).build();
		conf.addVariable("x");	
		
		Random rng = new DefaultRandom(12345L);
		org.nd4j.linalg.api.rng.distribution.Distribution dist
			= new org.nd4j.linalg.api.rng.distribution.impl.UniformDistribution(rng,-10, 10);
		Model m = new SphereFunctionModel(nDimensions,dist,conf);
		
		double scoreBefore = m.score();
		assertTrue(!Double.isNaN(scoreBefore) && !Double.isInfinite(scoreBefore));
		if( PRINT_OPT_RESULTS ){
			System.out.println("Before:");
			System.out.println(scoreBefore);
			System.out.println(m.params());
		}
		
		ConvexOptimizer opt = getOptimizer(oa,conf,m);
		
		opt.setupSearchState(m.gradientAndScore());
		opt.optimize();
		
		double scoreAfter = m.score();
		assertTrue(!Double.isNaN(scoreAfter) && !Double.isInfinite(scoreAfter));
		if( PRINT_OPT_RESULTS ){
			System.out.println("After:");
			System.out.println(scoreAfter);
			System.out.println(m.params());
		}
		
		
		
		
		assertTrue("Score did not improve after optimization (b="+scoreBefore+",a="+scoreAfter+")",scoreAfter < scoreBefore);
	}
	
	private static ConvexOptimizer getOptimizer( OptimizationAlgorithm oa, NeuralNetConfiguration conf, Model m ){
		switch(oa){
		case STOCHASTIC_GRADIENT_DESCENT:
			return new StochasticGradientDescent(conf,new DefaultStepFunction(),null,m);
		case LINE_GRADIENT_DESCENT:
			return new LineGradientDescent(conf,new DefaultStepFunction(),null,m);
		case CONJUGATE_GRADIENT:
			return new ConjugateGradient(conf,new DefaultStepFunction(),null,m);
		case LBFGS:
			return new LBFGS(conf,new DefaultStepFunction(),null,m);
		default:
			throw new UnsupportedOperationException();
		}
	}
	
	
	
	
	@Test
	public void testSphereFnOptStochGradDescentMultipleSteps(){
		
		
		
		
		
		
		testSphereFnMultipleStepsHelper(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT,5,5);
	}
	
	@Test
	public void testSphereFnOptLineGradDescentMultipleSteps(){
		testSphereFnMultipleStepsHelper(OptimizationAlgorithm.LINE_GRADIENT_DESCENT,5,5);
	}
	
	@Test
	public void testSphereFnOptCGMultipleSteps(){
		testSphereFnMultipleStepsHelper(OptimizationAlgorithm.CONJUGATE_GRADIENT,5,5);
	}
	
	@Test
	public void testSphereFnOptLBFGSMultipleSteps(){
		testSphereFnMultipleStepsHelper(OptimizationAlgorithm.LBFGS,5,5);
	}
	
	
	private static void testSphereFnMultipleStepsHelper( OptimizationAlgorithm oa, int nSteps, int numLineSearchIter ){
		Random rng = new DefaultRandom(12345L);
		org.nd4j.linalg.api.rng.distribution.Distribution dist
		= new org.nd4j.linalg.api.rng.distribution.impl.UniformDistribution(rng,-10, 10);
		NeuralNetConfiguration conf = new NeuralNetConfiguration.Builder()
		.numLineSearchIterations(numLineSearchIter)
		.iterations(1000)
		.learningRate(0.01)
		.layer(new RBM()).batchSize(1).build();
		conf.addVariable("x");	
		
		Model m = new SphereFunctionModel(100,dist,conf);
		double[] scores = new double[nSteps+1];
		scores[0] = m.score();
		
		
		ConvexOptimizer opt = getOptimizer(oa,conf,m);
		for( int i=0; i<nSteps; i++ ){
			opt.optimize();
			scores[i+1] = m.score();
		}
		
		if( PRINT_OPT_RESULTS ){
			System.out.println("Multiple step ("+nSteps+" steps) score vs iteration, nLineSearchIter=" + numLineSearchIter +": " + oa );
			System.out.println(Arrays.toString(scores));
		}
		for( int i=1; i<scores.length; i++ ){
			assertTrue( scores[i] < scores[i-1] || (scores[i] == 0.0 && scores[i-1] == 0.0) );
		}
		assertTrue(scores[scores.length-1]<1.0);	
	}
	
	
	
	
	
	private static class SphereFunctionModel implements Model, Layer {
		private static final long serialVersionUID = 239156313657395826L;
		private INDArray parameters;
		private final NeuralNetConfiguration conf;
		
		
		private SphereFunctionModel( INDArray parameterInit, NeuralNetConfiguration conf ){
			this.parameters = parameterInit.dup();
			this.conf = conf;
		}
		
		private SphereFunctionModel( int nParams, org.nd4j.linalg.api.rng.distribution.Distribution distribution,
				NeuralNetConfiguration conf ){
			this.parameters = distribution.sample(new int[]{1,nParams});
			this.conf = conf;
		}

		@Override
		public void fit() { throw new UnsupportedOperationException(); }

		@Override
		public void update(INDArray gradient, String paramType) {
			if(!"x".equals(paramType)) throw new UnsupportedOperationException();
			parameters.subi(gradient);
		}

		@Override
		public double score() {
			return Nd4j.getBlasWrapper().dot(parameters, parameters);	
		}

		@Override
		public void setScore() { }

		@Override
		public void accumulateScore(double accum) { throw new UnsupportedOperationException(); }

		@Override
		public INDArray transform(INDArray data) { throw new UnsupportedOperationException(); }

		@Override
		public INDArray params() {return parameters; }

		@Override
		public int numParams() { return parameters.length(); }

		@Override
		public void setParams(INDArray params) { this.parameters = params; }

		@Override
		public void fit(INDArray data) { throw new UnsupportedOperationException(); }

		@Override
		public void iterate(INDArray input) { throw new UnsupportedOperationException(); }

		@Override
		public Gradient gradient() {
			
			INDArray gradient = parameters.mul(2);
			Gradient g = new DefaultGradient();
			g.gradientForVariable().put("x", gradient);
			return g;
		}

		@Override
		public Pair<Gradient, Double> gradientAndScore() {
			return new Pair<Gradient,Double>(gradient(),score());
		}

		@Override
		public int batchSize() { return 1; }

		@Override
		public NeuralNetConfiguration conf() { return conf; }

		@Override
		public void setConf(NeuralNetConfiguration conf) { throw new UnsupportedOperationException(); }

		@Override
		public INDArray input() {
			
			
			return Nd4j.zeros(1);
		}

		@Override
		public void validateInput() { }

		@Override
		public ConvexOptimizer getOptimizer() { throw new UnsupportedOperationException(); }

		@Override
		public INDArray getParam(String param) { return parameters; }

		@Override
		public void initParams() { throw new UnsupportedOperationException(); }

		@Override
		public Map<String, INDArray> paramTable() { throw new UnsupportedOperationException(); }

		@Override
		public void setParamTable(Map<String, INDArray> paramTable) { throw new UnsupportedOperationException(); }

		@Override
		public void setParam(String key, INDArray val) { throw new UnsupportedOperationException(); }

		@Override
		public void clear() { throw new UnsupportedOperationException(); }

		@Override
		public Type type() { throw new UnsupportedOperationException(); }

		@Override
		public Gradient error(INDArray input) { throw new UnsupportedOperationException(); }

		@Override
		public INDArray derivativeActivation(INDArray input) { throw new UnsupportedOperationException(); }

		@Override
		public Gradient calcGradient(Gradient layerError, INDArray indArray) { throw new UnsupportedOperationException(); }

		@Override
		public Gradient errorSignal(Gradient error, INDArray input){ throw new UnsupportedOperationException(); }

		@Override
		public Gradient backwardGradient(INDArray z, Layer nextLayer,
				Gradient nextGradient, INDArray activation) { throw new UnsupportedOperationException(); }

		@Override
		public void merge(Layer layer, int batchSize) { throw new UnsupportedOperationException(); }

		@Override
		public INDArray activationMean() { throw new UnsupportedOperationException(); }

		@Override
		public INDArray preOutput(INDArray x) { throw new UnsupportedOperationException(); }

		@Override
		public INDArray activate() { throw new UnsupportedOperationException(); }

		@Override
		public INDArray activate(INDArray input) { throw new UnsupportedOperationException(); }

		@Override
		public Layer transpose() { throw new UnsupportedOperationException(); }

		@Override
		public Layer clone() { throw new UnsupportedOperationException(); }

		@Override
		public Pair<Gradient, Gradient> backWard(Gradient errors,
				Gradient deltas, INDArray activation, String previousActivation) { throw new UnsupportedOperationException(); }

		@Override
		public Collection<IterationListener> getIterationListeners() { return null; }

		@Override
		public void setIterationListeners(Collection<IterationListener> listeners) { throw new UnsupportedOperationException(); }

		@Override
		public void setIndex(int index) { throw new UnsupportedOperationException(); }

		@Override
		public int getIndex() { throw new UnsupportedOperationException(); }
	}
}

<code block>


package org.deeplearning4j.optimize.solvers;

import org.deeplearning4j.berkeley.Pair;
import org.deeplearning4j.exception.InvalidStepException;
import org.deeplearning4j.nn.api.Layer;
import org.deeplearning4j.nn.api.Model;
import org.deeplearning4j.nn.api.Updater;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.gradient.DefaultGradient;
import org.deeplearning4j.nn.gradient.Gradient;
import org.deeplearning4j.nn.updater.UpdaterCreator;
import org.deeplearning4j.optimize.api.ConvexOptimizer;
import org.deeplearning4j.optimize.api.IterationListener;
import org.deeplearning4j.optimize.api.StepFunction;
import org.deeplearning4j.optimize.api.TerminationCondition;
import org.deeplearning4j.optimize.terminations.EpsTermination;
import org.deeplearning4j.optimize.terminations.ZeroDirection;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.learning.GradientUpdater;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.*;
import java.util.concurrent.ConcurrentHashMap;


public abstract class BaseOptimizer implements ConvexOptimizer {

    protected NeuralNetConfiguration conf;
    protected int iteration = 0;
    protected static final Logger log = LoggerFactory.getLogger(BaseOptimizer.class);
    protected StepFunction stepFunction;
    protected Collection<IterationListener> iterationListeners = new ArrayList<>();
    protected Collection<TerminationCondition> terminationConditions = new ArrayList<>();
    protected Model model;
    protected BackTrackLineSearch lineMaximizer;
    protected Updater updater;
    protected double step;
    private int batchSize = 10;
    protected double score,oldScore;
    protected double stepMax = Double.MAX_VALUE;
    public final static String GRADIENT_KEY = "g";
    public final static String SCORE_KEY = "score";
    public final static String PARAMS_KEY = "params";
    public final static String SEARCH_DIR = "searchDirection";
    protected Map<String,Object> searchState = new ConcurrentHashMap<>();

    
    public BaseOptimizer(NeuralNetConfiguration conf,StepFunction stepFunction,Collection<IterationListener> iterationListeners,Model model) {
        this(conf,stepFunction,iterationListeners, Arrays.asList(new ZeroDirection(),new EpsTermination()),model);
    }


    
    public BaseOptimizer(NeuralNetConfiguration conf,StepFunction stepFunction,Collection<IterationListener> iterationListeners,Collection<TerminationCondition> terminationConditions,Model model) {
        this.conf = conf;
        this.stepFunction = stepFunction;
        this.iterationListeners = iterationListeners != null ? iterationListeners : new ArrayList<IterationListener>();
        this.terminationConditions = terminationConditions;
        this.model = model;
        lineMaximizer = new BackTrackLineSearch(model,stepFunction,this);
        lineMaximizer.setStepMax(stepMax);
        lineMaximizer.setMaxIterations(conf.getNumLineSearchIterations());

    }


    @Override
    public double score() {
        model.setScore();
        return model.score();
    }


    @Override
    public Pair<Gradient,Double> gradientAndScore() {
        model.setScore();
        Pair<Gradient,Double> pair = model.gradientAndScore();
        for(String paramType : pair.getFirst().gradientForVariable().keySet()) {
            INDArray gradient = pair.getFirst().getGradientFor(paramType);
            updateGradientAccordingToParams(gradient, model, model.batchSize(), paramType,iteration);
        }
        return pair;
    }


    
    @Override
    public  boolean optimize() {
        
        model.validateInput();
        Pair<Gradient,Double> pair = gradientAndScore();
        score = model.score();
        setupSearchState(pair);
        INDArray gradient = (INDArray) searchState.get(GRADIENT_KEY);
        INDArray searchDirection, parameters;

        
        

        
        preProcessLine(gradient);

        for(int i = 0; i < conf.getNumIterations(); i++) {
            
            try {
                gradient = (INDArray) searchState.get(GRADIENT_KEY);
                searchDirection = (INDArray) searchState.get(SEARCH_DIR);
                parameters = (INDArray) searchState.get(PARAMS_KEY);
                step = lineMaximizer.optimize(parameters, gradient, searchDirection);
            } catch (InvalidStepException e) {
                log.warn("Invalid step...continuing another iteration: {}",e.getMessage());
            }

            
            oldScore = score;
            pair = gradientAndScore();
            setupSearchState(pair);
            score = pair.getSecond();

            
            for(IterationListener listener : iterationListeners)
                listener.iterationDone(model,i);


            
            for(TerminationCondition condition : terminationConditions){
                if(condition.terminate(score,oldScore,new Object[]{gradient})){
                	log.debug("Hit termination condition: score={}, oldScore={}, condition={}",score,oldScore,condition);
                    return true;
                }
            }

            
            postStep();
            this.iteration++;

            
            for(TerminationCondition condition : terminationConditions)
                if(condition.terminate(score,oldScore,new Object[]{gradient}))
                    return true;


        }

        return true;
    }

    protected  void postFirstStep(INDArray gradient) {
        
    }

    @Override
    public int batchSize() {
        return batchSize;
    }

    @Override
    public void setBatchSize(int batchSize) {
        this.batchSize = batchSize;
    }


    
    @Override
    public  void preProcessLine(INDArray line) {
        
    }
    
    @Override
    public  void postStep() {
        
    }


    @Override
    public void updateGradientAccordingToParams(INDArray gradient, Model model, int batchSize, String paramType, int iteration) {
        if(updater == null)
            updater = UpdaterCreator.getUpdater(model.conf());
        Layer layer = (Layer) model;
        Gradient g = new DefaultGradient();
        g.setGradientFor(paramType,gradient);
        updater.update(layer,g,iteration);

    }

    
    @Override
    public  void setupSearchState(Pair<Gradient, Double> pair) {
        INDArray gradient = pair.getFirst().gradient(conf.variables());
        INDArray params = model.params();
        searchState.put(GRADIENT_KEY,gradient);
        searchState.put(SCORE_KEY,pair.getSecond());
        searchState.put(PARAMS_KEY,params);

    }




}

<code block>
package org.deeplearning4j.optimize.solver;

import static org.junit.Assert.*;

import java.util.Collection;
import java.util.Map;

import org.deeplearning4j.berkeley.Pair;
import org.deeplearning4j.datasets.iterator.DataSetIterator;
import org.deeplearning4j.datasets.iterator.impl.IrisDataSetIterator;
import org.deeplearning4j.nn.api.Layer;
import org.deeplearning4j.nn.api.Model;
import org.deeplearning4j.nn.api.OptimizationAlgorithm;
import org.deeplearning4j.nn.conf.MultiLayerConfiguration;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.conf.distribution.NormalDistribution;
import org.deeplearning4j.nn.conf.layers.OutputLayer;
import org.deeplearning4j.nn.conf.layers.RBM;
import org.deeplearning4j.nn.conf.override.ConfOverride;
import org.deeplearning4j.nn.gradient.DefaultGradient;
import org.deeplearning4j.nn.gradient.Gradient;
import org.deeplearning4j.nn.multilayer.MultiLayerNetwork;
import org.deeplearning4j.nn.weights.WeightInit;
import org.deeplearning4j.optimize.api.ConvexOptimizer;
import org.deeplearning4j.optimize.api.IterationListener;
import org.deeplearning4j.optimize.solvers.ConjugateGradient;
import org.deeplearning4j.optimize.solvers.LineGradientDescent;
import org.deeplearning4j.optimize.solvers.StochasticGradientDescent;
import org.deeplearning4j.optimize.solvers.LBFGS;
import org.deeplearning4j.optimize.stepfunctions.DefaultStepFunction;
import org.junit.Test;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.api.rng.DefaultRandom;
import org.nd4j.linalg.api.rng.Random;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.lossfunctions.LossFunctions.LossFunction;

public class TestOptimizers {
	
	@Test
	public void testOptimizersBasicMLPBackprop(){
		
		
		DataSetIterator iter = new IrisDataSetIterator(5,50);
		
		for( OptimizationAlgorithm oa : OptimizationAlgorithm.values() ){
			MultiLayerNetwork network = new MultiLayerNetwork(getMLPConfigIris(oa));
			network.init();
			
			iter.reset();
			network.fit(iter);
		}
	}
	
	private static MultiLayerConfiguration getMLPConfigIris( OptimizationAlgorithm oa ){
		MultiLayerConfiguration c = new NeuralNetConfiguration.Builder()
		.nIn(4).nOut(3)
		.weightInit(WeightInit.DISTRIBUTION)
		.dist(new NormalDistribution(0, 0.1))

		.activationFunction("sigmoid")
		.lossFunction(LossFunction.MCXENT)
		.optimizationAlgo(oa)
		.iterations(1)
		.batchSize(5)
		.constrainGradientToUnitNorm(false)
		.corruptionLevel(0.0)
		.layer(new RBM())
		.learningRate(0.1).useAdaGrad(false)
		.regularization(true)
		.l2(0.01)
		.applySparsity(false).sparsity(0.0)
		.seed(12345L)
		.list(4).hiddenLayerSizes(8,10,5)
		.backward(true).pretrain(false)
		.useDropConnect(false)

		.override(3, new ConfOverride() {
			@Override
			public void overrideLayer(int i, NeuralNetConfiguration.Builder builder) {
				builder.activationFunction("softmax");
				builder.layer(new OutputLayer());
				builder.weightInit(WeightInit.DISTRIBUTION);
				builder.dist(new NormalDistribution(0, 0.1));
			}
		}).build();

		return c;
	}
	
	@Test
	public void testSphereFnOptStochGradDescent(){
		testSphereFnOptHelper(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT,-1,2);
		testSphereFnOptHelper(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT,-1,10);
		testSphereFnOptHelper(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT,-1,100);
	}
	
	@Test
	public void testSphereFnOptLineGradDescent(){
		int[] numLineSearchIter = {1,2,5,10};
		for( int n : numLineSearchIter )
			testSphereFnOptHelper(OptimizationAlgorithm.LINE_GRADIENT_DESCENT,n,2);
		
		for( int n : numLineSearchIter )
			testSphereFnOptHelper(OptimizationAlgorithm.LINE_GRADIENT_DESCENT,n,10);

		for( int n : numLineSearchIter )
			testSphereFnOptHelper(OptimizationAlgorithm.LINE_GRADIENT_DESCENT,n,100);
	}
	
	@Test
	public void testSphereFnOptCG(){
		int[] numLineSearchIter = {1,2,5,10};
		for( int n : numLineSearchIter )
			testSphereFnOptHelper(OptimizationAlgorithm.CONJUGATE_GRADIENT,n,2);
		
		for( int n : numLineSearchIter )
			testSphereFnOptHelper(OptimizationAlgorithm.CONJUGATE_GRADIENT,n,10);
		
		for( int n : numLineSearchIter )
			testSphereFnOptHelper(OptimizationAlgorithm.CONJUGATE_GRADIENT,n,100);
	}
	
	@Test
	public void testSphereFnOptLBFGS(){
		int[] numLineSearchIter = {1,2,5,10};
		for( int n : numLineSearchIter )
			testSphereFnOptHelper(OptimizationAlgorithm.LBFGS,n,2);
		
		for( int n : numLineSearchIter )
			testSphereFnOptHelper(OptimizationAlgorithm.LBFGS,n,10);
		
		for( int n : numLineSearchIter )
			testSphereFnOptHelper(OptimizationAlgorithm.LBFGS,n,100);
	}
	
	
	private static final boolean PRINT_OPT_RESULTS = true;
	public void testSphereFnOptHelper( OptimizationAlgorithm oa, int numLineSearchIter, int nDimensions ){
		
		if( PRINT_OPT_RESULTS ) System.out.println("---------\n Alg=" + oa
				+ ", nIter=" + numLineSearchIter + ", nDimensions=" + nDimensions );
		
		NeuralNetConfiguration conf = new NeuralNetConfiguration.Builder()
		.numLineSearchIterations(numLineSearchIter)
		.iterations(1000)
		.learningRate(0.01)
		.layer(new RBM()).batchSize(1).build();
		conf.addVariable("x");	
		
		Random rng = new DefaultRandom(12345L);
		org.nd4j.linalg.api.rng.distribution.Distribution dist
			= new org.nd4j.linalg.api.rng.distribution.impl.UniformDistribution(rng,-10, 10);
		Model m = new SphereFunctionModel(nDimensions,dist,conf);
		
		double scoreBefore = m.score();
		assertTrue(!Double.isNaN(scoreBefore) && !Double.isInfinite(scoreBefore));
		if( PRINT_OPT_RESULTS ){
			System.out.println("Before:");
			System.out.println(scoreBefore);
			System.out.println(m.params());
		}
		
		ConvexOptimizer opt;
		switch(oa){
		case STOCHASTIC_GRADIENT_DESCENT:
			opt = new StochasticGradientDescent(conf,new DefaultStepFunction(),null,m);
			break;
		case LINE_GRADIENT_DESCENT:
			opt = new LineGradientDescent(conf,new DefaultStepFunction(),null,m);
			break;
		case CONJUGATE_GRADIENT:
			opt = new ConjugateGradient(conf,new DefaultStepFunction(),null,m);
			break;
		case LBFGS:
			opt = new LBFGS(conf,new DefaultStepFunction(),null,m);
			break;
		default:
			fail("Not supported: " + oa);	
			opt = null;
			break;
		}
		
		opt.setupSearchState(m.gradientAndScore());
		opt.optimize();
		
		double scoreAfter = m.score();
		assertTrue(!Double.isNaN(scoreAfter) && !Double.isInfinite(scoreAfter));
		if( PRINT_OPT_RESULTS ){
			System.out.println("After:");
			System.out.println(scoreAfter);
			System.out.println(m.params());
		}
		
		
		
		
		assertTrue("Score did not improve after optimization (b="+scoreBefore+",a="+scoreAfter+")",scoreAfter < scoreBefore);
		
	}
	
	
	
	
	
	private static class SphereFunctionModel implements Model, Layer {
		private static final long serialVersionUID = 239156313657395826L;
		private INDArray parameters;
		private final NeuralNetConfiguration conf;
		
		
		private SphereFunctionModel( INDArray parameterInit, NeuralNetConfiguration conf ){
			this.parameters = parameterInit.dup();
			this.conf = conf;
		}
		
		private SphereFunctionModel( int nParams, org.nd4j.linalg.api.rng.distribution.Distribution distribution,
				NeuralNetConfiguration conf ){
			this.parameters = distribution.sample(new int[]{1,nParams});
			this.conf = conf;
		}

		@Override
		public void fit() { throw new UnsupportedOperationException(); }

		@Override
		public void update(INDArray gradient, String paramType) {
			if(!"x".equals(paramType)) throw new UnsupportedOperationException();
			parameters.subi(gradient);
		}

		@Override
		public double score() {
			return Nd4j.getBlasWrapper().dot(parameters, parameters);	
		}

		@Override
		public void setScore() { }

		@Override
		public void accumulateScore(double accum) { throw new UnsupportedOperationException(); }

		@Override
		public INDArray transform(INDArray data) { throw new UnsupportedOperationException(); }

		@Override
		public INDArray params() {return parameters; }

		@Override
		public int numParams() { return parameters.length(); }

		@Override
		public void setParams(INDArray params) { this.parameters = params; }

		@Override
		public void fit(INDArray data) { throw new UnsupportedOperationException(); }

		@Override
		public void iterate(INDArray input) { throw new UnsupportedOperationException(); }

		@Override
		public Gradient gradient() {
			
			INDArray gradient = parameters.mul(2);
			Gradient g = new DefaultGradient();
			g.gradientForVariable().put("x", gradient);
			return g;
		}

		@Override
		public Pair<Gradient, Double> gradientAndScore() {
			return new Pair<Gradient,Double>(gradient(),score());
		}

		@Override
		public int batchSize() { return 1; }

		@Override
		public NeuralNetConfiguration conf() { return conf; }

		@Override
		public void setConf(NeuralNetConfiguration conf) { throw new UnsupportedOperationException(); }

		@Override
		public INDArray input() {
			
			
			return Nd4j.zeros(1);
		}

		@Override
		public void validateInput() { }

		@Override
		public ConvexOptimizer getOptimizer() { throw new UnsupportedOperationException(); }

		@Override
		public INDArray getParam(String param) { return parameters; }

		@Override
		public void initParams() { throw new UnsupportedOperationException(); }

		@Override
		public Map<String, INDArray> paramTable() { throw new UnsupportedOperationException(); }

		@Override
		public void setParamTable(Map<String, INDArray> paramTable) { throw new UnsupportedOperationException(); }

		@Override
		public void setParam(String key, INDArray val) { throw new UnsupportedOperationException(); }

		@Override
		public void clear() { throw new UnsupportedOperationException(); }

		@Override
		public Type type() { throw new UnsupportedOperationException(); }

		@Override
		public Gradient error(INDArray input) { throw new UnsupportedOperationException(); }

		@Override
		public INDArray derivativeActivation(INDArray input) { throw new UnsupportedOperationException(); }

		@Override
		public Gradient calcGradient(Gradient layerError, INDArray indArray) { throw new UnsupportedOperationException(); }

		@Override
		public Gradient errorSignal(Gradient error, INDArray input){ throw new UnsupportedOperationException(); }

		@Override
		public Gradient backwardGradient(INDArray z, Layer nextLayer,
				Gradient nextGradient, INDArray activation) { throw new UnsupportedOperationException(); }

		@Override
		public void merge(Layer layer, int batchSize) { throw new UnsupportedOperationException(); }

		@Override
		public INDArray activationMean() { throw new UnsupportedOperationException(); }

		@Override
		public INDArray preOutput(INDArray x) { throw new UnsupportedOperationException(); }

		@Override
		public INDArray activate() { throw new UnsupportedOperationException(); }

		@Override
		public INDArray activate(INDArray input) { throw new UnsupportedOperationException(); }

		@Override
		public Layer transpose() { throw new UnsupportedOperationException(); }

		@Override
		public Layer clone() { throw new UnsupportedOperationException(); }

		@Override
		public Pair<Gradient, Gradient> backWard(Gradient errors,
				Gradient deltas, INDArray activation, String previousActivation) { throw new UnsupportedOperationException(); }

		@Override
		public Collection<IterationListener> getIterationListeners() { return null; }

		@Override
		public void setIterationListeners(Collection<IterationListener> listeners) { throw new UnsupportedOperationException(); }

		@Override
		public void setIndex(int index) { throw new UnsupportedOperationException(); }

		@Override
		public int getIndex() { throw new UnsupportedOperationException(); }
	}
}

<code block>


package org.deeplearning4j.optimize.solvers;

import org.apache.commons.math3.util.FastMath;
import static org.nd4j.linalg.ops.transforms.Transforms.*;

import org.deeplearning4j.exception.InvalidStepException;
import org.deeplearning4j.nn.api.Model;
import org.deeplearning4j.optimize.api.ConvexOptimizer;
import org.deeplearning4j.optimize.api.StepFunction;
import org.deeplearning4j.optimize.stepfunctions.DefaultStepFunction;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.api.ops.impl.scalar.comparison.ScalarSetValue;
import org.nd4j.linalg.api.ops.impl.transforms.comparison.Eps;
import org.nd4j.linalg.factory.Nd4j;
import org.deeplearning4j.optimize.api.LineOptimizer;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;





public class BackTrackLineSearch implements LineOptimizer  {
    private static final Logger logger = LoggerFactory.getLogger(BackTrackLineSearch.class.getName());

    private Model layer;
    private StepFunction stepFunction = new DefaultStepFunction();
    private ConvexOptimizer optimizer;
    private int maxIterations = 5;
    double stpmax = 100;

    
    
    
    
    private double relTolx = 1e-10f;
    private double absTolx = 1e-4f; 
    final double ALF = 1e-4f;

    
    public BackTrackLineSearch(Model layer, StepFunction stepFunction, ConvexOptimizer optimizer) {
        this.layer = layer;
        this.stepFunction = stepFunction;
        this.optimizer = optimizer;
    }

    
    public BackTrackLineSearch(Model optimizable, ConvexOptimizer optimizer) {
        this(optimizable, new DefaultStepFunction(),optimizer);
    }


    public void setStpmax(double stpmax) {
        this.stpmax = stpmax;
    }


    public double getStpmax() {
        return stpmax;
    }

    
    public void setRelTolx (double tolx) { relTolx = tolx; }

    
    public void setAbsTolx (double tolx) { absTolx = tolx; }

    public int getMaxIterations() {
        return maxIterations;
    }

    public void setMaxIterations(int maxIterations) {
        this.maxIterations = maxIterations;
    }

    
    
    

    
    

    private double getNewScore(INDArray oldParameters){
        layer.setParams(oldParameters);
        layer.setScore();
        return layer.score();
    }

    
    public double optimize (double initialStep, INDArray parameters, INDArray gradients) throws InvalidStepException {
        double test, alamin, alam, alam2, oldAlam, tmplam;
        double rhs1, rhs2, a, b, disc, f, fold, f2;

        INDArray oldParameters = parameters.dup();
        INDArray gDup = gradients.dup();
        double sum = gradients.norm2(Integer.MAX_VALUE).getDouble(0);
        double slope = Nd4j.getBlasWrapper().dot(gDup, gradients);

        INDArray maxOldParams = abs(oldParameters);
        Nd4j.getExecutioner().exec(new ScalarSetValue(maxOldParams,1));
        INDArray testMatrix = abs(gradients).divi(maxOldParams);
        test = testMatrix.max(Integer.MAX_VALUE).getDouble(0);

        alam  = 1.0; 
        alamin = relTolx / test; 
        oldAlam = 0.0;
        alam2 = 0.0;

        f2 = fold = layer.score();

        if (logger.isDebugEnabled()) {
            logger.trace ("ENTERING BACKTRACK\n");
            logger.trace("Entering BackTrackLinnSearch, value = " + fold + ",\ndirection.oneNorm:"
                    +	gDup.norm1(Integer.MAX_VALUE) + "  direction.infNorm:"+
                    FastMath.max(Float.NEGATIVE_INFINITY,abs(gDup).max(Integer.MAX_VALUE).getDouble(0)));
        }

        if(sum > stpmax) {
            logger.warn("attempted step too big. scaling: sum= " + sum +
                    ", stpmax= "+ stpmax);
            gradients.muli(stpmax / sum);
        }

        logger.debug("slope = " + slope);

        if (slope < 0)
            throw new InvalidStepException("Slope = " + slope + " is negative");
        else if (slope == 0)
            throw new InvalidStepException ("Slope = " + slope + " is zero");

        
        
        
        

        for(int iteration = 0; iteration < maxIterations; iteration++) {
            logger.trace("BackTrack loop iteration " + iteration +" : alam=" + alam +" oldAlam=" + oldAlam);
            logger.trace ("before step, x.1norm: " + parameters.norm1(Integer.MAX_VALUE) +  "\nalam: " + alam + "\noldAlam: " + oldAlam);
            assert(alam != oldAlam) : "alam == oldAlam";

            if(stepFunction == null)
                stepFunction =  new DefaultStepFunction();
            
            stepFunction.step(parameters, gradients, new Object[]{alam,oldAlam}); 
            oldAlam = alam;

            if(logger.isDebugEnabled())  {
                double norm1 = parameters.norm1(Integer.MAX_VALUE).getDouble(0);
                logger.debug ("after step, x.1norm: " + norm1);
            }

            
            

            if ((alam < alamin) || Nd4j.getExecutioner().execAndReturn(new Eps(oldParameters, parameters,
                    parameters.dup(), parameters.length())).sum(Integer.MAX_VALUE).getDouble(0) == parameters.length()) {
                f = getNewScore(oldParameters);
                logger.trace("EXITING BACKTRACK: Jump too small (alamin = "+ alamin + "). Exiting and using xold. Value = " + f);
                return 0.0;
            }

            f = getNewScore(oldParameters);
            logger.debug("value = " + f);

            

            if(f >= fold + ALF * alam * slope) {

                logger.debug("EXITING BACKTRACK: value=" + f);
                if (f < fold)
                    throw new IllegalStateException
                            ("Function did not increase: f = " + f + " < " + fold + " = fold");
                return alam;
            }

            
            

            else if(Double.isInfinite(f) || Double.isInfinite(f2)) {
                logger.warn ("Value is infinite after jump " + oldAlam + ". f="+ f +", f2=" + f2 + ". Scaling back step size...");
                tmplam = .2 * alam;
                if(alam < alamin) { 
                    f = getNewScore(oldParameters);
                    logger.warn("EXITING BACKTRACK: Jump too small. Exiting and using xold. Value="+ f );
                    return 0.0;
                }
            }

            

            else {
                if(alam == 1.0) 
                    tmplam = -slope / (2.0 * ( f - fold - slope ));
                else {
                    rhs1 = f - fold- alam * slope;
                    rhs2 = f2 - fold - alam2 * slope;
                    if(alam == alam2)
                        throw new IllegalStateException("FAILURE: dividing by alam-alam2. alam=" + alam);
                    a = ( rhs1 / (FastMath.pow(alam, 2)) - rhs2 /  ( FastMath.pow(alam2, 2) )) / (alam - alam2);
                    b = ( -alam2 * rhs1/( alam* alam ) + alam * rhs2 / ( alam2 *  alam2 )) / ( alam - alam2);
                    if(a == 0.0)
                        tmplam = -slope / (2.0 * b);
                    else {
                        disc = b * b - 3.0 * a * slope;
                        if(disc < 0.0) {
                            tmplam = .5f * alam;
                        }
                        else if (b <= 0.0)
                            tmplam = (-b + FastMath.sqrt(disc))/(3.0f * a );
                        else
                            tmplam = -slope / (b +FastMath.sqrt(disc));
                    }
                    if (tmplam > .5f * alam)
                        tmplam = .5f * alam;    
                }
            }

            alam2 = alam;
            f2 = f;
            logger.debug("tmplam:" + tmplam);
            alam = Math.max(tmplam, .1f * alam);  

        }

        return 0.0;
    }




}


<code block>
package org.deeplearning4j.plot;

import org.deeplearning4j.base.MnistFetcher;
import org.deeplearning4j.datasets.fetchers.IrisDataFetcher;
import org.deeplearning4j.datasets.fetchers.MnistDataFetcher;
import org.deeplearning4j.datasets.iterator.DataSetIterator;
import org.deeplearning4j.datasets.iterator.impl.IrisDataSetIterator;
import org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator;
import org.deeplearning4j.nn.api.OptimizationAlgorithm;
import org.deeplearning4j.nn.conf.MultiLayerConfiguration;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.conf.distribution.NormalDistribution;
import org.deeplearning4j.nn.conf.distribution.UniformDistribution;
import org.deeplearning4j.nn.conf.layers.OutputLayer;
import org.deeplearning4j.nn.conf.layers.RBM;
import org.deeplearning4j.nn.conf.override.ClassifierOverride;
import org.deeplearning4j.nn.conf.override.ConfOverride;
import org.deeplearning4j.nn.multilayer.MultiLayerNetwork;
import org.deeplearning4j.nn.weights.WeightInit;
import org.deeplearning4j.optimize.api.IterationListener;
import org.deeplearning4j.plot.iterationlistener.*;
import org.junit.Test;
import org.nd4j.linalg.dataset.DataSet;
import org.nd4j.linalg.dataset.api.iterator.fetcher.DataSetFetcher;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.lossfunctions.LossFunctions;
import static org.junit.Assert.*;

import java.io.IOException;
import java.util.Arrays;
import java.util.Collections;


public class ListenerTest {

    

    private DataSetIterator irisIter = new IrisDataSetIterator(50,50);


    @Test
    public void testNeuralNetGraphsCapturedMLPNetwork() {
        MultiLayerNetwork network = new MultiLayerNetwork(getIrisMLPSimpleConfig(new int[]{5}, "sigmoid"));
        network.init();
        DataSet data = irisIter.next();
        IterationListener listener = new NeuralNetPlotterIterationListener(1,true);

        network.setListeners(Collections.singletonList(listener));
        network.fit(data.getFeatureMatrix(), data.getLabels());
        assertNotNull(network.getListeners().get(0));
        assertEquals(listener.invoked(), true);
    }


    @Test
    public void testAccuracyGraphCaptured() {
        MultiLayerNetwork network = new MultiLayerNetwork(getIrisSimpleConfig(new int[]{10}, "sigmoid", 10));
        network.init();
        DataSet data = irisIter.next();
        IterationListener listener = new AccuracyPlotterIterationListener(1, network, data);

        network.setListeners(Collections.singletonList(listener));
        network.fit(data.getFeatureMatrix(), data.getLabels());
        assertNotNull(network.getListeners().get(0).toString());
        assertEquals(listener.invoked(), true);
    }

    @Test
    public void testMultipleGraphsCapturedForMultipleLayers() {
        
        MultiLayerNetwork network = new MultiLayerNetwork(getIrisSimpleConfig(new int[]{10, 5}, "sigmoid", 5));
        network.init();
        IterationListener listener = new GradientPlotterIterationListener(4);
        IterationListener listener2 = new LossPlotterIterationListener(4);
        network.setListeners(Arrays.asList(listener, listener2));
        while( irisIter.hasNext() ) network.fit(irisIter.next());
        assertNotNull(network.getListeners().get(1).toString());
        assertEquals(listener2.invoked(), true);
    }

    private static MultiLayerConfiguration getIrisSimpleConfig( int[] hiddenLayerSizes, String activationFunction, int iterations ) {
        MultiLayerConfiguration c = new NeuralNetConfiguration.Builder()
                .nIn(4).nOut(3)
                .weightInit(WeightInit.DISTRIBUTION)
                .dist(new NormalDistribution(0, 0.1))

                .activationFunction(activationFunction)
                .lossFunction(LossFunctions.LossFunction.RMSE_XENT)
                .optimizationAlgo(OptimizationAlgorithm.GRADIENT_DESCENT)

                .iterations(iterations)
                .batchSize(1)
                .constrainGradientToUnitNorm(false)
                .corruptionLevel(0.0)

                .layer(new RBM())
                .learningRate(0.1).useAdaGrad(false)

                .regularization(false)
                .l1(0.0)
                .l2(0.0)
                .dropOut(0.0)
                .momentum(0.0)
                .applySparsity(false).sparsity(0.0)
                .seed(12345L)

                .list(hiddenLayerSizes.length + 1).hiddenLayerSizes(hiddenLayerSizes)
                .useDropConnect(false)

                .override(hiddenLayerSizes.length, new ConfOverride() {
                    @Override
                    public void overrideLayer(int i, NeuralNetConfiguration.Builder builder) {
                        builder.activationFunction("softmax");
                        builder.layer(new OutputLayer());
                        builder.weightInit(WeightInit.DISTRIBUTION);
                        builder.dist(new NormalDistribution(0, 0.1));
                    }
                }).build();


        return c;
    }

    private static MultiLayerConfiguration getIrisMLPSimpleConfig( int[] hiddenLayerSizes, String activationFunction ) {
        MultiLayerConfiguration c = new NeuralNetConfiguration.Builder()
                .nIn(4).nOut(3)
                .weightInit(WeightInit.DISTRIBUTION)
                .dist(new NormalDistribution(0, 0.1))

                .activationFunction(activationFunction)
                .lossFunction(LossFunctions.LossFunction.RMSE_XENT)
                .optimizationAlgo(OptimizationAlgorithm.GRADIENT_DESCENT)

                .iterations(1)
                .batchSize(1)
                .constrainGradientToUnitNorm(false)
                .corruptionLevel(0.0)

                .layer(new RBM())
                .learningRate(0.1).useAdaGrad(false)

                .regularization(false)
                .l1(0.0)
                .l2(0.0)
                .dropOut(0.0)
                .momentum(0.0)
                .applySparsity(false).sparsity(0.0)
                .seed(12345L)

                .list(hiddenLayerSizes.length + 1)
                .hiddenLayerSizes(hiddenLayerSizes)
                .backward(true).pretrain(false)
                .useDropConnect(false)

                .override(hiddenLayerSizes.length, new ConfOverride() {
                    @Override
                    public void overrideLayer(int i, NeuralNetConfiguration.Builder builder) {
                        builder.activationFunction("softmax");
                        builder.layer(new OutputLayer());
                        builder.weightInit(WeightInit.DISTRIBUTION);
                        builder.dist(new NormalDistribution(0, 0.1));
                    }
                }).build();


        return c;
    }



}

<code block>
package org.deeplearning4j.optimize.solver;

import org.deeplearning4j.datasets.iterator.DataSetIterator;
import org.deeplearning4j.datasets.iterator.impl.IrisDataSetIterator;
import org.deeplearning4j.nn.api.OptimizationAlgorithm;
import org.deeplearning4j.nn.conf.MultiLayerConfiguration;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.conf.distribution.NormalDistribution;
import org.deeplearning4j.nn.conf.layers.RBM;
import org.deeplearning4j.nn.conf.override.ConfOverride;
import org.deeplearning4j.nn.layers.OutputLayer;
import org.deeplearning4j.nn.layers.factory.LayerFactories;
import org.deeplearning4j.nn.multilayer.MultiLayerNetwork;
import org.deeplearning4j.nn.weights.WeightInit;
import org.deeplearning4j.optimize.api.IterationListener;
import org.deeplearning4j.optimize.listeners.ScoreIterationListener;
import org.deeplearning4j.optimize.solvers.BackTrackLineSearch;
import org.junit.Test;
import org.nd4j.linalg.dataset.DataSet;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.lossfunctions.LossFunctions;

import java.util.Arrays;

import static org.junit.Assert.*;


public class BackTrackLineSearchTest {

    @Test
    public void testLineSearch() throws Exception {
        DataSet data = new IrisDataSetIterator(1,1).next();
        data.normalizeZeroMeanZeroUnitVariance();

        OutputLayer layer = getIrisLogisticLayerConfig("softmax", 10);
        layer.setInput(data.getFeatureMatrix());
        layer.setLabels(data.getLabels());
        BackTrackLineSearch lineSearch = new BackTrackLineSearch(layer, layer.getOptimizer());

        double step = lineSearch.optimize(1.0, layer.params(), layer.gradient().gradient());
        assertEquals(0.0,step,1e-1);
    }

    @Test
    public void testBackTrackLine() {
        Nd4j.MAX_SLICES_TO_PRINT = -1;
        Nd4j.MAX_ELEMENTS_PER_SLICE = -1;

        DataSetIterator irisIter = new IrisDataSetIterator(1,1);
        DataSet data = irisIter.next();

        MultiLayerNetwork network = new MultiLayerNetwork(getIrisMultiLayerConfig(new int[]{5}, "sigmoid", 1));
        network.init();

        network.fit(data.getFeatureMatrix(), data.getLabels());
    }

    private static OutputLayer getIrisLogisticLayerConfig(String activationFunction, int iterations){
        NeuralNetConfiguration conf = new NeuralNetConfiguration.Builder()
                .layer(new org.deeplearning4j.nn.conf.layers.OutputLayer())
                .nIn(4)
                .nOut(3)
                .activationFunction(activationFunction)
                .lossFunction(LossFunctions.LossFunction.MCXENT)
                .optimizationAlgo(OptimizationAlgorithm.ITERATION_GRADIENT_DESCENT)
                .iterations(iterations)
                .weightInit(WeightInit.XAVIER)
                .learningRate(1e-1)
                .seed(12345L)
                .build();

        return LayerFactories.getFactory(conf.getLayer()).create(conf);


    }


    private static MultiLayerConfiguration getIrisMultiLayerConfig( int[] hiddenLayerSizes, String activationFunction, int iterations ) {
        MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
                .nIn(4).nOut(3)
                .weightInit(WeightInit.XAVIER)
                .dist(new NormalDistribution(0, 0.1))

                .activationFunction(activationFunction)
                .lossFunction(LossFunctions.LossFunction.RMSE_XENT)
                .optimizationAlgo(OptimizationAlgorithm.GRADIENT_DESCENT)
                .activationFunction("softmax")

                .iterations(iterations)
                .batchSize(1)
                .constrainGradientToUnitNorm(false)
                .corruptionLevel(0.0)

                .layer(new RBM())
                .learningRate(0.1)
                .useAdaGrad(false)
                .numLineSearchIterations(1)
                .regularization(false)
                .l1(0.0)
                .l2(0.0)
                .dropOut(0.0)
                .momentum(0.0)
                .applySparsity(false).sparsity(0.0)
                .seed(12345L)

                .list(hiddenLayerSizes.length + 1).hiddenLayerSizes(hiddenLayerSizes)
                .useDropConnect(false)

                .override(hiddenLayerSizes.length, new ConfOverride() {
                    @Override
                    public void overrideLayer(int i, NeuralNetConfiguration.Builder builder) {
                        builder.activationFunction("softmax");
                        builder.layer(new org.deeplearning4j.nn.conf.layers.OutputLayer());
                        builder.weightInit(WeightInit.DISTRIBUTION);
                        builder.dist(new NormalDistribution(0, 0.1));
                    }
                }).build();


        return conf;
    }

}

<code block>


package org.deeplearning4j.optimize.solvers;

import org.apache.commons.math3.util.FastMath;
import static org.nd4j.linalg.ops.transforms.Transforms.*;

import org.deeplearning4j.exception.InvalidStepException;
import org.deeplearning4j.nn.api.Model;
import org.deeplearning4j.optimize.api.ConvexOptimizer;
import org.deeplearning4j.optimize.api.StepFunction;
import org.deeplearning4j.optimize.stepfunctions.DefaultStepFunction;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.api.ops.impl.scalar.comparison.ScalarSetValue;
import org.nd4j.linalg.api.ops.impl.transforms.comparison.Eps;
import org.nd4j.linalg.factory.Nd4j;
import org.deeplearning4j.optimize.api.LineOptimizer;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;





public class BackTrackLineSearch implements LineOptimizer  {
    private static final Logger logger = LoggerFactory.getLogger(BackTrackLineSearch.class.getName());

    private Model function;
    private StepFunction stepFunction = new DefaultStepFunction();
    private ConvexOptimizer optimizer;
    private int maxIterations = 5;
    double stpmax = 100;

    
    
    
    
    private double relTolx = 1e-10f;
    private double absTolx = 1e-4f; 
    final double ALF = 1e-4f;

    
    public BackTrackLineSearch(Model function, StepFunction stepFunction, ConvexOptimizer optimizer) {
        this.function = function;
        this.stepFunction = stepFunction;
        this.optimizer = optimizer;
    }

    
    public BackTrackLineSearch(Model optimizable, ConvexOptimizer optimizer) {
        this(optimizable, new DefaultStepFunction(),optimizer);
    }


    public void setStpmax(double stpmax) {
        this.stpmax = stpmax;
    }


    public double getStpmax() {
        return stpmax;
    }

    
    public void setRelTolx (double tolx) { relTolx = tolx; }

    
    public void setAbsTolx (double tolx) { absTolx = tolx; }

    public int getMaxIterations() {
        return maxIterations;
    }

    public void setMaxIterations(int maxIterations) {
        this.maxIterations = maxIterations;
    }

    
    
    

    
    

    
    public double optimize (double initialStep,INDArray parameters,INDArray gradients) throws InvalidStepException {
        double slope, test, alamin, alam, alam2, tmplam;
        double rhs1, rhs2, a, b, disc, oldAlam;double f, fold, f2;
        INDArray oldParameters = parameters.dup();
        INDArray gDup = gradients.dup();

        alam2 = 0.0;
        f2 = fold = optimizer.score();

        if (logger.isDebugEnabled()) {
            logger.trace ("ENTERING BACKTRACK\n");
            logger.trace("Entering BackTrackLinnSearch, value = " + fold + ",\ndirection.oneNorm:"
                    +	gDup.norm1(Integer.MAX_VALUE) + "  direction.infNorm:"+ FastMath.max(Float.NEGATIVE_INFINITY,abs(gDup).max(Integer.MAX_VALUE).getDouble(0)));
        }

        double sum = gradients.norm2(Integer.MAX_VALUE).getDouble(0);
        if(sum > stpmax) {
            logger.warn("attempted step too big. scaling: sum= " + sum +
                    ", stpmax= "+ stpmax);
            gradients.muli(stpmax / sum);
        }

        
        slope = Nd4j.getBlasWrapper().dot(gDup, gradients);
        logger.debug("slope = " + slope);

        if (slope < 0)
            throw new InvalidStepException("Slope = " + slope + " is negative");

        if (slope == 0)
            throw new InvalidStepException ("Slope = " + slope + " is zero");

        
        
        
        
        INDArray maxOldParams = abs(oldParameters);
        Nd4j.getExecutioner().exec(new ScalarSetValue(maxOldParams,1));



        INDArray testMatrix = abs(gradients).divi(maxOldParams);
        test = testMatrix.max(Integer.MAX_VALUE).getDouble(0);

        alamin = relTolx / test;

        alam  = 1.0;
        oldAlam = 0.0;
        int iteration;
        
        for(iteration = 0; iteration < maxIterations; iteration++) {
            
            logger.trace("BackTrack loop iteration " + iteration +" : alam=" + alam +" oldAlam=" + oldAlam);
            logger.trace ("before step, x.1norm: " + parameters.norm1(Integer.MAX_VALUE) +  "\nalam: " + alam + "\noldAlam: " + oldAlam);
            assert(alam != oldAlam) : "alam == oldAlam";

            if(stepFunction == null)
                stepFunction =  new DefaultStepFunction();
            
            stepFunction.step(parameters, gradients, new Object[]{alam,oldAlam}); 

            if(logger.isDebugEnabled())  {
                double norm1 = parameters.norm1(Integer.MAX_VALUE).getDouble(0);
                logger.debug ("after step, x.1norm: " + norm1);
            }

            
            
            

            if ((alam < alamin) || Nd4j.getExecutioner().execAndReturn(new Eps(oldParameters, parameters,
                    parameters.dup(), parameters.length())).sum(Integer.MAX_VALUE).getDouble(0) == parameters.length()) {
                function.setParams(oldParameters);
                function.setScore();
                f = function.score();
                logger.trace("EXITING BACKTRACK: Jump too small (alamin = "+ alamin + "). Exiting and using xold. Value = " + f);
                return 0.0;
            }

            function.setParams(parameters);
            oldAlam = alam;
            function.setScore();
            f = function.score();

            logger.debug("value = " + f);

            
            if(f >= fold + ALF * alam * slope) {

                logger.debug("EXITING BACKTRACK: value=" + f);

                if (f < fold)
                    throw new IllegalStateException
                            ("Function did not increase: f = " + f + " < " + fold + " = fold");
                return alam;
            }


            
            
            else if(Double.isInfinite(f) || Double.isInfinite(f2)) {
                logger.warn ("Value is infinite after jump " + oldAlam + ". f="+ f +", f2=" + f2 + ". Scaling back step size...");
                tmplam = .2 * alam;
                if(alam < alamin) { 
                    function.setParams(oldParameters);
                    function.setScore();
                    f = function.score();
                    logger.warn("EXITING BACKTRACK: Jump too small. Exiting and using xold. Value="+ f );
                    return 0.0;
                }
            }
            else { 
                if(alam == 1.0) 
                    tmplam = -slope / (2.0 * ( f - fold - slope ));
                else {
                    rhs1 = f - fold- alam * slope;
                    rhs2 = f2 - fold - alam2 * slope;
                    if((alam - alam2) == 0)
                        throw new IllegalStateException("FAILURE: dividing by alam-alam2. alam=" + alam);
                    a = ( rhs1 / (FastMath.pow(alam, 2)) - rhs2 /  ( FastMath.pow(alam2, 2) )) / (alam - alam2);
                    b = ( -alam2 * rhs1/( alam* alam ) + alam * rhs2 / ( alam2 *  alam2 )) / ( alam - alam2);
                    if(a == 0.0)
                        tmplam = -slope / (2.0 * b);
                    else {
                        disc = b * b - 3.0 * a * slope;
                        if(disc < 0.0) {
                            tmplam = .5f * alam;
                        }
                        else if (b <= 0.0)
                            tmplam = (-b + FastMath.sqrt(disc))/(3.0f * a );
                        else
                            tmplam = -slope / (b +FastMath.sqrt(disc));
                    }
                    if (tmplam > .5f * alam)
                        tmplam = .5f * alam;    
                }
            }

            alam2 = alam;
            f2 = f;
            logger.debug("tmplam:" + tmplam);
            alam = Math.max(tmplam, .1f * alam);  

        }

        return 0.0;
    }




}


<code block>
package org.deeplearning4j.plot;

import org.deeplearning4j.base.MnistFetcher;
import org.deeplearning4j.datasets.fetchers.IrisDataFetcher;
import org.deeplearning4j.datasets.fetchers.MnistDataFetcher;
import org.deeplearning4j.datasets.iterator.DataSetIterator;
import org.deeplearning4j.datasets.iterator.impl.IrisDataSetIterator;
import org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator;
import org.deeplearning4j.nn.api.OptimizationAlgorithm;
import org.deeplearning4j.nn.conf.MultiLayerConfiguration;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.conf.distribution.NormalDistribution;
import org.deeplearning4j.nn.conf.distribution.UniformDistribution;
import org.deeplearning4j.nn.conf.layers.OutputLayer;
import org.deeplearning4j.nn.conf.layers.RBM;
import org.deeplearning4j.nn.conf.override.ClassifierOverride;
import org.deeplearning4j.nn.conf.override.ConfOverride;
import org.deeplearning4j.nn.multilayer.MultiLayerNetwork;
import org.deeplearning4j.nn.weights.WeightInit;
import org.deeplearning4j.optimize.api.IterationListener;
import org.deeplearning4j.plot.iterationlistener.*;
import org.junit.Test;
import org.nd4j.linalg.dataset.DataSet;
import org.nd4j.linalg.dataset.api.iterator.fetcher.DataSetFetcher;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.lossfunctions.LossFunctions;
import static org.junit.Assert.*;

import java.io.IOException;
import java.util.Arrays;
import java.util.Collections;


public class ListenerTest {

    

    private DataSetIterator irisIter = new IrisDataSetIterator(50,50);


    @Test
    public void testNeuralNetGraphsCapturedMLPNetwork() {
        MultiLayerNetwork network = new MultiLayerNetwork(getIrisMLPSimpleConfig(new int[]{5}, "sigmoid"));
        network.init();
        DataSet data = irisIter.next();
        IterationListener listener = new NeuralNetPlotterIterationListener(1,true);

        network.setListeners(Collections.singletonList(listener));
        network.fit(data.getFeatureMatrix(), data.getLabels());
        assertNotNull(network.getListeners().get(0));
        assertEquals(listener.invoked(), true);
    }


    @Test
    public void testAccuracyGraphCaptured() {
        MultiLayerNetwork network = new MultiLayerNetwork(getIrisSimpleConfig(new int[]{10}, "sigmoid", 10));
        network.init();
        DataSet data = irisIter.next();
        IterationListener listener = new AccuracyPlotterIterationListener(1, network, data);

        network.setListeners(Collections.singletonList(listener));
        network.fit(data.getFeatureMatrix(), data.getLabels());
        assertNotNull(network.getListeners().get(0).toString());
        assertEquals(listener.invoked(), true);
    }

    @Test
    public void testMultipleGraphsCapturedForMultipleLayers() {
        
        MultiLayerNetwork network = new MultiLayerNetwork(getIrisSimpleConfig(new int[]{10, 5}, "sigmoid", 5));
        network.init();
        IterationListener listener = new GradientPlotterIterationListener(4);
        IterationListener listener2 = new LossPlotterIterationListener(4);
        network.setListeners(Arrays.asList(listener, listener2));
        while( irisIter.hasNext() ) network.fit(irisIter.next());
        assertNotNull(network.getListeners().get(1).toString());
        assertEquals(listener2.invoked(), true);
    }

    private static MultiLayerConfiguration getIrisSimpleConfig( int[] hiddenLayerSizes, String activationFunction, int iterations ) {
        MultiLayerConfiguration c = new NeuralNetConfiguration.Builder()
                .nIn(4).nOut(3)
                .weightInit(WeightInit.DISTRIBUTION)
                .dist(new NormalDistribution(0, 0.1))

                .activationFunction(activationFunction)
                .lossFunction(LossFunctions.LossFunction.RMSE_XENT)
                .optimizationAlgo(OptimizationAlgorithm.GRADIENT_DESCENT)

                .iterations(iterations)
                .batchSize(1)
                .constrainGradientToUnitNorm(false)
                .corruptionLevel(0.0)

                .layer(new RBM())
                .learningRate(0.1).useAdaGrad(false)

                .regularization(false)
                .l1(0.0)
                .l2(0.0)
                .dropOut(0.0)
                .momentum(0.0)
                .applySparsity(false).sparsity(0.0)
                .seed(12345L)

                .list(hiddenLayerSizes.length + 1).hiddenLayerSizes(hiddenLayerSizes)
                .useDropConnect(false)

                .override(hiddenLayerSizes.length, new ConfOverride() {
                    @Override
                    public void overrideLayer(int i, NeuralNetConfiguration.Builder builder) {
                        builder.activationFunction("softmax");
                        builder.layer(new OutputLayer());
                        builder.weightInit(WeightInit.DISTRIBUTION);
                        builder.dist(new NormalDistribution(0, 0.1));
                    }
                }).build();


        return c;
    }

    private static MultiLayerConfiguration getIrisMLPSimpleConfig( int[] hiddenLayerSizes, String activationFunction ) {
        MultiLayerConfiguration c = new NeuralNetConfiguration.Builder()
                .nIn(4).nOut(3)
                .weightInit(WeightInit.DISTRIBUTION)
                .dist(new NormalDistribution(0, 0.1))

                .activationFunction(activationFunction)
                .lossFunction(LossFunctions.LossFunction.RMSE_XENT)
                .optimizationAlgo(OptimizationAlgorithm.GRADIENT_DESCENT)

                .iterations(1)
                .batchSize(1)
                .constrainGradientToUnitNorm(false)
                .corruptionLevel(0.0)

                .layer(new RBM())
                .learningRate(0.1).useAdaGrad(false)

                .regularization(false)
                .l1(0.0)
                .l2(0.0)
                .dropOut(0.0)
                .momentum(0.0)
                .applySparsity(false).sparsity(0.0)
                .seed(12345L)

                .list(hiddenLayerSizes.length + 1)
                .hiddenLayerSizes(hiddenLayerSizes)
                .backward(true).pretrain(false)
                .useDropConnect(false)

                .override(hiddenLayerSizes.length, new ConfOverride() {
                    @Override
                    public void overrideLayer(int i, NeuralNetConfiguration.Builder builder) {
                        builder.activationFunction("softmax");
                        builder.layer(new OutputLayer());
                        builder.weightInit(WeightInit.DISTRIBUTION);
                        builder.dist(new NormalDistribution(0, 0.1));
                    }
                }).build();


        return c;
    }


    @Test
    public void testBackTrackLine() {
        Nd4j.MAX_SLICES_TO_PRINT = -1;
        Nd4j.MAX_ELEMENTS_PER_SLICE = -1;

        DataSetIterator irisIter = new IrisDataSetIterator(1,1);
        DataSet data = irisIter.next();

        MultiLayerNetwork network = new MultiLayerNetwork(getIris1ItConfig(new int[]{5}, "sigmoid", 1));
        network.init();

        network.fit(data.getFeatureMatrix(), data.getLabels());
    }


    private static MultiLayerConfiguration getIris1ItConfig( int[] hiddenLayerSizes, String activationFunction, int iterations ) {
        MultiLayerConfiguration c = new NeuralNetConfiguration.Builder()
                .nIn(4).nOut(3)
                .weightInit(WeightInit.DISTRIBUTION)
                .dist(new NormalDistribution(0, 0.1))

                .activationFunction(activationFunction)
                .lossFunction(LossFunctions.LossFunction.RMSE_XENT)
                .optimizationAlgo(OptimizationAlgorithm.GRADIENT_DESCENT)

                .iterations(iterations)
                .batchSize(1)
                .constrainGradientToUnitNorm(false)
                .corruptionLevel(0.0)

                .layer(new RBM())
                .learningRate(0.1).useAdaGrad(false)
                .numLineSearchIterations(1)
                .regularization(false)
                .l1(0.0)
                .l2(0.0)
                .dropOut(0.0)
                .momentum(0.0)
                .applySparsity(false).sparsity(0.0)
                .seed(12345L)

                .list(hiddenLayerSizes.length + 1).hiddenLayerSizes(hiddenLayerSizes)
                .useDropConnect(false)

                .override(hiddenLayerSizes.length, new ConfOverride() {
                    @Override
                    public void overrideLayer(int i, NeuralNetConfiguration.Builder builder) {
                        builder.activationFunction("softmax");
                        builder.layer(new OutputLayer());
                        builder.weightInit(WeightInit.DISTRIBUTION);
                        builder.dist(new NormalDistribution(0, 0.1));
                    }
                }).build();


        return c;
    }


}

<code block>
package org.deeplearning4j.optimize.solver;

import org.deeplearning4j.datasets.iterator.impl.IrisDataSetIterator;
import org.deeplearning4j.nn.api.OptimizationAlgorithm;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.layers.OutputLayer;
import org.deeplearning4j.nn.layers.factory.LayerFactories;
import org.deeplearning4j.nn.weights.WeightInit;
import org.deeplearning4j.optimize.api.IterationListener;
import org.deeplearning4j.optimize.listeners.ScoreIterationListener;
import org.deeplearning4j.optimize.solvers.BackTrackLineSearch;
import org.junit.Test;
import org.nd4j.linalg.dataset.DataSet;
import org.nd4j.linalg.lossfunctions.LossFunctions;

import java.util.Arrays;

import static org.junit.Assert.*;


public class BackTrackLineSearchTest {
    @Test
    public void testLineSearch() throws Exception {
        NeuralNetConfiguration conf = new NeuralNetConfiguration.Builder()
                .lossFunction(LossFunctions.LossFunction.MCXENT).optimizationAlgo(OptimizationAlgorithm.ITERATION_GRADIENT_DESCENT)
                .activationFunction("softmax")
                .iterations(10).weightInit(WeightInit.XAVIER)
                .learningRate(1e-1).nIn(4).nOut(3).layer(new org.deeplearning4j.nn.conf.layers.OutputLayer()).build();

        OutputLayer l = LayerFactories.getFactory(conf.getLayer()).create(conf, Arrays.<IterationListener>asList(new ScoreIterationListener(1)),0);
        DataSet d = new IrisDataSetIterator(1,1).next();
        d.normalizeZeroMeanZeroUnitVariance();
        l.setInput(d.getFeatureMatrix());
        l.setLabels(d.getLabels());
        BackTrackLineSearch lineSearch = new BackTrackLineSearch(l,l.getOptimizer());
        double step = lineSearch.optimize(1.0,l.params(),l.gradient().gradient());
        assertEquals(0.0,step,1e-1);
    }

}

<code block>


package org.deeplearning4j.nn.layers;

import java.io.Serializable;


import org.deeplearning4j.berkeley.Pair;
import org.deeplearning4j.eval.Evaluation;
import org.deeplearning4j.nn.api.Classifier;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.gradient.DefaultGradient;
import org.deeplearning4j.nn.gradient.Gradient;
import org.deeplearning4j.nn.params.DefaultParamInitializer;
import org.deeplearning4j.optimize.Solver;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.dataset.api.DataSet;
import org.nd4j.linalg.dataset.api.iterator.DataSetIterator;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.indexing.NDArrayIndex;
import org.nd4j.linalg.lossfunctions.LossFunctions;
import org.nd4j.linalg.util.FeatureUtil;
import org.nd4j.linalg.util.LinAlgExceptions;

import static org.nd4j.linalg.ops.transforms.Transforms.log;
import static org.nd4j.linalg.ops.transforms.Transforms.pow;
import static org.nd4j.linalg.ops.transforms.Transforms.sqrt;



public class OutputLayer extends BaseLayer implements Serializable,Classifier {

    private static final long serialVersionUID = -7065564817460914364L;
    
    private INDArray labels;

    public OutputLayer(NeuralNetConfiguration conf) {
        super(conf);
    }

    public OutputLayer(NeuralNetConfiguration conf, INDArray input) {
        super(conf, input);
    }

    

    @Override
    public  double score() {
        LinAlgExceptions.assertRows(input, labels);
        INDArray output  = output(input);
        if(conf.getLossFunction() != LossFunctions.LossFunction.RECONSTRUCTION_CROSSENTROPY)
            return  LossFunctions.score(labels,conf.getLossFunction(),output,conf.getL2(),conf.isUseRegularization());

        return  -LossFunctions.score(labels,conf.getLossFunction(),output,conf.getL2(),conf.isUseRegularization());


    }

    @Override
    public void setScore() {
        LinAlgExceptions.assertRows(input,labels);
        INDArray output  = output(input);
        score =  LossFunctions.score(labels,conf.getLossFunction(),output,conf.getL2(),conf.isUseRegularization());

    }


    @Override
    public Pair<Gradient, Double> gradientAndScore() {
        return new Pair<>(gradient(),score());
    }


    
    @Override
    public Gradient gradient() {
        LinAlgExceptions.assertRows(input, labels);


        
        INDArray netOut = activate(input);
        
        INDArray dy = netOut.sub(labels);


        INDArray wGradient = getWeightGradient();
        INDArray bGradient = dy.mean(0);
        Gradient g = new DefaultGradient();

        g.gradientForVariable().put(DefaultParamInitializer.WEIGHT_KEY,wGradient);
        g.gradientForVariable().put(DefaultParamInitializer.BIAS_KEY, bGradient);

        return g;

    }



    private INDArray getWeightGradient() {
        INDArray z = output(input);

        switch (conf.getLossFunction()) {
            case MCXENT:
                INDArray preOut = preOutput(input);
                
                INDArray pYGivenX = Nd4j.getExecutioner().execAndReturn(Nd4j.getOpFactory().createTransform("softmax",preOut),1);
                
                INDArray dy = pYGivenX.sub(labels);
                return input.transpose().mmul(dy);

            case XENT:
                INDArray xEntDiff = labels.sub(z);
                return input.transpose().mmul(xEntDiff.div(z.mul(z.rsub(1))));
            case MSE:
                INDArray mseDelta = labels.sub(z);
                return input.transpose().mmul(mseDelta.neg());
            case EXPLL:
                return input.transpose().mmul(labels.rsub(1).divi(z));
            case RMSE_XENT:
                INDArray rmseXentDiff = labels.sub(z);
                INDArray squaredrmseXentDiff = pow(rmseXentDiff, 2.0);
                INDArray sqrt = sqrt(squaredrmseXentDiff);
                return input.transpose().mmul(sqrt);
            case SQUARED_LOSS:
                return input.transpose().mmul(pow(labels.sub(z),2));
            case NEGATIVELOGLIKELIHOOD:
                return input.transpose().mmul(log(z).negi());


        }

        throw new IllegalStateException("Invalid loss function");

    }


    @Override
    public INDArray activate(INDArray input) {
        return output(input);
    }

    @Override
    public INDArray activate() {
        return output(input);
    }

    
    @Override
    public double score(DataSet data) {
        return score(data.getFeatureMatrix(), data.getLabels());
    }

    
    @Override
    public double score(INDArray examples, INDArray labels) {
        Evaluation eval = new Evaluation();
        eval.eval(labels,labelProbabilities(examples));
        return  eval.f1();

    }

    
    @Override
    public int numLabels() {
        return labels.columns();
    }

    @Override
    public void fit(DataSetIterator iter) {
        while(iter.hasNext())
            fit(iter.next());
    }

    
    @Override
    public int[] predict(INDArray d) {
        INDArray output = output(d);
        int[] ret = new int[d.rows()];
        for(int i = 0; i < ret.length; i++)
            ret[i] = Nd4j.getBlasWrapper().iamax(output.getRow(i));
        return ret;
    }

    
    @Override
    public INDArray labelProbabilities(INDArray examples) {
        return output(examples);
    }

    
    @Override
    public void fit(INDArray examples, INDArray labels) {
        this.input = examples.dup();
        applyDropOutIfNecessary(this.input);
        this.labels = labels;
        Solver solver = new Solver.Builder()
                .configure(conf())
                .listeners(getIterationListeners())
                .model(this).build();
        solver.optimize();
    }

    
    @Override
    public void fit(DataSet data) {
        fit(data.getFeatureMatrix(), data.getLabels());
    }

    
    @Override
    public void fit(INDArray examples, int[] labels) {
        INDArray outcomeMatrix = FeatureUtil.toOutcomeMatrix(labels, numLabels());
        fit(examples,outcomeMatrix);

    }

    @Override
    public void clear() {
        super.clear();
        if(labels != null) {
            labels.data().destroy();
            labels = null;
        }
    }

    
    @Override
    public INDArray transform(INDArray data) {
        return preOutput(data);
    }



    
    @Override
    public void setParams(INDArray params) {
        INDArray wParams = params.get(NDArrayIndex.interval(0, conf.getNIn() * conf.getNOut()));
        INDArray W = getParam(DefaultParamInitializer.WEIGHT_KEY);
        W.assign(wParams);
        INDArray bias = getParam(DefaultParamInitializer.BIAS_KEY);
        int biasBegin = params.length() - bias.length();
        int biasEnd = params.length();
        INDArray biasAssign = params.get(NDArrayIndex.interval(biasBegin, biasEnd));
        bias.assign(biasAssign);
    }
    
    @Override
    public void fit(INDArray data) {
        

    }

    @Override
    public void iterate(INDArray input) {
       throw new UnsupportedOperationException();
    }


    
    public  INDArray output(INDArray x) {
        return output(x,false);

    }

    
    public  INDArray output(INDArray x,boolean test) {
        if(x == null)
            throw new IllegalArgumentException("No null input allowed");

        INDArray preOutput = preOutput(x);
        if(conf.getActivationFunction().equals("softmax")) {
            INDArray ret = Nd4j.getExecutioner().execAndReturn(Nd4j.getOpFactory().createTransform("softmax", preOutput), 1);
            return ret;
        }

        this.input = x.dup();
        if(!test)
            applyDropOutIfNecessary(input());

        return super.activate();

    }

    public  INDArray getLabels() {
        return labels;
    }

    public  void setLabels(INDArray labels) {
        this.labels = labels;
    }


}

<code block>


package org.deeplearning4j.nn.multilayer;


import org.apache.commons.lang3.ArrayUtils;
import org.apache.commons.lang3.EnumUtils;
import org.deeplearning4j.berkeley.Pair;
import org.deeplearning4j.eval.Evaluation;
import org.deeplearning4j.nn.api.*;
import org.deeplearning4j.nn.conf.MultiLayerConfiguration;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.gradient.DefaultGradient;
import org.deeplearning4j.nn.gradient.Gradient;
import org.deeplearning4j.nn.layers.OutputLayer;
import org.deeplearning4j.nn.layers.factory.LayerFactories;
import org.deeplearning4j.nn.params.DefaultParamInitializer;
import org.deeplearning4j.nn.weights.WeightInit;
import org.deeplearning4j.optimize.GradientAdjustment;
import org.deeplearning4j.optimize.api.ConvexOptimizer;
import org.deeplearning4j.optimize.api.IterationListener;
import org.deeplearning4j.util.MultiLayerUtil;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.api.ops.LossFunction;
import org.nd4j.linalg.dataset.DataSet;
import org.nd4j.linalg.dataset.api.iterator.DataSetIterator;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.indexing.NDArrayIndex;
import org.nd4j.linalg.lossfunctions.LossFunctions;
import org.nd4j.linalg.ops.transforms.Transforms;
import org.nd4j.linalg.util.FeatureUtil;
import org.nd4j.linalg.util.LinAlgExceptions;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.Serializable;
import java.lang.reflect.Constructor;
import java.util.*;



public class MultiLayerNetwork implements Serializable, Classifier {


    private static final Logger log = LoggerFactory.getLogger(MultiLayerNetwork.class);
    private static final long serialVersionUID = -5029161847383716484L;
    
    protected Layer[] layers;


    
    protected INDArray input, labels;
    
    protected boolean initCalled = false;
    private List<IterationListener> listeners = new ArrayList<>();

    protected NeuralNetConfiguration defaultConfiguration;
    protected MultiLayerConfiguration layerWiseConfigurations;


    
    protected INDArray mask;


    public MultiLayerNetwork(MultiLayerConfiguration conf) {
        this.layerWiseConfigurations = conf;
        this.defaultConfiguration = conf.getConf(0);
    }

    
    public MultiLayerNetwork(String conf, INDArray params) {
        this(MultiLayerConfiguration.fromJson(conf));
        init();
        setParameters(params);
    }


    
    public MultiLayerNetwork(MultiLayerConfiguration conf, INDArray params) {
        this(conf);
        init();
        setParameters(params);
    }


    protected void intializeConfigurations() {

        if (layerWiseConfigurations == null)
            layerWiseConfigurations = new MultiLayerConfiguration.Builder().build();

        if (layers == null)
            layers = new Layer[getnLayers()];

        if (defaultConfiguration == null)
            defaultConfiguration = new NeuralNetConfiguration.Builder()
                    .build();

        
        if (layerWiseConfigurations == null || layerWiseConfigurations.getConfs().isEmpty())
            for (int i = 0; i < layerWiseConfigurations.getHiddenLayerSizes().length + 1; i++) {
                layerWiseConfigurations.getConfs().add(defaultConfiguration.clone());
            }


    }


    
    public void pretrain(DataSetIterator iter) {
        if (!layerWiseConfigurations.isPretrain())
            return;

        INDArray layerInput;

        for (int i = 0; i < getnLayers(); i++) {
            if (i == 0) {
                while (iter.hasNext()) {
                    DataSet next = iter.next();
                    this.input = next.getFeatureMatrix();
                      
                    if (this.getInput() == null || this.getLayers() == null) {
                        setInput(input);
                        initializeLayers(input);
                    } else
                        setInput(input);
                    getLayers()[i].fit(next.getFeatureMatrix());
                    log.info("Training on layer " + (i + 1) + " with " + input.slices() + " examples");


                }

                iter.reset();
            } else {
                while (iter.hasNext()) {
                    DataSet next = iter.next();
                    layerInput = next.getFeatureMatrix();
                    for (int j = 1; j <= i; j++)
                        layerInput = activationFromPrevLayer(j - 1, layerInput);

                    log.info("Training on layer " + (i + 1) + " with " + layerInput.slices() + " examples");
                    getLayers()[i].fit(layerInput);

                }

                iter.reset();


            }
        }
    }


    
    public void pretrain(INDArray input) {

        if (!layerWiseConfigurations.isPretrain())
            return;
        


        INDArray layerInput = null;

        for (int i = 0; i < getnLayers() - 1; i++) {
            if (i == 0)
                layerInput = input;
            else
                layerInput = activationFromPrevLayer(i - 1, layerInput);
            log.info("Training on layer " + (i + 1) + " with " + layerInput.slices() + " examples");
            getLayers()[i].fit(layerInput);

        }
    }


    @Override
    public int batchSize() {
        return input.slices();
    }

    @Override
    public NeuralNetConfiguration conf() {
        throw new UnsupportedOperationException();
    }

    @Override
    public void setConf(NeuralNetConfiguration conf) {
        throw new UnsupportedOperationException();
    }

    @Override
    public INDArray input() {
        return input;
    }

    @Override
    public void validateInput() {

    }

    @Override
    public ConvexOptimizer getOptimizer() {
        throw new UnsupportedOperationException();
    }

    @Override
    public INDArray getParam(String param) {
        throw new UnsupportedOperationException();

    }

    @Override
    public void initParams() {
        throw new UnsupportedOperationException();
    }

    @Override
    public Map<String, INDArray> paramTable() {
        throw new UnsupportedOperationException();
    }

    @Override
    public void setParamTable(Map<String, INDArray> paramTable) {
        throw new UnsupportedOperationException();

    }

    @Override
    public void setParam(String key, INDArray val) {
        throw new UnsupportedOperationException();

    }

    
    @Override
    public INDArray transform(INDArray data) {
        return output(data);
    }


    public MultiLayerConfiguration getLayerWiseConfigurations() {
        return layerWiseConfigurations;
    }

    public void setLayerWiseConfigurations(MultiLayerConfiguration layerWiseConfigurations) {
        this.layerWiseConfigurations = layerWiseConfigurations;
    }

    
    public void initializeLayers(INDArray input) {


        if (input == null)
            throw new IllegalArgumentException("Unable to initialize neuralNets with empty input");
        int[] hiddenLayerSizes = getLayerWiseConfigurations().getHiddenLayerSizes();
        if (input.shape().length == 2)
            for (int i = 0; i < hiddenLayerSizes.length; i++)
                if (hiddenLayerSizes[i] < 1)
                    throw new IllegalArgumentException("All hidden layer sizes must be >= 1");


        this.input = input;

        if (!initCalled)
            init();
    }

    
    public void init() {
        if (layerWiseConfigurations == null || layers == null)
            intializeConfigurations();
        if(initCalled)
            return;

        INDArray layerInput = input();
        int inputSize;
        if (getnLayers() < 1)
            throw new IllegalStateException("Unable to createComplex network neuralNets; number specified is less than 1");

        int[] hiddenLayerSizes = layerWiseConfigurations.getHiddenLayerSizes();
        int numHiddenLayersSizesUsed = 0;

        if (this.layers == null || this.layers[0] == null) {
            if(this.layers == null)
                this.layers = new Layer[getnLayers()];

            
            for (int i = 0; i < getnLayers(); i++) {
                NeuralNetConfiguration conf = layerWiseConfigurations.getConf(i);
                Layer.Type type = LayerFactories.typeForFactory(conf);

                if (i == 0) {
                    inputSize = conf.getNIn();
                    if (input == null) {
                        input = Nd4j.ones(inputSize);
                        layerInput = input;
                    }
                    conf.setNIn(inputSize);

                    if (type == Layer.Type.FEED_FORWARD) {
                        conf.setNOut(hiddenLayerSizes[numHiddenLayersSizesUsed]);
                    }
                }
                else if (i < getLayers().length) {
                    if (input != null)
                        layerInput = activationFromPrevLayer(i - 1, layerInput);
                    
                    if(type == Layer.Type.FEED_FORWARD) {
                        if(i!=(layers.length-1)) {
                            numHiddenLayersSizesUsed++;
                            conf.setNIn(layerInput.columns());
                            conf.setNOut(hiddenLayerSizes[numHiddenLayersSizesUsed]);
                        } else {
                            conf.setNIn(hiddenLayerSizes[numHiddenLayersSizesUsed]);
                        }
                    }
                }
                layers[i] = LayerFactories.getFactory(conf).create(conf, listeners);
            }
            initCalled = true;
            initMask();
        }
    }


    
    public INDArray activate() {
        return getLayers()[getLayers().length - 1].activate();
    }

    
    public INDArray activate(int layer) {
        return getLayers()[layer].activate();
    }

    
    public INDArray activate(int layer, INDArray input) {
        return getLayers()[layer].activate(input);
    }


    
    public void initialize(DataSet data) {
        setInput(data.getFeatureMatrix());
        feedForward(getInput());
        this.labels = data.getLabels();
        if (getOutputLayer() instanceof OutputLayer) {
            OutputLayer o = (OutputLayer) getOutputLayer();
            o.setLabels(labels);

        }
    }


    
    public INDArray zFromPrevLayer(int curr, INDArray input) {
        if(getLayerWiseConfigurations().getInputPreProcess(curr) != null)
            input = getLayerWiseConfigurations().getInputPreProcess(curr).preProcess(input);
        INDArray ret = layers[curr].preOutput(input);
        if (getLayerWiseConfigurations().getProcessors() != null && getLayerWiseConfigurations().getPreProcessor(curr) != null) {
            ret = getLayerWiseConfigurations().getPreProcessor(curr).preProcess(ret);
            return ret;
        }
        return ret;
    }

    
    public INDArray activationFromPrevLayer(int curr, INDArray input) {
        if(getLayerWiseConfigurations().getInputPreProcess(curr) != null)
            input = getLayerWiseConfigurations().getInputPreProcess(curr).preProcess(input);
        INDArray ret = layers[curr].activate(input);
        if (getLayerWiseConfigurations().getProcessors() != null && getLayerWiseConfigurations().getPreProcessor(curr) != null) {
            ret = getLayerWiseConfigurations().getPreProcessor(curr).preProcess(ret);
            return ret;
        }
        return ret;
    }


    
    public List<INDArray> computeZ() {
        INDArray currInput = this.input;

        List<INDArray> activations = new ArrayList<>();
        activations.add(currInput);

        for (int i = 0; i < layers.length; i++) {
            currInput = zFromPrevLayer(i, currInput);
            
            applyDropConnectIfNecessary(currInput);
            activations.add(currInput);
        }


        return activations;
    }

    
    public List<INDArray> computeZ(INDArray input) {
        if (input == null)
            throw new IllegalStateException("Unable to perform feed forward; no input found");
        else if(this.getLayerWiseConfigurations().getInputPreProcess(0) != null)
            this.input = getLayerWiseConfigurations().getInputPreProcess(0).preProcess(input);
        else
            this.input = input;
        return computeZ();
    }

    
    public List<INDArray> feedForward(INDArray input,boolean test) {
        this.input = input;
        return feedForward(test);
    }

    
    public List<INDArray> feedForward(boolean test) {
        INDArray currInput = this.input;

        List<INDArray> activations = new ArrayList<>();
        activations.add(currInput);

        for (int i = 0; i < layers.length; i++) {
            currInput = activationFromPrevLayer(i, currInput);
            
            if(!test)
                applyDropConnectIfNecessary(currInput);
            activations.add(currInput);
        }


        return activations;
    }

    
    public List<INDArray> feedForward() {
        return feedForward(false);
    }


    
    public Pair<List<INDArray>,List<INDArray>> feedForwardActivationsAndDerivatives() {
        INDArray currInput = this.input;

        List<INDArray> activations = new ArrayList<>();
        List<INDArray> derivatives = new ArrayList<>();
        activations.add(currInput);

        for (int i = 0; i < layers.length; i++) {
            currInput = zFromPrevLayer(i, currInput); 
            applyDropConnectIfNecessary(currInput);
            activations.add(Nd4j.getExecutioner().execAndReturn(Nd4j.getOpFactory().createTransform(layerWiseConfigurations.getConf(i).getActivationFunction(), currInput)));
        }

        currInput = this.input;
        for (int i = 0; i < layers.length; i++) {
            currInput = zFromPrevLayer(i, currInput); 
            applyDropConnectIfNecessary(currInput);
            INDArray dup = currInput.dup();
            derivatives.add(Nd4j.getExecutioner().execAndReturn(Nd4j.getOpFactory().createTransform(layerWiseConfigurations.getConf(i).getActivationFunction(), dup).derivative()));
            Nd4j.getOpFactory().createTransform(layerWiseConfigurations.getConf(i).getActivationFunction(), currInput);
        }
        
        derivatives.add(derivatives.get(layers.length - 1));
        return new Pair<>(activations, derivatives);
    }


    
    public List<INDArray> feedForward(INDArray input) {
        if (input == null)
            throw new IllegalStateException("Unable to perform feed forward; no input found");
        else if(this.getLayerWiseConfigurations().getInputPreProcess(0) != null)
            this.input = getLayerWiseConfigurations().getInputPreProcess(0).preProcess(input);
        else
            this.input = input;
        return feedForward();
    }

    @Override
    public Gradient gradient() {
        Gradient ret = new DefaultGradient();
        for (int i = 0; i < layers.length; i ++) {
            ret.gradientForVariable().put(String.valueOf(i), layers[i].gradient().gradient());
        }

        return ret;
    }

    @Override
    public Pair<Gradient, Double> gradientAndScore() {
        return new Pair<>(gradient(), getOutputLayer().score());
    }

    
    protected void applyDropConnectIfNecessary(INDArray input) {
        if (layerWiseConfigurations.isUseDropConnect()) {
            INDArray mean = Nd4j.valueArrayOf(input.slices(), input.columns(), 0.5);
            INDArray mask =Nd4j.getDistributions().createBinomial(1,mean).sample(mean.shape());
            input.muli(mask);
            
            if (defaultConfiguration.getL2() > 0)
                input.muli(defaultConfiguration.getL2());
        }
    }


    
    protected List<INDArray> computeDeltasR(INDArray v) {
        List<INDArray> deltaRet = new ArrayList<>();

        INDArray[] deltas = new INDArray[getnLayers() + 1];
        List<INDArray> activations = feedForward();
        List<INDArray> rActivations = feedForwardR(activations, v);
      
        List<INDArray> weights = new ArrayList<>();
        List<INDArray> biases = new ArrayList<>();
        List<String> activationFunctions = new ArrayList<>();


        for (int j = 0; j < getLayers().length; j++) {
            weights.add(getLayers()[j].getParam(DefaultParamInitializer.WEIGHT_KEY));
            biases.add(getLayers()[j].getParam(DefaultParamInitializer.BIAS_KEY));
            activationFunctions.add(getLayers()[j].conf().getActivationFunction());
        }


        INDArray rix = rActivations.get(rActivations.size() - 1).divi((double) input.slices());
        LinAlgExceptions.assertValidNum(rix);

        
        for (int i = getnLayers() - 1; i >= 0; i--) {
            
            deltas[i] = activations.get(i).transpose().mmul(rix);
            applyDropConnectIfNecessary(deltas[i]);

            if (i > 0)
                rix = rix.mmul(weights.get(i).addRowVector(biases.get(i)).transpose()).muli(Nd4j.getExecutioner().execAndReturn(Nd4j.getOpFactory().createTransform(activationFunctions.get(i - 1),activations.get(i)).derivative()));

        }

        for (int i = 0; i < deltas.length - 1; i++) {
            if (defaultConfiguration.isConstrainGradientToUnitNorm()) {
                double sum = deltas[i].sum(Integer.MAX_VALUE).getDouble(0);
                if (sum > 0)
                    deltaRet.add(deltas[i].div(deltas[i].norm2(Integer.MAX_VALUE)));
                else
                    deltaRet.add(deltas[i]);
            } else
                deltaRet.add(deltas[i]);
            LinAlgExceptions.assertValidNum(deltaRet.get(i));
        }

        return deltaRet;
    }


    
    public void dampingUpdate(double rho, double boost, double decrease) {
        if (rho < 0.25 || Double.isNaN(rho))
            layerWiseConfigurations.setDampingFactor(getLayerWiseConfigurations().getDampingFactor() * boost);


        else if (rho > 0.75)
            layerWiseConfigurations.setDampingFactor(getLayerWiseConfigurations().getDampingFactor() * decrease);
    }

    
    public double reductionRatio(INDArray p, double currScore, double score, INDArray gradient) {
        double currentDamp = layerWiseConfigurations.getDampingFactor();
        layerWiseConfigurations.setDampingFactor(0);
        INDArray denom = getBackPropRGradient(p);
        denom.muli(0.5).muli(p.mul(denom)).sum(0);
        denom.subi(gradient.mul(p).sum(0));
        double rho = (currScore - score) / (double) denom.getScalar(0).element();
        layerWiseConfigurations.setDampingFactor(currentDamp);
        if (score - currScore > 0)
            return Float.NEGATIVE_INFINITY;
        return rho;
    }


    
    protected List<Pair<INDArray, INDArray>> computeDeltas2() {
        List<Pair<INDArray, INDArray>> deltaRet = new ArrayList<>();
        List<INDArray> activations = feedForward();
        INDArray[] deltas = new INDArray[activations.size() - 1];
        INDArray[] preCons = new INDArray[activations.size() - 1];


        
        INDArray ix = activations.get(activations.size() - 1).sub(labels).div(labels.slices());

       	
        List<INDArray> weights = new ArrayList<>();
        List<INDArray> biases = new ArrayList<>();

        List<String> activationFunctions = new ArrayList<>();
        for (int j = 0; j < getLayers().length; j++) {
            weights.add(getLayers()[j].getParam(DefaultParamInitializer.WEIGHT_KEY));
            biases.add(getLayers()[j].getParam(DefaultParamInitializer.BIAS_KEY));
            activationFunctions.add(getLayers()[j].conf().getActivationFunction());
        }


        
        for (int i = weights.size() - 1; i >= 0; i--) {
            deltas[i] = activations.get(i).transpose().mmul(ix);
            preCons[i] = Transforms.pow(activations.get(i).transpose(), 2).mmul(Transforms.pow(ix, 2)).muli(labels.slices());
            applyDropConnectIfNecessary(deltas[i]);

            if (i > 0) {
                
                ix = ix.mmul(weights.get(i).transpose()).muli(Nd4j.getExecutioner().execAndReturn(Nd4j.getOpFactory().createTransform(activationFunctions.get(i - 1),activations.get(i)).derivative()));
            }
        }

        for (int i = 0; i < deltas.length; i++) {
            if (defaultConfiguration.isConstrainGradientToUnitNorm())
                deltaRet.add(new Pair<>(deltas[i].divi(deltas[i].norm2(Integer.MAX_VALUE)), preCons[i]));

            else
                deltaRet.add(new Pair<>(deltas[i], preCons[i]));

        }

        return deltaRet;
    }



    
    public INDArray getBackPropRGradient(INDArray v) {
        return pack(backPropGradientR(v));
    }


    
    public Pair<INDArray, INDArray> getBackPropGradient2() {
        List<Pair<Pair<INDArray, INDArray>, Pair<INDArray, INDArray>>> deltas = backPropGradient2();
        List<Pair<INDArray, INDArray>> deltaNormal = new ArrayList<>();
        List<Pair<INDArray, INDArray>> deltasPreCon = new ArrayList<>();
        for (int i = 0; i < deltas.size(); i++) {
            deltaNormal.add(deltas.get(i).getFirst());
            deltasPreCon.add(deltas.get(i).getSecond());
        }


        return new Pair<>(pack(deltaNormal), pack(deltasPreCon));
    }


    @Override
    public MultiLayerNetwork clone() {
        MultiLayerNetwork ret;
        try {
            Constructor<MultiLayerNetwork> constructor = (Constructor<MultiLayerNetwork>) getClass().getDeclaredConstructor(MultiLayerConfiguration.class);
            ret = constructor.newInstance(getLayerWiseConfigurations());
            ret.update(this);

        } catch (Exception e) {
            throw new IllegalStateException("Unable to cloe network");
        }
        return ret;
    }


    
    @Override
    public INDArray params() {
        List<INDArray> params = new ArrayList<>();
        for (int i = 0; i < getnLayers(); i++)
            params.add(layers[i].params());

        return Nd4j.toFlattened(params);
    }

    
    @Override
    public void setParams(INDArray params) {
        setParameters(params);

    }


    
    @Override
    public int numParams() {
        int length = 0;
        for (int i = 0; i < layers.length; i++)
            length += layers[i].numParams();

        return length;

    }

    

    public INDArray pack() {
        return params();

    }

    
    public INDArray pack(List<Pair<INDArray, INDArray>> layers) {
        List<INDArray> list = new ArrayList<>();

        for (Pair<INDArray, INDArray> layer : layers) {
            list.add(layer.getFirst());
            list.add(layer.getSecond());
        }
        return Nd4j.toFlattened(list);
    }


    
    @Override
    public double score(org.nd4j.linalg.dataset.api.DataSet data) {
        return score(data.getFeatureMatrix(), data.getLabels());
    }



    
    public List<Pair<INDArray, INDArray>> unPack(INDArray param) {
        
        if (param.slices() != 1)
            param = param.reshape(1, param.length());
        List<Pair<INDArray, INDArray>> ret = new ArrayList<>();
        int curr = 0;
        for (int i = 0; i < layers.length; i++) {
            int layerLength = layers[i].getParam(DefaultParamInitializer.WEIGHT_KEY).length() + layers[i].getParam(DefaultParamInitializer.BIAS_KEY).length();
            INDArray subMatrix = param.get(NDArrayIndex.interval(curr, curr + layerLength));
            INDArray weightPortion = subMatrix.get(NDArrayIndex.interval(0, layers[i].getParam(DefaultParamInitializer.WEIGHT_KEY).length()));

            int beginHBias = layers[i].getParam(DefaultParamInitializer.WEIGHT_KEY).length();
            int endHbias = subMatrix.length();
            INDArray hBiasPortion = subMatrix.get(NDArrayIndex.interval(beginHBias, endHbias));
            int layerLengthSum = weightPortion.length() + hBiasPortion.length();
            if (layerLengthSum != layerLength) {
                if (hBiasPortion.length() != layers[i].getParam(DefaultParamInitializer.BIAS_KEY).length())
                    throw new IllegalStateException("Hidden bias on layer " + i + " was off");
                if (weightPortion.length() != layers[i].getParam(DefaultParamInitializer.WEIGHT_KEY).length())
                    throw new IllegalStateException("Weight portion on layer " + i + " was off");

            }

            ret.add(new Pair<>(weightPortion.reshape(layers[i].getParam(DefaultParamInitializer.WEIGHT_KEY).slices(), layers[i].getParam(DefaultParamInitializer.WEIGHT_KEY).columns()), hBiasPortion.reshape(layers[i].getParam(DefaultParamInitializer.BIAS_KEY).slices(), layers[i].getParam(DefaultParamInitializer.BIAS_KEY).columns())));
            curr += layerLength;
        }


        return ret;
    }

    
    protected List<Pair<Pair<INDArray, INDArray>, Pair<INDArray, INDArray>>> backPropGradient2() {
        
        

        
        List<Pair<INDArray, INDArray>> deltas = computeDeltas2();


        List<Pair<Pair<INDArray, INDArray>, Pair<INDArray, INDArray>>> list = new ArrayList<>();
        List<Pair<INDArray, INDArray>> grad = new ArrayList<>();
        List<Pair<INDArray, INDArray>> preCon = new ArrayList<>();

        for (int l = 0; l < deltas.size(); l++) {
            INDArray gradientChange = deltas.get(l).getFirst();
            INDArray preConGradientChange = deltas.get(l).getSecond();


            if (l < layers.length && gradientChange.length() != layers[l].getParam(DefaultParamInitializer.WEIGHT_KEY).length())
                throw new IllegalStateException("Gradient change not equal to weight change");

            
            INDArray deltaColumnSums = deltas.get(l).getFirst().mean(0);
            INDArray preConColumnSums = deltas.get(l).getSecond().mean(0);

            grad.add(new Pair<>(gradientChange, deltaColumnSums));
            preCon.add(new Pair<>(preConGradientChange, preConColumnSums));
            if (l < layers.length && deltaColumnSums.length() != layers[l].getParam(DefaultParamInitializer.BIAS_KEY).length())
                throw new IllegalStateException("Bias change not equal to weight change");
            else if (l == getLayers().length && deltaColumnSums.length() != getOutputLayer().getParam(DefaultParamInitializer.BIAS_KEY).length())
                throw new IllegalStateException("Bias change not equal to weight change");


        }

        INDArray g = pack(grad);
        INDArray con = pack(preCon);
        INDArray theta = params();


        if (mask == null)
            initMask();

        g.addi(theta.mul(defaultConfiguration.getL2()).muli(mask));

        INDArray conAdd = Transforms.pow(mask.mul(defaultConfiguration.getL2()).add(Nd4j.valueArrayOf(g.slices(), g.columns(), layerWiseConfigurations.getDampingFactor())), 3.0 / 4.0);

        con.addi(conAdd);

        List<Pair<INDArray, INDArray>> gUnpacked = unPack(g);

        List<Pair<INDArray, INDArray>> conUnpacked = unPack(con);

        for (int i = 0; i < gUnpacked.size(); i++)
            list.add(new Pair<>(gUnpacked.get(i), conUnpacked.get(i)));


        return list;

    }


    @Override
    public void fit(DataSetIterator iter) {
        if (layerWiseConfigurations.isPretrain()) {
            pretrain(iter);
            iter.reset();
            finetune(iter);
        }
        if(layerWiseConfigurations.isBackward()) {
            iter.reset();
            while(iter.hasNext()) {
                DataSet next = iter.next();
                doBackWard(next.getFeatureMatrix(),next.getLabels());

            }
        }
    }

    
    protected void doBackWard(INDArray input,INDArray labels) {
        setInput(input);
        this.labels = labels;
        Gradient nextGradients = new DefaultGradient();

        if(!(getOutputLayer() instanceof  OutputLayer)) {
            log.warn("Warning: final layer isn't output layer. You can ignore this message if you just intend on using a a deep neural network with no output layer.");
            return;
        }

        OutputLayer output = (OutputLayer) getOutputLayer();
        if(labels == null)
            throw new IllegalStateException("No labels found");
        if(output.conf().getWeightInit() == WeightInit.ZERO){
            throw new IllegalStateException("Output layer weights cannot be intialized to zero when using backprop.");
        };
        output.setLabels(labels);

        
        for(int i = 0; i < getLayerWiseConfigurations().getConf(0).getNumIterations(); i++) {
            
            int numLayers = getnLayers();
            List<Gradient> gradientUpdates = new ArrayList<>();
            Pair<List<INDArray>,List<INDArray>> activationsAndDeriv = feedForwardActivationsAndDerivatives();
            List<INDArray> activations = activationsAndDeriv.getFirst();
            INDArray outputActivation = activations.get(activations.size() - 1);

            List<INDArray> derivatives = activationsAndDeriv.getSecond();
            INDArray activationDeriv = derivatives.get(derivatives.size() - 1);
            INDArray layerInput = activations.get(activations.size() - 2);

            INDArray delta = outputActivation.sub(labels).transpose();

            
            if(output.conf().getLossFunction() != LossFunctions.LossFunction.XENT) {
                delta.muli(activationDeriv);
            }

            nextGradients.gradientForVariable().put(DefaultParamInitializer.WEIGHT_KEY, delta.mmul(layerInput).transpose());
            nextGradients.gradientForVariable().put(DefaultParamInitializer.BIAS_KEY, delta.transpose());

            gradientUpdates.add(nextGradients);

            
            for(int j = numLayers - 2; j >= 0; j--) {
                
                INDArray currActivation = activations.get(j);
                INDArray currDerivative = derivatives.get(j);
                Layer nextLayer = getLayers()[j + 1];
                nextGradients = getLayers()[j].backwardGradient(currDerivative, nextLayer, nextGradients, currActivation);
                gradientUpdates.add(nextGradients);
            }

            Collections.reverse(gradientUpdates);
            
            for(int k = 0; k < numLayers; k++) {
                Layer currLayer = getLayers()[k];
                for(String paramType : gradientUpdates.get(k).gradientForVariable().keySet()) {
                    INDArray gradient = gradientUpdates.get(k).getGradientFor(paramType);
                    
                    GradientAdjustment.updateGradientAccordingToParams(
                            i
                            ,input.slices()
                            ,currLayer.conf()
                            ,currLayer.getParam(paramType)
                            ,gradient
                            ,currLayer.getOptimizer().adaGradForVariables().get(paramType)
                            ,currLayer.getOptimizer().getLastStep().get(paramType)
                            ,paramType
                    );
                    currLayer.update(gradient, paramType);
                }
            }
            for(IterationListener listener :  listeners)
                listener.iterationDone(getOutputLayer(),i);
        }

    }


    public List<IterationListener> getListeners() {
        return listeners;
    }

    public void setListeners(List<IterationListener> listeners) {
        this.listeners = listeners;

        if(layers == null) {
            init();
        }
        for(Layer layer : layers) {
            layer.setIterationListeners(listeners);
        }
    }

    
    public void finetune(DataSetIterator iter) {
        log.info("Finetune phase ");
        iter.reset();

        while (iter.hasNext()) {
            DataSet data = iter.next();
            if (data.getFeatureMatrix() == null || data.getLabels() == null)
                break;

            setInput(data.getFeatureMatrix());
            setLabels(data.getLabels());
            if (getOutputLayer().conf().getOptimizationAlgo() != OptimizationAlgorithm.HESSIAN_FREE) {
                feedForward();
                if (getOutputLayer() instanceof OutputLayer) {
                    OutputLayer o = (OutputLayer) getOutputLayer();
                    o.setIterationListeners(getListeners());
                    o.fit(o.input(),getLabels());

                }
            } else {
                throw new UnsupportedOperationException();
            }

        }


    }


    
    public void finetune(INDArray labels) {
        if (labels != null)
            this.labels = labels;
        if (!(getOutputLayer() instanceof OutputLayer)) {
            log.warn("Output layer not instance of output layer returning.");
            return;
        }

        log.info("Finetune phase");
        OutputLayer o = (OutputLayer) getOutputLayer();
        if (getOutputLayer().conf().getOptimizationAlgo() != OptimizationAlgorithm.HESSIAN_FREE) {
            List<INDArray> activations = feedForward();
            o.setIterationListeners(getListeners());
            o.fit(activations.get(activations.size() - 2), labels);
        }

        else {
            throw new UnsupportedOperationException();
        }
    }


    
    @Override
    public int[] predict(INDArray d) {
        INDArray output = output(d);
        int[] ret = new int[d.slices()];
        if (d.isRowVector()) ret[0] = Nd4j.getBlasWrapper().iamax(output);
        else {
            for (int i = 0; i < ret.length; i++)
                ret[i] = Nd4j.getBlasWrapper().iamax(output.getRow(i));
        }
        return ret;
    }

    
    @Override
    public INDArray labelProbabilities(INDArray examples) {
        List<INDArray> feed = feedForward(examples);
        OutputLayer o = (OutputLayer) getOutputLayer();
        return o.labelProbabilities(feed.get(feed.size() - 1));
    }

    
    @Override
    public void fit(INDArray examples, INDArray labels) {
        setInput(examples.dup());



        if (layerWiseConfigurations.isPretrain()) {
            pretrain(getInput());
            finetune(labels);
        }

        if(layerWiseConfigurations.isBackward())
            doBackWard(getInput(),labels);

    }


    @Override
    public void fit(INDArray data) {
        pretrain(data);
    }

    @Override
    public void iterate(INDArray input) {
        pretrain(input);
    }


    
    @Override
    public void fit(org.nd4j.linalg.dataset.api.DataSet data) {
        fit(data.getFeatureMatrix(), data.getLabels());
    }

    
    @Override
    public void fit(INDArray examples, int[] labels) {
        fit(examples, FeatureUtil.toOutcomeMatrix(labels, getOutputLayer().conf().getNOut()));
    }


    
    public INDArray output(INDArray x,boolean test) {
        List<INDArray> activations = feedForward(x,test);
        
        return activations.get(activations.size() - 1);
    }

    
    public INDArray output(INDArray x) {
        return output(x,false);
    }


    
    public INDArray reconstruct(INDArray x, int layerNum) {
        List<INDArray> forward = feedForward(x);
        return forward.get(layerNum - 1);
    }


    
    public void printConfiguration() {
        StringBuilder sb = new StringBuilder();
        int count = 0;
        for (NeuralNetConfiguration conf : getLayerWiseConfigurations().getConfs()) {
            sb.append(" Layer " + count++ + " conf " + conf);
        }

        log.info(sb.toString());
    }


    
    public void update(MultiLayerNetwork network) {
        this.defaultConfiguration = network.defaultConfiguration;
        this.input = network.input;
        this.labels = network.labels;this.layers = ArrayUtils.clone(network.layers);
    }


    
    @Override
    public double score(INDArray input, INDArray labels) {
        feedForward(input);
        setLabels(labels);
        Evaluation eval = new Evaluation();
        eval.eval(labels, labelProbabilities(input));
        return eval.f1();
    }

    
    @Override
    public int numLabels() {
        return labels.columns();
    }


    
    public double score(DataSet data) {
        feedForward(data.getFeatureMatrix());
        setLabels(data.getLabels());
        return score();
    }


    @Override
    public void fit() {
        fit(input, labels);
    }

    @Override
    public void update(INDArray gradient, String paramType) {

    }


    
    @Override
    public double score() {
        if (getOutputLayer().input() == null)
            feedForward();
        return getOutputLayer().score();
    }

    @Override
    public void setScore() {

    }

    @Override
    public void accumulateScore(double accum) {

    }


    
    public void clear() {
        for(Layer layer : layers)
            layer.clear();


        input = null;
    }

    

    public double score(INDArray param) {
        INDArray params = params();
        setParameters(param);
        double ret = score();
        double regCost = 0.5f * defaultConfiguration.getL2() * (double) Transforms.pow(mask.mul(param), 2).sum(Integer.MAX_VALUE).element();
        setParameters(params);
        return ret + regCost;
    }


    
    public void merge(MultiLayerNetwork network, int batchSize) {
        if (network.layers.length != layers.length)
            throw new IllegalArgumentException("Unable to merge networks that are not of equal length");
        for (int i = 0; i < getnLayers(); i++) {
            Layer n = layers[i];
            Layer otherNetwork = network.layers[i];
            n.merge(otherNetwork, batchSize);

        }

        getOutputLayer().merge(network.getOutputLayer(), batchSize);
    }


    
    public void setInput(INDArray input) {
        if(getLayerWiseConfigurations().getInputPreProcess(0) != null)
            this.input = this.layerWiseConfigurations.getInputPreProcess(0).preProcess(input);
        else
            this.input =  input;
        if ( this.layers == null)
            this.initializeLayers(getInput());
        else if(this.input == null)
            this.input = input;

    }

    private void initMask() {
        setMask(Nd4j.ones(1, pack().length()));
    }



    
    public Layer getOutputLayer() {
        return getLayers()[getLayers().length - 1];
    }


    
    public void setParameters(INDArray params) {
        int idx = 0;
        for (int i = 0; i < getLayers().length; i++) {
            Layer layer = getLayers()[i];

            int range = layer.numParams();
            INDArray get = params.get(NDArrayIndex.interval(idx, range + idx));
            if(get.length() < 1)
                throw new IllegalStateException("Unable to retrieve layer. No params found (length was 0");
            layer.setParams(get);
            idx += range - 1;
        }

    }


    
    public List<INDArray> feedForwardR(List<INDArray> acts, INDArray v) {
        List<INDArray> R = new ArrayList<>();
        R.add(Nd4j.zeros(input.slices(), input.columns()));
        List<Pair<INDArray, INDArray>> vWvB = unPack(v);
        List<INDArray> W = MultiLayerUtil.weightMatrices(this);

        for (int i = 0; i < layers.length; i++) {
            String derivative = getLayers()[i].conf().getActivationFunction();
            
            R.add(R.get(i).mmul(W.get(i)).addi(acts.get(i)
                    .mmul(vWvB.get(i).getFirst().addiRowVector(vWvB.get(i).getSecond())))
                    .muli((Nd4j.getExecutioner().execAndReturn(Nd4j.getOpFactory().createTransform(derivative, acts.get(i + 1)).derivative()))));
        }

        return R;
    }


    
    public List<INDArray> feedForwardR(INDArray v) {
        return feedForwardR(feedForward(), v);
    }

    
    protected List<Pair<INDArray, INDArray>> backPropGradientR(INDArray v) {
        
        
        
        if (mask == null)
            initMask();
        
        List<INDArray> deltas = computeDeltasR(v);
        


        List<Pair<INDArray, INDArray>> list = new ArrayList<>();

        for (int l = 0; l < getnLayers(); l++) {
            INDArray gradientChange = deltas.get(l);

            if (gradientChange.length() != getLayers()[l].getParam(DefaultParamInitializer.WEIGHT_KEY).length())
                throw new IllegalStateException("Gradient change not equal to weight change");


            
            INDArray deltaColumnSums = deltas.get(l).mean(0);
            if (deltaColumnSums.length() != layers[l].getParam(DefaultParamInitializer.BIAS_KEY).length())
                throw new IllegalStateException("Bias change not equal to weight change");


            list.add(new Pair<>(gradientChange, deltaColumnSums));


        }

        INDArray pack = pack(list).addi(mask.mul(defaultConfiguration.getL2())
                .muli(v)).addi(v.mul(layerWiseConfigurations.getDampingFactor()));
        return unPack(pack);

    }


    public INDArray getLabels() {
        return labels;
    }

    public INDArray getInput() {
        return input;
    }




    public void setLabels(INDArray labels) {
        this.labels = labels;
    }

    
    public int getnLayers() {
        return layerWiseConfigurations.getConfs().size();
    }

    public Layer[] getLayers() {
        return layers;
    }

    public void setLayers(Layer[] layers) {
        this.layers = layers;
    }

    public INDArray getMask() {
        return mask;
    }

    public void setMask(INDArray mask) {
        this.mask = mask;
    }




}




<code block>


package org.deeplearning4j.nn.layers;

import java.io.Serializable;


import org.deeplearning4j.berkeley.Pair;
import org.deeplearning4j.eval.Evaluation;
import org.deeplearning4j.nn.api.Classifier;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.gradient.DefaultGradient;
import org.deeplearning4j.nn.gradient.Gradient;
import org.deeplearning4j.nn.params.DefaultParamInitializer;
import org.deeplearning4j.optimize.Solver;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.dataset.api.DataSet;
import org.nd4j.linalg.dataset.api.iterator.DataSetIterator;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.indexing.NDArrayIndex;
import org.nd4j.linalg.lossfunctions.LossFunctions;
import org.nd4j.linalg.util.FeatureUtil;
import org.nd4j.linalg.util.LinAlgExceptions;

import static org.nd4j.linalg.ops.transforms.Transforms.log;
import static org.nd4j.linalg.ops.transforms.Transforms.pow;
import static org.nd4j.linalg.ops.transforms.Transforms.sqrt;



public class OutputLayer extends BaseLayer implements Serializable,Classifier {

    private static final long serialVersionUID = -7065564817460914364L;
    
    private INDArray labels;

    public OutputLayer(NeuralNetConfiguration conf) {
        super(conf);
    }

    public OutputLayer(NeuralNetConfiguration conf, INDArray input) {
        super(conf, input);
    }

    

    @Override
    public  double score() {
        LinAlgExceptions.assertRows(input, labels);
        INDArray output  = output(input);
        if(conf.getLossFunction() != LossFunctions.LossFunction.RECONSTRUCTION_CROSSENTROPY)
            return  LossFunctions.score(labels,conf.getLossFunction(),output,conf.getL2(),conf.isUseRegularization());

        return  -LossFunctions.score(labels,conf.getLossFunction(),output,conf.getL2(),conf.isUseRegularization());


    }

    @Override
    public void setScore() {
        LinAlgExceptions.assertRows(input,labels);
        INDArray output  = output(input);
        score =  LossFunctions.score(labels,conf.getLossFunction(),output,conf.getL2(),conf.isUseRegularization());

    }


    @Override
    public Pair<Gradient, Double> gradientAndScore() {
        return new Pair<>(gradient(),score());
    }


    
    @Override
    public Gradient gradient() {
        LinAlgExceptions.assertRows(input, labels);


        
        INDArray netOut = activate(input);
        
        INDArray dy = netOut.sub(labels);


        INDArray wGradient = getWeightGradient();
        INDArray bGradient = dy.mean(0);
        Gradient g = new DefaultGradient();

        g.gradientForVariable().put(DefaultParamInitializer.WEIGHT_KEY,wGradient);
        g.gradientForVariable().put(DefaultParamInitializer.BIAS_KEY, bGradient);

        return g;

    }



    private INDArray getWeightGradient() {
        INDArray z = output(input);

        switch (conf.getLossFunction()) {
            case MCXENT:
                INDArray preOut = preOutput(input);
                
                INDArray pYGivenX = Nd4j.getExecutioner().execAndReturn(Nd4j.getOpFactory().createTransform("softmax",preOut),1);
                
                INDArray dy = pYGivenX.sub(labels);
                return input.transpose().mmul(dy);

            case XENT:
                INDArray xEntDiff = labels.sub(z);
                return input.transpose().mmul(xEntDiff.div(z.mul(z.rsub(1))));
            case MSE:
                INDArray mseDelta = labels.sub(z);
                return input.transpose().mmul(mseDelta.neg());
            case EXPLL:
                return input.transpose().mmul(labels.rsub(1).divi(z));
            case RMSE_XENT:
                INDArray rmseXentDiff = labels.sub(z);
                INDArray squaredrmseXentDiff = pow(rmseXentDiff, 2.0);
                INDArray sqrt = sqrt(squaredrmseXentDiff);
                return input.transpose().mmul(sqrt);
            case SQUARED_LOSS:
                return input.transpose().mmul(pow(labels.sub(z),2));
            case NEGATIVELOGLIKELIHOOD:
                return input.transpose().mmul(log(z).negi());


        }

        throw new IllegalStateException("Invalid loss function");

    }


    @Override
    public INDArray activate(INDArray input) {
        return output(input);
    }

    @Override
    public INDArray activate() {
        return output(input);
    }

    
    @Override
    public double score(DataSet data) {
        return score(data.getFeatureMatrix(),data.getLabels());
    }

    
    @Override
    public double score(INDArray examples, INDArray labels) {
        Evaluation eval = new Evaluation();
        eval.eval(labels,labelProbabilities(examples));
        return  eval.f1();

    }

    
    @Override
    public int numLabels() {
        return labels.columns();
    }

    @Override
    public void fit(DataSetIterator iter) {
        while(iter.hasNext())
            fit(iter.next());
    }

    
    @Override
    public int[] predict(INDArray d) {
        INDArray output = output(d);
        int[] ret = new int[d.rows()];
        for(int i = 0; i < ret.length; i++)
            ret[i] = Nd4j.getBlasWrapper().iamax(output.getRow(i));
        return ret;
    }

    
    @Override
    public INDArray labelProbabilities(INDArray examples) {
        return output(examples);
    }

    
    @Override
    public void fit(INDArray examples, INDArray labels) {
        this.input = examples.dup();
        applyDropOutIfNecessary(this.input);
        this.labels = labels;
        Solver solver = new Solver.Builder()
                .configure(conf())
                .listeners(getIterationListeners())
                .model(this).build();
        solver.optimize();
    }

    
    @Override
    public void fit(DataSet data) {
        fit(data.getFeatureMatrix(),data.getLabels());
    }

    
    @Override
    public void fit(INDArray examples, int[] labels) {
        INDArray outcomeMatrix = FeatureUtil.toOutcomeMatrix(labels, numLabels());
        fit(examples,outcomeMatrix);

    }

    @Override
    public void clear() {
        super.clear();
        if(labels != null) {
            labels.data().destroy();
            labels = null;
        }
    }

    
    @Override
    public INDArray transform(INDArray data) {
        return preOutput(data);
    }



    
    @Override
    public void setParams(INDArray params) {
        INDArray wParams = params.get(NDArrayIndex.interval(0, conf.getNIn() * conf.getNOut()));
        INDArray W = getParam(DefaultParamInitializer.WEIGHT_KEY);
        W.assign(wParams);
        INDArray bias = getParam(DefaultParamInitializer.BIAS_KEY);
        int biasBegin = params.length() - bias.length();
        int biasEnd = params.length();
        INDArray biasAssign = params.get(NDArrayIndex.interval(biasBegin, biasEnd));
        bias.assign(biasAssign);
    }
    
    @Override
    public void fit(INDArray data) {
        

    }

    @Override
    public void iterate(INDArray input) {

    }




    
    public  INDArray output(INDArray x) {
        if(x == null)
            throw new IllegalArgumentException("No null input allowed");

        INDArray preOutput = preOutput(x);
        if(conf.getActivationFunction().equals("softmax")) {
            INDArray ret = Nd4j.getExecutioner().execAndReturn(Nd4j.getOpFactory().createTransform("softmax", preOutput), 1);
            return ret;
        }

        this.input = x;
        return super.activate();

    }

    public  INDArray getLabels() {
        return labels;
    }

    public  void setLabels(INDArray labels) {
        this.labels = labels;
    }


}

<code block>


package org.deeplearning4j.nn.multilayer;


import org.apache.commons.lang3.ArrayUtils;
import org.apache.commons.lang3.EnumUtils;
import org.deeplearning4j.berkeley.Pair;
import org.deeplearning4j.eval.Evaluation;
import org.deeplearning4j.nn.api.*;
import org.deeplearning4j.nn.conf.MultiLayerConfiguration;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.gradient.DefaultGradient;
import org.deeplearning4j.nn.gradient.Gradient;
import org.deeplearning4j.nn.layers.OutputLayer;
import org.deeplearning4j.nn.layers.factory.LayerFactories;
import org.deeplearning4j.nn.params.DefaultParamInitializer;
import org.deeplearning4j.nn.weights.WeightInit;
import org.deeplearning4j.optimize.GradientAdjustment;
import org.deeplearning4j.optimize.api.ConvexOptimizer;
import org.deeplearning4j.optimize.api.IterationListener;
import org.deeplearning4j.util.MultiLayerUtil;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.api.ops.LossFunction;
import org.nd4j.linalg.dataset.DataSet;
import org.nd4j.linalg.dataset.api.iterator.DataSetIterator;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.indexing.NDArrayIndex;
import org.nd4j.linalg.lossfunctions.LossFunctions;
import org.nd4j.linalg.ops.transforms.Transforms;
import org.nd4j.linalg.util.FeatureUtil;
import org.nd4j.linalg.util.LinAlgExceptions;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.Serializable;
import java.lang.reflect.Constructor;
import java.util.*;



public class MultiLayerNetwork implements Serializable, Classifier {


    private static final Logger log = LoggerFactory.getLogger(MultiLayerNetwork.class);
    private static final long serialVersionUID = -5029161847383716484L;
    
    protected Layer[] layers;


    
    protected INDArray input, labels;
    
    protected boolean initCalled = false;
    private List<IterationListener> listeners = new ArrayList<>();

    protected NeuralNetConfiguration defaultConfiguration;
    protected MultiLayerConfiguration layerWiseConfigurations;


    
    protected INDArray mask;


    public MultiLayerNetwork(MultiLayerConfiguration conf) {
        this.layerWiseConfigurations = conf;
        this.defaultConfiguration = conf.getConf(0);
    }

    
    public MultiLayerNetwork(String conf, INDArray params) {
        this(MultiLayerConfiguration.fromJson(conf));
        init();
        setParameters(params);
    }


    
    public MultiLayerNetwork(MultiLayerConfiguration conf, INDArray params) {
        this(conf);
        init();
        setParameters(params);
    }


    protected void intializeConfigurations() {

        if (layerWiseConfigurations == null)
            layerWiseConfigurations = new MultiLayerConfiguration.Builder().build();

        if (layers == null)
            layers = new Layer[getnLayers()];

        if (defaultConfiguration == null)
            defaultConfiguration = new NeuralNetConfiguration.Builder()
                    .build();

        
        if (layerWiseConfigurations == null || layerWiseConfigurations.getConfs().isEmpty())
            for (int i = 0; i < layerWiseConfigurations.getHiddenLayerSizes().length + 1; i++) {
                layerWiseConfigurations.getConfs().add(defaultConfiguration.clone());
            }


    }


    
    public void pretrain(DataSetIterator iter) {
        if (!layerWiseConfigurations.isPretrain())
            return;

        INDArray layerInput;

        for (int i = 0; i < getnLayers(); i++) {
            if (i == 0) {
                while (iter.hasNext()) {
                    DataSet next = iter.next();
                    this.input = next.getFeatureMatrix();
                      
                    if (this.getInput() == null || this.getLayers() == null) {
                        setInput(input);
                        initializeLayers(input);
                    } else
                        setInput(input);
                    getLayers()[i].fit(next.getFeatureMatrix());
                    log.info("Training on layer " + (i + 1) + " with " + input.slices() + " examples");


                }

                iter.reset();
            } else {
                while (iter.hasNext()) {
                    DataSet next = iter.next();
                    layerInput = next.getFeatureMatrix();
                    for (int j = 1; j <= i; j++)
                        layerInput = activationFromPrevLayer(j - 1, layerInput);

                    log.info("Training on layer " + (i + 1) + " with " + layerInput.slices() + " examples");
                    getLayers()[i].fit(layerInput);

                }

                iter.reset();


            }
        }
    }


    
    public void pretrain(INDArray input) {

        if (!layerWiseConfigurations.isPretrain())
            return;
        


        INDArray layerInput = null;

        for (int i = 0; i < getnLayers() - 1; i++) {
            if (i == 0)
                layerInput = input;
            else
                layerInput = activationFromPrevLayer(i - 1, layerInput);
            log.info("Training on layer " + (i + 1) + " with " + layerInput.slices() + " examples");
            getLayers()[i].fit(layerInput);

        }
    }


    @Override
    public int batchSize() {
        return input.slices();
    }

    @Override
    public NeuralNetConfiguration conf() {
        throw new UnsupportedOperationException();
    }

    @Override
    public void setConf(NeuralNetConfiguration conf) {
        throw new UnsupportedOperationException();
    }

    @Override
    public INDArray input() {
        return input;
    }

    @Override
    public void validateInput() {

    }

    @Override
    public ConvexOptimizer getOptimizer() {
        throw new UnsupportedOperationException();
    }

    @Override
    public INDArray getParam(String param) {
        throw new UnsupportedOperationException();

    }

    @Override
    public void initParams() {
        throw new UnsupportedOperationException();
    }

    @Override
    public Map<String, INDArray> paramTable() {
        throw new UnsupportedOperationException();
    }

    @Override
    public void setParamTable(Map<String, INDArray> paramTable) {
        throw new UnsupportedOperationException();

    }

    @Override
    public void setParam(String key, INDArray val) {
        throw new UnsupportedOperationException();

    }

    
    @Override
    public INDArray transform(INDArray data) {
        return output(data);
    }


    public MultiLayerConfiguration getLayerWiseConfigurations() {
        return layerWiseConfigurations;
    }

    public void setLayerWiseConfigurations(MultiLayerConfiguration layerWiseConfigurations) {
        this.layerWiseConfigurations = layerWiseConfigurations;
    }

    
    public void initializeLayers(INDArray input) {


        if (input == null)
            throw new IllegalArgumentException("Unable to initialize neuralNets with empty input");
        int[] hiddenLayerSizes = getLayerWiseConfigurations().getHiddenLayerSizes();
        if (input.shape().length == 2)
            for (int i = 0; i < hiddenLayerSizes.length; i++)
                if (hiddenLayerSizes[i] < 1)
                    throw new IllegalArgumentException("All hidden layer sizes must be >= 1");


        this.input = input;

        if (!initCalled)
            init();
    }

    
    public void init() {
        if (layerWiseConfigurations == null || layers == null)
            intializeConfigurations();
        if(initCalled)
            return;

        INDArray layerInput = input();
        int inputSize;
        if (getnLayers() < 1)
            throw new IllegalStateException("Unable to createComplex network neuralNets; number specified is less than 1");

        int[] hiddenLayerSizes = layerWiseConfigurations.getHiddenLayerSizes();
        int numHiddenLayersSizesUsed = 0;

        if (this.layers == null || this.layers[0] == null) {
            if(this.layers == null)
                this.layers = new Layer[getnLayers()];

            
            for (int i = 0; i < getnLayers(); i++) {
                NeuralNetConfiguration conf = layerWiseConfigurations.getConf(i);
                Layer.Type type = LayerFactories.typeForFactory(conf);

                if (i == 0) {
                    inputSize = conf.getNIn();
                    if (input == null) {
                        input = Nd4j.ones(inputSize);
                        layerInput = input;
                    }
                    conf.setNIn(inputSize);

                    if (type == Layer.Type.FEED_FORWARD) {
                        conf.setNOut(hiddenLayerSizes[numHiddenLayersSizesUsed]);
                    }
                }
                else if (i < getLayers().length) {
                    if (input != null)
                        layerInput = activationFromPrevLayer(i - 1, layerInput);
                    
                    if(type == Layer.Type.FEED_FORWARD) {
                        if(i!=(layers.length-1)) {
                            numHiddenLayersSizesUsed++;
                            conf.setNIn(layerInput.columns());
                            conf.setNOut(hiddenLayerSizes[numHiddenLayersSizesUsed]);
                        } else {
                            conf.setNIn(hiddenLayerSizes[numHiddenLayersSizesUsed]);
                        }
                    }
                }
                layers[i] = LayerFactories.getFactory(conf).create(conf, listeners);
            }
            initCalled = true;
            initMask();
        }
    }


    
    public INDArray activate() {
        return getLayers()[getLayers().length - 1].activate();
    }

    
    public INDArray activate(int layer) {
        return getLayers()[layer].activate();
    }

    
    public INDArray activate(int layer, INDArray input) {
        return getLayers()[layer].activate(input);
    }


    
    public void initialize(DataSet data) {
        setInput(data.getFeatureMatrix());
        feedForward(getInput());
        this.labels = data.getLabels();
        if (getOutputLayer() instanceof OutputLayer) {
            OutputLayer o = (OutputLayer) getOutputLayer();
            o.setLabels(labels);

        }
    }


    
    public INDArray zFromPrevLayer(int curr, INDArray input) {
        if(getLayerWiseConfigurations().getInputPreProcess(curr) != null)
            input = getLayerWiseConfigurations().getInputPreProcess(curr).preProcess(input);
        INDArray ret = layers[curr].preOutput(input);
        if (getLayerWiseConfigurations().getProcessors() != null && getLayerWiseConfigurations().getPreProcessor(curr) != null) {
            ret = getLayerWiseConfigurations().getPreProcessor(curr).preProcess(ret);
            return ret;
        }
        return ret;
    }

    
    public INDArray activationFromPrevLayer(int curr, INDArray input) {
        if(getLayerWiseConfigurations().getInputPreProcess(curr) != null)
            input = getLayerWiseConfigurations().getInputPreProcess(curr).preProcess(input);
        INDArray ret = layers[curr].activate(input);
        if (getLayerWiseConfigurations().getProcessors() != null && getLayerWiseConfigurations().getPreProcessor(curr) != null) {
            ret = getLayerWiseConfigurations().getPreProcessor(curr).preProcess(ret);
            return ret;
        }
        return ret;
    }


    
    public List<INDArray> computeZ() {
        INDArray currInput = this.input;

        List<INDArray> activations = new ArrayList<>();
        activations.add(currInput);

        for (int i = 0; i < layers.length; i++) {
            currInput = zFromPrevLayer(i, currInput);
            
            applyDropConnectIfNecessary(currInput);
            activations.add(currInput);
        }


        return activations;
    }

    
    public List<INDArray> computeZ(INDArray input) {
        if (input == null)
            throw new IllegalStateException("Unable to perform feed forward; no input found");
        else if(this.getLayerWiseConfigurations().getInputPreProcess(0) != null)
            this.input = getLayerWiseConfigurations().getInputPreProcess(0).preProcess(input);
        else
            this.input = input;
        return computeZ();
    }

    
    public List<INDArray> feedForward(INDArray input,boolean test) {
        this.input = input;
        return feedForward(test);
    }

    
    public List<INDArray> feedForward(boolean test) {
        INDArray currInput = this.input;

        List<INDArray> activations = new ArrayList<>();
        activations.add(currInput);

        for (int i = 0; i < layers.length; i++) {
            currInput = activationFromPrevLayer(i, currInput);
            
            if(!test)
                applyDropConnectIfNecessary(currInput);
            activations.add(currInput);
        }


        return activations;
    }

    
    public List<INDArray> feedForward() {
        return feedForward(false);
    }


    
    public Pair<List<INDArray>,List<INDArray>> feedForwardActivationsAndDerivatives() {
        INDArray currInput = this.input;

        List<INDArray> activations = new ArrayList<>();
        List<INDArray> derivatives = new ArrayList<>();
        activations.add(currInput);

        for (int i = 0; i < layers.length; i++) {
            currInput = zFromPrevLayer(i, currInput); 
            applyDropConnectIfNecessary(currInput);
            activations.add(Nd4j.getExecutioner().execAndReturn(Nd4j.getOpFactory().createTransform(layerWiseConfigurations.getConf(i).getActivationFunction(), currInput)));
        }

        currInput = this.input;
        for (int i = 0; i < layers.length; i++) {
            currInput = zFromPrevLayer(i, currInput); 
            applyDropConnectIfNecessary(currInput);
            INDArray dup = currInput.dup();
            derivatives.add(Nd4j.getExecutioner().execAndReturn(Nd4j.getOpFactory().createTransform(layerWiseConfigurations.getConf(i).getActivationFunction(), dup).derivative()));
            Nd4j.getOpFactory().createTransform(layerWiseConfigurations.getConf(i).getActivationFunction(), currInput);
        }
        
        derivatives.add(derivatives.get(layers.length - 1));
        return new Pair<>(activations, derivatives);
    }


    
    public List<INDArray> feedForward(INDArray input) {
        if (input == null)
            throw new IllegalStateException("Unable to perform feed forward; no input found");
        else if(this.getLayerWiseConfigurations().getInputPreProcess(0) != null)
            this.input = getLayerWiseConfigurations().getInputPreProcess(0).preProcess(input);
        else
            this.input = input;
        return feedForward();
    }

    @Override
    public Gradient gradient() {
        Gradient ret = new DefaultGradient();
        for (int i = 0; i < layers.length; i ++) {
            ret.gradientForVariable().put(String.valueOf(i), layers[i].gradient().gradient());
        }

        return ret;
    }

    @Override
    public Pair<Gradient, Double> gradientAndScore() {
        return new Pair<>(gradient(), getOutputLayer().score());
    }

    
    protected void applyDropConnectIfNecessary(INDArray input) {
        if (layerWiseConfigurations.isUseDropConnect()) {
            INDArray mean = Nd4j.valueArrayOf(input.slices(), input.columns(), 0.5);
            INDArray mask =Nd4j.getDistributions().createBinomial(1,mean).sample(mean.shape());
            input.muli(mask);
            
            if (defaultConfiguration.getL2() > 0)
                input.muli(defaultConfiguration.getL2());
        }
    }


    
    protected List<INDArray> computeDeltasR(INDArray v) {
        List<INDArray> deltaRet = new ArrayList<>();

        INDArray[] deltas = new INDArray[getnLayers() + 1];
        List<INDArray> activations = feedForward();
        List<INDArray> rActivations = feedForwardR(activations, v);
      
        List<INDArray> weights = new ArrayList<>();
        List<INDArray> biases = new ArrayList<>();
        List<String> activationFunctions = new ArrayList<>();


        for (int j = 0; j < getLayers().length; j++) {
            weights.add(getLayers()[j].getParam(DefaultParamInitializer.WEIGHT_KEY));
            biases.add(getLayers()[j].getParam(DefaultParamInitializer.BIAS_KEY));
            activationFunctions.add(getLayers()[j].conf().getActivationFunction());
        }


        INDArray rix = rActivations.get(rActivations.size() - 1).divi((double) input.slices());
        LinAlgExceptions.assertValidNum(rix);

        
        for (int i = getnLayers() - 1; i >= 0; i--) {
            
            deltas[i] = activations.get(i).transpose().mmul(rix);
            applyDropConnectIfNecessary(deltas[i]);

            if (i > 0)
                rix = rix.mmul(weights.get(i).addRowVector(biases.get(i)).transpose()).muli(Nd4j.getExecutioner().execAndReturn(Nd4j.getOpFactory().createTransform(activationFunctions.get(i - 1),activations.get(i)).derivative()));

        }

        for (int i = 0; i < deltas.length - 1; i++) {
            if (defaultConfiguration.isConstrainGradientToUnitNorm()) {
                double sum = deltas[i].sum(Integer.MAX_VALUE).getDouble(0);
                if (sum > 0)
                    deltaRet.add(deltas[i].div(deltas[i].norm2(Integer.MAX_VALUE)));
                else
                    deltaRet.add(deltas[i]);
            } else
                deltaRet.add(deltas[i]);
            LinAlgExceptions.assertValidNum(deltaRet.get(i));
        }

        return deltaRet;
    }


    
    public void dampingUpdate(double rho, double boost, double decrease) {
        if (rho < 0.25 || Double.isNaN(rho))
            layerWiseConfigurations.setDampingFactor(getLayerWiseConfigurations().getDampingFactor() * boost);


        else if (rho > 0.75)
            layerWiseConfigurations.setDampingFactor(getLayerWiseConfigurations().getDampingFactor() * decrease);
    }

    
    public double reductionRatio(INDArray p, double currScore, double score, INDArray gradient) {
        double currentDamp = layerWiseConfigurations.getDampingFactor();
        layerWiseConfigurations.setDampingFactor(0);
        INDArray denom = getBackPropRGradient(p);
        denom.muli(0.5).muli(p.mul(denom)).sum(0);
        denom.subi(gradient.mul(p).sum(0));
        double rho = (currScore - score) / (double) denom.getScalar(0).element();
        layerWiseConfigurations.setDampingFactor(currentDamp);
        if (score - currScore > 0)
            return Float.NEGATIVE_INFINITY;
        return rho;
    }


    
    protected List<Pair<INDArray, INDArray>> computeDeltas2() {
        List<Pair<INDArray, INDArray>> deltaRet = new ArrayList<>();
        List<INDArray> activations = feedForward();
        INDArray[] deltas = new INDArray[activations.size() - 1];
        INDArray[] preCons = new INDArray[activations.size() - 1];


        
        INDArray ix = activations.get(activations.size() - 1).sub(labels).div(labels.slices());

       	
        List<INDArray> weights = new ArrayList<>();
        List<INDArray> biases = new ArrayList<>();

        List<String> activationFunctions = new ArrayList<>();
        for (int j = 0; j < getLayers().length; j++) {
            weights.add(getLayers()[j].getParam(DefaultParamInitializer.WEIGHT_KEY));
            biases.add(getLayers()[j].getParam(DefaultParamInitializer.BIAS_KEY));
            activationFunctions.add(getLayers()[j].conf().getActivationFunction());
        }


        
        for (int i = weights.size() - 1; i >= 0; i--) {
            deltas[i] = activations.get(i).transpose().mmul(ix);
            preCons[i] = Transforms.pow(activations.get(i).transpose(), 2).mmul(Transforms.pow(ix, 2)).muli(labels.slices());
            applyDropConnectIfNecessary(deltas[i]);

            if (i > 0) {
                
                ix = ix.mmul(weights.get(i).transpose()).muli(Nd4j.getExecutioner().execAndReturn(Nd4j.getOpFactory().createTransform(activationFunctions.get(i - 1),activations.get(i)).derivative()));
            }
        }

        for (int i = 0; i < deltas.length; i++) {
            if (defaultConfiguration.isConstrainGradientToUnitNorm())
                deltaRet.add(new Pair<>(deltas[i].divi(deltas[i].norm2(Integer.MAX_VALUE)), preCons[i]));

            else
                deltaRet.add(new Pair<>(deltas[i], preCons[i]));

        }

        return deltaRet;
    }



    
    public INDArray getBackPropRGradient(INDArray v) {
        return pack(backPropGradientR(v));
    }


    
    public Pair<INDArray, INDArray> getBackPropGradient2() {
        List<Pair<Pair<INDArray, INDArray>, Pair<INDArray, INDArray>>> deltas = backPropGradient2();
        List<Pair<INDArray, INDArray>> deltaNormal = new ArrayList<>();
        List<Pair<INDArray, INDArray>> deltasPreCon = new ArrayList<>();
        for (int i = 0; i < deltas.size(); i++) {
            deltaNormal.add(deltas.get(i).getFirst());
            deltasPreCon.add(deltas.get(i).getSecond());
        }


        return new Pair<>(pack(deltaNormal), pack(deltasPreCon));
    }


    @Override
    public MultiLayerNetwork clone() {
        MultiLayerNetwork ret;
        try {
            Constructor<MultiLayerNetwork> constructor = (Constructor<MultiLayerNetwork>) getClass().getDeclaredConstructor(MultiLayerConfiguration.class);
            ret = constructor.newInstance(getLayerWiseConfigurations());
            ret.update(this);

        } catch (Exception e) {
            throw new IllegalStateException("Unable to cloe network");
        }
        return ret;
    }


    
    @Override
    public INDArray params() {
        List<INDArray> params = new ArrayList<>();
        for (int i = 0; i < getnLayers(); i++)
            params.add(layers[i].params());

        return Nd4j.toFlattened(params);
    }

    
    @Override
    public void setParams(INDArray params) {
        setParameters(params);

    }


    
    @Override
    public int numParams() {
        int length = 0;
        for (int i = 0; i < layers.length; i++)
            length += layers[i].numParams();

        return length;

    }

    

    public INDArray pack() {
        return params();

    }

    
    public INDArray pack(List<Pair<INDArray, INDArray>> layers) {
        List<INDArray> list = new ArrayList<>();

        for (Pair<INDArray, INDArray> layer : layers) {
            list.add(layer.getFirst());
            list.add(layer.getSecond());
        }
        return Nd4j.toFlattened(list);
    }


    
    @Override
    public double score(org.nd4j.linalg.dataset.api.DataSet data) {
        return score(data.getFeatureMatrix(), data.getLabels());
    }



    
    public List<Pair<INDArray, INDArray>> unPack(INDArray param) {
        
        if (param.slices() != 1)
            param = param.reshape(1, param.length());
        List<Pair<INDArray, INDArray>> ret = new ArrayList<>();
        int curr = 0;
        for (int i = 0; i < layers.length; i++) {
            int layerLength = layers[i].getParam(DefaultParamInitializer.WEIGHT_KEY).length() + layers[i].getParam(DefaultParamInitializer.BIAS_KEY).length();
            INDArray subMatrix = param.get(NDArrayIndex.interval(curr, curr + layerLength));
            INDArray weightPortion = subMatrix.get(NDArrayIndex.interval(0, layers[i].getParam(DefaultParamInitializer.WEIGHT_KEY).length()));

            int beginHBias = layers[i].getParam(DefaultParamInitializer.WEIGHT_KEY).length();
            int endHbias = subMatrix.length();
            INDArray hBiasPortion = subMatrix.get(NDArrayIndex.interval(beginHBias, endHbias));
            int layerLengthSum = weightPortion.length() + hBiasPortion.length();
            if (layerLengthSum != layerLength) {
                if (hBiasPortion.length() != layers[i].getParam(DefaultParamInitializer.BIAS_KEY).length())
                    throw new IllegalStateException("Hidden bias on layer " + i + " was off");
                if (weightPortion.length() != layers[i].getParam(DefaultParamInitializer.WEIGHT_KEY).length())
                    throw new IllegalStateException("Weight portion on layer " + i + " was off");

            }

            ret.add(new Pair<>(weightPortion.reshape(layers[i].getParam(DefaultParamInitializer.WEIGHT_KEY).slices(), layers[i].getParam(DefaultParamInitializer.WEIGHT_KEY).columns()), hBiasPortion.reshape(layers[i].getParam(DefaultParamInitializer.BIAS_KEY).slices(), layers[i].getParam(DefaultParamInitializer.BIAS_KEY).columns())));
            curr += layerLength;
        }


        return ret;
    }

    
    protected List<Pair<Pair<INDArray, INDArray>, Pair<INDArray, INDArray>>> backPropGradient2() {
        
        

        
        List<Pair<INDArray, INDArray>> deltas = computeDeltas2();


        List<Pair<Pair<INDArray, INDArray>, Pair<INDArray, INDArray>>> list = new ArrayList<>();
        List<Pair<INDArray, INDArray>> grad = new ArrayList<>();
        List<Pair<INDArray, INDArray>> preCon = new ArrayList<>();

        for (int l = 0; l < deltas.size(); l++) {
            INDArray gradientChange = deltas.get(l).getFirst();
            INDArray preConGradientChange = deltas.get(l).getSecond();


            if (l < layers.length && gradientChange.length() != layers[l].getParam(DefaultParamInitializer.WEIGHT_KEY).length())
                throw new IllegalStateException("Gradient change not equal to weight change");

            
            INDArray deltaColumnSums = deltas.get(l).getFirst().mean(0);
            INDArray preConColumnSums = deltas.get(l).getSecond().mean(0);

            grad.add(new Pair<>(gradientChange, deltaColumnSums));
            preCon.add(new Pair<>(preConGradientChange, preConColumnSums));
            if (l < layers.length && deltaColumnSums.length() != layers[l].getParam(DefaultParamInitializer.BIAS_KEY).length())
                throw new IllegalStateException("Bias change not equal to weight change");
            else if (l == getLayers().length && deltaColumnSums.length() != getOutputLayer().getParam(DefaultParamInitializer.BIAS_KEY).length())
                throw new IllegalStateException("Bias change not equal to weight change");


        }

        INDArray g = pack(grad);
        INDArray con = pack(preCon);
        INDArray theta = params();


        if (mask == null)
            initMask();

        g.addi(theta.mul(defaultConfiguration.getL2()).muli(mask));

        INDArray conAdd = Transforms.pow(mask.mul(defaultConfiguration.getL2()).add(Nd4j.valueArrayOf(g.slices(), g.columns(), layerWiseConfigurations.getDampingFactor())), 3.0 / 4.0);

        con.addi(conAdd);

        List<Pair<INDArray, INDArray>> gUnpacked = unPack(g);

        List<Pair<INDArray, INDArray>> conUnpacked = unPack(con);

        for (int i = 0; i < gUnpacked.size(); i++)
            list.add(new Pair<>(gUnpacked.get(i), conUnpacked.get(i)));


        return list;

    }


    @Override
    public void fit(DataSetIterator iter) {
        if (layerWiseConfigurations.isPretrain()) {
            pretrain(iter);
            iter.reset();
            finetune(iter);
        }
        if(layerWiseConfigurations.isBackward()) {
            iter.reset();
            while(iter.hasNext()) {
                DataSet next = iter.next();
                doBackWard(next.getFeatureMatrix(),next.getLabels());

            }
        }
    }

    
    protected void doBackWard(INDArray input,INDArray labels) {
        setInput(input);
        this.labels = labels;
        Gradient nextGradients = new DefaultGradient();

        if(!(getOutputLayer() instanceof  OutputLayer)) {
            log.warn("Warning: final layer isn't output layer. You can ignore this message if you just intend on using a a deep neural network with no output layer.");
            return;
        }

        OutputLayer output = (OutputLayer) getOutputLayer();
        if(labels == null)
            throw new IllegalStateException("No labels found");
        if(output.conf().getWeightInit() == WeightInit.ZERO){
            throw new IllegalStateException("Output layer weights cannot be intialized to zero when using backprop.");
        };
        output.setLabels(labels);

        
        for(int i = 0; i < getLayerWiseConfigurations().getConf(0).getNumIterations(); i++) {
            
            int numLayers = getnLayers();
            List<Gradient> gradientUpdates = new ArrayList<>();
            Pair<List<INDArray>,List<INDArray>> activationsAndDeriv = feedForwardActivationsAndDerivatives();
            List<INDArray> activations = activationsAndDeriv.getFirst();
            INDArray outputActivation = activations.get(activations.size() - 1);

            List<INDArray> derivatives = activationsAndDeriv.getSecond();
            INDArray activationDeriv = derivatives.get(derivatives.size() - 1);
            INDArray layerInput = activations.get(activations.size() - 2);

            INDArray delta = outputActivation.sub(labels).transpose();

            
            if(output.conf().getLossFunction() != LossFunctions.LossFunction.XENT) {
                delta.muli(activationDeriv);
            }

            nextGradients.gradientForVariable().put(DefaultParamInitializer.WEIGHT_KEY, delta.mmul(layerInput).transpose());
            nextGradients.gradientForVariable().put(DefaultParamInitializer.BIAS_KEY, delta.transpose());

            gradientUpdates.add(nextGradients);

            
            for(int j = numLayers - 2; j >= 0; j--) {
                
                INDArray currActivation = activations.get(j);
                INDArray currDerivative = derivatives.get(j);
                Layer nextLayer = getLayers()[j + 1];
                nextGradients = getLayers()[j].backwardGradient(currDerivative, nextLayer, nextGradients, currActivation);
                gradientUpdates.add(nextGradients);
            }

            Collections.reverse(gradientUpdates);
            
            for(int k = 0; k < numLayers; k++) {
                Layer currLayer = getLayers()[k];
                for(String paramType : gradientUpdates.get(k).gradientForVariable().keySet()) {
                    INDArray gradient = gradientUpdates.get(k).getGradientFor(paramType);
                    
                    GradientAdjustment.updateGradientAccordingToParams(
                            i
                            ,input.slices()
                            ,currLayer.conf()
                            ,currLayer.getParam(paramType)
                            ,gradient
                            ,currLayer.getOptimizer().adaGradForVariables().get(paramType)
                            ,currLayer.getOptimizer().getLastStep().get(paramType)
                            ,paramType
                    );
                    currLayer.update(gradient, paramType);
                }
            }
            for(IterationListener listener :  listeners)
                listener.iterationDone(getOutputLayer(),i);
        }

    }


    public List<IterationListener> getListeners() {
        return listeners;
    }

    public void setListeners(List<IterationListener> listeners) {
        this.listeners = listeners;

        if(layers == null) {
            init();
        }
        for(Layer layer : layers) {
            layer.setIterationListeners(listeners);
        }
    }

    
    public void finetune(DataSetIterator iter) {
        log.info("Finetune phase ");
        iter.reset();

        while (iter.hasNext()) {
            DataSet data = iter.next();
            if (data.getFeatureMatrix() == null || data.getLabels() == null)
                break;

            setInput(data.getFeatureMatrix());
            setLabels(data.getLabels());
            if (getOutputLayer().conf().getOptimizationAlgo() != OptimizationAlgorithm.HESSIAN_FREE) {
                feedForward();
                if (getOutputLayer() instanceof OutputLayer) {
                    OutputLayer o = (OutputLayer) getOutputLayer();
                    o.setIterationListeners(getListeners());
                    o.fit(o.input(),getLabels());

                }
            } else {
                throw new UnsupportedOperationException();
            }

        }


    }


    
    public void finetune(INDArray labels) {
        if (labels != null)
            this.labels = labels;
        if (!(getOutputLayer() instanceof OutputLayer)) {
            log.warn("Output layer not instance of output layer returning.");
            return;
        }

        log.info("Finetune phase");
        OutputLayer o = (OutputLayer) getOutputLayer();
        if (getOutputLayer().conf().getOptimizationAlgo() != OptimizationAlgorithm.HESSIAN_FREE) {
            List<INDArray> activations = feedForward();
            o.setIterationListeners(getListeners());
            o.fit(activations.get(activations.size() - 2), labels);
        }

        else {
            throw new UnsupportedOperationException();
        }
    }


    
    @Override
    public int[] predict(INDArray d) {
        INDArray output = output(d);
        int[] ret = new int[d.slices()];
        if (d.isRowVector()) ret[0] = Nd4j.getBlasWrapper().iamax(output);
        else {
            for (int i = 0; i < ret.length; i++)
                ret[i] = Nd4j.getBlasWrapper().iamax(output.getRow(i));
        }
        return ret;
    }

    
    @Override
    public INDArray labelProbabilities(INDArray examples) {
        List<INDArray> feed = feedForward(examples);
        OutputLayer o = (OutputLayer) getOutputLayer();
        return o.labelProbabilities(feed.get(feed.size() - 1));
    }

    
    @Override
    public void fit(INDArray examples, INDArray labels) {
        setInput(examples);



        if (layerWiseConfigurations.isPretrain()) {
            pretrain(getInput());
            finetune(labels);
        }

        if(layerWiseConfigurations.isBackward())
            doBackWard(getInput(),labels);

    }


    @Override
    public void fit(INDArray data) {
        pretrain(data);
    }

    @Override
    public void iterate(INDArray input) {
        pretrain(input);
    }


    
    @Override
    public void fit(org.nd4j.linalg.dataset.api.DataSet data) {
        fit(data.getFeatureMatrix(), data.getLabels());
    }

    
    @Override
    public void fit(INDArray examples, int[] labels) {
        fit(examples, FeatureUtil.toOutcomeMatrix(labels, getOutputLayer().conf().getNOut()));
    }


    
    public INDArray output(INDArray x,boolean test) {
        List<INDArray> activations = feedForward(x,test);
        
        return activations.get(activations.size() - 1);
    }

    
    public INDArray output(INDArray x) {
        return output(x,false);
    }


    
    public INDArray reconstruct(INDArray x, int layerNum) {
        List<INDArray> forward = feedForward(x);
        return forward.get(layerNum - 1);
    }


    
    public void printConfiguration() {
        StringBuilder sb = new StringBuilder();
        int count = 0;
        for (NeuralNetConfiguration conf : getLayerWiseConfigurations().getConfs()) {
            sb.append(" Layer " + count++ + " conf " + conf);
        }

        log.info(sb.toString());
    }


    
    public void update(MultiLayerNetwork network) {
        this.defaultConfiguration = network.defaultConfiguration;
        this.input = network.input;
        this.labels = network.labels;this.layers = ArrayUtils.clone(network.layers);
    }


    
    @Override
    public double score(INDArray input, INDArray labels) {
        feedForward(input);
        setLabels(labels);
        Evaluation eval = new Evaluation();
        eval.eval(labels, labelProbabilities(input));
        return eval.f1();
    }

    
    @Override
    public int numLabels() {
        return labels.columns();
    }


    
    public double score(DataSet data) {
        feedForward(data.getFeatureMatrix());
        setLabels(data.getLabels());
        return score();
    }


    @Override
    public void fit() {
        fit(input, labels);
    }

    @Override
    public void update(INDArray gradient, String paramType) {

    }


    
    @Override
    public double score() {
        if (getOutputLayer().input() == null)
            feedForward();
        return getOutputLayer().score();
    }

    @Override
    public void setScore() {

    }

    @Override
    public void accumulateScore(double accum) {

    }


    
    public void clear() {
        for(Layer layer : layers)
            layer.clear();


        input = null;
    }

    

    public double score(INDArray param) {
        INDArray params = params();
        setParameters(param);
        double ret = score();
        double regCost = 0.5f * defaultConfiguration.getL2() * (double) Transforms.pow(mask.mul(param), 2).sum(Integer.MAX_VALUE).element();
        setParameters(params);
        return ret + regCost;
    }


    
    public void merge(MultiLayerNetwork network, int batchSize) {
        if (network.layers.length != layers.length)
            throw new IllegalArgumentException("Unable to merge networks that are not of equal length");
        for (int i = 0; i < getnLayers(); i++) {
            Layer n = layers[i];
            Layer otherNetwork = network.layers[i];
            n.merge(otherNetwork, batchSize);

        }

        getOutputLayer().merge(network.getOutputLayer(), batchSize);
    }


    
    public void setInput(INDArray input) {
        if(getLayerWiseConfigurations().getInputPreProcess(0) != null)
            this.input = this.layerWiseConfigurations.getInputPreProcess(0).preProcess(input);
        else
            this.input =  input;
        if ( this.layers == null)
            this.initializeLayers(getInput());
        else if(this.input == null)
            this.input = input;

    }

    private void initMask() {
        setMask(Nd4j.ones(1, pack().length()));
    }



    
    public Layer getOutputLayer() {
        return getLayers()[getLayers().length - 1];
    }


    
    public void setParameters(INDArray params) {
        int idx = 0;
        for (int i = 0; i < getLayers().length; i++) {
            Layer layer = getLayers()[i];

            int range = layer.numParams();
            INDArray get = params.get(NDArrayIndex.interval(idx, range + idx));
            if(get.length() < 1)
                throw new IllegalStateException("Unable to retrieve layer. No params found (length was 0");
            layer.setParams(get);
            idx += range - 1;
        }

    }


    
    public List<INDArray> feedForwardR(List<INDArray> acts, INDArray v) {
        List<INDArray> R = new ArrayList<>();
        R.add(Nd4j.zeros(input.slices(), input.columns()));
        List<Pair<INDArray, INDArray>> vWvB = unPack(v);
        List<INDArray> W = MultiLayerUtil.weightMatrices(this);

        for (int i = 0; i < layers.length; i++) {
            String derivative = getLayers()[i].conf().getActivationFunction();
            
            R.add(R.get(i).mmul(W.get(i)).addi(acts.get(i)
                    .mmul(vWvB.get(i).getFirst().addiRowVector(vWvB.get(i).getSecond())))
                    .muli((Nd4j.getExecutioner().execAndReturn(Nd4j.getOpFactory().createTransform(derivative, acts.get(i + 1)).derivative()))));
        }

        return R;
    }


    
    public List<INDArray> feedForwardR(INDArray v) {
        return feedForwardR(feedForward(), v);
    }

    
    protected List<Pair<INDArray, INDArray>> backPropGradientR(INDArray v) {
        
        
        
        if (mask == null)
            initMask();
        
        List<INDArray> deltas = computeDeltasR(v);
        


        List<Pair<INDArray, INDArray>> list = new ArrayList<>();

        for (int l = 0; l < getnLayers(); l++) {
            INDArray gradientChange = deltas.get(l);

            if (gradientChange.length() != getLayers()[l].getParam(DefaultParamInitializer.WEIGHT_KEY).length())
                throw new IllegalStateException("Gradient change not equal to weight change");


            
            INDArray deltaColumnSums = deltas.get(l).mean(0);
            if (deltaColumnSums.length() != layers[l].getParam(DefaultParamInitializer.BIAS_KEY).length())
                throw new IllegalStateException("Bias change not equal to weight change");


            list.add(new Pair<>(gradientChange, deltaColumnSums));


        }

        INDArray pack = pack(list).addi(mask.mul(defaultConfiguration.getL2())
                .muli(v)).addi(v.mul(layerWiseConfigurations.getDampingFactor()));
        return unPack(pack);

    }


    public INDArray getLabels() {
        return labels;
    }

    public INDArray getInput() {
        return input;
    }




    public void setLabels(INDArray labels) {
        this.labels = labels;
    }

    
    public int getnLayers() {
        return layerWiseConfigurations.getConfs().size();
    }

    public Layer[] getLayers() {
        return layers;
    }

    public void setLayers(Layer[] layers) {
        this.layers = layers;
    }

    public INDArray getMask() {
        return mask;
    }

    public void setMask(INDArray mask) {
        this.mask = mask;
    }




}




<code block>


package org.deeplearning4j.models.embeddings.loader;

import java.io.BufferedInputStream;
import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.DataInputStream;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.io.InputStream;
import java.util.ArrayList;
import java.util.List;
import java.util.zip.GZIPInputStream;

import org.apache.commons.compress.compressors.gzip.GzipUtils;
import org.apache.commons.io.IOUtils;
import org.apache.commons.io.LineIterator;
import org.deeplearning4j.berkeley.Pair;
import org.deeplearning4j.models.embeddings.WeightLookupTable;
import org.deeplearning4j.models.embeddings.inmemory.InMemoryLookupTable;
import org.deeplearning4j.models.embeddings.wordvectors.WordVectors;
import org.deeplearning4j.models.embeddings.wordvectors.WordVectorsImpl;
import org.deeplearning4j.models.glove.Glove;
import org.deeplearning4j.models.word2vec.VocabWord;
import org.deeplearning4j.models.word2vec.Word2Vec;
import org.deeplearning4j.models.word2vec.wordstore.VocabCache;
import org.deeplearning4j.models.word2vec.wordstore.inmemory.InMemoryLookupCache;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.ops.transforms.Transforms;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


public class WordVectorSerializer
{
    private static final boolean DEFAULT_LINEBREAKS = false;
    private static final int MAX_SIZE = 50;
    private static final Logger log = LoggerFactory.getLogger(WordVectorSerializer.class);

    
    public static Word2Vec loadGoogleModel(File modelFile, boolean binary)
        throws IOException
    {
        return loadGoogleModel(modelFile, binary, DEFAULT_LINEBREAKS);
    }

    
    public static Word2Vec loadGoogleModel(File modelFile, boolean binary, boolean lineBreaks)
        throws IOException
    {
        return binary ? readBinaryModel(modelFile, lineBreaks) : readTextModel(modelFile);
    }

    
    private static Word2Vec readTextModel(File modelFile)
        throws IOException, NumberFormatException
    {
        InMemoryLookupTable lookupTable;
        VocabCache cache;
        INDArray syn0;
        BufferedReader reader = new BufferedReader(new FileReader(modelFile));
        String line = reader.readLine();
        String[] initial = line.split(" ");
        int words = Integer.parseInt(initial[0]);
        int layerSize = Integer.parseInt(initial[1]);
        syn0 = Nd4j.create(words, layerSize);

        cache = new InMemoryLookupCache();

        int currLine = 0;
        while ((line = reader.readLine()) != null) {
            String[] split = line.split(" ");
            assert split.length == layerSize + 1;
            String word = split[0];

            float[] vector = new float[split.length - 1];
            for (int i = 1; i < split.length; i++) {
                vector[i - 1] = Float.parseFloat(split[i]);
            }

            syn0.putRow(currLine, Transforms.unitVec(Nd4j.create(vector)));

            cache.addWordToIndex(cache.numWords(), word);
            cache.addToken(new VocabWord(1, word));
            cache.putVocabWord(word);

            currLine++;
        }

        lookupTable = (InMemoryLookupTable) new InMemoryLookupTable.Builder().cache(cache)
                .vectorLength(layerSize).build();
        lookupTable.setSyn0(syn0);

        Word2Vec ret = new Word2Vec();
        ret.setVocab(cache);
        ret.setLookupTable(lookupTable);

        reader.close();
        return ret;
    }

    
    private static Word2Vec readBinaryModel(File modelFile, boolean linebreaks)
        throws NumberFormatException, IOException
    {
        InMemoryLookupTable lookupTable;
        VocabCache cache;
        INDArray syn0;
        int words, size;
        try (BufferedInputStream bis = new BufferedInputStream(
                GzipUtils.isCompressedFilename(modelFile.getName())
                        ? new GZIPInputStream(new FileInputStream(modelFile))
                        : new FileInputStream(modelFile));
                DataInputStream dis = new DataInputStream(bis)) {
            words = Integer.parseInt(readString(dis));
            size = Integer.parseInt(readString(dis));
            syn0 = Nd4j.create(words, size);
            cache = new InMemoryLookupCache(false);
            lookupTable = (InMemoryLookupTable) new InMemoryLookupTable.Builder().cache(cache)
                    .vectorLength(size).build();

            String word;
            for (int i = 0; i < words; i++) {

                word = readString(dis);
                log.trace("Loading " + word + " with word " + i);

                float[] vector = new float[size];

                for (int j = 0; j < size; j++) {
                    vector[j] = readFloat(dis);
                }

                syn0.putRow(i, Transforms.unitVec(Nd4j.create(vector)));

                cache.addWordToIndex(cache.numWords(), word);
                cache.addToken(new VocabWord(1, word));
                cache.putVocabWord(word);

                if (linebreaks) {
                    dis.readByte(); 
                }
            }
        }

        Word2Vec ret = new Word2Vec();

        lookupTable.setSyn0(syn0);
        ret.setVocab(cache);
        ret.setLookupTable(lookupTable);
        return ret;

    }

    
    public static float readFloat(InputStream is)
        throws IOException
    {
        byte[] bytes = new byte[4];
        is.read(bytes);
        return getFloat(bytes);
    }

    
    public static float getFloat(byte[] b)
    {
        int accum = 0;
        accum = accum | (b[0] & 0xff) << 0;
        accum = accum | (b[1] & 0xff) << 8;
        accum = accum | (b[2] & 0xff) << 16;
        accum = accum | (b[3] & 0xff) << 24;
        return Float.intBitsToFloat(accum);
    }

    
    public static String readString(DataInputStream dis)
        throws IOException
    {
        byte[] bytes = new byte[MAX_SIZE];
        byte b = dis.readByte();
        int i = -1;
        StringBuilder sb = new StringBuilder();
        while (b != 32 && b != 10) {
            i++;
            bytes[i] = b;
            b = dis.readByte();
            if (i == 49) {
                sb.append(new String(bytes));
                i = -1;
                bytes = new byte[MAX_SIZE];
            }
        }
        sb.append(new String(bytes, 0, i + 1));
        return sb.toString();
    }

    
    public static void writeWordVectors(InMemoryLookupTable lookupTable, InMemoryLookupCache cache,
            String path)
                throws IOException
    {
        BufferedWriter write = new BufferedWriter(new FileWriter(new File(path), false));
        for (int i = 0; i < lookupTable.getSyn0().rows(); i++) {
            String word = cache.wordAtIndex(i);
            if (word == null) {
                continue;
            }
            StringBuilder sb = new StringBuilder();
            sb.append(word.replaceAll(" ", "_"));
            sb.append(" ");
            INDArray wordVector = lookupTable.vector(word);
            for (int j = 0; j < wordVector.length(); j++) {
                sb.append(wordVector.getDouble(j));
                if (j < wordVector.length() - 1) {
                    sb.append(" ");
                }
            }
            sb.append("\n");
            write.write(sb.toString());

        }

        write.flush();
        write.close();

    }

    
    public static void writeWordVectors(Word2Vec vec, String path)
        throws IOException
    {
        BufferedWriter write = new BufferedWriter(new FileWriter(new File(path), false));
        int words = 0;
        for (String word : vec.vocab().words()) {
            if (word == null) {
                continue;
            }
            StringBuilder sb = new StringBuilder();
            sb.append(word.replaceAll(" ", "_"));
            sb.append(" ");
            INDArray wordVector = vec.getWordVectorMatrix(word);
            for (int j = 0; j < wordVector.length(); j++) {
                sb.append(wordVector.getDouble(j));
                if (j < wordVector.length() - 1) {
                    sb.append(" ");
                }
            }
            sb.append("\n");
            write.write(sb.toString());
            words++;

        }

        log.info("Wrote " + words + " with size of " + vec.lookupTable().layerSize());
        write.flush();
        write.close();

    }

    
    public static WordVectors fromTableAndVocab(WeightLookupTable table, VocabCache vocab)
    {
        WordVectorsImpl vectors = new WordVectorsImpl();
        vectors.setLookupTable(table);
        vectors.setVocab(vocab);
        return vectors;
    }

    
    public static WordVectors fromPair(Pair<InMemoryLookupTable, VocabCache> pair)
    {
        WordVectorsImpl vectors = new WordVectorsImpl();
        vectors.setLookupTable(pair.getFirst());
        vectors.setVocab(pair.getSecond());
        return vectors;
    }

    
    public static WordVectors loadTxtVectors(File vectorsFile)
        throws FileNotFoundException
    {
        Pair<InMemoryLookupTable, VocabCache> pair = loadTxt(vectorsFile);
        return fromPair(pair);
    }

    
    public static Pair<InMemoryLookupTable, VocabCache> loadTxt(File vectorsFile)
        throws FileNotFoundException
    {
        BufferedReader write = new BufferedReader(new FileReader(vectorsFile));
        VocabCache cache = new InMemoryLookupCache();

        InMemoryLookupTable lookupTable;

        LineIterator iter = IOUtils.lineIterator(write);
        List<INDArray> arrays = new ArrayList<>();
        while (iter.hasNext()) {
            String line = iter.nextLine();
            String[] split = line.split(" ");
            String word = split[0];
            VocabWord word1 = new VocabWord(1.0, word);
            cache.addToken(word1);
            cache.addWordToIndex(cache.numWords(), word);
            word1.setIndex(cache.numWords());
            cache.putVocabWord(word);
            INDArray row = Nd4j.create(Nd4j.createBuffer(split.length - 1));
            for (int i = 1; i < split.length; i++) {
                row.putScalar(i - 1, Float.parseFloat(split[i]));
            }
            arrays.add(row);
        }

        INDArray syn = Nd4j.create(new int[] { arrays.size(), arrays.get(0).columns() });
        for (int i = 0; i < syn.rows(); i++) {
            syn.putRow(i, arrays.get(i));
        }

        lookupTable = (InMemoryLookupTable) new InMemoryLookupTable.Builder()
                .vectorLength(arrays.get(0).columns())
                .useAdaGrad(false).cache(cache)
                .build();
        Nd4j.clearNans(syn);
        lookupTable.setSyn0(syn);

        iter.close();

        return new Pair<>(lookupTable, cache);
    }

    
    public static void writeTsneFormat(Glove vec, INDArray tsne, File csv)
        throws Exception
    {
        BufferedWriter write = new BufferedWriter(new FileWriter(csv));
        int words = 0;
        InMemoryLookupCache l = (InMemoryLookupCache) vec.vocab();
        for (String word : vec.vocab().words()) {
            if (word == null) {
                continue;
            }
            StringBuilder sb = new StringBuilder();
            INDArray wordVector = tsne.getRow(l.wordFor(word).getIndex());
            for (int j = 0; j < wordVector.length(); j++) {
                sb.append(wordVector.getDouble(j));
                if (j < wordVector.length() - 1) {
                    sb.append(",");
                }
            }
            sb.append(",");
            sb.append(word);
            sb.append(" ");

            sb.append("\n");
            write.write(sb.toString());

        }

        log.info("Wrote " + words + " with size of " + vec.lookupTable().getVectorLength());
        write.flush();
        write.close();

    }

    
    public static void writeTsneFormat(Word2Vec vec, INDArray tsne, File csv)
        throws Exception
    {
        BufferedWriter write = new BufferedWriter(new FileWriter(csv));
        int words = 0;
        InMemoryLookupCache l = (InMemoryLookupCache) vec.vocab();
        for (String word : vec.vocab().words()) {
            if (word == null) {
                continue;
            }
            StringBuilder sb = new StringBuilder();
            INDArray wordVector = tsne.getRow(l.wordFor(word).getIndex());
            for (int j = 0; j < wordVector.length(); j++) {
                sb.append(wordVector.getDouble(j));
                if (j < wordVector.length() - 1) {
                    sb.append(",");
                }
            }
            sb.append(",");
            sb.append(word);
            sb.append(" ");

            sb.append("\n");
            write.write(sb.toString());

        }

        log.info("Wrote " + words + " with size of " + vec.lookupTable().layerSize());
        write.flush();
        write.close();

    }

}

<code block>


package org.deeplearning4j.models.embeddings.loader;

import java.io.BufferedInputStream;
import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.DataInputStream;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.io.InputStream;
import java.util.ArrayList;
import java.util.List;
import java.util.zip.GZIPInputStream;

import org.apache.commons.compress.compressors.gzip.GzipUtils;
import org.apache.commons.io.IOUtils;
import org.apache.commons.io.LineIterator;
import org.deeplearning4j.berkeley.Pair;
import org.deeplearning4j.models.embeddings.WeightLookupTable;
import org.deeplearning4j.models.embeddings.inmemory.InMemoryLookupTable;
import org.deeplearning4j.models.embeddings.wordvectors.WordVectors;
import org.deeplearning4j.models.embeddings.wordvectors.WordVectorsImpl;
import org.deeplearning4j.models.glove.Glove;
import org.deeplearning4j.models.word2vec.VocabWord;
import org.deeplearning4j.models.word2vec.Word2Vec;
import org.deeplearning4j.models.word2vec.wordstore.VocabCache;
import org.deeplearning4j.models.word2vec.wordstore.inmemory.InMemoryLookupCache;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.ops.transforms.Transforms;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


public class WordVectorSerializer
{

    private static final int MAX_SIZE = 50;
    private static final Logger log = LoggerFactory.getLogger(WordVectorSerializer.class);

    
    public static Word2Vec loadGoogleModel(File modelFile, boolean binary)
        throws IOException
    {
        return binary ? readBinaryModel(modelFile) : readTextModel(modelFile);
    }

    
    private static Word2Vec readTextModel(File modelFile)
        throws  IOException, NumberFormatException
    {
        InMemoryLookupTable lookupTable;
        VocabCache cache;
        INDArray syn0;
        BufferedReader reader = new BufferedReader(new FileReader(modelFile));
        String line = reader.readLine();
        String[] initial = line.split(" ");
        int words = Integer.parseInt(initial[0]);
        int layerSize = Integer.parseInt(initial[1]);
        syn0 = Nd4j.create(words, layerSize);

        cache = new InMemoryLookupCache();

        int currLine = 0;
        while ((line = reader.readLine()) != null) {
            String[] split = line.split(" ");
            assert split.length == layerSize + 1;
            String word = split[0];

            if (word.isEmpty()) {
                continue;
            }

            float[] vector = new float[split.length - 1];
            for (int i = 1; i < split.length; i++) {
                vector[i - 1] = Float.parseFloat(split[i]);
            }

            syn0.putRow(currLine, Transforms.unitVec(Nd4j.create(vector)));

            cache.addWordToIndex(cache.numWords(), word);
            cache.addToken(new VocabWord(1, word));
            cache.putVocabWord(word);

            currLine++;
        }

        lookupTable = (InMemoryLookupTable) new InMemoryLookupTable.Builder().cache(cache)
                .vectorLength(layerSize).build();
        lookupTable.setSyn0(syn0);

        Word2Vec ret = new Word2Vec();
        ret.setVocab(cache);
        ret.setLookupTable(lookupTable);

        reader.close();
        return ret;
    }

    
    private static Word2Vec readBinaryModel(File modelFile)
        throws NumberFormatException, IOException {
        InMemoryLookupTable lookupTable;
        VocabCache cache;
        INDArray syn0;
        int words, size;
        try (BufferedInputStream bis =
                new BufferedInputStream(GzipUtils.isCompressedFilename(modelFile.getName()) ?
                        new GZIPInputStream(new FileInputStream(modelFile)) :
                        new FileInputStream(modelFile));
                DataInputStream dis = new DataInputStream(bis)) {
            words = Integer.parseInt(readString(dis));
            size = Integer.parseInt(readString(dis));
            syn0 = Nd4j.create(words, size);
            cache = new InMemoryLookupCache(false);
            lookupTable = (InMemoryLookupTable) new InMemoryLookupTable.Builder().cache(cache)
                    .vectorLength(size).build();

            String word;
            for (int i = 0; i < words; i++) {

                word = readString(dis);
                log.trace("Loading " + word + " with word " + i);
                if (word.isEmpty()) {
                    continue;
                }

                float[] vector = new float[size];

                for (int j = 0; j < size; j++) {
                    vector[j] = readFloat(dis);
                }

                syn0.putRow(i, Transforms.unitVec(Nd4j.create(vector)));

                cache.addWordToIndex(cache.numWords(), word);
                cache.addToken(new VocabWord(1, word));
                cache.putVocabWord(word);
            }
        }

        Word2Vec ret = new Word2Vec();

        lookupTable.setSyn0(syn0);
        ret.setVocab(cache);
        ret.setLookupTable(lookupTable);
        return ret;
    }


    
    public static float readFloat(InputStream is) throws IOException {
        byte[] bytes = new byte[4];
        is.read(bytes);
        return getFloat(bytes);
    }

    
    public static float getFloat(byte[] b) {
        int accum = 0;
        accum = accum | (b[0] & 0xff) << 0;
        accum = accum | (b[1] & 0xff) << 8;
        accum = accum | (b[2] & 0xff) << 16;
        accum = accum | (b[3] & 0xff) << 24;
        return Float.intBitsToFloat(accum);
    }


    
    public static String readString(DataInputStream dis)
        throws IOException
    {
        byte[] bytes = new byte[MAX_SIZE];
        byte b = dis.readByte();
        int i = -1;
        StringBuilder sb = new StringBuilder();
        while (b != 32 && b != 10) {
            i++;
            bytes[i] = b;
            b = dis.readByte();
            if (i == 49) {
                sb.append(new String(bytes));
                i = -1;
                bytes = new byte[MAX_SIZE];
            }
        }
        sb.append(new String(bytes, 0, i + 1));
        return sb.toString();
    }

    
    public static void writeWordVectors(InMemoryLookupTable lookupTable, InMemoryLookupCache cache,
            String path)
        throws IOException {
        BufferedWriter write = new BufferedWriter(new FileWriter(new File(path), false));
        for (int i = 0; i < lookupTable.getSyn0().rows(); i++) {
            String word = cache.wordAtIndex(i);
            if (word == null) {
                continue;
            }
            StringBuilder sb = new StringBuilder();
            sb.append(word.replaceAll(" ", "_"));
            sb.append(" ");
            INDArray wordVector = lookupTable.vector(word);
            for (int j = 0; j < wordVector.length(); j++) {
                sb.append(wordVector.getDouble(j));
                if (j < wordVector.length() - 1) {
                    sb.append(" ");
                }
            }
            sb.append("\n");
            write.write(sb.toString());

        }

        write.flush();
        write.close();

    }

    
    public static void writeWordVectors(Word2Vec vec, String path)
        throws IOException
    {
        BufferedWriter write = new BufferedWriter(new FileWriter(new File(path), false));
        int words = 0;
        for (String word : vec.vocab().words()) {
            if (word == null) {
                continue;
            }
            StringBuilder sb = new StringBuilder();
            sb.append(word.replaceAll(" ", "_"));
            sb.append(" ");
            INDArray wordVector = vec.getWordVectorMatrix(word);
            for (int j = 0; j < wordVector.length(); j++) {
                sb.append(wordVector.getDouble(j));
                if (j < wordVector.length() - 1) {
                    sb.append(" ");
                }
            }
            sb.append("\n");
            write.write(sb.toString());
            words++;

        }

        log.info("Wrote " + words + " with size of " + vec.lookupTable().layerSize());
        write.flush();
        write.close();

    }

    
    public static WordVectors fromTableAndVocab(WeightLookupTable table, VocabCache vocab)
    {
        WordVectorsImpl vectors = new WordVectorsImpl();
        vectors.setLookupTable(table);
        vectors.setVocab(vocab);
        return vectors;
    }

    
    public static WordVectors fromPair(Pair<InMemoryLookupTable, VocabCache> pair) {
        WordVectorsImpl vectors = new WordVectorsImpl();
        vectors.setLookupTable(pair.getFirst());
        vectors.setVocab(pair.getSecond());
        return vectors;
    }

    
    public static WordVectors loadTxtVectors(File vectorsFile)
        throws FileNotFoundException {
        Pair<InMemoryLookupTable, VocabCache> pair = loadTxt(vectorsFile);
        return fromPair(pair);
    }

    
    public static Pair<InMemoryLookupTable, VocabCache> loadTxt(File vectorsFile)
        throws FileNotFoundException {
        BufferedReader write = new BufferedReader(new FileReader(vectorsFile));
        VocabCache cache = new InMemoryLookupCache();

        InMemoryLookupTable lookupTable;

        LineIterator iter = IOUtils.lineIterator(write);
        List<INDArray> arrays = new ArrayList<>();
        while (iter.hasNext()) {
            String line = iter.nextLine();
            String[] split = line.split(" ");
            String word = split[0];
            VocabWord word1 = new VocabWord(1.0, word);
            cache.addToken(word1);
            cache.addWordToIndex(cache.numWords(), word);
            word1.setIndex(cache.numWords());
            cache.putVocabWord(word);
            INDArray row = Nd4j.create(Nd4j.createBuffer(split.length - 1));
            for (int i = 1; i < split.length; i++) {
                row.putScalar(i - 1, Float.parseFloat(split[i]));
            }
            arrays.add(row);
        }

        INDArray syn = Nd4j.create(new int[] { arrays.size(), arrays.get(0).columns() });
        for (int i = 0; i < syn.rows(); i++) {
            syn.putRow(i, arrays.get(i));
        }

        lookupTable = (InMemoryLookupTable) new InMemoryLookupTable.Builder()
                .vectorLength(arrays.get(0).columns())
                .useAdaGrad(false).cache(cache)
                .build();
        Nd4j.clearNans(syn);
        lookupTable.setSyn0(syn);

        iter.close();

        return new Pair<>(lookupTable, cache);
    }

    
    public static void writeTsneFormat(Glove vec, INDArray tsne, File csv)
        throws Exception
    {
        BufferedWriter write = new BufferedWriter(new FileWriter(csv));
        int words = 0;
        InMemoryLookupCache l = (InMemoryLookupCache) vec.vocab();
        for (String word : vec.vocab().words()) {
            if (word == null) {
                continue;
            }
            StringBuilder sb = new StringBuilder();
            INDArray wordVector = tsne.getRow(l.wordFor(word).getIndex());
            for (int j = 0; j < wordVector.length(); j++) {
                sb.append(wordVector.getDouble(j));
                if (j < wordVector.length() - 1) {
                    sb.append(",");
                }
            }
            sb.append(",");
            sb.append(word);
            sb.append(" ");

            sb.append("\n");
            write.write(sb.toString());

        }

        log.info("Wrote " + words + " with size of " + vec.lookupTable().getVectorLength());
        write.flush();
        write.close();

    }

    
    public static void writeTsneFormat(Word2Vec vec, INDArray tsne, File csv)
        throws Exception
    {
        BufferedWriter write = new BufferedWriter(new FileWriter(csv));
        int words = 0;
        InMemoryLookupCache l = (InMemoryLookupCache) vec.vocab();
        for (String word : vec.vocab().words()) {
            if (word == null) {
                continue;
            }
            StringBuilder sb = new StringBuilder();
            INDArray wordVector = tsne.getRow(l.wordFor(word).getIndex());
            for (int j = 0; j < wordVector.length(); j++) {
                sb.append(wordVector.getDouble(j));
                if (j < wordVector.length() - 1) {
                    sb.append(",");
                }
            }
            sb.append(",");
            sb.append(word);
            sb.append(" ");

            sb.append("\n");
            write.write(sb.toString());

        }

        log.info("Wrote " + words + " with size of " + vec.lookupTable().layerSize());
        write.flush();
        write.close();

    }

}
