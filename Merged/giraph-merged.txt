

package org.apache.giraph;

import org.apache.giraph.bsp.CentralizedServiceWorker;
import org.apache.giraph.conf.GiraphConfiguration;
import org.apache.giraph.conf.GiraphConstants;
import org.apache.giraph.examples.GeneratedVertexReader;
import org.apache.giraph.examples.SimplePageRankComputation.SimplePageRankVertexInputFormat;
import org.apache.giraph.examples.SimplePageRankComputation.SimplePageRankVertexOutputFormat;
import org.apache.giraph.graph.BasicComputation;
import org.apache.giraph.graph.Vertex;
import org.apache.giraph.job.GiraphJob;
import org.apache.giraph.ooc.CheckMemoryCallable;
import org.apache.giraph.ooc.DiskBackedPartitionStore;
import org.apache.giraph.ooc.MemoryEstimator;
import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.FloatWritable;
import org.apache.hadoop.io.LongWritable;
import org.junit.Test;

import java.io.IOException;

import static org.junit.Assert.assertTrue;


public class TestOutOfCore extends BspCase {
  final static int NUM_PARTITIONS = 32;
  final static int MAX_NUM_PARTITIONS_IN_MEMORY = 16;
  final static int FAIR_NUM_PARTITIONS_IN_MEMORY = 12;

  public TestOutOfCore() {
      super(TestOutOfCore.class.getName());
  }

  public static class EmptyComputation extends BasicComputation<
      LongWritable, DoubleWritable, FloatWritable, DoubleWritable> {

    @Override
    public void compute(
        Vertex<LongWritable, DoubleWritable, FloatWritable> vertex,
        Iterable<DoubleWritable> messages) throws IOException {
      if (getSuperstep() > 5) {
        vertex.voteToHalt();
      }
    }
  }

  public static class TestMemoryEstimator implements MemoryEstimator {
    private DiskBackedPartitionStore partitionStore;
    @Override
    public void initialize(CentralizedServiceWorker serviceWorker) {
      partitionStore =
          (DiskBackedPartitionStore) serviceWorker.getPartitionStore();
    }

    @Override
    public double freeMemoryMB() {
      int numPartitions = partitionStore.getNumPartitionInMemory();
      if (numPartitions > MAX_NUM_PARTITIONS_IN_MEMORY) {
        return 1;
      } else if (numPartitions > FAIR_NUM_PARTITIONS_IN_MEMORY) {
        return 10;
      } else {
        return 40;
      }
    }

    @Override
    public double maxMemoryMB() {
      return 100;
    }
  }
  
  @Test
  public void testOutOfCore()
          throws IOException, InterruptedException, ClassNotFoundException {
    GiraphConfiguration conf = new GiraphConfiguration();
    conf.setComputationClass(EmptyComputation.class);
    conf.setVertexInputFormatClass(SimplePageRankVertexInputFormat.class);
    conf.setVertexOutputFormatClass(SimplePageRankVertexOutputFormat.class);
    GiraphConstants.USER_PARTITION_COUNT.set(conf, NUM_PARTITIONS);
    GiraphConstants.USE_OUT_OF_CORE_GRAPH.set(conf, true);
    GiraphConstants.OUT_OF_CORE_MEM_ESTIMATOR
        .set(conf, TestMemoryEstimator.class);
    CheckMemoryCallable.CHECK_MEMORY_INTERVAL.set(conf, 5);
    GiraphConstants.NUM_COMPUTE_THREADS.set(conf, 8);
    GiraphConstants.NUM_INPUT_THREADS.set(conf, 8);
    GiraphConstants.NUM_OOC_THREADS.set(conf, 4);
    GiraphConstants.NUM_OUTPUT_THREADS.set(conf, 8);
    GiraphJob job = prepareJob(getCallingMethodName(), conf,
        getTempPath(getCallingMethodName()));
    
    GeneratedVertexReader.READER_VERTICES.set(conf, 400);
    assertTrue(job.run(true));
  }
}

<code block>


package org.apache.giraph.partition;

import org.apache.giraph.utils.ExtendedDataOutput;
import org.apache.giraph.utils.VertexIdEdges;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;


public abstract class PartitionStore<I extends WritableComparable,
    V extends Writable, E extends Writable> {
  
  public abstract boolean addPartition(Partition<I, V, E> partition);

  
  public abstract Partition<I, V, E> removePartition(Integer partitionId);

  
  public abstract boolean hasPartition(Integer partitionId);

  
  public abstract Iterable<Integer> getPartitionIds();

  
  public abstract int getNumPartitions();

  
  public abstract long getPartitionVertexCount(Integer partitionId);

  
  public abstract long getPartitionEdgeCount(Integer partitionId);

  
  public boolean isEmpty() {
    return getNumPartitions() == 0;
  }

  
  public void shutdown() { }

  
  public void initialize() { }

  
  public abstract void startIteration();

  
  public abstract Partition<I, V, E> getNextPartition();

  
  public abstract void putPartition(Partition<I, V, E> partition);

  
  public abstract void addPartitionVertices(Integer partitionId,
      ExtendedDataOutput extendedDataOutput);

  
  public abstract void addPartitionEdges(Integer partitionId,
      VertexIdEdges<I, E> edges);

  
  public abstract void moveEdgesToVertices();
}

<code block>


package org.apache.giraph.partition;

import com.google.common.collect.Maps;
import org.apache.giraph.bsp.CentralizedServiceWorker;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.giraph.edge.EdgeStore;
import org.apache.giraph.edge.EdgeStoreFactory;
import org.apache.giraph.utils.ExtendedDataOutput;
import org.apache.giraph.utils.VertexIdEdges;
import org.apache.giraph.utils.VertexIterator;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.mapreduce.Mapper;

import java.util.concurrent.ArrayBlockingQueue;
import java.util.concurrent.BlockingQueue;
import java.util.concurrent.ConcurrentMap;


public class SimplePartitionStore<I extends WritableComparable,
    V extends Writable, E extends Writable>
    extends PartitionStore<I, V, E> {
  
  private final ConcurrentMap<Integer, Partition<I, V, E>> partitions =
      Maps.newConcurrentMap();
  
  private final EdgeStore<I, V, E> edgeStore;
  
  private final ImmutableClassesGiraphConfiguration<I, V, E> conf;
  
  private final Mapper<?, ?, ?, ?>.Context context;
  
  private BlockingQueue<Partition<I, V, E>> partitionQueue;

  
  public SimplePartitionStore(ImmutableClassesGiraphConfiguration<I, V, E> conf,
      Mapper<?, ?, ?, ?>.Context context,
      CentralizedServiceWorker<I, V, E> serviceWorker) {
    this.conf = conf;
    this.context = context;
    EdgeStoreFactory<I, V, E> edgeStoreFactory = conf.createEdgeStoreFactory();
    edgeStoreFactory.initialize(serviceWorker, conf, context);
    edgeStore = edgeStoreFactory.newStore();
  }

  @Override
  public boolean addPartition(Partition<I, V, E> partition) {
    return partitions.putIfAbsent(partition.getId(), partition) == null;
  }

  @Override
  public Partition<I, V, E> removePartition(Integer partitionId) {
    return partitions.remove(partitionId);
  }

  @Override
  public boolean hasPartition(Integer partitionId) {
    return partitions.containsKey(partitionId);
  }

  @Override
  public Iterable<Integer> getPartitionIds() {
    return partitions.keySet();
  }

  @Override
  public int getNumPartitions() {
    return partitions.size();
  }

  @Override
  public long getPartitionVertexCount(Integer partitionId) {
    Partition partition = partitions.get(partitionId);
    if (partition == null) {
      return 0;
    } else {
      return partition.getVertexCount();
    }
  }

  @Override
  public long getPartitionEdgeCount(Integer partitionId) {
    Partition partition = partitions.get(partitionId);
    if (partition == null) {
      return 0;
    } else {
      return partition.getEdgeCount();
    }
  }

  @Override
  public void startIteration() {
    if (partitionQueue != null && !partitionQueue.isEmpty()) {
      throw new IllegalStateException("startIteration: It seems that some of " +
          "of the partitions from previous iteration over partition store are" +
          " not yet processed.");
    }
    partitionQueue =
        new ArrayBlockingQueue<Partition<I, V, E>>(getNumPartitions());
    for (Partition<I, V, E> partition : partitions.values()) {
      partitionQueue.add(partition);
    }
  }

  @Override
  public Partition<I, V, E> getNextPartition() {
    return partitionQueue.poll();
  }

  @Override
  public void putPartition(Partition<I, V, E> partition) { }

  
  private Partition<I, V, E> getOrCreatePartition(Integer partitionId) {
    Partition<I, V, E> oldPartition = partitions.get(partitionId);
    if (oldPartition == null) {
      Partition<I, V, E> newPartition =
          conf.createPartition(partitionId, context);
      oldPartition = partitions.putIfAbsent(partitionId, newPartition);
      if (oldPartition == null) {
        return newPartition;
      }
    }
    return oldPartition;
  }

  @Override
  public void addPartitionVertices(Integer partitionId,
      ExtendedDataOutput extendedDataOutput) {
    VertexIterator<I, V, E> vertexIterator =
        new VertexIterator<I, V, E>(extendedDataOutput, conf);

    Partition<I, V, E> partition = getOrCreatePartition(partitionId);
    partition.addPartitionVertices(vertexIterator);
    putPartition(partition);
  }

  @Override
  public void addPartitionEdges(Integer partitionId,
      VertexIdEdges<I, E> edges) {
    edgeStore.addPartitionEdges(partitionId, edges);
  }

  @Override
  public void moveEdgesToVertices() {
    edgeStore.moveEdgesToVertices();
  }
}

<code block>


package org.apache.giraph.ooc;

<code block>


package org.apache.giraph.ooc;

import org.apache.giraph.bsp.CentralizedServiceWorker;
import org.apache.giraph.utils.MemoryUtils;


public class JVMMemoryEstimator implements MemoryEstimator {
  
  public JVMMemoryEstimator() { }

  @Override
  public void initialize(CentralizedServiceWorker serviceWorker) { }

  @Override
  public double freeMemoryMB() {
    return MemoryUtils.freePlusUnallocatedMemoryMB();
  }

  @Override public double maxMemoryMB() {
    return MemoryUtils.maxMemoryMB();
  }
}

<code block>


package org.apache.giraph.ooc;

import com.google.common.collect.Iterables;
import com.google.common.collect.Maps;
import com.google.common.collect.Sets;
import com.google.common.hash.HashFunction;
import com.google.common.hash.Hashing;
import org.apache.commons.lang3.tuple.MutablePair;
import org.apache.commons.lang3.tuple.Pair;
import org.apache.giraph.bsp.CentralizedServiceWorker;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.giraph.conf.IntConfOption;
import org.apache.giraph.edge.EdgeStore;
import org.apache.giraph.edge.EdgeStoreFactory;
import org.apache.giraph.edge.OutEdges;
import org.apache.giraph.graph.Vertex;
import org.apache.giraph.partition.Partition;
import org.apache.giraph.partition.PartitionStore;
import org.apache.giraph.utils.ByteArrayVertexIdEdges;
import org.apache.giraph.utils.ExtendedDataOutput;
import org.apache.giraph.utils.PairList;
import org.apache.giraph.utils.VertexIdEdges;
import org.apache.giraph.utils.VertexIterator;
import org.apache.giraph.utils.WritableUtils;
import org.apache.giraph.worker.BspServiceWorker;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Mapper.Context;
import org.apache.log4j.Logger;

import java.io.BufferedInputStream;
import java.io.BufferedOutputStream;
import java.io.DataInput;
import java.io.DataInputStream;
import java.io.DataOutput;
import java.io.DataOutputStream;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Set;
import java.util.concurrent.ConcurrentMap;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.locks.ReadWriteLock;
import java.util.concurrent.locks.ReentrantReadWriteLock;

import static org.apache.giraph.conf.GiraphConstants.MAX_PARTITIONS_IN_MEMORY;
import static org.apache.giraph.conf.GiraphConstants.ONE_MB;
import static org.apache.giraph.conf.GiraphConstants.PARTITIONS_DIRECTORY;


@SuppressWarnings("rawtypes")
public class DiskBackedPartitionStore<I extends WritableComparable,
    V extends Writable, E extends Writable>
    extends PartitionStore<I, V, E> {
  
  public static final IntConfOption MINIMUM_BUFFER_SIZE_TO_FLUSH =
      new IntConfOption("giraph.flushBufferSize", 8 * ONE_MB,
          "Minimum size of a buffer (in bytes) to flush to disk. ");

  
  private static final Logger LOG =
      Logger.getLogger(DiskBackedPartitionStore.class);

  
  private final int minBuffSize;
  
  private enum State { INIT, ACTIVE, INACTIVE, IN_TRANSIT, ON_DISK };

  
  private final ConcurrentMap<Integer, MetaPartition> partitions =
    Maps.newConcurrentMap();

  
  private final Map<State, Set<Integer>> processedPartitions;
  
  private final Map<State, Set<Integer>> unProcessedPartitions;

  
  private ReadWriteLock rwLock = new ReentrantReadWriteLock();

  
  private final
  ImmutableClassesGiraphConfiguration<I, V, E> conf;
  
  private final Context context;
  
  private final String[] basePaths;
  
  private final HashFunction hasher = Hashing.murmur3_32();
  
  private final AtomicInteger maxPartitionsInMem = new AtomicInteger(-1);
  
  private final AtomicInteger numPartitionsInMem = new AtomicInteger(0);
  
  private CentralizedServiceWorker<I, V, E> serviceWorker;

  
  private final OutOfCoreEngine oocEngine;
  
  private final EdgeStore<I, V, E> edgeStore;
  
  private volatile boolean movingEdges;
  
  private volatile AtomicBoolean isInitialized;
  
  private final boolean isNumPartitionsFixed;

  
  private final ConcurrentMap<Integer, Pair<Integer, List<ExtendedDataOutput>>>
      pendingInputVertices = Maps.newConcurrentMap();
  
  private final ConcurrentMap<Integer, Integer> numPendingInputVerticesOnDisk =
      Maps.newConcurrentMap();
  
  private ReadWriteLock vertexBufferRWLock = new ReentrantReadWriteLock();

  
  private final ConcurrentMap<Integer, Pair<Integer, List<VertexIdEdges<I, E>>>>
      pendingInputEdges = Maps.newConcurrentMap();
  
  private final ConcurrentMap<Integer, Integer> numPendingInputEdgesOnDisk =
      Maps.newConcurrentMap();
  
  private ReadWriteLock edgeBufferRWLock = new ReentrantReadWriteLock();

  
  private final ConcurrentMap<Integer, Boolean> hasEdgeStoreOnDisk =
      Maps.newConcurrentMap();

  
  public DiskBackedPartitionStore(
      ImmutableClassesGiraphConfiguration<I, V, E> conf,
      Mapper<?, ?, ?, ?>.Context context,
      CentralizedServiceWorker<I, V, E> serviceWorker) {
    this.conf = conf;
    this.context = context;
    this.serviceWorker = serviceWorker;
    this.minBuffSize = MINIMUM_BUFFER_SIZE_TO_FLUSH.get(conf);
    int userMaxNumPartitions = MAX_PARTITIONS_IN_MEMORY.get(conf);
    if (userMaxNumPartitions > 0) {
      this.isNumPartitionsFixed = true;
      this.maxPartitionsInMem.set(userMaxNumPartitions);
      oocEngine = null;
    } else {
      this.isNumPartitionsFixed = false;
      this.oocEngine =
          new AdaptiveOutOfCoreEngine<I, V, E>(conf, serviceWorker);
    }
    EdgeStoreFactory<I, V, E> edgeStoreFactory = conf.createEdgeStoreFactory();
    edgeStoreFactory.initialize(serviceWorker, conf, context);
    this.edgeStore = edgeStoreFactory.newStore();
    this.movingEdges = false;
    this.isInitialized = new AtomicBoolean(false);

    this.processedPartitions = Maps.newHashMap();
    this.processedPartitions
        .put(State.INACTIVE, Sets.<Integer>newLinkedHashSet());
    this.processedPartitions
        .put(State.IN_TRANSIT, Sets.<Integer>newLinkedHashSet());
    this.processedPartitions
        .put(State.ON_DISK, Sets.<Integer>newLinkedHashSet());

    this.unProcessedPartitions = Maps.newHashMap();
    this.unProcessedPartitions
        .put(State.INACTIVE, Sets.<Integer>newLinkedHashSet());
    this.unProcessedPartitions
        .put(State.IN_TRANSIT, Sets.<Integer>newLinkedHashSet());
    this.unProcessedPartitions
        .put(State.ON_DISK, Sets.<Integer>newLinkedHashSet());

    
    String[] userPaths = PARTITIONS_DIRECTORY.getArray(conf);
    basePaths = new String[userPaths.length];
    int i = 0;
    for (String path : userPaths) {
      basePaths[i++] = path + "/" + conf.get("mapred.job.id", "Unknown Job");
    }
    if (LOG.isInfoEnabled()) {
      LOG.info("DiskBackedPartitionStore with isStaticGraph=" +
          conf.isStaticGraph() + ((userMaxNumPartitions > 0) ?
          (" with maximum " + userMaxNumPartitions + " partitions in memory.") :
          "."));
    }
  }

  
  public int getNumPartitionSlots() {
    return maxPartitionsInMem.get();
  }

  
  public int getNumPartitionInMemory() {
    return numPartitionsInMem.get();
  }

  
  public void setNumPartitionSlots(int numPartitions) {
    maxPartitionsInMem.set(numPartitions);
  }

  @Override
  public void initialize() {
    
    
    
    
    
    
    if (isInitialized.compareAndSet(false, true)) {
      
      if (maxPartitionsInMem.get() == -1) {
        maxPartitionsInMem.set(serviceWorker.getNumPartitionsOwned());
        
        
        if (maxPartitionsInMem.get() == 0) {
          LOG.warn("initialize: partitions assigned to this worker is not " +
              "known yet");
          maxPartitionsInMem.set(partitions.size());
          if (maxPartitionsInMem.get() == 0) {
            maxPartitionsInMem.set(Integer.MAX_VALUE);
          }
        }
        if (LOG.isInfoEnabled()) {
          LOG.info("initialize: set the max number of partitions in memory " +
              "to " + maxPartitionsInMem.get());
        }
        oocEngine.initialize();
      }
    }
  }

  @Override
  public Iterable<Integer> getPartitionIds() {
    return Iterables.unmodifiableIterable(partitions.keySet());
  }

  @Override
  public boolean hasPartition(final Integer id) {
    return partitions.containsKey(id);
  }

  @Override
  public int getNumPartitions() {
    return partitions.size();
  }

  @Override
  public long getPartitionVertexCount(Integer partitionId) {
    MetaPartition meta = partitions.get(partitionId);
    if (meta == null) {
      return 0;
    } else if (meta.getState() == State.ON_DISK) {
      return meta.getVertexCount();
    } else {
      return meta.getPartition().getVertexCount();
    }
  }

  @Override
  public long getPartitionEdgeCount(Integer partitionId) {
    MetaPartition meta = partitions.get(partitionId);
    if (meta == null) {
      return 0;
    } else if (meta.getState() == State.ON_DISK) {
      return meta.getEdgeCount();
    } else {
      return meta.getPartition().getEdgeCount();
    }
  }

  
  @edu.umd.cs.findbugs.annotations.SuppressWarnings("TLW_TWO_LOCK_WAIT")
  private void swapOnePartitionToDisk() {
    Integer partitionId;
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    rwLock.readLock().lock();
    synchronized (processedPartitions) {
      partitionId = popFromSet(processedPartitions.get(State.INACTIVE));
    }
    if (partitionId == null) {
      synchronized (unProcessedPartitions) {
        partitionId = popFromSet(unProcessedPartitions.get(State.INACTIVE));
      }
      if (partitionId == null) {
        
        
        synchronized (processedPartitions) {
          partitionId = popFromSet(processedPartitions.get(State.INACTIVE));
          while (partitionId == null) {
            try {
              
              
              
              
              
              
              processedPartitions.wait();
            } catch (InterruptedException e) {
              throw new IllegalStateException("swapOnePartitionToDisk: Caught" +
                  "InterruptedException while waiting on a partition to" +
                  "become inactive in memory and swapping it to disk");
            }
            partitionId = popFromSet(processedPartitions.get(State.INACTIVE));
          }
        }
      }
    }

    if (LOG.isInfoEnabled()) {
      LOG.info("swapOnePartitionToDisk: decided to swap partition " +
          partitionId + " to disk");
    }
    MetaPartition swapOutPartition = partitions.get(partitionId);
    if (swapOutPartition == null) {
      throw new IllegalStateException("swapOnePartitionToDisk: the partition " +
          "is not found to spill to disk (impossible)");
    }

    
    
    
    
    
    
    Map<State, Set<Integer>> ownerMap = (swapOutPartition.isProcessed()) ?
        processedPartitions :
        unProcessedPartitions;

    
    
    
    
    
    synchronized (swapOutPartition) {
      swapOutPartition.setState(State.IN_TRANSIT);
      synchronized (ownerMap) {
        ownerMap.get(State.IN_TRANSIT).add(partitionId);
      }
    }

    try {
      if (LOG.isInfoEnabled()) {
        LOG.info("swapOnePartitionToDisk: start swapping partition " +
            partitionId + " to disk.");
      }
      offloadPartition(swapOutPartition);
      if (LOG.isInfoEnabled()) {
        LOG.info("swapOnePartitionToDisk: done swapping partition " +
            partitionId + " to disk.");
      }
    } catch (IOException e) {
      throw new IllegalStateException(
          "swapOnePartitionToDisk: Failed while offloading partition " +
              partitionId);
    }

    synchronized (swapOutPartition) {
      synchronized (ownerMap) {
        boolean stillInMap = ownerMap.get(State.IN_TRANSIT).remove(partitionId);
        swapOutPartition.setOnDisk();
        numPartitionsInMem.getAndDecrement();
        
        
        
        
        if (stillInMap) {
          ownerMap.get(State.ON_DISK).add(partitionId);
        }
      }
      
      
      swapOutPartition.notifyAll();
    }
    rwLock.readLock().unlock();
  }

  
  public void spillOnePartition() {
    if (maxPartitionsInMem.getAndDecrement() <= numPartitionsInMem.get()) {
      swapOnePartitionToDisk();
    }
  }

  @Override
  public Partition<I, V, E> getNextPartition() {
    Integer partitionId;
    
    
    
    
    
    
    synchronized (unProcessedPartitions) {
      partitionId = popFromSet(unProcessedPartitions.get(State.INACTIVE));
      if (partitionId == null) {
        partitionId = popFromSet(unProcessedPartitions.get(State.ON_DISK));
        if (partitionId == null) {
          partitionId = popFromSet(unProcessedPartitions.get(State.IN_TRANSIT));
        }
      }
    }

    
    if (partitionId == null) {
      return null;
    }

    MetaPartition meta = partitions.get(partitionId);
    if (meta == null) {
      throw new IllegalStateException("getNextPartition: partition " +
          partitionId + " does not exist (impossible)");
    }

    
    
    
    
    
    
    if (movingEdges) {
      boolean shouldProcess = false;
      synchronized (meta) {
        if (meta.getState() == State.INACTIVE) {
          shouldProcess = edgeStore.hasPartitionEdges(partitionId);
        } else { 
          Integer numBuf = numPendingInputEdgesOnDisk.get(partitionId);
          Boolean hasStore = hasEdgeStoreOnDisk.get(partitionId);
          shouldProcess =
              (numBuf != null && numBuf != 0) || (hasStore != null && hasStore);
        }
        if (!shouldProcess) {
          meta.setProcessed(true);
          synchronized (processedPartitions) {
            processedPartitions.get(meta.getState()).add(partitionId);
            if (meta.getState() == State.INACTIVE) {
              processedPartitions.notifyAll();
            }
          }
        }
      }
      if (!shouldProcess) {
        return getNextPartition();
      }
    }
    getPartition(meta);
    return meta.getPartition();
  }

  
  private void getPartition(MetaPartition meta) {
    int partitionId = meta.getId();
    synchronized (meta) {
      boolean partitionInMemory = false;
      while (!partitionInMemory) {
        switch (meta.getState()) {
        case INACTIVE:
          meta.setState(State.ACTIVE);
          partitionInMemory = true;
          break;
        case IN_TRANSIT:
          try {
            
            meta.wait();
          } catch (InterruptedException e) {
            throw new IllegalStateException("getPartition: exception " +
                "while waiting on IN_TRANSIT partition " + partitionId + " to" +
                " fully spill to disk.");
          }
          break;
        case ON_DISK:
          boolean spaceAvailable = false;

          while (numPartitionsInMem.get() >= maxPartitionsInMem.get()) {
            swapOnePartitionToDisk();
          }

          
          if (numPartitionsInMem.incrementAndGet() <=
              maxPartitionsInMem.get()) {
            spaceAvailable = true;
          } else {
            numPartitionsInMem.decrementAndGet();
          }

          if (spaceAvailable) {
            Partition<I, V, E> partition;
            try {
              if (LOG.isInfoEnabled()) {
                LOG.info("getPartition: start reading partition " +
                    partitionId + " from disk");
              }
              partition = loadPartition(partitionId, meta.getVertexCount());
              if (LOG.isInfoEnabled()) {
                LOG.info("getPartition: done reading partition " +
                    partitionId + " from disk");
              }
            } catch (IOException e) {
              LOG.error("getPartition: Failed while Loading Partition " +
                  "from disk: " + e.getMessage());
              throw new IllegalStateException(e);
            }
            meta.setActive(partition);
            partitionInMemory = true;
          }
          break;
        default:
          throw new IllegalStateException("illegal state " + meta.getState() +
              " for partition " + meta.getId());
        }
      }
    }
  }

  
  public void spillPartitionInputEdgeStore(Integer partitionId)
      throws IOException {
    rwLock.readLock().lock();
    if (movingEdges) {
      rwLock.readLock().unlock();
      return;
    }
    Pair<Integer, List<VertexIdEdges<I, E>>> entry;

    
    
    edgeBufferRWLock.writeLock().lock();
    entry = pendingInputEdges.remove(partitionId);
    edgeBufferRWLock.writeLock().unlock();

    
    if (entry == null) {
      rwLock.readLock().unlock();
      return;
    }

    
    if (entry.getRight().isEmpty()) {
      throw new IllegalStateException("spillPartitionInputEdgeStore: " +
          "the edge buffer that is supposed to be flushed to disk does not" +
          "exist.");
    }

    List<VertexIdEdges<I, E>> bufferList = entry.getRight();
    Integer numBuffers = numPendingInputEdgesOnDisk.putIfAbsent(partitionId,
        bufferList.size());
    if (numBuffers != null) {
      numPendingInputEdgesOnDisk.replace(
          partitionId, numBuffers + bufferList.size());
    }

    File file = new File(getPendingEdgesBufferPath(partitionId));
    FileOutputStream fos = new FileOutputStream(file, true);
    BufferedOutputStream bos = new BufferedOutputStream(fos);
    DataOutputStream dos = new DataOutputStream(bos);
    for (VertexIdEdges<I, E> edges : entry.getRight()) {
      edges.write(dos);
    }
    dos.close();
    rwLock.readLock().unlock();
  }

  
  public PairList<Integer, Integer> getOocPartitionIdsWithPendingInputEdges() {
    PairList<Integer, Integer> pairList = new PairList<>();
    pairList.initialize();
    if (!movingEdges) {
      for (Entry<Integer, Pair<Integer, List<VertexIdEdges<I, E>>>> entry :
          pendingInputEdges.entrySet()) {
        if (entry.getValue().getLeft() > minBuffSize) {
          pairList.add(entry.getKey(), entry.getValue().getLeft());
        }
      }
    }
    return pairList;
  }

  @Override
  @edu.umd.cs.findbugs.annotations.SuppressWarnings("SF_SWITCH_FALLTHROUGH")
  public void addPartitionEdges(Integer partitionId,
      VertexIdEdges<I, E> edges) {
    if (!isInitialized.get()) {
      initialize();
    }

    MetaPartition meta = new MetaPartition(partitionId);
    MetaPartition temp = partitions.putIfAbsent(partitionId, meta);
    if (temp != null) {
      meta = temp;
    }

    boolean createPartition = false;
    synchronized (meta) {
      switch (meta.getState()) {
      case INIT:
        Partition<I, V, E> partition =
            conf.createPartition(partitionId, context);
        meta.setPartition(partition);
        
        
        
        
        meta.setProcessed(true);
        numPartitionsInMem.getAndIncrement();
        meta.setState(State.INACTIVE);
      synchronized (processedPartitions) {
        processedPartitions.get(State.INACTIVE).add(partitionId);
        processedPartitions.notifyAll();
      }
        createPartition = true;
        
        
      case INACTIVE:
        
        edgeStore.addPartitionEdges(partitionId, edges);
        break;
      case IN_TRANSIT:
      case ON_DISK:
        
        List<VertexIdEdges<I, E>> newEdges =
            new ArrayList<VertexIdEdges<I, E>>();
        newEdges.add(edges);
        int length = edges.getSerializedSize();
        Pair<Integer, List<VertexIdEdges<I, E>>> newPair =
            new MutablePair<>(length, newEdges);
        edgeBufferRWLock.readLock().lock();
        Pair<Integer, List<VertexIdEdges<I, E>>> oldPair =
            pendingInputEdges.putIfAbsent(partitionId, newPair);
        if (oldPair != null) {
          synchronized (oldPair) {
            MutablePair<Integer, List<VertexIdEdges<I, E>>> pair =
                (MutablePair<Integer, List<VertexIdEdges<I, E>>>) oldPair;
            pair.setLeft(pair.getLeft() + length);
            pair.getRight().add(edges);
          }
        }
        edgeBufferRWLock.readLock().unlock();
        
        
        if (isNumPartitionsFixed &&
            pendingInputEdges.get(partitionId).getLeft() > minBuffSize) {
          try {
            spillPartitionInputEdgeStore(partitionId);
          } catch (IOException e) {
            throw new IllegalStateException("addPartitionEdges: spilling " +
                "edge store for partition " + partitionId + " failed!");
          }
        }
        break;
      default:
        throw new IllegalStateException("illegal state " + meta.getState() +
            " for partition " + meta.getId());
      }
    }
    
    
    if (createPartition &&
        numPartitionsInMem.get() > maxPartitionsInMem.get()) {
      swapOnePartitionToDisk();
    }
  }

  
  public void spillPartitionInputVertexBuffer(Integer partitionId)
      throws IOException {
    rwLock.readLock().lock();
    if (movingEdges) {
      rwLock.readLock().unlock();
      return;
    }
    Pair<Integer, List<ExtendedDataOutput>> entry;
    
    
    
    
    
    
    
    
    vertexBufferRWLock.writeLock().lock();
    entry = pendingInputVertices.remove(partitionId);
    vertexBufferRWLock.writeLock().unlock();

    
    if (entry == null) {
      rwLock.readLock().unlock();
      return;
    }
    
    if (entry.getRight().isEmpty()) {
      throw new IllegalStateException("spillPartitionInputVertexBuffer: " +
          "the vertex buffer that is supposed to be flushed to disk does not" +
          "exist.");
    }

    List<ExtendedDataOutput> bufferList = entry.getRight();
    Integer numBuffers = numPendingInputVerticesOnDisk.putIfAbsent(partitionId,
        bufferList.size());
    if (numBuffers != null) {
      numPendingInputVerticesOnDisk.replace(partitionId,
          numBuffers + bufferList.size());
    }

    File file = new File(getPendingVerticesBufferPath(partitionId));
    FileOutputStream fos = new FileOutputStream(file, true);
    BufferedOutputStream bos = new BufferedOutputStream(fos);
    DataOutputStream dos = new DataOutputStream(bos);
    for (ExtendedDataOutput extendedDataOutput : bufferList) {
      WritableUtils.writeExtendedDataOutput(extendedDataOutput, dos);
    }
    dos.close();
    rwLock.readLock().unlock();
  }

  
  public PairList<Integer, Integer>
  getOocPartitionIdsWithPendingInputVertices() {
    PairList<Integer, Integer> pairList = new PairList<>();
    pairList.initialize();
    if (!movingEdges) {
      for (Entry<Integer, Pair<Integer, List<ExtendedDataOutput>>> entry :
          pendingInputVertices.entrySet()) {
        if (entry.getValue().getLeft() > minBuffSize) {
          pairList.add(entry.getKey(), entry.getValue().getLeft());
        }
      }
    }
    return pairList;
  }

  @Override
  @edu.umd.cs.findbugs.annotations.SuppressWarnings("SF_SWITCH_FALLTHROUGH")
  public void addPartitionVertices(Integer partitionId,
      ExtendedDataOutput extendedDataOutput) {
    if (!isInitialized.get()) {
      initialize();
    }

    MetaPartition meta = new MetaPartition(partitionId);
    MetaPartition temp = partitions.putIfAbsent(partitionId, meta);
    if (temp != null) {
      meta = temp;
    }

    boolean createPartition = false;
    synchronized (meta) {
      switch (meta.getState()) {
      case INIT:
        Partition<I, V, E> partition =
            conf.createPartition(partitionId, context);
        meta.setPartition(partition);
        
        
        meta.setProcessed(true);
        numPartitionsInMem.getAndIncrement();
        meta.setState(State.INACTIVE);
      synchronized (processedPartitions) {
        processedPartitions.get(State.INACTIVE).add(partitionId);
        processedPartitions.notifyAll();
      }
        createPartition = true;
        
        
      case INACTIVE:
        
        meta.getPartition().addPartitionVertices(
            new VertexIterator<I, V, E>(extendedDataOutput, conf));
        break;
      case IN_TRANSIT:
      case ON_DISK:
        
        List<ExtendedDataOutput> vertices = new ArrayList<ExtendedDataOutput>();
        vertices.add(extendedDataOutput);
        int length = extendedDataOutput.getPos();
        Pair<Integer, List<ExtendedDataOutput>> newPair =
            new MutablePair<>(length, vertices);
        vertexBufferRWLock.readLock().lock();
        Pair<Integer, List<ExtendedDataOutput>> oldPair =
            pendingInputVertices.putIfAbsent(partitionId, newPair);
        if (oldPair != null) {
          synchronized (oldPair) {
            MutablePair<Integer, List<ExtendedDataOutput>> pair =
                (MutablePair<Integer, List<ExtendedDataOutput>>) oldPair;
            pair.setLeft(pair.getLeft() + length);
            pair.getRight().add(extendedDataOutput);
          }
        }
        vertexBufferRWLock.readLock().unlock();
        
        
        if (isNumPartitionsFixed &&
            pendingInputVertices.get(partitionId).getLeft() > minBuffSize) {
          try {
            spillPartitionInputVertexBuffer(partitionId);
          } catch (IOException e) {
            throw new IllegalStateException("addPartitionVertices: spilling " +
                "vertex buffer for partition " + partitionId + " failed!");
          }
        }
        break;
      default:
        throw new IllegalStateException("illegal state " + meta.getState() +
            " for partition " + meta.getId());
      }
    }
    
    
    if (createPartition &&
        numPartitionsInMem.get() > maxPartitionsInMem.get()) {
      swapOnePartitionToDisk();
    }
  }

  @Override
  public void putPartition(Partition<I, V, E> partition) {
    if (partition == null) {
      throw new IllegalStateException("putPartition: partition to put is null" +
          " (impossible)");
    }
    Integer id = partition.getId();
    MetaPartition meta = partitions.get(id);
    if (meta == null) {
      throw new IllegalStateException("putPartition: partition to put does" +
          "not exist in the store (impossible)");
    }
    synchronized (meta) {
      if (meta.getState() != State.ACTIVE) {
        String msg = "It is not possible to put back a partition which is " +
            "not ACTIVE.\n" + meta.toString();
        LOG.error(msg);
        throw new IllegalStateException(msg);
      }

      meta.setState(State.INACTIVE);
      meta.setProcessed(true);
      synchronized (processedPartitions) {
        processedPartitions.get(State.INACTIVE).add(id);
        
        
        processedPartitions.notifyAll();
      }
    }
  }

  @Override
  public Partition<I, V, E> removePartition(Integer partitionId) {
    if (hasPartition(partitionId)) {
      MetaPartition meta = partitions.remove(partitionId);
      
      
      if (!processedPartitions.get(meta.getState()).remove(partitionId)) {
        throw new IllegalStateException("removePartition: partition that is" +
            "about to remove is not in processed list (impossible)");
      }
      getPartition(meta);
      numPartitionsInMem.getAndDecrement();
      return meta.getPartition();
    }
    return null;
  }

  @Override
  public boolean addPartition(Partition<I, V, E> partition) {
    if (!isInitialized.get()) {
      initialize();
    }

    Integer id = partition.getId();
    MetaPartition meta = new MetaPartition(id);
    MetaPartition temp = partitions.putIfAbsent(id, meta);
    if (temp != null) {
      return false;
    }

    if (LOG.isInfoEnabled()) {
      LOG.info("addPartition: partition " + id + "is  added to the store.");
    }

    meta.setPartition(partition);
    meta.setState(State.INACTIVE);
    meta.setProcessed(true);
    synchronized (processedPartitions) {
      processedPartitions.get(State.INACTIVE).add(id);
      processedPartitions.notifyAll();
    }
    numPartitionsInMem.getAndIncrement();
    
    
    if (numPartitionsInMem.get() > maxPartitionsInMem.get()) {
      swapOnePartitionToDisk();
    }
    return true;
  }

  @Override
  public void shutdown() {
    
    if (!unProcessedPartitions.get(State.INACTIVE).isEmpty() ||
        !unProcessedPartitions.get(State.IN_TRANSIT).isEmpty() ||
        !unProcessedPartitions.get(State.ON_DISK).isEmpty()) {
      throw new IllegalStateException("shutdown: There are some " +
          "unprocessed partitions left from the " +
          "previous superstep. This should not be possible");
    }

    for (MetaPartition meta : partitions.values()) {
      synchronized (meta) {
        while (meta.getState() == State.IN_TRANSIT) {
          try {
            meta.wait();
          } catch (InterruptedException e) {
            throw new IllegalStateException("shutdown: exception while" +
                "waiting on an IN_TRANSIT partition to be written on disk");
          }
        }
        if (meta.getState() == State.ON_DISK) {
          deletePartitionFiles(meta.getId());
        }
      }
    }

    if (oocEngine != null) {
      oocEngine.shutdown();
    }
  }

  @Override
  public void startIteration() {
    if (!isInitialized.get()) {
      initialize();
    }
    if (LOG.isInfoEnabled()) {
      LOG.info("startIteration: with " + numPartitionsInMem.get() +
          " partitions in memory, there can be maximum " + maxPartitionsInMem +
          " partitions in memory out of " + partitions.size() + " that " +
          "belongs to this worker.");
    }
    
    
    if (!unProcessedPartitions.get(State.INACTIVE).isEmpty() ||
        !unProcessedPartitions.get(State.IN_TRANSIT).isEmpty() ||
        !unProcessedPartitions.get(State.ON_DISK).isEmpty()) {
      throw new IllegalStateException("startIteration: There are some " +
          "unprocessed and/or in-transition partitions left from the " +
          "previous superstep. This should not be possible");
    }

    rwLock.writeLock().lock();
    for (MetaPartition meta : partitions.values()) {
      
      if (!meta.isProcessed()) {
        throw new IllegalStateException("startIteration: meta-partition " +
            meta + " has not been processed in the previous superstep.");
      }
      
      
      
      
      
      if (meta.getState() == State.IN_TRANSIT) {
        throw new IllegalStateException("startIteration: meta-partition " +
            meta + " is still IN_TRANSIT (impossible)");
      }

      meta.setProcessed(false);
    }

    unProcessedPartitions.clear();
    unProcessedPartitions.putAll(processedPartitions);
    processedPartitions.clear();
    processedPartitions
        .put(State.INACTIVE, Sets.<Integer>newLinkedHashSet());
    processedPartitions
        .put(State.IN_TRANSIT, Sets.<Integer>newLinkedHashSet());
    processedPartitions
        .put(State.ON_DISK, Sets.<Integer>newLinkedHashSet());
    rwLock.writeLock().unlock();
    LOG.info("startIteration: done preparing the iteration");
  }

  @Override
  public void moveEdgesToVertices() {
    movingEdges = true;
    edgeStore.moveEdgesToVertices();
    movingEdges = false;
  }

  
  private Integer popFromSet(Set<Integer> set) {
    if (!set.isEmpty()) {
      Iterator<Integer> it = set.iterator();
      Integer id = it.next();
      it.remove();
      return id;
    }
    return null;
  }

  
  public void increasePartitionSlots(Integer numPartitionsToIncrease) {
    maxPartitionsInMem.getAndAdd(numPartitionsToIncrease);
    if (LOG.isInfoEnabled()) {
      LOG.info("increasePartitionSlots: allowing partition store to have " +
          numPartitionsToIncrease + " more partitions. Now, partition store " +
          "can have up to " + maxPartitionsInMem.get() + " partitions.");
    }
  }

  @Override
  public String toString() {
    StringBuilder sb = new StringBuilder();
    for (MetaPartition e : partitions.values()) {
      sb.append(e.toString() + "\n");
    }
    return sb.toString();
  }

  
  private void writeVertexData(DataOutput output, Vertex<I, V, E> vertex)
    throws IOException {

    vertex.getId().write(output);
    vertex.getValue().write(output);
    output.writeBoolean(vertex.isHalted());
  }

  
  @SuppressWarnings("unchecked")
  private void writeOutEdges(DataOutput output, Vertex<I, V, E> vertex)
    throws IOException {

    vertex.getId().write(output);
    OutEdges<I, E> edges = (OutEdges<I, E>) vertex.getEdges();
    edges.write(output);
  }

  
  private void readVertexData(DataInput in, Vertex<I, V, E> vertex)
    throws IOException {

    I id = conf.createVertexId();
    id.readFields(in);
    V value = conf.createVertexValue();
    value.readFields(in);
    OutEdges<I, E> edges = conf.createAndInitializeOutEdges(0);
    vertex.initialize(id, value, edges);
    if (in.readBoolean()) {
      vertex.voteToHalt();
    } else {
      vertex.wakeUp();
    }
  }

  
  @SuppressWarnings("unchecked")
  private void readOutEdges(DataInput in, Partition<I, V, E> partition)
    throws IOException {

    I id = conf.createVertexId();
    id.readFields(in);
    Vertex<I, V, E> v = partition.getVertex(id);
    OutEdges<I, E> edges = (OutEdges<I, E>) v.getEdges();
    edges.readFields(in);
    partition.saveVertex(v);
  }

  
  @SuppressWarnings("unchecked")
  private Partition<I, V, E> loadPartition(int id, long numVertices)
    throws IOException {
    Partition<I, V, E> partition = conf.createPartition(id, context);

    
    File file = new File(getVerticesPath(id));
    if (LOG.isDebugEnabled()) {
      LOG.debug("loadPartition: loading partition vertices " +
        partition.getId() + " from " + file.getAbsolutePath());
    }

    FileInputStream filein = new FileInputStream(file);
    BufferedInputStream bufferin = new BufferedInputStream(filein);
    DataInputStream inputStream  = new DataInputStream(bufferin);
    for (int i = 0; i < numVertices; ++i) {
      Vertex<I, V , E> vertex = conf.createVertex();
      readVertexData(inputStream, vertex);
      partition.putVertex(vertex);
    }
    inputStream.close();
    if (!file.delete()) {
      String msg = "loadPartition: failed to delete " + file.getAbsolutePath();
      LOG.error(msg);
      throw new IllegalStateException(msg);
    }

    
    file = new File(getEdgesPath(id));

    if (LOG.isDebugEnabled()) {
      LOG.debug("loadPartition: loading partition edges " +
        partition.getId() + " from " + file.getAbsolutePath());
    }

    filein = new FileInputStream(file);
    bufferin = new BufferedInputStream(filein);
    inputStream  = new DataInputStream(bufferin);
    for (int i = 0; i < numVertices; ++i) {
      readOutEdges(inputStream, partition);
    }
    inputStream.close();
    
    
    if (!conf.isStaticGraph() ||
        serviceWorker.getSuperstep() == BspServiceWorker.INPUT_SUPERSTEP) {
      if (!file.delete()) {
        String msg =
            "loadPartition: failed to delete " + file.getAbsolutePath();
        LOG.error(msg);
        throw new IllegalStateException(msg);
      }
    }

    if (serviceWorker.getSuperstep() == BspServiceWorker.INPUT_SUPERSTEP) {
      
      
      Integer numBuffers = numPendingInputVerticesOnDisk.remove(id);
      if (numBuffers != null) {
        file = new File(getPendingVerticesBufferPath(id));
        if (LOG.isDebugEnabled()) {
          LOG.debug("loadPartition: loading " + numBuffers + " input vertex " +
              "buffers of partition " + id + " from " + file.getAbsolutePath());
        }
        filein = new FileInputStream(file);
        bufferin = new BufferedInputStream(filein);
        inputStream = new DataInputStream(bufferin);
        for (int i = 0; i < numBuffers; ++i) {
          ExtendedDataOutput extendedDataOutput =
              WritableUtils.readExtendedDataOutput(inputStream, conf);
          partition.addPartitionVertices(
              new VertexIterator<I, V, E>(extendedDataOutput, conf));
        }
        inputStream.close();
        if (!file.delete()) {
          String msg =
              "loadPartition: failed to delete " + file.getAbsolutePath();
          LOG.error(msg);
          throw new IllegalStateException(msg);
        }
      }
      
      Pair<Integer, List<ExtendedDataOutput>> vertexPair;
      vertexBufferRWLock.writeLock().lock();
      vertexPair = pendingInputVertices.remove(id);
      vertexBufferRWLock.writeLock().unlock();
      if (vertexPair != null) {
        for (ExtendedDataOutput extendedDataOutput : vertexPair.getRight()) {
          partition.addPartitionVertices(
              new VertexIterator<I, V, E>(extendedDataOutput, conf));
        }
      }

      
      if (!hasEdgeStoreOnDisk.containsKey(id)) {
        throw new IllegalStateException("loadPartition: partition is written" +
            " to disk in INPUT_SUPERSTEP, but it is not clear whether its " +
            "edge store is on disk or not (impossible)");
      }
      if (hasEdgeStoreOnDisk.remove(id)) {
        file = new File(getEdgeStorePath(id));
        if (LOG.isDebugEnabled()) {
          LOG.debug("loadPartition: loading edge store of partition " + id +
              " from " + file.getAbsolutePath());
        }
        filein = new FileInputStream(file);
        bufferin = new BufferedInputStream(filein);
        inputStream = new DataInputStream(bufferin);
        edgeStore.readPartitionEdgeStore(id, inputStream);
        inputStream.close();
        if (!file.delete()) {
          String msg =
              "loadPartition: failed to delete " + file.getAbsolutePath();
          LOG.error(msg);
          throw new IllegalStateException(msg);
        }
      }

      
      
      numBuffers = numPendingInputEdgesOnDisk.remove(id);
      if (numBuffers != null) {
        file = new File(getPendingEdgesBufferPath(id));
        if (LOG.isDebugEnabled()) {
          LOG.debug("loadPartition: loading " + numBuffers + " input edge " +
              "buffers of partition " + id + " from " + file.getAbsolutePath());
        }
        filein = new FileInputStream(file);
        bufferin = new BufferedInputStream(filein);
        inputStream = new DataInputStream(bufferin);
        for (int i = 0; i < numBuffers; ++i) {
          VertexIdEdges<I, E> vertexIdEdges =
              new ByteArrayVertexIdEdges<I, E>();
          vertexIdEdges.setConf(conf);
          vertexIdEdges.readFields(inputStream);
          edgeStore.addPartitionEdges(id, vertexIdEdges);
        }
        inputStream.close();
        if (!file.delete()) {
          String msg =
              "loadPartition: failed to delete " + file.getAbsolutePath();
          LOG.error(msg);
          throw new IllegalStateException(msg);
        }
      }
      
      Pair<Integer, List<VertexIdEdges<I, E>>> edgePair = null;
      edgeBufferRWLock.writeLock().lock();
      edgePair = pendingInputEdges.remove(id);
      edgeBufferRWLock.writeLock().unlock();
      if (edgePair != null) {
        for (VertexIdEdges<I, E> vertexIdEdges : edgePair.getRight()) {
          edgeStore.addPartitionEdges(id, vertexIdEdges);
        }
      }
    }
    return partition;
  }

  
  private void offloadPartition(MetaPartition meta) throws IOException {
    Partition<I, V, E> partition = meta.getPartition();
    int partitionId = meta.getId();
    File file = new File(getVerticesPath(partitionId));
    File parent = file.getParentFile();
    if (!parent.exists() && !parent.mkdirs() && LOG.isDebugEnabled()) {
      LOG.debug("offloadPartition: directory " + parent.getAbsolutePath() +
        " already exists.");
    }

    if (!file.createNewFile()) {
      String msg = "offloadPartition: file " + parent.getAbsolutePath() +
        " already exists.";
      LOG.error(msg);
      throw new IllegalStateException(msg);
    }

    if (LOG.isDebugEnabled()) {
      LOG.debug("offloadPartition: writing partition vertices " +
        partitionId + " to " + file.getAbsolutePath());
    }

    FileOutputStream fileout = new FileOutputStream(file);
    BufferedOutputStream bufferout = new BufferedOutputStream(fileout);
    DataOutputStream outputStream  = new DataOutputStream(bufferout);
    for (Vertex<I, V, E> vertex : partition) {
      writeVertexData(outputStream, vertex);
    }
    outputStream.close();

    
    
    
    
    file = new File(getEdgesPath(partitionId));
    if (serviceWorker.getSuperstep() == BspServiceWorker.INPUT_SUPERSTEP ||
        meta.getPrevVertexCount() != partition.getVertexCount() ||
        !conf.isStaticGraph() || !file.exists()) {

      meta.setPrevVertexCount(partition.getVertexCount());

      if (!file.createNewFile() && LOG.isDebugEnabled()) {
        LOG.debug("offloadPartition: file " + file.getAbsolutePath() +
            " already exists.");
      }

      if (LOG.isDebugEnabled()) {
        LOG.debug("offloadPartition: writing partition edges " +
          partitionId + " to " + file.getAbsolutePath());
      }

      fileout = new FileOutputStream(file);
      bufferout = new BufferedOutputStream(fileout);
      outputStream = new DataOutputStream(bufferout);
      for (Vertex<I, V, E> vertex : partition) {
        writeOutEdges(outputStream, vertex);
      }
      outputStream.close();
    }

    
    if (serviceWorker.getSuperstep() == BspServiceWorker.INPUT_SUPERSTEP) {
      if (edgeStore.hasPartitionEdges(partitionId)) {
        hasEdgeStoreOnDisk.put(partitionId, true);
        file = new File(getEdgeStorePath(partitionId));
        if (!file.createNewFile() && LOG.isDebugEnabled()) {
          LOG.debug("offloadPartition: file " + file.getAbsolutePath() +
              " already exists.");
        }

        if (LOG.isDebugEnabled()) {
          LOG.debug("offloadPartition: writing partition edge store of " +
              partitionId + " to " + file.getAbsolutePath());
        }

        fileout = new FileOutputStream(file);
        bufferout = new BufferedOutputStream(fileout);
        outputStream = new DataOutputStream(bufferout);
        edgeStore.writePartitionEdgeStore(partitionId, outputStream);
        outputStream.close();
      } else {
        hasEdgeStoreOnDisk.put(partitionId, false);
      }
    }
  }

  
  public void deletePartitionFiles(Integer id) {
    
    File file = new File(getVerticesPath(id));
    if (file.exists() && !file.delete()) {
      String msg = "deletePartitionFiles: Failed to delete file " +
        file.getAbsolutePath();
      LOG.error(msg);
      throw new IllegalStateException(msg);
    }

    
    file = new File(getEdgesPath(id));
    if (file.exists() && !file.delete()) {
      String msg = "deletePartitionFiles: Failed to delete file " +
        file.getAbsolutePath();
      LOG.error(msg);
      throw new IllegalStateException(msg);
    }
  }

  
  private String getPartitionPath(Integer partitionId) {
    int hash = hasher.hashInt(partitionId).asInt();
    int idx = Math.abs(hash % basePaths.length);
    return basePaths[idx] + "/partition-" + partitionId;
  }

  
  private String getVerticesPath(Integer partitionId) {
    return getPartitionPath(partitionId) + "_vertices";
  }

  
  private String getPendingVerticesBufferPath(Integer partitionId) {
    return getPartitionPath(partitionId) + "_pending_vertices";
  }

  
  private String getEdgeStorePath(Integer partitionId) {
    return getPartitionPath(partitionId) + "_edge_store";
  }

  
  private String getPendingEdgesBufferPath(Integer partitionId) {
    return getPartitionPath(partitionId) + "_pending_edges";
  }

  
  private String getEdgesPath(Integer partitionId) {
    return getPartitionPath(partitionId) + "_edges";
  }

  
  private class MetaPartition {
    
    
    private int id;
    
    private State state;
    
    private long vertexCount;
    
    private long prevVertexCount;
    
    private long edgeCount;
    
    private boolean isProcessed;

    
    
    private Partition<I, V, E> partition;

    
    public MetaPartition(int id) {
      this.id = id;
      this.state = State.INIT;
      this.vertexCount = 0;
      this.prevVertexCount = 0;
      this.edgeCount = 0;
      this.isProcessed = false;

      this.partition = null;
    }

    
    public int getId() {
      return id;
    }

    
    public State getState() {
      return state;
    }

    
    public void setOnDisk() {
      this.state = State.ON_DISK;
      this.vertexCount = partition.getVertexCount();
      this.edgeCount = partition.getEdgeCount();
      this.partition = null;
    }

    
    public void setActive(Partition<I, V, E> partition) {
      if (partition != null) {
        this.partition = partition;
      }
      this.state = State.ACTIVE;
      this.prevVertexCount = this.vertexCount;
      this.vertexCount = 0;
    }

    
    public void setState(State state) {
      this.state = state;
    }

    
    public void setPrevVertexCount(long vertexCount) {
      this.prevVertexCount = vertexCount;
    }

    
    public long getPrevVertexCount() {
      return prevVertexCount;
    }

    
    public long getVertexCount() {
      return vertexCount;
    }

    
    public long getEdgeCount() {
      return edgeCount;
    }

    
    public boolean isProcessed() {
      return isProcessed;
    }

    
    public void setProcessed(boolean isProcessed) {
      this.isProcessed = isProcessed;
    }

    
    public Partition<I, V, E> getPartition() {
      return partition;
    }

    
    public void setPartition(Partition<I, V, E> partition) {
      this.partition = partition;
    }

    @Override
    public String toString() {
      StringBuffer sb = new StringBuffer();

      sb.append("Meta Data: { ");
      sb.append("ID: " + id + "; ");
      sb.append("State: " + state + "; ");
      sb.append("Number of Vertices: " + vertexCount + "; ");
      sb.append("Previous number of Vertices: " + prevVertexCount + "; ");
      sb.append("Number of edges: " + edgeCount + "; ");
      sb.append("Is processed: " + isProcessed + "; }");
      sb.append("Partition: " + partition + "; }");

      return sb.toString();
    }
  }
}

<code block>


package org.apache.giraph.ooc;

import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;


public interface OutOfCoreEngine<I extends WritableComparable,
    V extends Writable, E extends Writable> {
  
  void initialize();

  
  void shutdown();
}

<code block>


package org.apache.giraph.ooc;

import org.apache.giraph.bsp.BspService;
import org.apache.giraph.bsp.CentralizedServiceWorker;
import org.apache.giraph.conf.FloatConfOption;
import org.apache.giraph.conf.GiraphConstants;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.giraph.conf.IntConfOption;
import org.apache.giraph.utils.PairList;
import org.apache.giraph.utils.ReflectionUtils;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;
import org.apache.log4j.Logger;

import java.util.Stack;
import java.util.concurrent.BlockingQueue;
import java.util.concurrent.BrokenBarrierException;
import java.util.concurrent.Callable;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicInteger;


public class CheckMemoryCallable<I extends WritableComparable,
    V extends Writable, E extends Writable> implements Callable<Void> {
  
  public static final FloatConfOption LOW_FREE_MEMORY_FRACTION =
      new FloatConfOption("giraph.lowFreeMemoryFraction", 0.1f,
          "If free memory fraction goes below this value, GC is called " +
              "manually and necessary actions are taken if we have to go " +
              "out-of-core");
  
  public static final FloatConfOption MID_FREE_MEMORY_FRACTION =
      new FloatConfOption("giraph.midFreeMemoryFraction", 0.15f,
          "Once out-of-core mechanism decides to offload data on disk, it " +
              "offloads data on disk until free memory fraction reaches this " +
              "fraction.");
  
  public static final FloatConfOption FAIR_FREE_MEMORY_FRACTION =
      new FloatConfOption("giraph.fairFreeMemoryFraction", 0.3f,
          "The fraction of free memory at which the job shows the best GC " +
              "performance. This fraction might be dependent on GC strategy " +
              "used in running the job, but generally 0.3 is a reasonable " +
              "fraction for most strategies.");
  
  public static final FloatConfOption HIGH_FREE_MEMORY_FRACTION =
      new FloatConfOption("giraph.highFreeMemoryFraction", 0.4f,
          "Once free memory reaches at this fraction, last out-of-core " +
              "decision is lazily rolled back, i.e. we back off from " +
              "out-of-core.");
  
  public static final IntConfOption CHECK_MEMORY_INTERVAL =
      new IntConfOption("giraph.checkMemoryInterval", 5000,
          "Time interval (in milliseconds) at which checking memory is done" +
              " to decide if there should be any out-of-core action.");
  
  public static final FloatConfOption OOC_GRAPH_MODIFICATION_COEFFICIENT =
      new FloatConfOption("giraph.graphPartitionModificationCoefficient", 0.3f,
          "If we decide to go out-of-core or back-off from out-of-core, this " +
              "is the multiplier by which the number of in-memory partitions" +
              "will change.");

  
  private static final Logger LOG = Logger.getLogger(CheckMemoryCallable.class);

  
  private final CentralizedServiceWorker<I, V, E> serviceWorker;
  
  private final DiskBackedPartitionStore<I, V, E> partitionStore;

  
  
  private float lowFreeMemoryFraction;
  
  private float midFreeMemoryFraction;
  
  private float fairFreeMemoryFraction;
  
  private float highFreeMemoryFraction;
  
  private int checkInterval;
  
  private float modificationCoefficient;

  
  private Stack<Integer> oocPartitionCounts;
  
  private final MemoryEstimator memoryEstimator;
  
  private final AdaptiveOutOfCoreEngine<I, V, E> oocEngine;

  
  public CheckMemoryCallable(AdaptiveOutOfCoreEngine<I, V, E> oocEngine,
      ImmutableClassesGiraphConfiguration<I, V, E> conf,
      CentralizedServiceWorker<I, V, E> serviceWorker) {
    this.oocEngine = oocEngine;
    this.serviceWorker = serviceWorker;
    this.partitionStore =
        (DiskBackedPartitionStore<I, V, E>) serviceWorker.getPartitionStore();

    this.oocPartitionCounts = new Stack<>();

    this.lowFreeMemoryFraction = LOW_FREE_MEMORY_FRACTION.get(conf);
    this.midFreeMemoryFraction = MID_FREE_MEMORY_FRACTION.get(conf);
    this.fairFreeMemoryFraction = FAIR_FREE_MEMORY_FRACTION.get(conf);
    this.highFreeMemoryFraction = HIGH_FREE_MEMORY_FRACTION.get(conf);
    this.checkInterval = CHECK_MEMORY_INTERVAL.get(conf);
    this.modificationCoefficient = OOC_GRAPH_MODIFICATION_COEFFICIENT.get(conf);

    memoryEstimator = ReflectionUtils
        .newInstance(GiraphConstants.OUT_OF_CORE_MEM_ESTIMATOR.get(conf));
  }

  
  @Override
  @edu.umd.cs.findbugs.annotations.SuppressWarnings("DM_GC")
  public Void call() {
    if (LOG.isInfoEnabled()) {
      LOG.info("call: check-memory thread started.");
    }
    memoryEstimator.initialize(serviceWorker);
    CountDownLatch doneCompute = oocEngine.getDoneCompute();
    while (doneCompute.getCount() != 0) {
      double maxMemory = memoryEstimator.maxMemoryMB();
      double freeMemory = memoryEstimator.freeMemoryMB();
      boolean gcDone = false;
      if (freeMemory < lowFreeMemoryFraction * maxMemory) {
        
        
        
        
        
        
        if (LOG.isInfoEnabled()) {
          LOG.info("call: Memory is very limited now. Calling GC manually. " +
              String.format("freeMemory = %.2fMB", freeMemory));
        }
        long gcStartTime = System.currentTimeMillis();
        System.gc();
        gcDone = true;
        freeMemory = memoryEstimator.freeMemoryMB();
        if (LOG.isInfoEnabled()) {
          LOG.info("call: GC is done. " + String
              .format("GC time = %.2f sec, and freeMemory = %.2fMB",
                  (System.currentTimeMillis() - gcStartTime) / 1000.0,
                  freeMemory));
        }
      }

      
      
      
      
      
      
      int numInMemory = partitionStore.getNumPartitionInMemory();
      int maxInMemory = partitionStore.getNumPartitionSlots();
      int numInTotal = partitionStore.getNumPartitions();
      if (freeMemory > highFreeMemoryFraction * maxMemory) {
        if (numInMemory >= maxInMemory && !oocPartitionCounts.isEmpty()) {
          partitionStore.increasePartitionSlots(oocPartitionCounts.pop());
        }
      } else if (freeMemory > fairFreeMemoryFraction * maxMemory) {
        
        
        if (!oocPartitionCounts.isEmpty() || maxInMemory < numInTotal) {
          if (numInMemory >= maxInMemory) {
            partitionStore.increasePartitionSlots(1);
            if (!oocPartitionCounts.isEmpty()) {
              int num = oocPartitionCounts.pop();
              if (num > 1) {
                oocPartitionCounts.push(num - 1);
              }
            }
          }
        }
      } else if (gcDone && freeMemory < midFreeMemoryFraction * maxMemory) {
        BlockingQueue<Integer> partitionsWithInputVertices =
            oocEngine.getPartitionsWithInputVertices();
        BlockingQueue<Integer> partitionsWithInputEdges =
            oocEngine.getPartitionsWithInputEdges();
        AtomicInteger numPartitionsToSpill =
            oocEngine.getNumPartitionsToSpill();

        while (freeMemory < midFreeMemoryFraction * maxMemory) {
          
          
          if (serviceWorker.getSuperstep() == BspService.INPUT_SUPERSTEP) {
            
            
            PairList<Integer, Integer> pairs =
                partitionStore.getOocPartitionIdsWithPendingInputVertices();
            freeMemory -= createCommand(pairs, partitionsWithInputVertices);
          }

          
          if (freeMemory < midFreeMemoryFraction * maxMemory &&
              serviceWorker.getSuperstep() == BspService.INPUT_SUPERSTEP) {
            PairList<Integer, Integer> pairs =
                partitionStore.getOocPartitionIdsWithPendingInputEdges();
            freeMemory -= createCommand(pairs, partitionsWithInputEdges);
          }

          
          if (freeMemory < midFreeMemoryFraction * maxMemory) {
            numPartitionsToSpill
                .set(getNextOocPartitionCount(freeMemory, maxMemory));
          }

          if (!partitionsWithInputVertices.isEmpty() ||
              !partitionsWithInputEdges.isEmpty() ||
              numPartitionsToSpill.get() != 0) {
            if (LOG.isInfoEnabled()) {
              LOG.info("call: signal out-of-core processor threads to start " +
                  "offloading. These threads will spill vertex buffer of " +
                  partitionsWithInputVertices.size() + " partitions, edge " +
                  "buffers of " + partitionsWithInputEdges.size() +
                  " partitions, and " + numPartitionsToSpill.get() + " whole " +
                  "partition");
            }
            
            
            try {
              oocEngine.waitOnGate();
            } catch (InterruptedException e) {
              throw new IllegalStateException("call: Caught " +
                  "InterruptedException while opening the gate for OOC " +
                  "processing threads");
            } catch (BrokenBarrierException e) {
              throw new IllegalStateException("call: Caught " +
                  "BrokenBarrierException while opening the gate for OOC " +
                  "processing threads");
            }
            oocEngine.resetGate();

            if (LOG.isInfoEnabled()) {
              LOG.info("call: waiting on OOC processors to finish offloading " +
                  "data to disk");
            }
            
            
            try {
              oocEngine.waitOnOocSignal();
            } catch (InterruptedException e) {
              throw new IllegalStateException("call: Caught " +
                  "InterruptedException. Looks like memory check thread is " +
                  "interrupted while waiting on OOC processing threads.");
            } catch (BrokenBarrierException e) {
              throw new IllegalStateException("call: Caught " +
                  "BrokenBarrierException. Looks like some OOC processing " +
                  "threads  broke while writing data on disk.");
            }
            oocEngine.resetOocSignal();
          }

          gcDone = false;
          long gcStartTime = 0;
          if (freeMemory < midFreeMemoryFraction * maxMemory) {
            
            
            if (LOG.isInfoEnabled()) {
              LOG.info("call: calling GC manually to free up space for " +
                  "recently offloaded data.");
            }
            gcStartTime = System.currentTimeMillis();
            System.gc();
            gcDone = true;
          }
          freeMemory = memoryEstimator.freeMemoryMB();
          if (LOG.isInfoEnabled()) {
            LOG.info("call: " +
                (gcDone ?
                    ("GC is done. " + String.format("GC time = %.2f sec.",
                        (System.currentTimeMillis() - gcStartTime) / 1000.0)) :
                    "") +
                "Finished offloading data to disk. " +
                String.format("freeMemory = %.2fMB", freeMemory));
          }
        }
      }

      
      try {
        doneCompute.await(checkInterval, TimeUnit.MILLISECONDS);
      } catch (InterruptedException e) {
        throw new IllegalStateException("call: Caught InterruptedException " +
            "while waiting for computation to be done and/or " + checkInterval +
            "milliseconds passes.");
      }
    }

    
    
    
    oocEngine.setDone();
    try {
      oocEngine.waitOnGate();
    } catch (InterruptedException e) {
      throw new IllegalStateException("call: Caught InterruptedException " +
          "while waiting for the last time on gate in the current superstep");
    } catch (BrokenBarrierException e) {
      throw new IllegalStateException("call: Caught BrokenBarrierException " +
          "while waiting for the last time on gate in the current superstep");
    }
    return null;
  }

  
  private int getNextOocPartitionCount(double freeMemory, double maxMemory) {
    int numSlots = partitionStore.getNumPartitionSlots();
    if (numSlots == Integer.MAX_VALUE) {
      numSlots = partitionStore.getNumPartitions();
      partitionStore.setNumPartitionSlots(numSlots);
    }

    double freeFraction = freeMemory / maxMemory;
    double multiplier = Math.min(
        
        modificationCoefficient,
        
        
        (fairFreeMemoryFraction - freeFraction) / (1 - freeFraction));
    int count = Math.max((int) (numSlots * multiplier), 1);
    if (count >= numSlots) {
      LOG.warn("getNextOocPartitionCount: Memory capacity is " +
          numSlots + " partitions, and OOC mechanism is " +
          "trying to put " + count + " partitions to disk. This is not " +
          "possible");
      
      count = numSlots - 1;
      if (count == 0) {
        LOG.warn("It seems that size of one partition is too large for the " +
            "available memory.  Try to run the job with more partitions!");
      }
    }
    if (count != 0) {
      oocPartitionCounts.push(count);
    }
    return count;
  }

  
  private double createCommand(PairList<Integer, Integer> pairs,
      BlockingQueue<Integer> commands) {
    double usedMemory = 0;
    if (pairs.getSize() != 0) {
      PairList<Integer, Integer>.Iterator iterator = pairs.getIterator();
      
      
      while (iterator.hasNext() &&
          commands.remainingCapacity() > 0) {
        iterator.next();
        commands.add(iterator.getCurrentFirst());
        
        
        
        
        usedMemory += iterator.getCurrentSecond() / 1024.0 / 1024.0;
      }
    }
    return usedMemory;
  }
}

<code block>


package org.apache.giraph.ooc;

import org.apache.giraph.bsp.CentralizedServiceWorker;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;
import org.apache.log4j.Logger;

import java.io.IOException;
import java.util.concurrent.BlockingQueue;
import java.util.concurrent.BrokenBarrierException;
import java.util.concurrent.Callable;
import java.util.concurrent.atomic.AtomicInteger;


public class OutOfCoreProcessorCallable<I extends WritableComparable,
    V extends Writable, E extends Writable> implements Callable<Void> {
  
  private static final Logger LOG =
      Logger.getLogger(OutOfCoreProcessorCallable.class);
  
  private final DiskBackedPartitionStore<I, V, E> partitionStore;
  
  private final AdaptiveOutOfCoreEngine<I, V, E> oocEngine;

  
  public OutOfCoreProcessorCallable(AdaptiveOutOfCoreEngine<I, V, E> oocEngine,
      CentralizedServiceWorker<I, V, E> serviceWorker) {
    this.oocEngine = oocEngine;
    this.partitionStore =
        (DiskBackedPartitionStore<I, V, E>) serviceWorker.getPartitionStore();
  }

  @Override
  public Void call() {
    while (true) {
      
      
      
      try {
        oocEngine.waitOnGate();
      } catch (InterruptedException e) {
        throw new IllegalStateException("call: Caught InterruptedException " +
            "while waiting on memory check thread signal on available " +
            "partitions to put on disk");
      } catch (BrokenBarrierException e) {
        throw new IllegalStateException("call Caught BrokenBarrierException. " +
            "Looks like some other threads broke while waiting on barrier");
      }

      
      
      
      if (oocEngine.isDone()) {
        break;
      }

      BlockingQueue<Integer> partitionsWithInputVertices =
          oocEngine.getPartitionsWithInputVertices();
      BlockingQueue<Integer> partitionsWithInputEdges =
          oocEngine.getPartitionsWithInputEdges();
      AtomicInteger numPartitionsToSpill =
          oocEngine.getNumPartitionsToSpill();

      while (!partitionsWithInputVertices.isEmpty()) {
        Integer partitionId = partitionsWithInputVertices.poll();
        if (partitionId == null) {
          break;
        }
        LOG.info("call: spilling vertex buffer of partition " + partitionId);
        try {
          partitionStore.spillPartitionInputVertexBuffer(partitionId);
        } catch (IOException e) {
          throw new IllegalStateException("call: caught IOException while " +
              "spilling vertex buffers to disk");
        }
      }

      while (!partitionsWithInputEdges.isEmpty()) {
        Integer partitionId = partitionsWithInputEdges.poll();
        if (partitionId == null) {
          break;
        }
        LOG.info("call: spilling edge buffer of partition " + partitionId);
        try {
          partitionStore.spillPartitionInputEdgeStore(partitionId);
        } catch (IOException e) {
          throw new IllegalStateException("call: caught IOException while " +
              "spilling edge buffers/store to disk");
        }
      }

      
      while (numPartitionsToSpill.getAndDecrement() > 0) {
        LOG.info("call: start offloading a partition");
        partitionStore.spillOnePartition();
      }

      
      try {
        oocEngine.waitOnOocSignal();
      } catch (InterruptedException e) {
        throw new IllegalStateException("call: Caught InterruptedException " +
            "while waiting to notify memory check thread that I am done");
      } catch (BrokenBarrierException e) {
        throw new IllegalStateException("call: Caught BrokenBarrierException " +
            "while waiting to notify memory check thread that I am done");
      }
    }
    return null;
  }
}

<code block>


package org.apache.giraph.ooc;

import com.google.common.collect.Lists;
import com.google.common.util.concurrent.ThreadFactoryBuilder;
import org.apache.giraph.bsp.CentralizedServiceWorker;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.giraph.utils.CallableFactory;
import org.apache.giraph.utils.LogStacktraceCallable;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;
import org.apache.log4j.Logger;

import java.util.List;
import java.util.concurrent.ArrayBlockingQueue;
import java.util.concurrent.BlockingQueue;
import java.util.concurrent.BrokenBarrierException;
import java.util.concurrent.Callable;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.CyclicBarrier;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.Future;
import java.util.concurrent.atomic.AtomicInteger;


public class AdaptiveOutOfCoreEngine<I extends WritableComparable,
    V extends Writable, E extends Writable> implements
    OutOfCoreEngine<I, V, E> {
  
  private static final Logger LOG =
      Logger.getLogger(AdaptiveOutOfCoreEngine.class);

  
  
  private final CyclicBarrier gate;
  
  private final CyclicBarrier doneOocSignal;
  
  private final CountDownLatch doneCompute;
  
  private volatile boolean done;

  
  
  private final BlockingQueue<Integer> partitionsWithInputVertices;
  
  private final BlockingQueue<Integer> partitionsWithInputEdges;
  
  private final AtomicInteger numPartitionsToSpill;

  
  private ExecutorService checkMemoryExecutor;
  
  private ExecutorService outOfCoreProcessorExecutor;

  
  private ImmutableClassesGiraphConfiguration<I, V, E> conf;
  
  private final CentralizedServiceWorker<I, V, E> serviceWorker;

  
  private int numOocThreads;

  
  private Future<Void> checkMemoryResult;
  
  private List<Future<Void>> oocProcessorResults;

  
  public AdaptiveOutOfCoreEngine(ImmutableClassesGiraphConfiguration conf,
      CentralizedServiceWorker<I, V, E> serviceWorker) {
    this.conf = conf;
    this.serviceWorker = serviceWorker;

    this.numOocThreads = conf.getNumOocThreads();
    this.gate = new CyclicBarrier(numOocThreads + 1);
    this.doneOocSignal = new CyclicBarrier(numOocThreads + 1);
    this.doneCompute = new CountDownLatch(1);
    this.done = false;
    this.partitionsWithInputVertices = new ArrayBlockingQueue<Integer>(100);
    this.partitionsWithInputEdges = new ArrayBlockingQueue<Integer>(100);
    this.numPartitionsToSpill = new AtomicInteger(0);
  }

  @Override
  public void initialize() {
    if (LOG.isInfoEnabled()) {
      LOG.info("initialize: initializing out-of-core engine");
    }
    CallableFactory<Void> checkMemoryCallableFactory =
      new CallableFactory<Void>() {
        @Override
        public Callable<Void> newCallable(int callableId) {
          return new CheckMemoryCallable<I, V, E>(
              AdaptiveOutOfCoreEngine.this, conf, serviceWorker);
        }
      };
    checkMemoryExecutor = Executors.newSingleThreadExecutor(
        new ThreadFactoryBuilder().setNameFormat("check-memory").build());
    checkMemoryResult = checkMemoryExecutor.submit(new LogStacktraceCallable<>(
        checkMemoryCallableFactory.newCallable(0)));

    CallableFactory<Void> outOfCoreProcessorCallableFactory =
      new CallableFactory<Void>() {
        @Override
        public Callable<Void> newCallable(int callableId) {
          return new OutOfCoreProcessorCallable<I, V, E>(
              AdaptiveOutOfCoreEngine.this, serviceWorker);
        }
      };
    outOfCoreProcessorExecutor = Executors
        .newFixedThreadPool(numOocThreads,
            new ThreadFactoryBuilder().setNameFormat("ooc-%d").build());
    oocProcessorResults = Lists.newArrayListWithCapacity(numOocThreads);
    for (int i = 0; i < numOocThreads; ++i) {
      Future<Void> future = outOfCoreProcessorExecutor.submit(
          new LogStacktraceCallable<>(
              outOfCoreProcessorCallableFactory.newCallable(i)));
      oocProcessorResults.add(future);
    }
  }

  @Override
  public void shutdown() {
    doneCompute.countDown();
    checkMemoryExecutor.shutdown();
    if (checkMemoryResult.isCancelled()) {
      throw new IllegalStateException(
          "shutdown: memory check thread did not " + "terminate gracefully!");
    }
    outOfCoreProcessorExecutor.shutdown();
    for (int i = 0; i < numOocThreads; ++i) {
      if (oocProcessorResults.get(i).isCancelled()) {
        throw new IllegalStateException("shutdown: out-of-core processor " +
            "thread " + i + " did not terminate gracefully.");
      }
    }
  }

  
  public CountDownLatch getDoneCompute() {
    return doneCompute;
  }

  
  public boolean isDone() {
    return done;
  }

  
  public BlockingQueue<Integer> getPartitionsWithInputVertices() {
    return partitionsWithInputVertices;
  }

  
  public BlockingQueue<Integer> getPartitionsWithInputEdges() {
    return partitionsWithInputEdges;
  }

  
  public AtomicInteger getNumPartitionsToSpill() {
    return numPartitionsToSpill;
  }

  
  public void waitOnGate() throws BrokenBarrierException, InterruptedException {
    gate.await();
  }

  
  public void resetGate() {
    gate.reset();
  }

  
  public void waitOnOocSignal()
      throws BrokenBarrierException, InterruptedException {
    doneOocSignal.await();
  }

  
  public void resetOocSignal() {
    doneOocSignal.reset();
  }

  
  public void setDone() {
    done = true;
  }
}

<code block>


package org.apache.giraph.ooc;

import org.apache.giraph.bsp.CentralizedServiceWorker;


public interface MemoryEstimator {
  
  void initialize(CentralizedServiceWorker serviceWorker);

  
  double freeMemoryMB();

  
  double maxMemoryMB();
}

<code block>


package org.apache.giraph.graph;

import java.io.IOException;
import java.net.URL;
import java.net.URLDecoder;
import java.util.ArrayList;
import java.util.Collection;
import java.util.Enumeration;
import java.util.List;
import java.util.concurrent.Callable;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.TimeUnit;

import org.apache.giraph.bsp.BspService;
import org.apache.giraph.bsp.CentralizedServiceMaster;
import org.apache.giraph.bsp.CentralizedServiceWorker;
import org.apache.giraph.bsp.checkpoints.CheckpointStatus;
import org.apache.giraph.comm.messages.MessageStore;
import org.apache.giraph.conf.ClassConfOption;
import org.apache.giraph.conf.GiraphConstants;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.giraph.job.JobProgressTracker;
import org.apache.giraph.master.BspServiceMaster;
import org.apache.giraph.master.MasterThread;
import org.apache.giraph.metrics.GiraphMetrics;
import org.apache.giraph.metrics.GiraphMetricsRegistry;
import org.apache.giraph.metrics.GiraphTimer;
import org.apache.giraph.metrics.GiraphTimerContext;
import org.apache.giraph.metrics.ResetSuperstepMetricsObserver;
import org.apache.giraph.metrics.SuperstepMetricsRegistry;
import org.apache.giraph.partition.PartitionOwner;
import org.apache.giraph.partition.PartitionStats;
import org.apache.giraph.partition.PartitionStore;
import org.apache.giraph.scripting.ScriptLoader;
import org.apache.giraph.utils.CallableFactory;
import org.apache.giraph.utils.MemoryUtils;
import org.apache.giraph.utils.ProgressableUtils;
import org.apache.giraph.worker.BspServiceWorker;
import org.apache.giraph.worker.InputSplitsCallable;
import org.apache.giraph.worker.WorkerContext;
import org.apache.giraph.worker.WorkerObserver;
import org.apache.giraph.worker.WorkerProgress;
import org.apache.giraph.zk.ZooKeeperManager;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.log4j.Appender;
import org.apache.log4j.Level;
import org.apache.log4j.LogManager;
import org.apache.log4j.Logger;
import org.apache.log4j.PatternLayout;


@SuppressWarnings("rawtypes")
public class GraphTaskManager<I extends WritableComparable, V extends Writable,
  E extends Writable> implements
  ResetSuperstepMetricsObserver {

  
  public static final ClassConfOption<CheckerIfWorkerShouldFailAfterException>
  CHECKER_IF_WORKER_SHOULD_FAIL_AFTER_EXCEPTION_CLASS = ClassConfOption.create(
      "giraph.checkerIfWorkerShouldFailAfterExceptionClass",
      FailWithEveryExceptionOccurred.class,
      CheckerIfWorkerShouldFailAfterException.class,
      "Class which checks if an exception on some thread should cause worker " +
          "to fail, by default all exceptions cause failure");
  
  public static final String TIMER_SUPERSTEP_TIME = "superstep-time-ms";
  
  public static final String TIMER_COMPUTE_ALL = "compute-all-ms";
  
  public static final String TIMER_TIME_TO_FIRST_MSG =
      "time-to-first-message-ms";
  
  public static final String TIMER_COMMUNICATION_TIME = "communication-time-ms";

  
  private static final Logger LOG = Logger.getLogger(GraphTaskManager.class);
  
  private CentralizedServiceWorker<I, V, E> serviceWorker;
  
  private CentralizedServiceMaster<I, V, E> serviceMaster;
  
  private Thread masterThread = null;
  
  private boolean alreadyRun = false;
  
  private ZooKeeperManager zkManager;
  
  private ImmutableClassesGiraphConfiguration<I, V, E> conf;
  
  private boolean done = false;
  
  private GraphFunctions graphFunctions = GraphFunctions.UNKNOWN;
  
  private FinishedSuperstepStats finishedSuperstepStats =
      new FinishedSuperstepStats(0, false, 0, 0, false, CheckpointStatus.NONE);
  
  private JobProgressTrackerClient jobProgressTracker;

  
  
  private GiraphTimer wcPreAppTimer;
  
  private GiraphTimer wcPostAppTimer;

  
  
  private GiraphTimer superstepTimer;
  
  private GiraphTimer computeAll;
  
  private GiraphTimer timeToFirstMessage;
  
  private GiraphTimerContext timeToFirstMessageTimerContext;
  
  private GiraphTimer communicationTimer;
  
  private GiraphTimerContext communicationTimerContext;
  
  private GiraphTimer wcPreSuperstepTimer;
  
  private final Mapper<?, ?, ?, ?>.Context context;
  
  private boolean isMaster;

  
  public GraphTaskManager(Mapper<?, ?, ?, ?>.Context context) {
    this.context = context;
    this.isMaster = false;
  }

  
  private void checkInput() {
    if (conf.hasEdgeInputFormat()) {
      conf.createWrappedEdgeInputFormat().checkInputSpecs(conf);
    }
    if (conf.hasVertexInputFormat()) {
      conf.createWrappedVertexInputFormat().checkInputSpecs(conf);
    }
  }

  
  private void createZooKeeperCounter(String serverPortList) {
    
    context.getCounter(GiraphConstants.ZOOKEEPER_SERVER_PORT_COUNTER_GROUP,
        serverPortList);
  }

  
  public void setup(Path[] zkPathList)
    throws IOException, InterruptedException {
    context.setStatus("setup: Beginning worker setup.");
    Configuration hadoopConf = context.getConfiguration();
    conf = new ImmutableClassesGiraphConfiguration<I, V, E>(hadoopConf);
    initializeJobProgressTracker();
    
    
    
    conf.getGiraphTypes().writeIfUnset(conf);
    
    initializeAndConfigureLogging();
    
    setupAndInitializeGiraphMetrics();
    
    checkInput();
    
    ScriptLoader.loadScripts(conf);
    
    conf.createComputationFactory().initialize(conf);
    
    context.setStatus("setup: Initializing Zookeeper services.");
    String serverPortList = conf.getZookeeperList();
    if (serverPortList.isEmpty()) {
      if (startZooKeeperManager()) {
        return; 
      }
    } else {
      createZooKeeperCounter(serverPortList);
    }
    if (zkManager != null && zkManager.runsZooKeeper()) {
      if (LOG.isInfoEnabled()) {
        LOG.info("setup: Chosen to run ZooKeeper...");
      }
    }
    context
        .setStatus("setup: Connected to Zookeeper service " + serverPortList);
    this.graphFunctions = determineGraphFunctions(conf, zkManager);
    
    if (conf.getZooKeeperServerCount() > 1) {
      Thread.sleep(GiraphConstants.DEFAULT_ZOOKEEPER_INIT_LIMIT *
        GiraphConstants.DEFAULT_ZOOKEEPER_TICK_TIME);
    }
    try {
      instantiateBspService();
    } catch (IOException e) {
      LOG.error("setup: Caught exception just before end of setup", e);
      if (zkManager != null) {
        zkManager.offlineZooKeeperServers(ZooKeeperManager.State.FAILED);
      }
      throw new RuntimeException(
        "setup: Offlining servers due to exception...", e);
    }
    context.setStatus(getGraphFunctions().toString() + " starting...");
  }

  
  private void initializeJobProgressTracker() {
    if (!conf.trackJobProgressOnClient()) {
      jobProgressTracker = new JobProgressTrackerClientNoOp();
    } else {
      try {
        jobProgressTracker = new RetryableJobProgressTrackerClient(conf);
      } catch (InterruptedException | ExecutionException e) {
        LOG.warn("createJobProgressClient: Exception occurred while trying to" +
            " connect to JobProgressTracker - not reporting progress", e);
        jobProgressTracker = new JobProgressTrackerClientNoOp();
      }
    }
    jobProgressTracker.mapperStarted();
  }

  
  public void execute() throws IOException, InterruptedException {
    if (checkTaskState()) {
      return;
    }
    preLoadOnWorkerObservers();
    finishedSuperstepStats = serviceWorker.setup();
    if (collectInputSuperstepStats(finishedSuperstepStats)) {
      return;
    }
    prepareGraphStateAndWorkerContext();
    List<PartitionStats> partitionStatsList = new ArrayList<PartitionStats>();
    int numComputeThreads = conf.getNumComputeThreads();

    
    while (!finishedSuperstepStats.allVerticesHalted()) {
      final long superstep = serviceWorker.getSuperstep();
      GiraphTimerContext superstepTimerContext =
        getTimerForThisSuperstep(superstep);
      GraphState graphState = new GraphState(superstep,
          finishedSuperstepStats.getVertexCount(),
          finishedSuperstepStats.getEdgeCount(),
          context);
      Collection<? extends PartitionOwner> masterAssignedPartitionOwners =
        serviceWorker.startSuperstep();
      if (LOG.isDebugEnabled()) {
        LOG.debug("execute: " + MemoryUtils.getRuntimeMemoryStats());
      }
      context.progress();
      serviceWorker.exchangeVertexPartitions(masterAssignedPartitionOwners);
      context.progress();
      boolean hasBeenRestarted = checkSuperstepRestarted(superstep);

      GlobalStats globalStats = serviceWorker.getGlobalStats();

      if (hasBeenRestarted) {
        graphState = new GraphState(superstep,
            finishedSuperstepStats.getVertexCount(),
            finishedSuperstepStats.getEdgeCount(),
            context);
      } else if (storeCheckpoint(globalStats.getCheckpointStatus())) {
        break;
      }
      serviceWorker.getServerData().prepareResolveMutations();
      context.progress();
      prepareForSuperstep(graphState);
      context.progress();
      MessageStore<I, Writable> messageStore =
        serviceWorker.getServerData().getCurrentMessageStore();
      int numPartitions = serviceWorker.getPartitionStore().getNumPartitions();
      int numThreads = Math.min(numComputeThreads, numPartitions);
      if (LOG.isInfoEnabled()) {
        LOG.info("execute: " + numPartitions + " partitions to process with " +
          numThreads + " compute thread(s), originally " +
          numComputeThreads + " thread(s) on superstep " + superstep);
      }
      partitionStatsList.clear();
      
      if (numPartitions > 0) {
        processGraphPartitions(context, partitionStatsList, graphState,
          messageStore, numThreads);
      }
      finishedSuperstepStats = completeSuperstepAndCollectStats(
        partitionStatsList, superstepTimerContext);

      
    }

    if (LOG.isInfoEnabled()) {
      LOG.info("execute: BSP application done (global vertices marked done)");
    }
    updateSuperstepGraphState();
    postApplication();
  }

  
  private void postApplication() throws IOException, InterruptedException {
    GiraphTimerContext postAppTimerContext = wcPostAppTimer.time();
    serviceWorker.getWorkerContext().postApplication();
    serviceWorker.getSuperstepOutput().postApplication();
    postAppTimerContext.stop();
    context.progress();

    for (WorkerObserver obs : serviceWorker.getWorkerObservers()) {
      obs.postApplication();
      context.progress();
    }
  }

  
  public void setIsMaster(final boolean im) {
    this.isMaster = im;
  }

  
  public boolean isMaster() {
    return isMaster;
  }

  
  private GiraphTimerContext getTimerForThisSuperstep(long superstep) {
    GiraphMetrics.get().resetSuperstepMetrics(superstep);
    return superstepTimer.time();
  }

  
  private void setupAndInitializeGiraphMetrics() {
    GiraphMetrics.init(conf);
    GiraphMetrics.get().addSuperstepResetObserver(this);
    initJobMetrics();
    MemoryUtils.initMetrics();
    InputSplitsCallable.initMetrics();
  }

  
  private boolean startZooKeeperManager()
    throws IOException, InterruptedException {
    zkManager = new ZooKeeperManager(context, conf);
    context.setStatus("setup: Setting up Zookeeper manager.");
    zkManager.setup();
    if (zkManager.computationDone()) {
      done = true;
      return true;
    }
    zkManager.onlineZooKeeperServers();
    String serverPortList = zkManager.getZooKeeperServerPortString();
    conf.setZookeeperList(serverPortList);
    createZooKeeperCounter(serverPortList);
    return false;
  }

  
  private void updateSuperstepGraphState() {
    serviceWorker.getWorkerContext().setGraphState(
        new GraphState(serviceWorker.getSuperstep(),
            finishedSuperstepStats.getVertexCount(),
            finishedSuperstepStats.getEdgeCount(), context));
  }

  
  private FinishedSuperstepStats completeSuperstepAndCollectStats(
    List<PartitionStats> partitionStatsList,
    GiraphTimerContext superstepTimerContext) {

    
    
    
    finishedSuperstepStats =
      serviceWorker.finishSuperstep(partitionStatsList, superstepTimerContext);
    if (conf.metricsEnabled()) {
      GiraphMetrics.get().perSuperstep().printSummary(System.err);
    }
    return finishedSuperstepStats;
  }

  
  private void prepareForSuperstep(GraphState graphState) {
    serviceWorker.prepareSuperstep();

    serviceWorker.getWorkerContext().setGraphState(graphState);
    serviceWorker.getWorkerContext().setupSuperstep(serviceWorker);
    GiraphTimerContext preSuperstepTimer = wcPreSuperstepTimer.time();
    serviceWorker.getWorkerContext().preSuperstep();
    preSuperstepTimer.stop();
    context.progress();

    for (WorkerObserver obs : serviceWorker.getWorkerObservers()) {
      obs.preSuperstep(graphState.getSuperstep());
      context.progress();
    }
  }

  
  private void prepareGraphStateAndWorkerContext() {
    updateSuperstepGraphState();
    workerContextPreApp();
  }

  
  public GraphFunctions getGraphFunctions() {
    return graphFunctions;
  }

  public final WorkerContext getWorkerContext() {
    return serviceWorker.getWorkerContext();
  }

  public JobProgressTracker getJobProgressTracker() {
    return jobProgressTracker;
  }

  
  private static String findContainingJar(Class<?> myClass) {
    ClassLoader loader = myClass.getClassLoader();
    String classFile =
        myClass.getName().replaceAll("\\.", "/") + ".class";
    try {
      for (Enumeration<?> itr = loader.getResources(classFile);
          itr.hasMoreElements();) {
        URL url = (URL) itr.nextElement();
        if ("jar".equals(url.getProtocol())) {
          String toReturn = url.getPath();
          if (toReturn.startsWith("file:")) {
            toReturn = toReturn.substring("file:".length());
          }
          toReturn = URLDecoder.decode(toReturn, "UTF-8");
          return toReturn.replaceAll("!.*$", "");
        }
      }
    } catch (IOException e) {
      throw new RuntimeException(e);
    }
    return null;
  }

  
  private static GraphFunctions determineGraphFunctions(
      ImmutableClassesGiraphConfiguration conf,
      ZooKeeperManager zkManager) {
    boolean splitMasterWorker = conf.getSplitMasterWorker();
    int taskPartition = conf.getTaskPartition();
    boolean zkAlreadyProvided = conf.isZookeeperExternal();
    GraphFunctions functions = GraphFunctions.UNKNOWN;
    
    if (!splitMasterWorker) {
      if ((zkManager != null) && zkManager.runsZooKeeper()) {
        functions = GraphFunctions.ALL;
      } else {
        functions = GraphFunctions.ALL_EXCEPT_ZOOKEEPER;
      }
    } else {
      if (zkAlreadyProvided) {
        int masterCount = conf.getZooKeeperServerCount();
        if (taskPartition < masterCount) {
          functions = GraphFunctions.MASTER_ONLY;
        } else {
          functions = GraphFunctions.WORKER_ONLY;
        }
      } else {
        if ((zkManager != null) && zkManager.runsZooKeeper()) {
          functions = GraphFunctions.MASTER_ZOOKEEPER_ONLY;
        } else {
          functions = GraphFunctions.WORKER_ONLY;
        }
      }
    }
    return functions;
  }

  
  private void instantiateBspService()
    throws IOException, InterruptedException {
    if (graphFunctions.isMaster()) {
      if (LOG.isInfoEnabled()) {
        LOG.info("setup: Starting up BspServiceMaster " +
          "(master thread)...");
      }
      serviceMaster = new BspServiceMaster<I, V, E>(context, this);
      masterThread = new MasterThread<I, V, E>(serviceMaster, context);
      masterThread.start();
    }
    if (graphFunctions.isWorker()) {
      if (LOG.isInfoEnabled()) {
        LOG.info("setup: Starting up BspServiceWorker...");
      }
      serviceWorker = new BspServiceWorker<I, V, E>(context, this);
      if (LOG.isInfoEnabled()) {
        LOG.info("setup: Registering health of this worker...");
      }
    }
  }

  
  private void initializeAndConfigureLogging() {
    
    String logLevel = conf.getLocalLevel();
    if (!Logger.getRootLogger().getLevel().equals(Level.toLevel(logLevel))) {
      Logger.getRootLogger().setLevel(Level.toLevel(logLevel));
      if (LOG.isInfoEnabled()) {
        LOG.info("setup: Set log level to " + logLevel);
      }
    } else {
      if (LOG.isInfoEnabled()) {
        LOG.info("setup: Log level remains at " + logLevel);
      }
    }
    
    if (conf.useLogThreadLayout()) {
      PatternLayout layout =
        new PatternLayout("%-7p %d [%t] %c %x - %m%n");
      Enumeration<Appender> appenderEnum =
        Logger.getRootLogger().getAllAppenders();
      while (appenderEnum.hasMoreElements()) {
        appenderEnum.nextElement().setLayout(layout);
      }
    }
    
    
    if (conf.getLocalTestMode()) {
      LogManager.getLogger(org.apache.zookeeper.server.PrepRequestProcessor.
          class.getName()).setLevel(Level.ERROR);
    }
  }

  
  private void initJobMetrics() {
    GiraphMetricsRegistry jobMetrics = GiraphMetrics.get().perJobOptional();
    wcPreAppTimer = new GiraphTimer(jobMetrics, "worker-context-pre-app",
        TimeUnit.MILLISECONDS);
    wcPostAppTimer = new GiraphTimer(jobMetrics, "worker-context-post-app",
        TimeUnit.MILLISECONDS);
  }

  @Override
  public void newSuperstep(SuperstepMetricsRegistry superstepMetrics) {
    superstepTimer = new GiraphTimer(superstepMetrics,
        TIMER_SUPERSTEP_TIME, TimeUnit.MILLISECONDS);
    computeAll = new GiraphTimer(superstepMetrics,
        TIMER_COMPUTE_ALL, TimeUnit.MILLISECONDS);
    timeToFirstMessage = new GiraphTimer(superstepMetrics,
        TIMER_TIME_TO_FIRST_MSG, TimeUnit.MICROSECONDS);
    communicationTimer = new GiraphTimer(superstepMetrics,
        TIMER_COMMUNICATION_TIME, TimeUnit.MILLISECONDS);
    wcPreSuperstepTimer = new GiraphTimer(superstepMetrics,
        "worker-context-pre-superstep", TimeUnit.MILLISECONDS);
  }

  
  public void notifySentMessages() {
    
    
    GiraphTimerContext tmp = timeToFirstMessageTimerContext;
    if (tmp != null) {
      synchronized (timeToFirstMessage) {
        if (timeToFirstMessageTimerContext != null) {
          timeToFirstMessageTimerContext.stop();
          timeToFirstMessageTimerContext = null;
          communicationTimerContext = communicationTimer.time();
        }
      }
    }
  }

  
  public void notifyFinishedCommunication() {
    GiraphTimerContext tmp = communicationTimerContext;
    if (tmp != null) {
      synchronized (communicationTimer) {
        if (communicationTimerContext != null) {
          communicationTimerContext.stop();
          communicationTimerContext = null;
        }
      }
    }
  }

  
  private void processGraphPartitions(final Mapper<?, ?, ?, ?>.Context context,
      List<PartitionStats> partitionStatsList,
      final GraphState graphState,
      final MessageStore<I, Writable> messageStore,
      int numThreads) {
    PartitionStore<I, V, E> partitionStore = serviceWorker.getPartitionStore();
    long verticesToCompute = 0;
    for (Integer partitionId : partitionStore.getPartitionIds()) {
      verticesToCompute += partitionStore.getPartitionVertexCount(partitionId);
    }
    WorkerProgress.get().startSuperstep(
        serviceWorker.getSuperstep(), verticesToCompute,
        serviceWorker.getPartitionStore().getNumPartitions());
    partitionStore.startIteration();

    GiraphTimerContext computeAllTimerContext = computeAll.time();
    timeToFirstMessageTimerContext = timeToFirstMessage.time();

    CallableFactory<Collection<PartitionStats>> callableFactory =
      new CallableFactory<Collection<PartitionStats>>() {
        @Override
        public Callable<Collection<PartitionStats>> newCallable(
            int callableId) {
          return new ComputeCallable<I, V, E, Writable, Writable>(
              context,
              graphState,
              messageStore,
              conf,
              serviceWorker);
        }
      };
    List<Collection<PartitionStats>> results =
        ProgressableUtils.getResultsWithNCallables(callableFactory, numThreads,
            "compute-%d", context);

    for (Collection<PartitionStats> result : results) {
      partitionStatsList.addAll(result);
    }

    computeAllTimerContext.stop();
  }

  
  private boolean checkSuperstepRestarted(long superstep) throws IOException {
    
    
    if (serviceWorker.getRestartedSuperstep() == superstep) {
      if (LOG.isInfoEnabled()) {
        LOG.info("execute: Loading from checkpoint " + superstep);
      }
      VertexEdgeCount vertexEdgeCount = serviceWorker.loadCheckpoint(
        serviceWorker.getRestartedSuperstep());
      finishedSuperstepStats = new FinishedSuperstepStats(0, false,
          vertexEdgeCount.getVertexCount(), vertexEdgeCount.getEdgeCount(),
          false, CheckpointStatus.NONE);
      return true;
    }
    return false;
  }

  
  private boolean storeCheckpoint(CheckpointStatus checkpointStatus)
    throws IOException {
    if (checkpointStatus != CheckpointStatus.NONE) {
      serviceWorker.storeCheckpoint();
    }
    return checkpointStatus == CheckpointStatus.CHECKPOINT_AND_HALT;
  }

  
  private boolean collectInputSuperstepStats(
    FinishedSuperstepStats inputSuperstepStats) {
    if (inputSuperstepStats.getVertexCount() == 0 &&
        !inputSuperstepStats.mustLoadCheckpoint()) {
      LOG.warn("map: No vertices in the graph, exiting.");
      return true;
    }
    if (conf.metricsEnabled()) {
      GiraphMetrics.get().perSuperstep().printSummary(System.err);
    }
    return false;
  }

  
  private boolean checkTaskState() {
    if (done) {
      return true;
    }
    GiraphMetrics.get().resetSuperstepMetrics(BspService.INPUT_SUPERSTEP);
    if (graphFunctions.isNotAWorker()) {
      if (LOG.isInfoEnabled()) {
        LOG.info("map: No need to do anything when not a worker");
      }
      return true;
    }
    if (alreadyRun) {
      throw new RuntimeException("map: In BSP, map should have only been" +
        " run exactly once, (already run)");
    }
    alreadyRun = true;
    return false;
  }

  
  private void workerContextPreApp() {
    GiraphTimerContext preAppTimerContext = wcPreAppTimer.time();
    try {
      serviceWorker.getWorkerContext().preApplication();
    } catch (InstantiationException e) {
      LOG.fatal("execute: preApplication failed in instantiation", e);
      throw new RuntimeException(
          "execute: preApplication failed in instantiation", e);
    } catch (IllegalAccessException e) {
      LOG.fatal("execute: preApplication failed in access", e);
      throw new RuntimeException(
          "execute: preApplication failed in access", e);
    }
    preAppTimerContext.stop();
    context.progress();

    for (WorkerObserver obs : serviceWorker.getWorkerObservers()) {
      obs.preApplication();
      context.progress();
    }
  }

  
  private void preLoadOnWorkerObservers() {
    for (WorkerObserver obs : serviceWorker.getWorkerObservers()) {
      obs.preLoad();
      context.progress();
    }
  }

  
  private void postSaveOnWorkerObservers() {
    for (WorkerObserver obs : serviceWorker.getWorkerObservers()) {
      obs.postSave();
      context.progress();
    }
  }

  
  public void cleanup()
    throws IOException, InterruptedException {
    if (LOG.isInfoEnabled()) {
      LOG.info("cleanup: Starting for " + getGraphFunctions());
    }
    jobProgressTracker.cleanup();
    if (done) {
      return;
    }

    if (serviceWorker != null) {
      serviceWorker.cleanup(finishedSuperstepStats);
      postSaveOnWorkerObservers();
    }
    try {
      if (masterThread != null) {
        masterThread.join();
        LOG.info("cleanup: Joined with master thread");
      }
    } catch (InterruptedException e) {
      
      LOG.error("cleanup: Master thread couldn't join");
    }
    if (zkManager != null) {
      LOG.info("cleanup: Offlining ZooKeeper servers");
      try {
        zkManager.offlineZooKeeperServers(ZooKeeperManager.State.FINISHED);
      
      
      
      
      
      
      
      } catch (Throwable e) {
      
        LOG.error("cleanup: Error offlining zookeeper", e);
      }
    }

    
    GiraphMetrics.get().shutdown();
  }

  
  public void zooKeeperCleanup() {
    if (graphFunctions.isZooKeeper()) {
      
      if (zkManager != null) {
        zkManager.cleanup();
      }
    }
  }

  
  public void workerFailureCleanup() {
    try {
      if (graphFunctions.isWorker()) {
        serviceWorker.failureCleanup();
      }
      
      GiraphMetrics.get().shutdown();
    
    
    
    } catch (RuntimeException e1) {
    
      LOG.error("run: Worker failure failed on another RuntimeException, " +
          "original expection will be rethrown", e1);
    }
  }

  
  public Thread.UncaughtExceptionHandler createUncaughtExceptionHandler() {
    return new OverrideExceptionHandler(
        CHECKER_IF_WORKER_SHOULD_FAIL_AFTER_EXCEPTION_CLASS.newInstance(
            getConf()));
  }

  public ImmutableClassesGiraphConfiguration<I, V, E> getConf() {
    return conf;
  }


  
  class OverrideExceptionHandler implements Thread.UncaughtExceptionHandler {
    
    private final CheckerIfWorkerShouldFailAfterException checker;

    
    public OverrideExceptionHandler(
        CheckerIfWorkerShouldFailAfterException checker) {
      this.checker = checker;
    }

    @Override
    public void uncaughtException(final Thread t, final Throwable e) {
      if (!checker.checkIfWorkerShouldFail(t, e)) {
        return;
      }
      try {
        LOG.fatal(
            "uncaughtException: OverrideExceptionHandler on thread " +
                t.getName() + ", msg = " +  e.getMessage() + ", exiting...", e);

        zooKeeperCleanup();
        workerFailureCleanup();
      } finally {
        System.exit(1);
      }
    }
  }

  
  public interface CheckerIfWorkerShouldFailAfterException {
    
    boolean checkIfWorkerShouldFail(Thread thread, Throwable exception);
  }

  
  public static class FailWithEveryExceptionOccurred
      implements CheckerIfWorkerShouldFailAfterException {
    @Override
    public boolean checkIfWorkerShouldFail(Thread thread, Throwable exception) {
      return true;
    }
  }
}

<code block>

package org.apache.giraph.graph;

import java.io.IOException;
import java.util.Collection;
import java.util.List;
import java.util.concurrent.Callable;

import org.apache.giraph.bsp.CentralizedServiceWorker;
import org.apache.giraph.comm.WorkerClientRequestProcessor;
import org.apache.giraph.comm.messages.MessageStore;
import org.apache.giraph.comm.netty.NettyWorkerClientRequestProcessor;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.giraph.io.SimpleVertexWriter;
import org.apache.giraph.metrics.GiraphMetrics;
import org.apache.giraph.metrics.MetricNames;
import org.apache.giraph.metrics.SuperstepMetricsRegistry;
import org.apache.giraph.partition.Partition;
import org.apache.giraph.partition.PartitionStats;
import org.apache.giraph.partition.PartitionStore;
import org.apache.giraph.time.SystemTime;
import org.apache.giraph.time.Time;
import org.apache.giraph.time.Times;
import org.apache.giraph.utils.MemoryUtils;
import org.apache.giraph.utils.TimedLogger;
import org.apache.giraph.utils.Trimmable;
import org.apache.giraph.worker.WorkerContext;
import org.apache.giraph.worker.WorkerProgress;
import org.apache.giraph.worker.WorkerThreadGlobalCommUsage;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.log4j.Logger;

import com.google.common.collect.Iterables;
import com.google.common.collect.Lists;
import com.yammer.metrics.core.Counter;
import com.yammer.metrics.core.Histogram;


public class ComputeCallable<I extends WritableComparable, V extends Writable,
    E extends Writable, M1 extends Writable, M2 extends Writable>
    implements Callable<Collection<PartitionStats>> {
  
  private static final Logger LOG  = Logger.getLogger(ComputeCallable.class);
  
  private static final Time TIME = SystemTime.get();
  
  private static final long VERTICES_TO_UPDATE_PROGRESS = 100000;
  
  private final Mapper<?, ?, ?, ?>.Context context;
  
  private final GraphState graphState;
  
  private final MessageStore<I, M1> messageStore;
  
  private final ImmutableClassesGiraphConfiguration<I, V, E> configuration;
  
  private final CentralizedServiceWorker<I, V, E> serviceWorker;
  
  private final TimedLogger timedLogger = new TimedLogger(30 * 1000, LOG);
  
  private SimpleVertexWriter<I, V, E> vertexWriter;
  
  private final long startNanos = TIME.getNanoseconds();

  
  
  private final Counter messagesSentCounter;
  
  private final Counter messageBytesSentCounter;
  
  private final Histogram histogramComputePerPartition;

  
  public ComputeCallable(Mapper<?, ?, ?, ?>.Context context,
      GraphState graphState, MessageStore<I, M1> messageStore,
      ImmutableClassesGiraphConfiguration<I, V, E> configuration,
      CentralizedServiceWorker<I, V, E> serviceWorker) {
    this.context = context;
    this.configuration = configuration;
    this.messageStore = messageStore;
    this.serviceWorker = serviceWorker;
    this.graphState = graphState;

    SuperstepMetricsRegistry metrics = GiraphMetrics.get().perSuperstep();
    messagesSentCounter = metrics.getCounter(MetricNames.MESSAGES_SENT);
    messageBytesSentCounter =
      metrics.getCounter(MetricNames.MESSAGE_BYTES_SENT);
    histogramComputePerPartition = metrics.getUniformHistogram(
        MetricNames.HISTOGRAM_COMPUTE_PER_PARTITION);
  }

  @Override
  public Collection<PartitionStats> call() {
    
    WorkerClientRequestProcessor<I, V, E> workerClientRequestProcessor =
        new NettyWorkerClientRequestProcessor<I, V, E>(
            context, configuration, serviceWorker,
            configuration.getOutgoingMessageEncodeAndStoreType().
              useOneMessageToManyIdsEncoding());
    WorkerThreadGlobalCommUsage aggregatorUsage =
        serviceWorker.getAggregatorHandler().newThreadAggregatorUsage();
    WorkerContext workerContext = serviceWorker.getWorkerContext();

    vertexWriter = serviceWorker.getSuperstepOutput().getVertexWriter();

    Computation<I, V, E, M1, M2> computation =
        (Computation<I, V, E, M1, M2>) configuration.createComputation();
    computation.initialize(graphState, workerClientRequestProcessor,
        serviceWorker.getGraphTaskManager(), aggregatorUsage, workerContext);
    computation.preSuperstep();

    List<PartitionStats> partitionStatsList = Lists.newArrayList();
    PartitionStore<I, V, E> partitionStore = serviceWorker.getPartitionStore();
    while (true) {
      long startTime = System.currentTimeMillis();
      Partition<I, V, E> partition = partitionStore.getNextPartition();
      if (partition == null) {
        break;
      }

      try {
        serviceWorker.getServerData().resolvePartitionMutation(partition);
        PartitionStats partitionStats =
            computePartition(computation, partition);
        partitionStatsList.add(partitionStats);
        long partitionMsgs = workerClientRequestProcessor.resetMessageCount();
        partitionStats.addMessagesSentCount(partitionMsgs);
        messagesSentCounter.inc(partitionMsgs);
        long partitionMsgBytes =
          workerClientRequestProcessor.resetMessageBytesCount();
        partitionStats.addMessageBytesSentCount(partitionMsgBytes);
        messageBytesSentCounter.inc(partitionMsgBytes);
        timedLogger.info("call: Completed " +
            partitionStatsList.size() + " partitions, " +
            partitionStore.getNumPartitions() + " remaining " +
            MemoryUtils.getRuntimeMemoryStats());
      } catch (IOException e) {
        throw new IllegalStateException("call: Caught unexpected IOException," +
            " failing.", e);
      } catch (InterruptedException e) {
        throw new IllegalStateException("call: Caught unexpected " +
            "InterruptedException, failing.", e);
      } finally {
        partitionStore.putPartition(partition);
      }
      histogramComputePerPartition.update(
          System.currentTimeMillis() - startTime);
    }

    computation.postSuperstep();

    
    serviceWorker.getSuperstepOutput().returnVertexWriter(vertexWriter);

    if (LOG.isInfoEnabled()) {
      float seconds = Times.getNanosSince(TIME, startNanos) /
          Time.NS_PER_SECOND_AS_FLOAT;
      LOG.info("call: Computation took " + seconds + " secs for "  +
          partitionStatsList.size() + " partitions on superstep " +
          graphState.getSuperstep() + ".  Flushing started");
    }
    try {
      workerClientRequestProcessor.flush();
      
      
      if (partitionStatsList.size() > 0) {
        long partitionMsgBytes =
          workerClientRequestProcessor.resetMessageBytesCount();
        partitionStatsList.get(partitionStatsList.size() - 1).
          addMessageBytesSentCount(partitionMsgBytes);
        messageBytesSentCounter.inc(partitionMsgBytes);
      }
      aggregatorUsage.finishThreadComputation();
    } catch (IOException e) {
      throw new IllegalStateException("call: Flushing failed.", e);
    }
    return partitionStatsList;
  }

  
  private PartitionStats computePartition(
      Computation<I, V, E, M1, M2> computation,
      Partition<I, V, E> partition) throws IOException, InterruptedException {
    PartitionStats partitionStats =
        new PartitionStats(partition.getId(), 0, 0, 0, 0, 0);
    long verticesComputedProgress = 0;
    
    synchronized (partition) {
      for (Vertex<I, V, E> vertex : partition) {
        Iterable<M1> messages = messageStore.getVertexMessages(vertex.getId());
        if (vertex.isHalted() && !Iterables.isEmpty(messages)) {
          vertex.wakeUp();
        }
        if (!vertex.isHalted()) {
          context.progress();
          computation.compute(vertex, messages);
          
          vertex.unwrapMutableEdges();
          
          if (vertex instanceof Trimmable) {
            ((Trimmable) vertex).trim();
          }
          
          vertexWriter.writeVertex(vertex);
          
          partition.saveVertex(vertex);
        }
        if (vertex.isHalted()) {
          partitionStats.incrFinishedVertexCount();
        }
        
        messageStore.clearVertexMessages(vertex.getId());

        
        partitionStats.incrVertexCount();
        partitionStats.addEdgeCount(vertex.getNumEdges());

        verticesComputedProgress++;
        if (verticesComputedProgress == VERTICES_TO_UPDATE_PROGRESS) {
          WorkerProgress.get().addVerticesComputed(verticesComputedProgress);
          verticesComputedProgress = 0;
        }
      }

      messageStore.clearPartition(partition.getId());
    }
    WorkerProgress.get().addVerticesComputed(verticesComputedProgress);
    WorkerProgress.get().incrementPartitionsComputed();
    return partitionStats;
  }
}


<code block>


package org.apache.giraph.edge;

import com.google.common.collect.MapMaker;
import org.apache.giraph.bsp.CentralizedServiceWorker;
import org.apache.giraph.conf.DefaultImmutableClassesGiraphConfigurable;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.giraph.graph.Vertex;
import org.apache.giraph.partition.Partition;
import org.apache.giraph.utils.CallableFactory;
import org.apache.giraph.utils.ProgressableUtils;
import org.apache.giraph.utils.Trimmable;
import org.apache.giraph.utils.VertexIdEdgeIterator;
import org.apache.giraph.utils.VertexIdEdges;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.util.Progressable;
import org.apache.log4j.Logger;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;
import java.util.Iterator;
import java.util.Map;
import java.util.concurrent.Callable;
import java.util.concurrent.ConcurrentMap;


public abstract class AbstractEdgeStore<I extends WritableComparable,
  V extends Writable, E extends Writable, K, Et>
  extends DefaultImmutableClassesGiraphConfigurable<I, V, E>
  implements EdgeStore<I, V, E> {
  
  private static final Logger LOG = Logger.getLogger(AbstractEdgeStore.class);
  
  protected CentralizedServiceWorker<I, V, E> service;
  
  protected ImmutableClassesGiraphConfiguration<I, V, E> configuration;
  
  protected Progressable progressable;
  
  protected ConcurrentMap<Integer, Map<K, OutEdges<I, E>>> transientEdges;
  
  protected boolean reuseEdgeObjects;
  
  protected boolean useInputOutEdges;

  
  public AbstractEdgeStore(
    CentralizedServiceWorker<I, V, E> service,
    ImmutableClassesGiraphConfiguration<I, V, E> configuration,
    Progressable progressable) {
    this.service = service;
    this.configuration = configuration;
    this.progressable = progressable;
    transientEdges = new MapMaker().concurrencyLevel(
      configuration.getNettyServerExecutionConcurrency()).makeMap();
    reuseEdgeObjects = configuration.reuseEdgeObjects();
    useInputOutEdges = configuration.useInputOutEdges();
  }

  
  protected abstract I getVertexId(Et entry, I representativeVertexId);

  
  protected abstract I createVertexId(Et entry);

  
  protected abstract Map<K, OutEdges<I, E>> getPartitionEdges(int partitionId);

  
  protected abstract OutEdges<I, E> getPartitionEdges(Et entry);

  
  protected abstract void writeVertexKey(K key, DataOutput output)
  throws IOException;

  
  protected abstract K readVertexKey(DataInput input) throws IOException;

  
  protected abstract Iterator<Et>
  getPartitionEdgesIterator(Map<K, OutEdges<I, E>> partitionEdges);

  @Override
  public boolean hasPartitionEdges(int partitionId) {
    return transientEdges.containsKey(partitionId);
  }

  @Override
  public void writePartitionEdgeStore(int partitionId, DataOutput output)
      throws IOException {
    Map<K, OutEdges<I, E>> edges = transientEdges.remove(partitionId);
    output.writeInt(edges.size());
    for (Map.Entry<K, OutEdges<I, E>> edge : edges.entrySet()) {
      writeVertexKey(edge.getKey(), output);
      edge.getValue().write(output);
    }
  }

  @Override
  public void readPartitionEdgeStore(int partitionId, DataInput input)
      throws IOException {
    if (transientEdges.containsKey(partitionId)) {
      throw new IllegalStateException("readPartitionEdgeStore: reading a " +
          "partition that is already there in the partition store " +
          "(impossible)");
    }
    Map<K, OutEdges<I, E>> partitionEdges = getPartitionEdges(partitionId);
    int numEntries = input.readInt();
    for (int i = 0; i < numEntries; ++i) {
      K vertexKey = readVertexKey(input);
      OutEdges<I, E> edges = configuration.createAndInitializeInputOutEdges();
      edges.readFields(input);
      partitionEdges.put(vertexKey, edges);
    }
  }

  
  protected abstract OutEdges<I, E> getVertexOutEdges(
    VertexIdEdgeIterator<I, E> vertexIdEdgeIterator,
    Map<K, OutEdges<I, E>> partitionEdgesIn);

  @Override
  public void addPartitionEdges(
    int partitionId, VertexIdEdges<I, E> edges) {
    Map<K, OutEdges<I, E>> partitionEdges = getPartitionEdges(partitionId);

    VertexIdEdgeIterator<I, E> vertexIdEdgeIterator =
        edges.getVertexIdEdgeIterator();
    while (vertexIdEdgeIterator.hasNext()) {
      vertexIdEdgeIterator.next();
      Edge<I, E> edge = reuseEdgeObjects ?
          vertexIdEdgeIterator.getCurrentEdge() :
          vertexIdEdgeIterator.releaseCurrentEdge();
      OutEdges<I, E> outEdges = getVertexOutEdges(vertexIdEdgeIterator,
          partitionEdges);
      synchronized (outEdges) {
        outEdges.add(edge);
      }
    }
  }

  
  private OutEdges<I, E> convertInputToComputeEdges(
    OutEdges<I, E> inputEdges) {
    if (!useInputOutEdges) {
      return inputEdges;
    } else {
      return configuration.createAndInitializeOutEdges(inputEdges);
    }
  }

  @Override
  public void moveEdgesToVertices() {
    final boolean createSourceVertex = configuration.getCreateSourceVertex();
    if (transientEdges.isEmpty()) {
      if (LOG.isInfoEnabled()) {
        LOG.info("moveEdgesToVertices: No edges to move");
      }
      return;
    }

    if (LOG.isInfoEnabled()) {
      LOG.info("moveEdgesToVertices: Moving incoming edges to vertices.");
    }

    service.getPartitionStore().startIteration();
    int numThreads = configuration.getNumInputSplitsThreads();

    CallableFactory<Void> callableFactory = new CallableFactory<Void>() {
      @Override
      public Callable<Void> newCallable(int callableId) {
        return new Callable<Void>() {
          @Override
          public Void call() throws Exception {
            Integer partitionId;
            I representativeVertexId = configuration.createVertexId();
            while (true) {
              Partition<I, V, E> partition =
                  service.getPartitionStore().getNextPartition();
              if (partition == null) {
                break;
              }
              Map<K, OutEdges<I, E>> partitionEdges =
                  transientEdges.remove(partition.getId());
              if (partitionEdges == null) {
                service.getPartitionStore().putPartition(partition);
                continue;
              }

              Iterator<Et> iterator =
                  getPartitionEdgesIterator(partitionEdges);
              
              while (iterator.hasNext()) {
                Et entry = iterator.next();
                I vertexId = getVertexId(entry, representativeVertexId);
                OutEdges<I, E> outEdges = convertInputToComputeEdges(
                  getPartitionEdges(entry));
                Vertex<I, V, E> vertex = partition.getVertex(vertexId);
                
                
                if (vertex == null) {
                  if (createSourceVertex) {
                    
                    vertex = configuration.createVertex();
                    vertex.initialize(createVertexId(entry),
                        configuration.createVertexValue(), outEdges);
                    partition.putVertex(vertex);
                  }
                } else {
                  
                  
                  if (vertex.getNumEdges() == 0) {
                    vertex.setEdges(outEdges);
                  } else {
                    for (Edge<I, E> edge : outEdges) {
                      vertex.addEdge(edge);
                    }
                  }
                  if (vertex instanceof Trimmable) {
                    ((Trimmable) vertex).trim();
                  }
                  
                  
                  partition.saveVertex(vertex);
                }
                iterator.remove();
              }
              
              
              
              service.getPartitionStore().putPartition(partition);
            }
            return null;
          }
        };
      }
    };
    ProgressableUtils.getResultsWithNCallables(callableFactory, numThreads,
        "move-edges-%d", progressable);

    
    transientEdges.clear();

    if (LOG.isInfoEnabled()) {
      LOG.info("moveEdgesToVertices: Finished moving incoming edges to " +
          "vertices.");
    }
  }
}

<code block>


package org.apache.giraph.edge;

import org.apache.giraph.utils.VertexIdEdges;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;


public interface EdgeStore<I extends WritableComparable,
   V extends Writable, E extends Writable> {
  
  void addPartitionEdges(int partitionId, VertexIdEdges<I, E> edges);

  
  void moveEdgesToVertices();

  
  boolean hasPartitionEdges(int partitionId);

  
  void writePartitionEdgeStore(int partitionId, DataOutput output)
      throws IOException;

  
  void readPartitionEdgeStore(int partitionId, DataInput input)
      throws IOException;
}

<code block>


package org.apache.giraph.edge;

import org.apache.giraph.bsp.CentralizedServiceWorker;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.giraph.utils.VertexIdEdgeIterator;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.util.Progressable;

import com.google.common.collect.MapMaker;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;
import java.util.Iterator;
import java.util.Map;
import java.util.concurrent.ConcurrentMap;


public class SimpleEdgeStore<I extends WritableComparable,
  V extends Writable, E extends Writable>
  extends AbstractEdgeStore<I, V, E, I,
  Map.Entry<I, OutEdges<I, E>>> {

  
  public SimpleEdgeStore(
    CentralizedServiceWorker<I, V, E> service,
    ImmutableClassesGiraphConfiguration<I, V, E> configuration,
    Progressable progressable) {
    super(service, configuration, progressable);
  }

  @Override
  protected I getVertexId(Map.Entry<I, OutEdges<I, E>> entry,
    I representativeVertexId) {
    return entry.getKey();
  }

  @Override
  protected I createVertexId(Map.Entry<I, OutEdges<I, E>> entry) {
    return entry.getKey();
  }

  @Override
  protected ConcurrentMap<I, OutEdges<I, E>> getPartitionEdges(
    int partitionId) {
    ConcurrentMap<I, OutEdges<I, E>> partitionEdges =
        (ConcurrentMap<I, OutEdges<I, E>>) transientEdges.get(partitionId);
    if (partitionEdges == null) {
      ConcurrentMap<I, OutEdges<I, E>> newPartitionEdges =
          new MapMaker().concurrencyLevel(
              configuration.getNettyServerExecutionConcurrency()).makeMap();
      partitionEdges = (ConcurrentMap<I, OutEdges<I, E>>)
          transientEdges.putIfAbsent(partitionId, newPartitionEdges);
      if (partitionEdges == null) {
        partitionEdges = newPartitionEdges;
      }
    }
    return partitionEdges;
  }

  @Override
  protected OutEdges<I, E> getPartitionEdges(
    Map.Entry<I, OutEdges<I, E>> entry) {
    return entry.getValue();
  }

  @Override
  protected void writeVertexKey(I key, DataOutput output) throws IOException {
    key.write(output);
  }

  @Override
  protected I readVertexKey(DataInput input) throws IOException {
    I id = configuration.createVertexId();
    id.readFields(input);
    return id;
  }

  @Override
  protected Iterator<Map.Entry<I, OutEdges<I, E>>>
  getPartitionEdgesIterator(Map<I, OutEdges<I, E>> partitionEdges) {
    return partitionEdges.entrySet().iterator();
  }

  @Override
  protected OutEdges<I, E> getVertexOutEdges(
      VertexIdEdgeIterator<I, E> vertexIdEdgeIterator,
      Map<I, OutEdges<I, E>> partitionEdgesIn) {
    ConcurrentMap<I, OutEdges<I, E>> partitionEdges =
        (ConcurrentMap<I, OutEdges<I, E>>) partitionEdgesIn;
    I vertexId = vertexIdEdgeIterator.getCurrentVertexId();
    OutEdges<I, E> outEdges = partitionEdges.get(vertexId);
    if (outEdges == null) {
      OutEdges<I, E> newOutEdges =
          configuration.createAndInitializeInputOutEdges();
      outEdges = partitionEdges.putIfAbsent(vertexId, newOutEdges);
      if (outEdges == null) {
        outEdges = newOutEdges;
        
        
        vertexIdEdgeIterator.releaseCurrentVertexId();
      }
    }
    return outEdges;
  }
}

<code block>


package org.apache.giraph.edge.primitives;

import org.apache.giraph.bsp.CentralizedServiceWorker;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.giraph.edge.AbstractEdgeStore;
import org.apache.giraph.edge.OutEdges;
import org.apache.giraph.utils.VertexIdEdgeIterator;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.util.Progressable;

import it.unimi.dsi.fastutil.longs.Long2ObjectMap;
import it.unimi.dsi.fastutil.longs.Long2ObjectMaps;
import it.unimi.dsi.fastutil.longs.Long2ObjectOpenHashMap;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;
import java.util.Iterator;
import java.util.Map;


public class LongEdgeStore<V extends Writable, E extends Writable>
  extends AbstractEdgeStore<LongWritable, V, E, Long,
  Long2ObjectMap.Entry<OutEdges<LongWritable, E>>> {

  
  public LongEdgeStore(
    CentralizedServiceWorker<LongWritable, V, E> service,
    ImmutableClassesGiraphConfiguration<LongWritable, V, E> configuration,
    Progressable progressable) {
    super(service, configuration, progressable);
  }

  @Override
  protected LongWritable getVertexId(
    Long2ObjectMap.Entry<OutEdges<LongWritable, E>> entry,
    LongWritable representativeVertexId) {
    representativeVertexId.set(entry.getLongKey());
    return representativeVertexId;
  }

  @Override
  protected LongWritable createVertexId(
    Long2ObjectMap.Entry<OutEdges<LongWritable, E>> entry) {
    return new LongWritable(entry.getLongKey());
  }


  @Override
  protected OutEdges<LongWritable, E> getPartitionEdges(
    Long2ObjectMap.Entry<OutEdges<LongWritable, E>> entry) {
    return entry.getValue();
  }

  @Override
  protected void writeVertexKey(Long key, DataOutput output)
      throws IOException {
    output.writeLong(key);
  }

  @Override
  protected Long readVertexKey(DataInput input) throws IOException {
    return input.readLong();
  }

  @Override
  protected Iterator<Long2ObjectMap.Entry<OutEdges<LongWritable, E>>>
  getPartitionEdgesIterator(
      Map<Long, OutEdges<LongWritable, E>> partitionEdges) {
    return ((Long2ObjectMap<OutEdges<LongWritable, E>>) partitionEdges)
        .long2ObjectEntrySet()
        .iterator();
  }

  @Override
  protected Long2ObjectMap<OutEdges<LongWritable, E>> getPartitionEdges(
    int partitionId) {
    Long2ObjectMap<OutEdges<LongWritable, E>> partitionEdges =
      (Long2ObjectMap<OutEdges<LongWritable, E>>)
        transientEdges.get(partitionId);
    if (partitionEdges == null) {
      Long2ObjectMap<OutEdges<LongWritable, E>> newPartitionEdges =
          Long2ObjectMaps.synchronize(
              new Long2ObjectOpenHashMap<OutEdges<LongWritable, E>>());
      partitionEdges = (Long2ObjectMap<OutEdges<LongWritable, E>>)
          transientEdges.putIfAbsent(partitionId,
          newPartitionEdges);
      if (partitionEdges == null) {
        partitionEdges = newPartitionEdges;
      }
    }
    return partitionEdges;
  }

  @Override
  protected OutEdges<LongWritable, E> getVertexOutEdges(
    VertexIdEdgeIterator<LongWritable, E> vertexIdEdgeIterator,
    Map<Long, OutEdges<LongWritable, E>> partitionEdgesIn) {
    Long2ObjectMap<OutEdges<LongWritable, E>> partitionEdges =
        (Long2ObjectMap<OutEdges<LongWritable, E>>) partitionEdgesIn;
    LongWritable vertexId = vertexIdEdgeIterator.getCurrentVertexId();
    OutEdges<LongWritable, E> outEdges = partitionEdges.get(vertexId.get());
    if (outEdges == null) {
      synchronized (partitionEdges) {
        outEdges = partitionEdges.get(vertexId.get());
        if (outEdges == null) {
          outEdges = configuration.createAndInitializeInputOutEdges();
          partitionEdges.put(vertexId.get(), outEdges);
        }
      }
    }
    return outEdges;
  }
}

<code block>


package org.apache.giraph.edge.primitives;

import org.apache.giraph.bsp.CentralizedServiceWorker;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.giraph.edge.AbstractEdgeStore;
import org.apache.giraph.edge.OutEdges;
import org.apache.giraph.utils.VertexIdEdgeIterator;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.util.Progressable;

import it.unimi.dsi.fastutil.ints.Int2ObjectMaps;
import it.unimi.dsi.fastutil.ints.Int2ObjectOpenHashMap;
import it.unimi.dsi.fastutil.ints.Int2ObjectMap;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;
import java.util.Iterator;
import java.util.Map;


public class IntEdgeStore<V extends Writable, E extends Writable>
  extends AbstractEdgeStore<IntWritable, V, E, Integer,
  Int2ObjectMap.Entry<OutEdges<IntWritable, E>>> {

  
  public IntEdgeStore(
      CentralizedServiceWorker<IntWritable, V, E> service,
      ImmutableClassesGiraphConfiguration<IntWritable, V, E> configuration,
      Progressable progressable) {
    super(service, configuration, progressable);
  }

  @Override
  protected IntWritable getVertexId(
    Int2ObjectMap.Entry<OutEdges<IntWritable, E>> entry,
    IntWritable representativeVertexId) {
    representativeVertexId.set(entry.getIntKey());
    return representativeVertexId;
  }

  @Override
  protected IntWritable createVertexId(
    Int2ObjectMap.Entry<OutEdges<IntWritable, E>> entry) {
    return new IntWritable(entry.getIntKey());
  }

  @Override
  protected OutEdges<IntWritable, E> getPartitionEdges(
    Int2ObjectMap.Entry<OutEdges<IntWritable, E>> entry) {
    return entry.getValue();
  }

  @Override
  protected void writeVertexKey(Integer key, DataOutput output)
      throws IOException {
    output.writeInt(key);
  }

  @Override
  protected Integer readVertexKey(DataInput input)
      throws IOException {
    return input.readInt();
  }

  @Override
  protected Iterator<Int2ObjectMap.Entry<OutEdges<IntWritable, E>>>
  getPartitionEdgesIterator(
    Map<Integer, OutEdges<IntWritable, E>> partitionEdges) {
    return  ((Int2ObjectMap<OutEdges<IntWritable, E>>) partitionEdges)
        .int2ObjectEntrySet()
        .iterator();
  }

  @Override
  protected Int2ObjectMap<OutEdges<IntWritable, E>> getPartitionEdges(
      int partitionId) {
    Int2ObjectMap<OutEdges<IntWritable, E>> partitionEdges =
        (Int2ObjectMap<OutEdges<IntWritable, E>>)
            transientEdges.get(partitionId);
    if (partitionEdges == null) {
      Int2ObjectMap<OutEdges<IntWritable, E>> newPartitionEdges =
          Int2ObjectMaps.synchronize(
              new Int2ObjectOpenHashMap<OutEdges<IntWritable, E>>());
      partitionEdges = (Int2ObjectMap<OutEdges<IntWritable, E>>)
          transientEdges.putIfAbsent(partitionId,
              newPartitionEdges);
      if (partitionEdges == null) {
        partitionEdges = newPartitionEdges;
      }
    }
    return partitionEdges;
  }

  @Override
  protected OutEdges<IntWritable, E> getVertexOutEdges(
      VertexIdEdgeIterator<IntWritable, E> vertexIdEdgeIterator,
      Map<Integer, OutEdges<IntWritable, E>> partitionEdgesIn) {
    Int2ObjectMap<OutEdges<IntWritable, E>> partitionEdges =
        (Int2ObjectMap<OutEdges<IntWritable, E>>) partitionEdgesIn;
    IntWritable vertexId = vertexIdEdgeIterator.getCurrentVertexId();
    OutEdges<IntWritable, E> outEdges = partitionEdges.get(vertexId.get());
    if (outEdges == null) {
      synchronized (partitionEdges) {
        outEdges = partitionEdges.get(vertexId.get());
        if (outEdges == null) {
          outEdges = configuration.createAndInitializeInputOutEdges();
          partitionEdges.put(vertexId.get(), outEdges);
        }
      }
    }
    return outEdges;
  }
}

<code block>


package org.apache.giraph.comm;

import java.io.IOException;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
import java.util.Map;
import java.util.concurrent.ConcurrentMap;

import com.google.common.collect.Iterables;
import com.google.common.collect.Maps;

import org.apache.giraph.bsp.CentralizedServiceWorker;
import org.apache.giraph.comm.aggregators.AllAggregatorServerData;
import org.apache.giraph.comm.aggregators.OwnerAggregatorServerData;
import org.apache.giraph.comm.messages.MessageStore;
import org.apache.giraph.comm.messages.MessageStoreFactory;
import org.apache.giraph.comm.messages.queue.AsyncMessageStoreWrapper;
import org.apache.giraph.conf.GiraphConstants;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.giraph.graph.Vertex;
import org.apache.giraph.graph.VertexMutations;
import org.apache.giraph.graph.VertexResolver;
import org.apache.giraph.ooc.DiskBackedPartitionStore;
import org.apache.giraph.partition.Partition;
import org.apache.giraph.partition.PartitionStore;
import org.apache.giraph.partition.SimplePartitionStore;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.log4j.Logger;


@SuppressWarnings("rawtypes")
public class ServerData<I extends WritableComparable,
    V extends Writable, E extends Writable> {
  
  private static final Logger LOG =
      Logger.getLogger(ServerData.class);
  
  private final ImmutableClassesGiraphConfiguration<I, V, E> conf;
  
  private volatile PartitionStore<I, V, E> partitionStore;
  
  private final MessageStoreFactory<I, Writable, MessageStore<I, Writable>>
  messageStoreFactory;
  
  private volatile MessageStore<I, Writable> incomingMessageStore;
  
  private volatile MessageStore<I, Writable> currentMessageStore;
  
  private ConcurrentMap<Integer,
      ConcurrentMap<I, VertexMutations<I, V, E>>>
      oldPartitionMutations = Maps.newConcurrentMap();
  
  private ConcurrentMap<Integer,
      ConcurrentMap<I, VertexMutations<I, V, E>>>
      partitionMutations = Maps.newConcurrentMap();
  
  private final OwnerAggregatorServerData ownerAggregatorData;
  
  private final AllAggregatorServerData allAggregatorData;
  
  private final CentralizedServiceWorker<I, V, E> serviceWorker;

  
  private volatile List<Writable> currentWorkerToWorkerMessages =
      Collections.synchronizedList(new ArrayList<Writable>());
  
  private volatile List<Writable> incomingWorkerToWorkerMessages =
      Collections.synchronizedList(new ArrayList<Writable>());

  
  private final Mapper<?, ?, ?, ?>.Context context;

  
  public ServerData(
      CentralizedServiceWorker<I, V, E> service,
      ImmutableClassesGiraphConfiguration<I, V, E> conf,
      MessageStoreFactory<I, Writable, MessageStore<I, Writable>>
          messageStoreFactory,
      Mapper<?, ?, ?, ?>.Context context) {
    this.serviceWorker = service;
    this.conf = conf;
    this.messageStoreFactory = messageStoreFactory;
    if (GiraphConstants.USE_OUT_OF_CORE_GRAPH.get(conf)) {
      partitionStore =
          new DiskBackedPartitionStore<I, V, E>(conf, context,
              getServiceWorker());
    } else {
      partitionStore =
          new SimplePartitionStore<I, V, E>(conf, context, getServiceWorker());
    }
    ownerAggregatorData = new OwnerAggregatorServerData(context);
    allAggregatorData = new AllAggregatorServerData(context, conf);
    this.context = context;
  }

  
  public PartitionStore<I, V, E> getPartitionStore() {
    return partitionStore;
  }

  
  public <M extends Writable> MessageStore<I, M> getIncomingMessageStore() {
    return (MessageStore<I, M>) incomingMessageStore;
  }

  
  public <M extends Writable> MessageStore<I, M> getCurrentMessageStore() {
    return (MessageStore<I, M>) currentMessageStore;
  }

  
  public void resetMessageStores() throws IOException {
    if (currentMessageStore != null) {
      currentMessageStore.clearAll();
      currentMessageStore = null;
    }
    if (incomingMessageStore != null) {
      incomingMessageStore.clearAll();
      incomingMessageStore = null;
    }
    prepareSuperstep();
  }

  
  public void prepareSuperstep() {
    if (currentMessageStore != null) {
      try {
        currentMessageStore.clearAll();
      } catch (IOException e) {
        throw new IllegalStateException(
            "Failed to clear previous message store");
      }
    }
    currentMessageStore =
        incomingMessageStore != null ? incomingMessageStore :
            messageStoreFactory.newStore(conf.getIncomingMessageClasses());
    incomingMessageStore =
        messageStoreFactory.newStore(conf.getOutgoingMessageClasses());
    
    currentMessageStore.finalizeStore();

    currentWorkerToWorkerMessages = incomingWorkerToWorkerMessages;
    incomingWorkerToWorkerMessages =
        Collections.synchronizedList(new ArrayList<Writable>());
  }

  
  public void waitForComplete() {
    if (incomingMessageStore instanceof AsyncMessageStoreWrapper) {
      ((AsyncMessageStoreWrapper) incomingMessageStore).waitToComplete();
    }
  }

  
  public ConcurrentMap<Integer, ConcurrentMap<I, VertexMutations<I, V, E>>>
  getPartitionMutations() {
    return partitionMutations;
  }

  
  public OwnerAggregatorServerData getOwnerAggregatorData() {
    return ownerAggregatorData;
  }

  
  public AllAggregatorServerData getAllAggregatorData() {
    return allAggregatorData;
  }

  
  public CentralizedServiceWorker<I, V, E> getServiceWorker() {
    return this.serviceWorker;
  }

  
  public List<Writable> getAndClearCurrentWorkerToWorkerMessages() {
    List<Writable> ret = currentWorkerToWorkerMessages;
    currentWorkerToWorkerMessages = null;
    return ret;
  }

  
  public void addIncomingWorkerToWorkerMessage(Writable message) {
    incomingWorkerToWorkerMessages.add(message);
  }


  
  public List<Writable> getCurrentWorkerToWorkerMessages() {
    return currentWorkerToWorkerMessages;
  }

  
  public void prepareResolveMutations() {
    oldPartitionMutations = partitionMutations;
    partitionMutations = Maps.newConcurrentMap();
  }

  
  public void resolvePartitionMutation(Partition<I, V, E> partition) {
    Integer partitionId = partition.getId();
    VertexResolver<I, V, E> vertexResolver = conf.createVertexResolver();
    ConcurrentMap<I, VertexMutations<I, V, E>> prevPartitionMutations =
        oldPartitionMutations.get(partitionId);

    
    if (prevPartitionMutations != null) {
      for (Map.Entry<I, VertexMutations<I, V, E>> entry : prevPartitionMutations
          .entrySet()) {
        I vertexId = entry.getKey();
        Vertex<I, V, E> originalVertex = partition.getVertex(vertexId);
        VertexMutations<I, V, E> vertexMutations = entry.getValue();
        Vertex<I, V, E> vertex = vertexResolver.resolve(vertexId,
            originalVertex, vertexMutations,
            getCurrentMessageStore().hasMessagesForVertex(entry.getKey()));

        if (LOG.isDebugEnabled()) {
          LOG.debug("resolvePartitionMutations: Resolved vertex index " +
              vertexId + " in partition index " + partitionId +
              " with original vertex " + originalVertex +
              ", returned vertex " + vertex + " on superstep " +
              serviceWorker.getSuperstep() + " with mutations " +
              vertexMutations);
        }

        if (vertex != null) {
          partition.putVertex(vertex);
        } else if (originalVertex != null) {
          partition.removeVertex(vertexId);
          try {
            getCurrentMessageStore().clearVertexMessages(vertexId);
          } catch (IOException e) {
            throw new IllegalStateException("resolvePartitionMutations: " +
                "Caught IOException while clearing messages for a deleted " +
                "vertex due to a mutation");
          }
        }
        context.progress();
      }
    }

    
    
    Iterable<I> destinations = getCurrentMessageStore().
        getPartitionDestinationVertices(partitionId);
    if (!Iterables.isEmpty(destinations)) {
      for (I vertexId : destinations) {
        if (partition.getVertex(vertexId) == null) {
          Vertex<I, V, E> vertex =
              vertexResolver.resolve(vertexId, null, null, true);

          if (LOG.isDebugEnabled()) {
            LOG.debug("resolvePartitionMutations: A non-existing vertex has " +
                "message(s). Added vertex index " + vertexId +
                " in partition index " + partitionId +
                ", vertex = " + vertex + ", on superstep " +
                serviceWorker.getSuperstep());
          }

          if (vertex != null) {
            partition.putVertex(vertex);
          }
          context.progress();
        }
      }
    }
  }
}

<code block>


package org.apache.giraph.comm.messages.primitives.long_id;

import it.unimi.dsi.fastutil.ints.Int2ObjectOpenHashMap;
import it.unimi.dsi.fastutil.longs.Long2ObjectOpenHashMap;
import org.apache.giraph.bsp.CentralizedServiceWorker;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.giraph.factories.MessageValueFactory;
import org.apache.giraph.graph.Vertex;
import org.apache.giraph.partition.Partition;
import org.apache.giraph.partition.PartitionOwner;
import org.apache.giraph.utils.VertexIdIterator;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Writable;

import java.util.List;


public abstract class LongAbstractListMessageStore<M extends Writable,
  L extends List> extends LongAbstractMessageStore<M, L> {
  
  private final
  Int2ObjectOpenHashMap<Long2ObjectOpenHashMap<L>> nascentMap;

  
  public LongAbstractListMessageStore(
      MessageValueFactory<M> messageValueFactory,
      CentralizedServiceWorker<LongWritable, Writable, Writable> service,
      ImmutableClassesGiraphConfiguration<LongWritable,
          Writable, Writable> config) {
    super(messageValueFactory, service, config);
    populateMap();

    
    nascentMap = new Int2ObjectOpenHashMap<>();
    for (int partitionId : service.getPartitionStore().getPartitionIds()) {
      nascentMap.put(partitionId, new Long2ObjectOpenHashMap<L>());
    }
  }

  
  private void populateMap() { 
    
    service.getPartitionStore().startIteration();
    while (true) {
      Partition partition = service.getPartitionStore().getNextPartition();
      if (partition == null) {
        break;
      }
      Long2ObjectOpenHashMap<L> partitionMap = map.get(partition.getId());
      for (Object obj : partition) {
        Vertex vertex = (Vertex) obj;
        LongWritable vertexId = (LongWritable) vertex.getId();
        partitionMap.put(vertexId.get(), createList());
      }
      service.getPartitionStore().putPartition(partition);
    }
  }

  
  protected abstract L createList();

  
  protected L getList(
    VertexIdIterator<LongWritable> iterator) {
    PartitionOwner owner =
        service.getVertexPartitionOwner(iterator.getCurrentVertexId());
    long vertexId = iterator.getCurrentVertexId().get();
    int partitionId = owner.getPartitionId();
    Long2ObjectOpenHashMap<L> partitionMap = map.get(partitionId);
    if (!partitionMap.containsKey(vertexId)) {
      synchronized (nascentMap) {
        
        
        Long2ObjectOpenHashMap<L> nascentPartitionMap =
          nascentMap.get(partitionId);
        if (nascentPartitionMap.get(vertexId) == null) {
          nascentPartitionMap.put(vertexId, createList());
        }
        return nascentPartitionMap.get(vertexId);
      }
    }
    return partitionMap.get(vertexId);
  }

  @Override
  public void finalizeStore() {
    for (int partitionId : nascentMap.keySet()) {
      
      map.get(partitionId).putAll(nascentMap.get(partitionId));
    }
    nascentMap.clear();
  }

  
  
}

<code block>


package org.apache.giraph.comm.requests;

import org.apache.giraph.comm.ServerData;
import org.apache.giraph.edge.Edge;
import org.apache.giraph.utils.ByteArrayVertexIdEdges;
import org.apache.giraph.utils.PairList;
import org.apache.giraph.utils.VertexIdEdges;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;


@SuppressWarnings("unchecked")
public class SendWorkerEdgesRequest<I extends WritableComparable,
    E extends Writable>
    extends SendWorkerDataRequest<I, Edge<I, E>,
    VertexIdEdges<I, E>> {
  
  public SendWorkerEdgesRequest() { }

  
  public SendWorkerEdgesRequest(
      PairList<Integer, VertexIdEdges<I, E>> partVertEdges) {
    this.partitionVertexData = partVertEdges;
  }

  @Override
  public VertexIdEdges<I, E> createVertexIdData() {
    return new ByteArrayVertexIdEdges<>();
  }

  @Override
  public RequestType getType() {
    return RequestType.SEND_WORKER_EDGES_REQUEST;
  }

  @Override
  public void doRequest(ServerData serverData) {
    PairList<Integer, VertexIdEdges<I, E>>.Iterator
        iterator = partitionVertexData.getIterator();
    while (iterator.hasNext()) {
      iterator.next();
      serverData.getPartitionStore()
          .addPartitionEdges(iterator.getCurrentFirst(),
              iterator.getCurrentSecond());
    }
  }
}

<code block>


package org.apache.giraph.comm.requests;

import org.apache.giraph.comm.ServerData;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.giraph.utils.ExtendedDataOutput;
import org.apache.giraph.utils.PairList;
import org.apache.giraph.utils.WritableUtils;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;
import org.apache.log4j.Logger;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;


@SuppressWarnings("rawtypes")
public class SendWorkerVerticesRequest<I extends WritableComparable,
    V extends Writable, E extends Writable> extends
    WritableRequest<I, V, E> implements WorkerRequest<I, V, E> {
  
  private static final Logger LOG =
      Logger.getLogger(SendWorkerVerticesRequest.class);
  
  private PairList<Integer, ExtendedDataOutput> workerPartitions;

  
  public SendWorkerVerticesRequest() { }

  
  public SendWorkerVerticesRequest(
      ImmutableClassesGiraphConfiguration<I, V, E> conf,
      PairList<Integer, ExtendedDataOutput> workerPartitions) {
    this.workerPartitions = workerPartitions;
    setConf(conf);
  }

  @Override
  public void readFieldsRequest(DataInput input) throws IOException {
    int numPartitions = input.readInt();
    workerPartitions = new PairList<Integer, ExtendedDataOutput>();
    workerPartitions.initialize(numPartitions);
    while (numPartitions-- > 0) {
      final int partitionId = input.readInt();
      ExtendedDataOutput partitionData =
          WritableUtils.readExtendedDataOutput(input, getConf());
      workerPartitions.add(partitionId, partitionData);
    }
  }

  @Override
  public void writeRequest(DataOutput output) throws IOException {
    output.writeInt(workerPartitions.getSize());
    PairList<Integer, ExtendedDataOutput>.Iterator
        iterator = workerPartitions.getIterator();
    while (iterator.hasNext()) {
      iterator.next();
      output.writeInt(iterator.getCurrentFirst());
      WritableUtils.writeExtendedDataOutput(
          iterator.getCurrentSecond(), output);
    }
  }

  @Override
  public RequestType getType() {
    return RequestType.SEND_WORKER_VERTICES_REQUEST;
  }

  @Override
  public void doRequest(ServerData<I, V, E> serverData) {
    PairList<Integer, ExtendedDataOutput>.Iterator
        iterator = workerPartitions.getIterator();
    while (iterator.hasNext()) {
      iterator.next();
      serverData.getPartitionStore()
          .addPartitionVertices(iterator.getCurrentFirst(),
              iterator.getCurrentSecond());
    }
  }

  @Override
  public int getSerializedSize() {
    
    int size = super.getSerializedSize() + 4;
    PairList<Integer, ExtendedDataOutput>.Iterator iterator =
        workerPartitions.getIterator();
    while (iterator.hasNext()) {
      iterator.next();
      
      size += 8 + iterator.getCurrentSecond().getPos();
    }
    return size;
  }
}


<code block>


package org.apache.giraph.worker;

import java.io.DataInputStream;
import java.io.DataOutputStream;
import java.io.IOException;
import java.nio.charset.Charset;
import java.util.ArrayList;
import java.util.Collection;
import java.util.Collections;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Queue;
import java.util.Set;
import java.util.concurrent.Callable;
import java.util.concurrent.ConcurrentLinkedQueue;
import java.util.concurrent.TimeUnit;

import net.iharder.Base64;

import org.apache.giraph.bsp.ApplicationState;
import org.apache.giraph.bsp.BspService;
import org.apache.giraph.bsp.CentralizedServiceWorker;
import org.apache.giraph.bsp.checkpoints.CheckpointStatus;
import org.apache.giraph.comm.ServerData;
import org.apache.giraph.comm.WorkerClient;
import org.apache.giraph.comm.WorkerClientRequestProcessor;
import org.apache.giraph.comm.WorkerServer;
import org.apache.giraph.comm.aggregators.WorkerAggregatorRequestProcessor;
import org.apache.giraph.comm.messages.MessageStore;
import org.apache.giraph.comm.messages.queue.AsyncMessageStoreWrapper;
import org.apache.giraph.comm.netty.NettyWorkerAggregatorRequestProcessor;
import org.apache.giraph.comm.netty.NettyWorkerClient;
import org.apache.giraph.comm.netty.NettyWorkerClientRequestProcessor;
import org.apache.giraph.comm.netty.NettyWorkerServer;
import org.apache.giraph.conf.GiraphConstants;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.giraph.edge.Edge;
import org.apache.giraph.graph.AddressesAndPartitionsWritable;
import org.apache.giraph.graph.FinishedSuperstepStats;
import org.apache.giraph.graph.GlobalStats;
import org.apache.giraph.graph.GraphTaskManager;
import org.apache.giraph.graph.InputSplitEvents;
import org.apache.giraph.graph.InputSplitPaths;
import org.apache.giraph.graph.Vertex;
import org.apache.giraph.graph.VertexEdgeCount;
import org.apache.giraph.io.EdgeOutputFormat;
import org.apache.giraph.io.EdgeWriter;
import org.apache.giraph.io.VertexOutputFormat;
import org.apache.giraph.io.VertexWriter;
import org.apache.giraph.io.superstep_output.SuperstepOutput;
import org.apache.giraph.mapping.translate.TranslateEdge;
import org.apache.giraph.master.MasterInfo;
import org.apache.giraph.master.SuperstepClasses;
import org.apache.giraph.metrics.GiraphMetrics;
import org.apache.giraph.metrics.GiraphTimer;
import org.apache.giraph.metrics.GiraphTimerContext;
import org.apache.giraph.metrics.ResetSuperstepMetricsObserver;
import org.apache.giraph.metrics.SuperstepMetricsRegistry;
import org.apache.giraph.metrics.WorkerSuperstepMetrics;
import org.apache.giraph.partition.Partition;
import org.apache.giraph.partition.PartitionExchange;
import org.apache.giraph.partition.PartitionOwner;
import org.apache.giraph.partition.PartitionStats;
import org.apache.giraph.partition.PartitionStore;
import org.apache.giraph.partition.WorkerGraphPartitioner;
import org.apache.giraph.utils.CallableFactory;
import org.apache.giraph.utils.CheckpointingUtils;
import org.apache.giraph.utils.JMapHistoDumper;
import org.apache.giraph.utils.LoggerUtils;
import org.apache.giraph.utils.MemoryUtils;
import org.apache.giraph.utils.ProgressableUtils;
import org.apache.giraph.utils.ReactiveJMapHistoDumper;
import org.apache.giraph.utils.WritableUtils;
import org.apache.giraph.zk.BspEvent;
import org.apache.giraph.zk.PredicateLock;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.io.compress.CompressionCodec;
import org.apache.hadoop.io.compress.CompressionCodecFactory;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.OutputCommitter;
import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.WatchedEvent;
import org.apache.zookeeper.Watcher.Event.EventType;
import org.apache.zookeeper.ZooDefs.Ids;
import org.apache.zookeeper.data.Stat;
import org.json.JSONArray;
import org.json.JSONException;
import org.json.JSONObject;

import com.google.common.collect.Lists;


@SuppressWarnings("rawtypes")
public class BspServiceWorker<I extends WritableComparable,
    V extends Writable, E extends Writable>
    extends BspService<I, V, E>
    implements CentralizedServiceWorker<I, V, E>,
    ResetSuperstepMetricsObserver {
  
  public static final String TIMER_WAIT_REQUESTS = "wait-requests-us";
  
  private static final Logger LOG = Logger.getLogger(BspServiceWorker.class);
  
  private String myHealthZnode;
  
  private final WorkerInfo workerInfo;
  
  private final WorkerGraphPartitioner<I, V, E> workerGraphPartitioner;
  
  private final LocalData<I, V, E, ? extends Writable> localData;
  
  private final TranslateEdge<I, E> translateEdge;
  
  private final WorkerClient<I, V, E> workerClient;
  
  private final WorkerServer<I, V, E> workerServer;
  
  private final WorkerAggregatorRequestProcessor
  workerAggregatorRequestProcessor;
  
  private MasterInfo masterInfo = new MasterInfo();
  
  private List<WorkerInfo> workerInfoList = Lists.newArrayList();
  
  private final BspEvent partitionExchangeChildrenChanged;

  
  private final WorkerContext workerContext;

  
  private final WorkerAggregatorHandler globalCommHandler;

  
  private final SuperstepOutput<I, V, E> superstepOutput;

  
  private final WorkerObserver[] observers;
  
  private final WorkerProgressWriter workerProgressWriter;

  
  
  private GiraphTimer wcPostSuperstepTimer;
  
  private GiraphTimer waitRequestsTimer;

  
  private InputSplitsHandler vertexSplitsHandler;
  
  private InputSplitsHandler edgeSplitsHandler;

  
  public BspServiceWorker(
    Mapper<?, ?, ?, ?>.Context context,
    GraphTaskManager<I, V, E> graphTaskManager)
    throws IOException, InterruptedException {
    super(context, graphTaskManager);
    ImmutableClassesGiraphConfiguration<I, V, E> conf = getConfiguration();
    localData = new LocalData<>(conf);
    translateEdge = getConfiguration().edgeTranslationInstance();
    if (translateEdge != null) {
      translateEdge.initialize(this);
    }
    partitionExchangeChildrenChanged = new PredicateLock(context);
    registerBspEvent(partitionExchangeChildrenChanged);
    workerGraphPartitioner =
        getGraphPartitionerFactory().createWorkerGraphPartitioner();
    workerInfo = new WorkerInfo();
    workerServer = new NettyWorkerServer<I, V, E>(conf, this, context,
        graphTaskManager.createUncaughtExceptionHandler());
    workerInfo.setInetSocketAddress(workerServer.getMyAddress());
    workerInfo.setTaskId(getTaskPartition());
    workerClient = new NettyWorkerClient<I, V, E>(context, conf, this,
        graphTaskManager.createUncaughtExceptionHandler());

    workerAggregatorRequestProcessor =
        new NettyWorkerAggregatorRequestProcessor(getContext(), conf, this);

    globalCommHandler = new WorkerAggregatorHandler(this, conf, context);

    workerContext = conf.createWorkerContext();
    workerContext.setWorkerGlobalCommUsage(globalCommHandler);

    superstepOutput = conf.createSuperstepOutput(context);

    if (conf.isJMapHistogramDumpEnabled()) {
      conf.addWorkerObserverClass(JMapHistoDumper.class);
    }
    if (conf.isReactiveJmapHistogramDumpEnabled()) {
      conf.addWorkerObserverClass(ReactiveJMapHistoDumper.class);
    }
    observers = conf.createWorkerObservers();

    WorkerProgress.get().setTaskId(getTaskPartition());
    workerProgressWriter = conf.trackJobProgressOnClient() ?
        new WorkerProgressWriter(graphTaskManager.getJobProgressTracker()) :
        null;

    GiraphMetrics.get().addSuperstepResetObserver(this);
    vertexSplitsHandler = null;
    edgeSplitsHandler = null;
  }

  @Override
  public void newSuperstep(SuperstepMetricsRegistry superstepMetrics) {
    waitRequestsTimer = new GiraphTimer(superstepMetrics,
        TIMER_WAIT_REQUESTS, TimeUnit.MICROSECONDS);
    wcPostSuperstepTimer = new GiraphTimer(superstepMetrics,
        "worker-context-post-superstep", TimeUnit.MICROSECONDS);
  }

  @Override
  public WorkerContext getWorkerContext() {
    return workerContext;
  }

  @Override
  public WorkerObserver[] getWorkerObservers() {
    return observers;
  }

  @Override
  public WorkerClient<I, V, E> getWorkerClient() {
    return workerClient;
  }

  public LocalData<I, V, E, ? extends Writable> getLocalData() {
    return localData;
  }

  public TranslateEdge<I, E> getTranslateEdge() {
    return translateEdge;
  }

  
  public boolean isHealthy() {
    return true;
  }

  
  private VertexEdgeCount loadInputSplits(
      List<String> inputSplitPathList,
      CallableFactory<VertexEdgeCount> inputSplitsCallableFactory)
    throws KeeperException, InterruptedException {
    VertexEdgeCount vertexEdgeCount = new VertexEdgeCount();
    
    int maxInputSplitThreads = (inputSplitPathList.size() - 1) /
        getConfiguration().getMaxWorkers() + 1;
    int numThreads = Math.min(getConfiguration().getNumInputSplitsThreads(),
        maxInputSplitThreads);
    if (LOG.isInfoEnabled()) {
      LOG.info("loadInputSplits: Using " + numThreads + " thread(s), " +
          "originally " + getConfiguration().getNumInputSplitsThreads() +
          " threads(s) for " + inputSplitPathList.size() + " total splits.");
    }

    List<VertexEdgeCount> results =
        ProgressableUtils.getResultsWithNCallables(inputSplitsCallableFactory,
            numThreads, "load-%d", getContext());
    for (VertexEdgeCount result : results) {
      vertexEdgeCount = vertexEdgeCount.incrVertexEdgeCount(result);
    }

    workerClient.waitAllRequests();
    return vertexEdgeCount;
  }

  
  private long loadMapping() throws KeeperException,
    InterruptedException {
    List<String> inputSplitPathList =
        getZkExt().getChildrenExt(mappingInputSplitsPaths.getPath(),
        false, false, true);

    InputSplitPathOrganizer splitOrganizer =
        new InputSplitPathOrganizer(getZkExt(),
            inputSplitPathList, getWorkerInfo().getHostname(),
            getConfiguration().useInputSplitLocality());

    MappingInputSplitsCallableFactory<I, V, E, ? extends Writable>
        mappingInputSplitsCallableFactory =
        new MappingInputSplitsCallableFactory<>(
            getConfiguration().createWrappedMappingInputFormat(),
            splitOrganizer,
            getContext(),
            getConfiguration(),
            this,
            getZkExt());

    long entriesLoaded = 0;
    
    int maxInputSplitThreads = inputSplitPathList.size();
    int numThreads = Math.min(getConfiguration().getNumInputSplitsThreads(),
        maxInputSplitThreads);
    if (LOG.isInfoEnabled()) {
      LOG.info("loadInputSplits: Using " + numThreads + " thread(s), " +
          "originally " + getConfiguration().getNumInputSplitsThreads() +
          " threads(s) for " + inputSplitPathList.size() + " total splits.");
    }

    List<Integer> results =
        ProgressableUtils.getResultsWithNCallables(
            mappingInputSplitsCallableFactory,
            numThreads, "load-mapping-%d", getContext());
    for (Integer result : results) {
      entriesLoaded += result;
    }
    
    localData.getMappingStore().postFilling();
    return entriesLoaded;
  }

  
  private VertexEdgeCount loadVertices() throws KeeperException,
      InterruptedException {
    List<String> inputSplitPathList =
        getZkExt().getChildrenExt(vertexInputSplitsPaths.getPath(),
            false, false, true);

    InputSplitPathOrganizer splitOrganizer =
        new InputSplitPathOrganizer(getZkExt(),
            inputSplitPathList, getWorkerInfo().getHostname(),
            getConfiguration().useInputSplitLocality());
    vertexSplitsHandler = new InputSplitsHandler(
        splitOrganizer,
        getZkExt(),
        getContext(),
        BspService.VERTEX_INPUT_SPLIT_RESERVED_NODE,
        BspService.VERTEX_INPUT_SPLIT_FINISHED_NODE);

    VertexInputSplitsCallableFactory<I, V, E> inputSplitsCallableFactory =
        new VertexInputSplitsCallableFactory<I, V, E>(
            getConfiguration().createWrappedVertexInputFormat(),
            getContext(),
            getConfiguration(),
            this,
            vertexSplitsHandler,
            getZkExt());

    return loadInputSplits(inputSplitPathList, inputSplitsCallableFactory);
  }

  
  private long loadEdges() throws KeeperException, InterruptedException {
    List<String> inputSplitPathList =
        getZkExt().getChildrenExt(edgeInputSplitsPaths.getPath(),
            false, false, true);

    InputSplitPathOrganizer splitOrganizer =
        new InputSplitPathOrganizer(getZkExt(),
            inputSplitPathList, getWorkerInfo().getHostname(),
            getConfiguration().useInputSplitLocality());
    edgeSplitsHandler = new InputSplitsHandler(
        splitOrganizer,
        getZkExt(),
        getContext(),
        BspService.EDGE_INPUT_SPLIT_RESERVED_NODE,
        BspService.EDGE_INPUT_SPLIT_FINISHED_NODE);

    EdgeInputSplitsCallableFactory<I, V, E> inputSplitsCallableFactory =
        new EdgeInputSplitsCallableFactory<I, V, E>(
            getConfiguration().createWrappedEdgeInputFormat(),
            getContext(),
            getConfiguration(),
            this,
            edgeSplitsHandler,
            getZkExt());

    return loadInputSplits(inputSplitPathList, inputSplitsCallableFactory).
        getEdgeCount();
  }

  @Override
  public MasterInfo getMasterInfo() {
    return masterInfo;
  }

  @Override
  public List<WorkerInfo> getWorkerInfoList() {
    return workerInfoList;
  }

  
  private void ensureInputSplitsReady(InputSplitPaths inputSplitPaths,
                                      InputSplitEvents inputSplitEvents) {
    while (true) {
      Stat inputSplitsReadyStat;
      try {
        inputSplitsReadyStat = getZkExt().exists(
            inputSplitPaths.getAllReadyPath(), true);
      } catch (KeeperException e) {
        throw new IllegalStateException("ensureInputSplitsReady: " +
            "KeeperException waiting on input splits", e);
      } catch (InterruptedException e) {
        throw new IllegalStateException("ensureInputSplitsReady: " +
            "InterruptedException waiting on input splits", e);
      }
      if (inputSplitsReadyStat != null) {
        break;
      }
      inputSplitEvents.getAllReadyChanged().waitForever();
      inputSplitEvents.getAllReadyChanged().reset();
    }
  }

  
  private void markCurrentWorkerDoneThenWaitForOthers(
    InputSplitPaths inputSplitPaths,
    InputSplitEvents inputSplitEvents) {
    String workerInputSplitsDonePath =
        inputSplitPaths.getDonePath() + "/" +
            getWorkerInfo().getHostnameId();
    try {
      getZkExt().createExt(workerInputSplitsDonePath,
          null,
          Ids.OPEN_ACL_UNSAFE,
          CreateMode.PERSISTENT,
          true);
    } catch (KeeperException e) {
      throw new IllegalStateException(
          "markCurrentWorkerDoneThenWaitForOthers: " +
          "KeeperException creating worker done splits", e);
    } catch (InterruptedException e) {
      throw new IllegalStateException(
          "markCurrentWorkerDoneThenWaitForOthers: " +
          "InterruptedException creating worker done splits", e);
    }
    while (true) {
      Stat inputSplitsDoneStat;
      try {
        inputSplitsDoneStat =
            getZkExt().exists(inputSplitPaths.getAllDonePath(),
                true);
      } catch (KeeperException e) {
        throw new IllegalStateException(
            "markCurrentWorkerDoneThenWaitForOthers: " +
            "KeeperException waiting on worker done splits", e);
      } catch (InterruptedException e) {
        throw new IllegalStateException(
            "markCurrentWorkerDoneThenWaitForOthers: " +
            "InterruptedException waiting on worker done splits", e);
      }
      if (inputSplitsDoneStat != null) {
        break;
      }
      inputSplitEvents.getAllDoneChanged().waitForever();
      inputSplitEvents.getAllDoneChanged().reset();
    }
  }

  @Override
  public FinishedSuperstepStats setup() {
    
    
    
    
    
    
    
    if (getRestartedSuperstep() != UNSET_SUPERSTEP) {
      setCachedSuperstep(getRestartedSuperstep());
      return new FinishedSuperstepStats(0, false, 0, 0, true,
          CheckpointStatus.NONE);
    }

    JSONObject jobState = getJobState();
    if (jobState != null) {
      try {
        if ((ApplicationState.valueOf(jobState.getString(JSONOBJ_STATE_KEY)) ==
            ApplicationState.START_SUPERSTEP) &&
            jobState.getLong(JSONOBJ_SUPERSTEP_KEY) ==
            getSuperstep()) {
          if (LOG.isInfoEnabled()) {
            LOG.info("setup: Restarting from an automated " +
                "checkpointed superstep " +
                getSuperstep() + ", attempt " +
                getApplicationAttempt());
          }
          setRestartedSuperstep(getSuperstep());
          return new FinishedSuperstepStats(0, false, 0, 0, true,
              CheckpointStatus.NONE);
        }
      } catch (JSONException e) {
        throw new RuntimeException(
            "setup: Failed to get key-values from " +
                jobState.toString(), e);
      }
    }

    
    Collection<? extends PartitionOwner> masterSetPartitionOwners =
        startSuperstep();
    workerGraphPartitioner.updatePartitionOwners(
        getWorkerInfo(), masterSetPartitionOwners);
    getPartitionStore().initialize();


    workerClient.setup(getConfiguration().authenticate());


    
    
    globalCommHandler.prepareSuperstep(workerAggregatorRequestProcessor);

    VertexEdgeCount vertexEdgeCount;
    long entriesLoaded;

    if (getConfiguration().hasMappingInputFormat()) {
      
      ensureInputSplitsReady(mappingInputSplitsPaths, mappingInputSplitsEvents);
      getContext().progress();
      try {
        entriesLoaded = loadMapping();
        
        
        getGraphPartitionerFactory().initialize(localData);
      } catch (InterruptedException e) {
        throw new IllegalStateException(
            "setup: loadMapping failed with InterruptedException", e);
      } catch (KeeperException e) {
        throw new IllegalStateException(
            "setup: loadMapping failed with KeeperException", e);
      }
      getContext().progress();
      if (LOG.isInfoEnabled()) {
        LOG.info("setup: Finally loaded a total of " +
            entriesLoaded + " entries from inputSplits");
      }

      
      markCurrentWorkerDoneThenWaitForOthers(mappingInputSplitsPaths,
          mappingInputSplitsEvents);
      
      
      localData.printStats();
    }

    if (getConfiguration().hasVertexInputFormat()) {
      
      ensureInputSplitsReady(vertexInputSplitsPaths, vertexInputSplitsEvents);
      getContext().progress();
      try {
        vertexEdgeCount = loadVertices();
      } catch (InterruptedException e) {
        throw new IllegalStateException(
            "setup: loadVertices failed with InterruptedException", e);
      } catch (KeeperException e) {
        throw new IllegalStateException(
            "setup: loadVertices failed with KeeperException", e);
      }
      getContext().progress();
    } else {
      vertexEdgeCount = new VertexEdgeCount();
    }
    WorkerProgress.get().finishLoadingVertices();

    if (getConfiguration().hasEdgeInputFormat()) {
      
      ensureInputSplitsReady(edgeInputSplitsPaths, edgeInputSplitsEvents);
      getContext().progress();
      try {
        vertexEdgeCount = vertexEdgeCount.incrVertexEdgeCount(0, loadEdges());
      } catch (InterruptedException e) {
        throw new IllegalStateException(
            "setup: loadEdges failed with InterruptedException", e);
      } catch (KeeperException e) {
        throw new IllegalStateException(
            "setup: loadEdges failed with KeeperException", e);
      }
      getContext().progress();
    }
    WorkerProgress.get().finishLoadingEdges();

    if (LOG.isInfoEnabled()) {
      LOG.info("setup: Finally loaded a total of " + vertexEdgeCount);
    }

    if (getConfiguration().hasVertexInputFormat()) {
      
      markCurrentWorkerDoneThenWaitForOthers(vertexInputSplitsPaths,
          vertexInputSplitsEvents);
    }

    if (getConfiguration().hasEdgeInputFormat()) {
      
      markCurrentWorkerDoneThenWaitForOthers(edgeInputSplitsPaths,
          edgeInputSplitsEvents);
    }

    
    for (PartitionOwner partitionOwner : masterSetPartitionOwners) {
      if (partitionOwner.getWorkerInfo().equals(getWorkerInfo()) &&
          !getPartitionStore().hasPartition(
              partitionOwner.getPartitionId())) {
        Partition<I, V, E> partition =
            getConfiguration().createPartition(
                partitionOwner.getPartitionId(), getContext());
        getPartitionStore().addPartition(partition);
      }
    }

    
    localData.removeMappingStoreIfPossible();

    if (getConfiguration().hasEdgeInputFormat()) {
      
      getServerData().getPartitionStore().moveEdgesToVertices();
    }

    
    
    List<PartitionStats> partitionStatsList =
        new ArrayList<PartitionStats>();
    PartitionStore<I, V, E> partitionStore = getPartitionStore();
    for (Integer partitionId : partitionStore.getPartitionIds()) {
      PartitionStats partitionStats =
          new PartitionStats(partitionId,
              partitionStore.getPartitionVertexCount(partitionId),
              0,
              partitionStore.getPartitionEdgeCount(partitionId),
              0, 0);
      partitionStatsList.add(partitionStats);
    }
    workerGraphPartitioner.finalizePartitionStats(
        partitionStatsList, getPartitionStore());

    return finishSuperstep(partitionStatsList, null);
  }

  
  private void registerHealth(long superstep) {
    JSONArray hostnamePort = new JSONArray();
    hostnamePort.put(getHostname());

    hostnamePort.put(workerInfo.getPort());

    String myHealthPath = null;
    if (isHealthy()) {
      myHealthPath = getWorkerInfoHealthyPath(getApplicationAttempt(),
          getSuperstep());
    } else {
      myHealthPath = getWorkerInfoUnhealthyPath(getApplicationAttempt(),
          getSuperstep());
    }
    myHealthPath = myHealthPath + "/" + workerInfo.getHostnameId();
    try {
      myHealthZnode = getZkExt().createExt(
          myHealthPath,
          WritableUtils.writeToByteArray(workerInfo),
          Ids.OPEN_ACL_UNSAFE,
          CreateMode.EPHEMERAL,
          true);
    } catch (KeeperException.NodeExistsException e) {
      LOG.warn("registerHealth: myHealthPath already exists (likely " +
          "from previous failure): " + myHealthPath +
          ".  Waiting for change in attempts " +
          "to re-join the application");
      getApplicationAttemptChangedEvent().waitForever();
      if (LOG.isInfoEnabled()) {
        LOG.info("registerHealth: Got application " +
            "attempt changed event, killing self");
      }
      throw new IllegalStateException(
          "registerHealth: Trying " +
              "to get the new application attempt by killing self", e);
    } catch (KeeperException e) {
      throw new IllegalStateException("Creating " + myHealthPath +
          " failed with KeeperException", e);
    } catch (InterruptedException e) {
      throw new IllegalStateException("Creating " + myHealthPath +
          " failed with InterruptedException", e);
    }
    if (LOG.isInfoEnabled()) {
      LOG.info("registerHealth: Created my health node for attempt=" +
          getApplicationAttempt() + ", superstep=" +
          getSuperstep() + " with " + myHealthZnode +
          " and workerInfo= " + workerInfo);
    }
  }

  
  private void unregisterHealth() {
    LOG.error("unregisterHealth: Got failure, unregistering health on " +
        myHealthZnode + " on superstep " + getSuperstep());
    try {
      getZkExt().deleteExt(myHealthZnode, -1, false);
    } catch (InterruptedException e) {
      throw new IllegalStateException(
          "unregisterHealth: InterruptedException - Couldn't delete " +
              myHealthZnode, e);
    } catch (KeeperException e) {
      throw new IllegalStateException(
          "unregisterHealth: KeeperException - Couldn't delete " +
              myHealthZnode, e);
    }
  }

  @Override
  public void failureCleanup() {
    unregisterHealth();
  }

  @Override
  public Collection<? extends PartitionOwner> startSuperstep() {
    
    
    
    
    
    
    if (getSuperstep() != INPUT_SUPERSTEP) {
      workerServer.prepareSuperstep();
    }

    registerHealth(getSuperstep());

    String addressesAndPartitionsPath =
        getAddressesAndPartitionsPath(getApplicationAttempt(),
            getSuperstep());
    AddressesAndPartitionsWritable addressesAndPartitions =
        new AddressesAndPartitionsWritable(
            workerGraphPartitioner.createPartitionOwner().getClass());
    try {
      while (getZkExt().exists(addressesAndPartitionsPath, true) ==
          null) {
        getAddressesAndPartitionsReadyChangedEvent().waitForever();
        getAddressesAndPartitionsReadyChangedEvent().reset();
      }
      WritableUtils.readFieldsFromZnode(
          getZkExt(),
          addressesAndPartitionsPath,
          false,
          null,
          addressesAndPartitions);
    } catch (KeeperException e) {
      throw new IllegalStateException(
          "startSuperstep: KeeperException getting assignments", e);
    } catch (InterruptedException e) {
      throw new IllegalStateException(
          "startSuperstep: InterruptedException getting assignments", e);
    }

    workerInfoList.clear();
    workerInfoList = addressesAndPartitions.getWorkerInfos();
    masterInfo = addressesAndPartitions.getMasterInfo();

    if (LOG.isInfoEnabled()) {
      LOG.info("startSuperstep: " + masterInfo);
      LOG.info("startSuperstep: Ready for computation on superstep " +
          getSuperstep() + " since worker " +
          "selection and vertex range assignments are done in " +
          addressesAndPartitionsPath);
    }

    getContext().setStatus("startSuperstep: " +
        getGraphTaskManager().getGraphFunctions().toString() +
        " - Attempt=" + getApplicationAttempt() +
        ", Superstep=" + getSuperstep());

    if (LOG.isDebugEnabled()) {
      LOG.debug("startSuperstep: addressesAndPartitions" +
          addressesAndPartitions.getWorkerInfos());
      for (PartitionOwner partitionOwner : addressesAndPartitions
          .getPartitionOwners()) {
        LOG.debug(partitionOwner.getPartitionId() + " " +
            partitionOwner.getWorkerInfo());
      }
    }

    return addressesAndPartitions.getPartitionOwners();
  }

  @Override
  public FinishedSuperstepStats finishSuperstep(
      List<PartitionStats> partitionStatsList,
      GiraphTimerContext superstepTimerContext) {
    
    
    
    
    
    
    
    
    
    
    
    
    waitForRequestsToFinish();

    getGraphTaskManager().notifyFinishedCommunication();

    long workerSentMessages = 0;
    long workerSentMessageBytes = 0;
    long localVertices = 0;
    for (PartitionStats partitionStats : partitionStatsList) {
      workerSentMessages += partitionStats.getMessagesSentCount();
      workerSentMessageBytes += partitionStats.getMessageBytesSentCount();
      localVertices += partitionStats.getVertexCount();
    }

    if (getSuperstep() != INPUT_SUPERSTEP) {
      postSuperstepCallbacks();
    } else {
      if (getConfiguration().hasVertexInputFormat()) {
        vertexSplitsHandler.setDoneReadingGraph(true);
      }
      if (getConfiguration().hasEdgeInputFormat()) {
        edgeSplitsHandler.setDoneReadingGraph(true);
      }
    }

    globalCommHandler.finishSuperstep(workerAggregatorRequestProcessor);

    MessageStore<I, Writable> incomingMessageStore =
        getServerData().getIncomingMessageStore();
    if (incomingMessageStore instanceof AsyncMessageStoreWrapper) {
      ((AsyncMessageStoreWrapper) incomingMessageStore).waitToComplete();
    }

    if (LOG.isInfoEnabled()) {
      LOG.info("finishSuperstep: Superstep " + getSuperstep() +
          ", messages = " + workerSentMessages + " " +
          ", message bytes = " + workerSentMessageBytes + " , " +
          MemoryUtils.getRuntimeMemoryStats());
    }

    if (superstepTimerContext != null) {
      superstepTimerContext.stop();
    }
    writeFinshedSuperstepInfoToZK(partitionStatsList,
      workerSentMessages, workerSentMessageBytes);

    LoggerUtils.setStatusAndLog(getContext(), LOG, Level.INFO,
        "finishSuperstep: (waiting for rest " +
            "of workers) " +
            getGraphTaskManager().getGraphFunctions().toString() +
            " - Attempt=" + getApplicationAttempt() +
            ", Superstep=" + getSuperstep());

    String superstepFinishedNode =
        getSuperstepFinishedPath(getApplicationAttempt(), getSuperstep());

    waitForOtherWorkers(superstepFinishedNode);

    GlobalStats globalStats = new GlobalStats();
    SuperstepClasses superstepClasses = SuperstepClasses.createToRead(
        getConfiguration());
    WritableUtils.readFieldsFromZnode(
        getZkExt(), superstepFinishedNode, false, null, globalStats,
        superstepClasses);
    if (LOG.isInfoEnabled()) {
      LOG.info("finishSuperstep: Completed superstep " + getSuperstep() +
          " with global stats " + globalStats + " and classes " +
          superstepClasses);
    }
    getContext().setStatus("finishSuperstep: (all workers done) " +
        getGraphTaskManager().getGraphFunctions().toString() +
        " - Attempt=" + getApplicationAttempt() +
        ", Superstep=" + getSuperstep());
    incrCachedSuperstep();
    getConfiguration().updateSuperstepClasses(superstepClasses);

    return new FinishedSuperstepStats(
        localVertices,
        globalStats.getHaltComputation(),
        globalStats.getVertexCount(),
        globalStats.getEdgeCount(),
        false,
        globalStats.getCheckpointStatus());
  }

  
  private void postSuperstepCallbacks() {
    GiraphTimerContext timerContext = wcPostSuperstepTimer.time();
    getWorkerContext().postSuperstep();
    timerContext.stop();
    getContext().progress();

    for (WorkerObserver obs : getWorkerObservers()) {
      obs.postSuperstep(getSuperstep());
      getContext().progress();
    }
  }

  
  private void waitForRequestsToFinish() {
    if (LOG.isInfoEnabled()) {
      LOG.info("finishSuperstep: Waiting on all requests, superstep " +
          getSuperstep() + " " +
          MemoryUtils.getRuntimeMemoryStats());
    }
    GiraphTimerContext timerContext = waitRequestsTimer.time();
    workerClient.waitAllRequests();
    timerContext.stop();
  }

  
  private void waitForOtherWorkers(String superstepFinishedNode) {
    try {
      while (getZkExt().exists(superstepFinishedNode, true) == null) {
        getSuperstepFinishedEvent().waitForever();
        getSuperstepFinishedEvent().reset();
      }
    } catch (KeeperException e) {
      throw new IllegalStateException(
          "finishSuperstep: Failed while waiting for master to " +
              "signal completion of superstep " + getSuperstep(), e);
    } catch (InterruptedException e) {
      throw new IllegalStateException(
          "finishSuperstep: Failed while waiting for master to " +
              "signal completion of superstep " + getSuperstep(), e);
    }
  }

  
  private void writeFinshedSuperstepInfoToZK(
      List<PartitionStats> partitionStatsList, long workerSentMessages,
      long workerSentMessageBytes) {
    Collection<PartitionStats> finalizedPartitionStats =
        workerGraphPartitioner.finalizePartitionStats(
            partitionStatsList, getPartitionStore());
    List<PartitionStats> finalizedPartitionStatsList =
        new ArrayList<PartitionStats>(finalizedPartitionStats);
    byte[] partitionStatsBytes =
        WritableUtils.writeListToByteArray(finalizedPartitionStatsList);
    WorkerSuperstepMetrics metrics = new WorkerSuperstepMetrics();
    metrics.readFromRegistry();
    byte[] metricsBytes = WritableUtils.writeToByteArray(metrics);

    JSONObject workerFinishedInfoObj = new JSONObject();
    try {
      workerFinishedInfoObj.put(JSONOBJ_PARTITION_STATS_KEY,
          Base64.encodeBytes(partitionStatsBytes));
      workerFinishedInfoObj.put(JSONOBJ_NUM_MESSAGES_KEY, workerSentMessages);
      workerFinishedInfoObj.put(JSONOBJ_NUM_MESSAGE_BYTES_KEY,
        workerSentMessageBytes);
      workerFinishedInfoObj.put(JSONOBJ_METRICS_KEY,
          Base64.encodeBytes(metricsBytes));
    } catch (JSONException e) {
      throw new RuntimeException(e);
    }

    String finishedWorkerPath =
        getWorkerFinishedPath(getApplicationAttempt(), getSuperstep()) +
        "/" + getHostnamePartitionId();
    try {
      getZkExt().createExt(finishedWorkerPath,
          workerFinishedInfoObj.toString().getBytes(Charset.defaultCharset()),
          Ids.OPEN_ACL_UNSAFE,
          CreateMode.PERSISTENT,
          true);
    } catch (KeeperException.NodeExistsException e) {
      LOG.warn("finishSuperstep: finished worker path " +
          finishedWorkerPath + " already exists!");
    } catch (KeeperException e) {
      throw new IllegalStateException("Creating " + finishedWorkerPath +
          " failed with KeeperException", e);
    } catch (InterruptedException e) {
      throw new IllegalStateException("Creating " + finishedWorkerPath +
          " failed with InterruptedException", e);
    }
  }

  
  private void saveVertices(long numLocalVertices) throws IOException,
      InterruptedException {
    ImmutableClassesGiraphConfiguration<I, V, E>  conf = getConfiguration();

    if (conf.getVertexOutputFormatClass() == null) {
      LOG.warn("saveVertices: " +
          GiraphConstants.VERTEX_OUTPUT_FORMAT_CLASS +
          " not specified -- there will be no saved output");
      return;
    }
    if (conf.doOutputDuringComputation()) {
      if (LOG.isInfoEnabled()) {
        LOG.info("saveVertices: The option for doing output during " +
            "computation is selected, so there will be no saving of the " +
            "output in the end of application");
      }
      return;
    }

    final int numPartitions = getPartitionStore().getNumPartitions();
    int numThreads = Math.min(getConfiguration().getNumOutputThreads(),
        numPartitions);
    LoggerUtils.setStatusAndLog(getContext(), LOG, Level.INFO,
        "saveVertices: Starting to save " + numLocalVertices + " vertices " +
            "using " + numThreads + " threads");
    final VertexOutputFormat<I, V, E> vertexOutputFormat =
        getConfiguration().createWrappedVertexOutputFormat();

    getPartitionStore().startIteration();

    long verticesToStore = 0;
    PartitionStore<I, V, E> partitionStore = getPartitionStore();
    for (int partitionId : partitionStore.getPartitionIds()) {
      verticesToStore += partitionStore.getPartitionVertexCount(partitionId);
    }
    WorkerProgress.get().startStoring(
        verticesToStore, getPartitionStore().getNumPartitions());

    CallableFactory<Void> callableFactory = new CallableFactory<Void>() {
      @Override
      public Callable<Void> newCallable(int callableId) {
        return new Callable<Void>() {
          
          private static final long VERTICES_TO_UPDATE_PROGRESS = 100000;

          @Override
          public Void call() throws Exception {
            VertexWriter<I, V, E> vertexWriter =
                vertexOutputFormat.createVertexWriter(getContext());
            vertexWriter.setConf(getConfiguration());
            vertexWriter.initialize(getContext());
            long nextPrintVertices = 0;
            long nextUpdateProgressVertices = VERTICES_TO_UPDATE_PROGRESS;
            long nextPrintMsecs = System.currentTimeMillis() + 15000;
            int partitionIndex = 0;
            int numPartitions = getPartitionStore().getNumPartitions();
            while (true) {
              Partition<I, V, E> partition =
                  getPartitionStore().getNextPartition();
              if (partition == null) {
                break;
              }

              long verticesWritten = 0;
              for (Vertex<I, V, E> vertex : partition) {
                vertexWriter.writeVertex(vertex);
                ++verticesWritten;

                
                if (verticesWritten > nextPrintVertices &&
                    System.currentTimeMillis() > nextPrintMsecs) {
                  LoggerUtils.setStatusAndLog(getContext(), LOG, Level.INFO,
                      "saveVertices: Saved " + verticesWritten + " out of " +
                          partition.getVertexCount() + " partition vertices, " +
                          "on partition " + partitionIndex +
                          " out of " + numPartitions);
                  nextPrintMsecs = System.currentTimeMillis() + 15000;
                  nextPrintVertices = verticesWritten + 250000;
                }

                if (verticesWritten >= nextUpdateProgressVertices) {
                  WorkerProgress.get().addVerticesStored(
                      VERTICES_TO_UPDATE_PROGRESS);
                  nextUpdateProgressVertices += VERTICES_TO_UPDATE_PROGRESS;
                }
              }
              getPartitionStore().putPartition(partition);
              ++partitionIndex;
              WorkerProgress.get().addVerticesStored(
                  verticesWritten % VERTICES_TO_UPDATE_PROGRESS);
              WorkerProgress.get().incrementPartitionsStored();
            }
            vertexWriter.close(getContext()); 
            return null;
          }
        };
      }
    };
    ProgressableUtils.getResultsWithNCallables(callableFactory, numThreads,
        "save-vertices-%d", getContext());

    LoggerUtils.setStatusAndLog(getContext(), LOG, Level.INFO,
      "saveVertices: Done saving vertices.");
    
    if (getConfiguration().isPureYarnJob() &&
      getConfiguration().getVertexOutputFormatClass() != null) {
      try {
        OutputCommitter outputCommitter =
          vertexOutputFormat.getOutputCommitter(getContext());
        if (outputCommitter.needsTaskCommit(getContext())) {
          LoggerUtils.setStatusAndLog(getContext(), LOG, Level.INFO,
            "OutputCommitter: committing task output.");
          
          
          outputCommitter.commitTask(getContext());
        }
      } catch (InterruptedException ie) {
        LOG.error("Interrupted while attempting to obtain " +
          "OutputCommitter.", ie);
      } catch (IOException ioe) {
        LOG.error("Master task's attempt to commit output has " +
          "FAILED.", ioe);
      }
    }
  }

  
  private void saveEdges() throws IOException, InterruptedException {
    final ImmutableClassesGiraphConfiguration<I, V, E>  conf =
      getConfiguration();

    if (conf.getEdgeOutputFormatClass() == null) {
      LOG.warn("saveEdges: " +
               GiraphConstants.EDGE_OUTPUT_FORMAT_CLASS +
               "Make sure that the EdgeOutputFormat is not required.");
      return;
    }

    final int numPartitions = getPartitionStore().getNumPartitions();
    int numThreads = Math.min(conf.getNumOutputThreads(),
        numPartitions);
    LoggerUtils.setStatusAndLog(getContext(), LOG, Level.INFO,
        "saveEdges: Starting to save the edges using " +
        numThreads + " threads");
    final EdgeOutputFormat<I, V, E> edgeOutputFormat =
        conf.createWrappedEdgeOutputFormat();

    getPartitionStore().startIteration();

    CallableFactory<Void> callableFactory = new CallableFactory<Void>() {
      @Override
      public Callable<Void> newCallable(int callableId) {
        return new Callable<Void>() {
          @Override
          public Void call() throws Exception {
            EdgeWriter<I, V, E>  edgeWriter =
                edgeOutputFormat.createEdgeWriter(getContext());
            edgeWriter.setConf(conf);
            edgeWriter.initialize(getContext());

            long nextPrintVertices = 0;
            long nextPrintMsecs = System.currentTimeMillis() + 15000;
            int partitionIndex = 0;
            int numPartitions = getPartitionStore().getNumPartitions();
            while (true) {
              Partition<I, V, E> partition =
                  getPartitionStore().getNextPartition();
              if (partition == null) {
                break;
              }

              long vertices = 0;
              long edges = 0;
              long partitionEdgeCount = partition.getEdgeCount();
              for (Vertex<I, V, E> vertex : partition) {
                for (Edge<I, E> edge : vertex.getEdges()) {
                  edgeWriter.writeEdge(vertex.getId(), vertex.getValue(), edge);
                  ++edges;
                }
                ++vertices;

                
                if (vertices > nextPrintVertices &&
                    System.currentTimeMillis() > nextPrintMsecs) {
                  LoggerUtils.setStatusAndLog(getContext(), LOG, Level.INFO,
                      "saveEdges: Saved " + edges +
                      " edges out of " + partitionEdgeCount +
                      " partition edges, on partition " + partitionIndex +
                      " out of " + numPartitions);
                  nextPrintMsecs = System.currentTimeMillis() + 15000;
                  nextPrintVertices = vertices + 250000;
                }
              }
              getPartitionStore().putPartition(partition);
              ++partitionIndex;
            }
            edgeWriter.close(getContext()); 
            return null;
          }
        };
      }
    };
    ProgressableUtils.getResultsWithNCallables(callableFactory, numThreads,
        "save-vertices-%d", getContext());

    LoggerUtils.setStatusAndLog(getContext(), LOG, Level.INFO,
      "saveEdges: Done saving edges.");
    
    if (conf.isPureYarnJob() &&
      conf.getVertexOutputFormatClass() != null) {
      try {
        OutputCommitter outputCommitter =
          edgeOutputFormat.getOutputCommitter(getContext());
        if (outputCommitter.needsTaskCommit(getContext())) {
          LoggerUtils.setStatusAndLog(getContext(), LOG, Level.INFO,
            "OutputCommitter: committing task output.");
          
          
          outputCommitter.commitTask(getContext());
        }
      } catch (InterruptedException ie) {
        LOG.error("Interrupted while attempting to obtain " +
          "OutputCommitter.", ie);
      } catch (IOException ioe) {
        LOG.error("Master task's attempt to commit output has " +
          "FAILED.", ioe);
      }
    }
  }

  @Override
  public void cleanup(FinishedSuperstepStats finishedSuperstepStats)
    throws IOException, InterruptedException {
    workerClient.closeConnections();
    setCachedSuperstep(getSuperstep() - 1);
    if (finishedSuperstepStats.getCheckpointStatus() !=
        CheckpointStatus.CHECKPOINT_AND_HALT) {
      saveVertices(finishedSuperstepStats.getLocalVertexCount());
      saveEdges();
    }
    WorkerProgress.get().finishStoring();
    if (workerProgressWriter != null) {
      workerProgressWriter.stop();
    }
    getPartitionStore().shutdown();
    
    
    
    
    String workerCleanedUpPath = cleanedUpPath  + "/" +
        getTaskPartition() + WORKER_SUFFIX;
    try {
      String finalFinishedPath =
          getZkExt().createExt(workerCleanedUpPath,
              null,
              Ids.OPEN_ACL_UNSAFE,
              CreateMode.PERSISTENT,
              true);
      if (LOG.isInfoEnabled()) {
        LOG.info("cleanup: Notifying master its okay to cleanup with " +
            finalFinishedPath);
      }
    } catch (KeeperException.NodeExistsException e) {
      if (LOG.isInfoEnabled()) {
        LOG.info("cleanup: Couldn't create finished node '" +
            workerCleanedUpPath);
      }
    } catch (KeeperException e) {
      
      LOG.error("cleanup: Got KeeperException on notification " +
          "to master about cleanup", e);
    } catch (InterruptedException e) {
      
      LOG.error("cleanup: Got InterruptedException on notification " +
          "to master about cleanup", e);
    }
    try {
      getZkExt().close();
    } catch (InterruptedException e) {
      
      LOG.error("cleanup: Zookeeper failed to close with " + e);
    }

    if (getConfiguration().metricsEnabled()) {
      GiraphMetrics.get().dumpToStream(System.err);
    }

    
    
    
    workerServer.close();
  }

  @Override
  public void storeCheckpoint() throws IOException {
    LoggerUtils.setStatusAndLog(getContext(), LOG, Level.INFO,
        "storeCheckpoint: Starting checkpoint " +
            getGraphTaskManager().getGraphFunctions().toString() +
            " - Attempt=" + getApplicationAttempt() +
            ", Superstep=" + getSuperstep());

    
    
    Path metadataFilePath = createCheckpointFilePathSafe(
        CheckpointingUtils.CHECKPOINT_METADATA_POSTFIX);
    Path validFilePath = createCheckpointFilePathSafe(
        CheckpointingUtils.CHECKPOINT_VALID_POSTFIX);
    Path checkpointFilePath = createCheckpointFilePathSafe(
        CheckpointingUtils.CHECKPOINT_DATA_POSTFIX);


    
    
    FSDataOutputStream metadataOutputStream =
        getFs().create(metadataFilePath);
    metadataOutputStream.writeInt(getPartitionStore().getNumPartitions());

    for (Integer partitionId : getPartitionStore().getPartitionIds()) {
      metadataOutputStream.writeInt(partitionId);
    }
    metadataOutputStream.close();

    storeCheckpointVertices();

    FSDataOutputStream checkpointOutputStream =
        getFs().create(checkpointFilePath);
    workerContext.write(checkpointOutputStream);
    getContext().progress();

    for (Integer partitionId : getPartitionStore().getPartitionIds()) {
      
      checkpointOutputStream.writeInt(partitionId);
      getServerData().getCurrentMessageStore().writePartition(
          checkpointOutputStream, partitionId);
      getContext().progress();

    }

    List<Writable> w2wMessages =
        getServerData().getCurrentWorkerToWorkerMessages();
    WritableUtils.writeList(w2wMessages, checkpointOutputStream);

    checkpointOutputStream.close();

    getFs().createNewFile(validFilePath);

    
    String workerWroteCheckpoint =
        getWorkerWroteCheckpointPath(getApplicationAttempt(),
            getSuperstep()) + "/" + getHostnamePartitionId();
    try {
      getZkExt().createExt(workerWroteCheckpoint,
          new byte[0],
          Ids.OPEN_ACL_UNSAFE,
          CreateMode.PERSISTENT,
          true);
    } catch (KeeperException.NodeExistsException e) {
      LOG.warn("storeCheckpoint: wrote checkpoint worker path " +
          workerWroteCheckpoint + " already exists!");
    } catch (KeeperException e) {
      throw new IllegalStateException("Creating " + workerWroteCheckpoint +
          " failed with KeeperException", e);
    } catch (InterruptedException e) {
      throw new IllegalStateException("Creating " +
          workerWroteCheckpoint +
          " failed with InterruptedException", e);
    }
  }

  
  private Path createCheckpointFilePathSafe(String name) throws IOException {
    Path validFilePath = new Path(getCheckpointBasePath(getSuperstep()) + '.' +
        getWorkerId(workerInfo) + name);
    
    
    if (getFs().delete(validFilePath, false)) {
      LOG.warn("storeCheckpoint: Removed " + name + " file " +
          validFilePath);
    }
    return validFilePath;
  }

  
  private Path getSavedCheckpoint(long superstep, String name) {
    return new Path(getSavedCheckpointBasePath(superstep) + '.' +
        getWorkerId(workerInfo) + name);
  }

  
  private void storeCheckpointVertices() {
    final int numPartitions = getPartitionStore().getNumPartitions();
    int numThreads = Math.min(
        GiraphConstants.NUM_CHECKPOINT_IO_THREADS.get(getConfiguration()),
        numPartitions);

    getPartitionStore().startIteration();

    final CompressionCodec codec =
        new CompressionCodecFactory(getConfiguration())
            .getCodec(new Path(
                GiraphConstants.CHECKPOINT_COMPRESSION_CODEC
                    .get(getConfiguration())));

    long t0 = System.currentTimeMillis();

    CallableFactory<Void> callableFactory = new CallableFactory<Void>() {
      @Override
      public Callable<Void> newCallable(int callableId) {
        return new Callable<Void>() {

          @Override
          public Void call() throws Exception {
            while (true) {
              Partition<I, V, E> partition =
                  getPartitionStore().getNextPartition();
              if (partition == null) {
                break;
              }
              Path path =
                  createCheckpointFilePathSafe("_" + partition.getId() +
                      CheckpointingUtils.CHECKPOINT_VERTICES_POSTFIX);

              FSDataOutputStream uncompressedStream =
                  getFs().create(path);


              DataOutputStream stream = codec == null ? uncompressedStream :
                  new DataOutputStream(
                      codec.createOutputStream(uncompressedStream));


              partition.write(stream);

              getPartitionStore().putPartition(partition);

              stream.close();
              uncompressedStream.close();
            }
            return null;
          }


        };
      }
    };

    ProgressableUtils.getResultsWithNCallables(callableFactory, numThreads,
        "checkpoint-vertices-%d", getContext());

    LOG.info("Save checkpoint in " + (System.currentTimeMillis() - t0) +
        " ms, using " + numThreads + " threads");
  }

  
  private void loadCheckpointVertices(final long superstep,
                                      List<Integer> partitions) {
    int numThreads = Math.min(
        GiraphConstants.NUM_CHECKPOINT_IO_THREADS.get(getConfiguration()),
        partitions.size());

    final Queue<Integer> partitionIdQueue =
        new ConcurrentLinkedQueue<>(partitions);

    final CompressionCodec codec =
        new CompressionCodecFactory(getConfiguration())
            .getCodec(new Path(
                GiraphConstants.CHECKPOINT_COMPRESSION_CODEC
                    .get(getConfiguration())));

    long t0 = System.currentTimeMillis();

    CallableFactory<Void> callableFactory = new CallableFactory<Void>() {
      @Override
      public Callable<Void> newCallable(int callableId) {
        return new Callable<Void>() {

          @Override
          public Void call() throws Exception {
            while (!partitionIdQueue.isEmpty()) {
              Integer partitionId = partitionIdQueue.poll();
              if (partitionId == null) {
                break;
              }
              Path path =
                  getSavedCheckpoint(superstep, "_" + partitionId +
                      CheckpointingUtils.CHECKPOINT_VERTICES_POSTFIX);

              FSDataInputStream compressedStream =
                  getFs().open(path);

              DataInputStream stream = codec == null ? compressedStream :
                  new DataInputStream(
                      codec.createInputStream(compressedStream));

              Partition<I, V, E> partition =
                  getConfiguration().createPartition(partitionId, getContext());

              partition.readFields(stream);

              getPartitionStore().addPartition(partition);

              stream.close();
            }
            return null;
          }

        };
      }
    };

    ProgressableUtils.getResultsWithNCallables(callableFactory, numThreads,
        "load-vertices-%d", getContext());

    LOG.info("Loaded checkpoint in " + (System.currentTimeMillis() - t0) +
        " ms, using " + numThreads + " threads");
  }

  @Override
  public VertexEdgeCount loadCheckpoint(long superstep) {
    Path metadataFilePath = getSavedCheckpoint(
        superstep, CheckpointingUtils.CHECKPOINT_METADATA_POSTFIX);

    Path checkpointFilePath = getSavedCheckpoint(
        superstep, CheckpointingUtils.CHECKPOINT_DATA_POSTFIX);
    
    
    
    
    try {
      DataInputStream metadataStream =
          getFs().open(metadataFilePath);

      int partitions = metadataStream.readInt();
      List<Integer> partitionIds = new ArrayList<>(partitions);
      for (int i = 0; i < partitions; i++) {
        int partitionId = metadataStream.readInt();
        partitionIds.add(partitionId);
      }

      loadCheckpointVertices(superstep, partitionIds);

      getContext().progress();

      metadataStream.close();

      DataInputStream checkpointStream =
          getFs().open(checkpointFilePath);
      workerContext.readFields(checkpointStream);

      
      GlobalStats globalStats = new GlobalStats();
      SuperstepClasses superstepClasses = SuperstepClasses.createToRead(
          getConfiguration());
      String finalizedCheckpointPath = getSavedCheckpointBasePath(superstep) +
          CheckpointingUtils.CHECKPOINT_FINALIZED_POSTFIX;
      DataInputStream finalizedStream =
          getFs().open(new Path(finalizedCheckpointPath));
      globalStats.readFields(finalizedStream);
      superstepClasses.readFields(finalizedStream);
      getConfiguration().updateSuperstepClasses(superstepClasses);
      getServerData().resetMessageStores();

      for (int i = 0; i < partitions; i++) {
        int partitionId = checkpointStream.readInt();
        getServerData().getCurrentMessageStore().readFieldsForPartition(
            checkpointStream, partitionId);
      }

      List<Writable> w2wMessages = (List<Writable>) WritableUtils.readList(
          checkpointStream);
      getServerData().getCurrentWorkerToWorkerMessages().addAll(w2wMessages);

      checkpointStream.close();

      if (LOG.isInfoEnabled()) {
        LOG.info("loadCheckpoint: Loaded " +
            workerGraphPartitioner.getPartitionOwners().size() +
            " total.");
      }

      
      

      workerClient.setup(getConfiguration().authenticate());

      return new VertexEdgeCount(globalStats.getVertexCount(),
          globalStats.getEdgeCount());

    } catch (IOException e) {
      throw new RuntimeException(
          "loadCheckpoint: Failed for superstep=" + superstep, e);
    }
  }

  
  private void sendWorkerPartitions(
      Map<WorkerInfo, List<Integer>> workerPartitionMap) {
    List<Entry<WorkerInfo, List<Integer>>> randomEntryList =
        new ArrayList<Entry<WorkerInfo, List<Integer>>>(
            workerPartitionMap.entrySet());
    Collections.shuffle(randomEntryList);
    WorkerClientRequestProcessor<I, V, E> workerClientRequestProcessor =
        new NettyWorkerClientRequestProcessor<I, V, E>(getContext(),
            getConfiguration(), this,
            false );
    for (Entry<WorkerInfo, List<Integer>> workerPartitionList :
      randomEntryList) {
      for (Integer partitionId : workerPartitionList.getValue()) {
        Partition<I, V, E> partition =
            getPartitionStore().removePartition(partitionId);
        if (partition == null) {
          throw new IllegalStateException(
              "sendWorkerPartitions: Couldn't find partition " +
                  partitionId + " to send to " +
                  workerPartitionList.getKey());
        }
        if (LOG.isInfoEnabled()) {
          LOG.info("sendWorkerPartitions: Sending worker " +
              workerPartitionList.getKey() + " partition " +
              partitionId);
        }
        workerClientRequestProcessor.sendPartitionRequest(
            workerPartitionList.getKey(),
            partition);
      }
    }

    try {
      workerClientRequestProcessor.flush();
      workerClient.waitAllRequests();
    } catch (IOException e) {
      throw new IllegalStateException("sendWorkerPartitions: Flush failed", e);
    }
    String myPartitionExchangeDonePath =
        getPartitionExchangeWorkerPath(
            getApplicationAttempt(), getSuperstep(), getWorkerInfo());
    try {
      getZkExt().createExt(myPartitionExchangeDonePath,
          null,
          Ids.OPEN_ACL_UNSAFE,
          CreateMode.PERSISTENT,
          true);
    } catch (KeeperException e) {
      throw new IllegalStateException(
          "sendWorkerPartitions: KeeperException to create " +
              myPartitionExchangeDonePath, e);
    } catch (InterruptedException e) {
      throw new IllegalStateException(
          "sendWorkerPartitions: InterruptedException to create " +
              myPartitionExchangeDonePath, e);
    }
    if (LOG.isInfoEnabled()) {
      LOG.info("sendWorkerPartitions: Done sending all my partitions.");
    }
  }

  @Override
  public final void exchangeVertexPartitions(
      Collection<? extends PartitionOwner> masterSetPartitionOwners) {
    
    
    
    
    
    
    PartitionExchange partitionExchange =
        workerGraphPartitioner.updatePartitionOwners(
            getWorkerInfo(), masterSetPartitionOwners);
    workerClient.openConnections();

    Map<WorkerInfo, List<Integer>> sendWorkerPartitionMap =
        partitionExchange.getSendWorkerPartitionMap();
    if (!getPartitionStore().isEmpty()) {
      sendWorkerPartitions(sendWorkerPartitionMap);
    }

    Set<WorkerInfo> myDependencyWorkerSet =
        partitionExchange.getMyDependencyWorkerSet();
    Set<String> workerIdSet = new HashSet<String>();
    for (WorkerInfo tmpWorkerInfo : myDependencyWorkerSet) {
      if (!workerIdSet.add(tmpWorkerInfo.getHostnameId())) {
        throw new IllegalStateException(
            "exchangeVertexPartitions: Duplicate entry " + tmpWorkerInfo);
      }
    }
    if (myDependencyWorkerSet.isEmpty() && getPartitionStore().isEmpty()) {
      if (LOG.isInfoEnabled()) {
        LOG.info("exchangeVertexPartitions: Nothing to exchange, " +
            "exiting early");
      }
      return;
    }

    String vertexExchangePath =
        getPartitionExchangePath(getApplicationAttempt(), getSuperstep());
    List<String> workerDoneList;
    try {
      while (true) {
        workerDoneList = getZkExt().getChildrenExt(
            vertexExchangePath, true, false, false);
        workerIdSet.removeAll(workerDoneList);
        if (workerIdSet.isEmpty()) {
          break;
        }
        if (LOG.isInfoEnabled()) {
          LOG.info("exchangeVertexPartitions: Waiting for workers " +
              workerIdSet);
        }
        getPartitionExchangeChildrenChangedEvent().waitForever();
        getPartitionExchangeChildrenChangedEvent().reset();
      }
    } catch (KeeperException | InterruptedException e) {
      throw new RuntimeException(
          "exchangeVertexPartitions: Got runtime exception", e);
    }

    if (LOG.isInfoEnabled()) {
      LOG.info("exchangeVertexPartitions: Done with exchange.");
    }
  }

  
  public final BspEvent getPartitionExchangeChildrenChangedEvent() {
    return partitionExchangeChildrenChanged;
  }

  @Override
  protected boolean processEvent(WatchedEvent event) {
    boolean foundEvent = false;
    if (event.getPath().startsWith(masterJobStatePath) &&
        (event.getType() == EventType.NodeChildrenChanged)) {
      if (LOG.isInfoEnabled()) {
        LOG.info("processEvent: Job state changed, checking " +
            "to see if it needs to restart");
      }
      JSONObject jsonObj = getJobState();
      
      
      if (getConfiguration().isPureYarnJob() && null == jsonObj) {
        LOG.error("BspServiceWorker#getJobState() came back NULL.");
        return false; 
      }
      try {
        if ((ApplicationState.valueOf(jsonObj.getString(JSONOBJ_STATE_KEY)) ==
            ApplicationState.START_SUPERSTEP) &&
            jsonObj.getLong(JSONOBJ_APPLICATION_ATTEMPT_KEY) !=
            getApplicationAttempt()) {
          LOG.fatal("processEvent: Worker will restart " +
              "from command - " + jsonObj.toString());
          System.exit(-1);
        }
      } catch (JSONException e) {
        throw new RuntimeException(
            "processEvent: Couldn't properly get job state from " +
                jsonObj.toString());
      }
      foundEvent = true;
    } else if (event.getPath().contains(PARTITION_EXCHANGE_DIR) &&
        event.getType() == EventType.NodeChildrenChanged) {
      if (LOG.isInfoEnabled()) {
        LOG.info("processEvent : partitionExchangeChildrenChanged " +
            "(at least one worker is done sending partitions)");
      }
      partitionExchangeChildrenChanged.signal();
      foundEvent = true;
    }

    return foundEvent;
  }

  @Override
  public WorkerInfo getWorkerInfo() {
    return workerInfo;
  }

  @Override
  public PartitionStore<I, V, E> getPartitionStore() {
    return getServerData().getPartitionStore();
  }

  @Override
  public PartitionOwner getVertexPartitionOwner(I vertexId) {
    return workerGraphPartitioner.getPartitionOwner(vertexId);
  }

  @Override
  public Iterable<? extends PartitionOwner> getPartitionOwners() {
    return workerGraphPartitioner.getPartitionOwners();
  }

  @Override
  public int getPartitionId(I vertexId) {
    PartitionOwner partitionOwner = getVertexPartitionOwner(vertexId);
    return partitionOwner.getPartitionId();
  }

  @Override
  public boolean hasPartition(Integer partitionId) {
    return getPartitionStore().hasPartition(partitionId);
  }

  @Override
  public ServerData<I, V, E> getServerData() {
    return workerServer.getServerData();
  }


  @Override
  public WorkerAggregatorHandler getAggregatorHandler() {
    return globalCommHandler;
  }

  @Override
  public void prepareSuperstep() {
    if (getSuperstep() != INPUT_SUPERSTEP) {
      globalCommHandler.prepareSuperstep(workerAggregatorRequestProcessor);
    }
  }

  @Override
  public SuperstepOutput<I, V, E> getSuperstepOutput() {
    return superstepOutput;
  }

  @Override
  public GlobalStats getGlobalStats() {
    GlobalStats globalStats = new GlobalStats();
    if (getSuperstep() > Math.max(INPUT_SUPERSTEP, getRestartedSuperstep())) {
      String superstepFinishedNode =
          getSuperstepFinishedPath(getApplicationAttempt(),
              getSuperstep() - 1);
      WritableUtils.readFieldsFromZnode(
          getZkExt(), superstepFinishedNode, false, null,
          globalStats);
    }
    return globalStats;
  }

  @Override
  public int getNumPartitionsOwned() {
    int count = 0;
    for (PartitionOwner partitionOwner : getPartitionOwners()) {
      if (partitionOwner.getWorkerInfo().equals(getWorkerInfo())) {
        count++;
      }
    }
    return count;
  }
}

<code block>


package org.apache.giraph.conf;

import io.netty.buffer.ByteBufAllocator;
import io.netty.buffer.PooledByteBufAllocator;
import io.netty.buffer.UnpooledByteBufAllocator;

import java.net.UnknownHostException;

import org.apache.giraph.aggregators.AggregatorWriter;
import org.apache.giraph.bsp.checkpoints.CheckpointSupportedChecker;
import org.apache.giraph.combiner.MessageCombiner;
import org.apache.giraph.edge.OutEdges;
import org.apache.giraph.edge.ReuseObjectsOutEdges;
import org.apache.giraph.factories.ComputationFactory;
import org.apache.giraph.factories.VertexValueFactory;
import org.apache.giraph.graph.Computation;
import org.apache.giraph.graph.Vertex;
import org.apache.giraph.graph.VertexResolver;
import org.apache.giraph.graph.VertexValueCombiner;
import org.apache.giraph.io.EdgeInputFormat;
import org.apache.giraph.io.EdgeOutputFormat;
import org.apache.giraph.io.MappingInputFormat;
import org.apache.giraph.io.VertexInputFormat;
import org.apache.giraph.io.VertexOutputFormat;
import org.apache.giraph.io.filters.EdgeInputFilter;
import org.apache.giraph.io.filters.VertexInputFilter;
import org.apache.giraph.job.GiraphJobObserver;
import org.apache.giraph.job.GiraphJobRetryChecker;
import org.apache.giraph.master.MasterCompute;
import org.apache.giraph.master.MasterObserver;
import org.apache.giraph.partition.GraphPartitionerFactory;
import org.apache.giraph.partition.Partition;
import org.apache.giraph.partition.ReusesObjectsPartition;
import org.apache.giraph.utils.ReflectionUtils;
import org.apache.giraph.worker.WorkerContext;
import org.apache.giraph.worker.WorkerObserver;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.net.DNS;


public class GiraphConfiguration extends Configuration
    implements GiraphConstants {
  
  private ByteBufAllocator nettyBufferAllocator = null;

  
  public GiraphConfiguration() {
    configureHadoopSecurity();
  }

  
  public GiraphConfiguration(Configuration conf) {
    super(conf);
    configureHadoopSecurity();
  }

  
  public String getComputationName() {
    ComputationFactory compFactory = ReflectionUtils.newInstance(
        getComputationFactoryClass());
    return compFactory.computationName(this);
  }

  
  public Class<? extends ComputationFactory> getComputationFactoryClass() {
    return COMPUTATION_FACTORY_CLASS.get(this);
  }

  
  public Class<? extends Computation> getComputationClass() {
    return COMPUTATION_CLASS.get(this);
  }

  
  public void setComputationClass(
      Class<? extends Computation> computationClass) {
    COMPUTATION_CLASS.set(this, computationClass);
  }

  
  public final void setVertexValueFactoryClass(
      Class<? extends VertexValueFactory> vertexValueFactoryClass) {
    VERTEX_VALUE_FACTORY_CLASS.set(this, vertexValueFactoryClass);
  }

  
  public void setEdgeInputFilterClass(
      Class<? extends EdgeInputFilter> edgeFilterClass) {
    EDGE_INPUT_FILTER_CLASS.set(this, edgeFilterClass);
  }

  
  public void setVertexInputFilterClass(
      Class<? extends VertexInputFilter> vertexFilterClass) {
    VERTEX_INPUT_FILTER_CLASS.set(this, vertexFilterClass);
  }

  
  public Class<? extends OutEdges> getOutEdgesClass() {
    return VERTEX_EDGES_CLASS.get(this);
  }

  
  public final void setOutEdgesClass(
      Class<? extends OutEdges> outEdgesClass) {
    VERTEX_EDGES_CLASS.set(this, outEdgesClass);
  }

  
  public final void setVertexClass(Class<? extends Vertex> vertexClass) {
    VERTEX_CLASS.set(this, vertexClass);
  }


  
  public final void setInputOutEdgesClass(
      Class<? extends OutEdges> inputOutEdgesClass) {
    INPUT_VERTEX_EDGES_CLASS.set(this, inputOutEdgesClass);
  }

  
  public boolean reuseEdgeObjects() {
    return ReuseObjectsOutEdges.class.isAssignableFrom(
        getOutEdgesClass());
  }

  
  public boolean reuseVertexObjects() {
    return ReusesObjectsPartition.class.isAssignableFrom(getPartitionClass());
  }

  
  public Class<? extends Partition> getPartitionClass() {
    return PARTITION_CLASS.get(this);
  }

  
  public boolean hasVertexInputFormat() {
    return VERTEX_INPUT_FORMAT_CLASS.get(this) != null;
  }

  
  public void setVertexInputFormatClass(
      Class<? extends VertexInputFormat> vertexInputFormatClass) {
    VERTEX_INPUT_FORMAT_CLASS.set(this, vertexInputFormatClass);
  }

  
  public boolean hasEdgeInputFormat() {
    return EDGE_INPUT_FORMAT_CLASS.get(this) != null;
  }

  
  public void setEdgeInputFormatClass(
      Class<? extends EdgeInputFormat> edgeInputFormatClass) {
    EDGE_INPUT_FORMAT_CLASS.set(this, edgeInputFormatClass);
  }

  
  public void setMappingInputFormatClass(
    Class<? extends MappingInputFormat> mappingInputFormatClass) {
    MAPPING_INPUT_FORMAT_CLASS.set(this, mappingInputFormatClass);
  }

  
  public final void setMasterComputeClass(
      Class<? extends MasterCompute> masterComputeClass) {
    MASTER_COMPUTE_CLASS.set(this, masterComputeClass);
  }

  
  public final void addMasterObserverClass(
      Class<? extends MasterObserver> masterObserverClass) {
    MASTER_OBSERVER_CLASSES.add(this, masterObserverClass);
  }

  
  public final void addWorkerObserverClass(
      Class<? extends WorkerObserver> workerObserverClass) {
    WORKER_OBSERVER_CLASSES.add(this, workerObserverClass);
  }

  
  public Class<? extends GiraphJobObserver> getJobObserverClass() {
    return JOB_OBSERVER_CLASS.get(this);
  }

  
  public void setJobObserverClass(Class<? extends GiraphJobObserver> klass) {
    JOB_OBSERVER_CLASS.set(this, klass);
  }

  
  public Class<? extends GiraphJobRetryChecker> getJobRetryCheckerClass() {
    return JOB_RETRY_CHECKER_CLASS.get(this);
  }

  
  public void setJobRetryCheckerClass(
      Class<? extends GiraphJobRetryChecker> klass) {
    JOB_RETRY_CHECKER_CLASS.set(this, klass);
  }

  
  public boolean isJMapHistogramDumpEnabled() {
    return JMAP_ENABLE.get(this);
  }

  
  public boolean isReactiveJmapHistogramDumpEnabled() {
    return REACTIVE_JMAP_ENABLE.get(this);
  }

  
  public final void setClasses(String name, Class<?> xface,
                               Class<?> ... klasses) {
    String[] klassNames = new String[klasses.length];
    for (int i = 0; i < klasses.length; ++i) {
      Class<?> klass = klasses[i];
      if (!xface.isAssignableFrom(klass)) {
        throw new RuntimeException(klass + " does not implement " +
            xface.getName());
      }
      klassNames[i] = klasses[i].getName();
    }
    setStrings(name, klassNames);
  }

  
  public boolean hasVertexOutputFormat() {
    return VERTEX_OUTPUT_FORMAT_CLASS.get(this) != null;
  }

  
  public final void setVertexOutputFormatClass(
      Class<? extends VertexOutputFormat> vertexOutputFormatClass) {
    VERTEX_OUTPUT_FORMAT_CLASS.set(this, vertexOutputFormatClass);
  }


  
  public boolean hasVertexOutputFormatSubdir() {
    return !VERTEX_OUTPUT_FORMAT_SUBDIR.get(this).isEmpty();
  }

  
  public final void setVertexOutputFormatSubdir(String path) {
    VERTEX_OUTPUT_FORMAT_SUBDIR.set(this, path);
  }

  
  public final boolean doOutputDuringComputation() {
    return DO_OUTPUT_DURING_COMPUTATION.get(this);
  }

  
  public final void setDoOutputDuringComputation(
      boolean doOutputDuringComputation) {
    DO_OUTPUT_DURING_COMPUTATION.set(this, doOutputDuringComputation);
  }

  
  public final boolean vertexOutputFormatThreadSafe() {
    return VERTEX_OUTPUT_FORMAT_THREAD_SAFE.get(this);
  }

  
  public final void setVertexOutputFormatThreadSafe(
      boolean vertexOutputFormatThreadSafe) {
    VERTEX_OUTPUT_FORMAT_THREAD_SAFE.set(this, vertexOutputFormatThreadSafe);
  }

  
  public boolean hasEdgeOutputFormat() {
    return EDGE_OUTPUT_FORMAT_CLASS.get(this) != null;
  }

  
  public final void setEdgeOutputFormatClass(
      Class<? extends EdgeOutputFormat> edgeOutputFormatClass) {
    EDGE_OUTPUT_FORMAT_CLASS.set(this, edgeOutputFormatClass);
  }

  
  public boolean hasEdgeOutputFormatSubdir() {
    return !EDGE_OUTPUT_FORMAT_SUBDIR.get(this).isEmpty();
  }

  
  public final void setEdgeOutputFormatSubdir(String path) {
    EDGE_OUTPUT_FORMAT_SUBDIR.set(this, path);
  }

  
  public final int getNumOutputThreads() {
    if (!vertexOutputFormatThreadSafe()) {
      return 1;
    } else {
      return NUM_OUTPUT_THREADS.get(this);
    }
  }

  
  public void setNumOutputThreads(int numOutputThreads) {
    NUM_OUTPUT_THREADS.set(this, numOutputThreads);
  }

  
  public void setMessageCombinerClass(
      Class<? extends MessageCombiner> messageCombinerClass) {
    MESSAGE_COMBINER_CLASS.set(this, messageCombinerClass);
  }

  
  public final void setGraphPartitionerFactoryClass(
      Class<? extends GraphPartitionerFactory> graphPartitionerFactoryClass) {
    GRAPH_PARTITIONER_FACTORY_CLASS.set(this, graphPartitionerFactoryClass);
  }

  
  public final void setVertexResolverClass(
      Class<? extends VertexResolver> vertexResolverClass) {
    VERTEX_RESOLVER_CLASS.set(this, vertexResolverClass);
  }

  
  public final boolean getResolverCreateVertexOnMessages() {
    return RESOLVER_CREATE_VERTEX_ON_MSGS.get(this);
  }

  
  public final void setResolverCreateVertexOnMessages(boolean v) {
    RESOLVER_CREATE_VERTEX_ON_MSGS.set(this, v);
  }

  
  public final void setVertexValueCombinerClass(
      Class<? extends VertexValueCombiner> vertexValueCombinerClass) {
    VERTEX_VALUE_COMBINER_CLASS.set(this, vertexValueCombinerClass);
  }

  
  public final void setWorkerContextClass(
      Class<? extends WorkerContext> workerContextClass) {
    WORKER_CONTEXT_CLASS.set(this, workerContextClass);
  }

  
  public final void setAggregatorWriterClass(
      Class<? extends AggregatorWriter> aggregatorWriterClass) {
    AGGREGATOR_WRITER_CLASS.set(this, aggregatorWriterClass);
  }

  
  public final void setPartitionClass(
      Class<? extends Partition> partitionClass) {
    PARTITION_CLASS.set(this, partitionClass);
  }

  
  public final void setWorkerConfiguration(int minWorkers,
                                           int maxWorkers,
                                           float minPercentResponded) {
    setInt(MIN_WORKERS, minWorkers);
    setInt(MAX_WORKERS, maxWorkers);
    MIN_PERCENT_RESPONDED.set(this, minPercentResponded);
  }

  public final int getMinWorkers() {
    return getInt(MIN_WORKERS, -1);
  }

  public final int getMaxWorkers() {
    return getInt(MAX_WORKERS, -1);
  }

  public final float getMinPercentResponded() {
    return MIN_PERCENT_RESPONDED.get(this);
  }

  
  public final void setZooKeeperConfiguration(String serverList) {
    ZOOKEEPER_LIST.set(this, serverList);
  }

  
  public final boolean getSplitMasterWorker() {
    return SPLIT_MASTER_WORKER.get(this);
  }

  
  public Class<? extends MasterObserver>[] getMasterObserverClasses() {
    return MASTER_OBSERVER_CLASSES.getArray(this);
  }

  
  public Class<? extends WorkerObserver>[] getWorkerObserverClasses() {
    return WORKER_OBSERVER_CLASSES.getArray(this);
  }

  
  public boolean metricsEnabled() {
    return METRICS_ENABLE.isTrue(this);
  }

  
  public int getTaskPartition() {
    return getInt("mapred.task.partition", -1);
  }

  
  public boolean isPureYarnJob() {
    return IS_PURE_YARN_JOB.get(this);
  }

  
  public String getYarnLibJars() {
    return GIRAPH_YARN_LIBJARS.get(this);
  }

  
  public void setYarnLibJars(String jarList) {
    GIRAPH_YARN_LIBJARS.set(this, jarList);
  }

  
  public int getYarnTaskHeapMb() {
    return GIRAPH_YARN_TASK_HEAP_MB.get(this);
  }

  
  public void setYarnTaskHeapMb(int heapMb) {
    GIRAPH_YARN_TASK_HEAP_MB.set(this, heapMb);
  }

  
  public String getZookeeperList() {
    return ZOOKEEPER_LIST.get(this);
  }

  
  public void setZookeeperList(String zkList) {
    ZOOKEEPER_LIST.set(this, zkList);
    ZOOKEEPER_IS_EXTERNAL.set(this, false);
  }

  
  public boolean isZookeeperExternal() {
    return ZOOKEEPER_IS_EXTERNAL.get(this);
  }

  public String getLocalLevel() {
    return LOG_LEVEL.get(this);
  }

  
  public boolean useLogThreadLayout() {
    return LOG_THREAD_LAYOUT.get(this);
  }

  
  public boolean getLocalTestMode() {
    return LOCAL_TEST_MODE.get(this);
  }

  
  public void setLocalTestMode(boolean flag) {
    LOCAL_TEST_MODE.set(this, flag);
  }

  
  public int getZooKeeperServerCount() {
    return ZOOKEEPER_SERVER_COUNT.get(this);
  }

  public int getZooKeeperSessionTimeout() {
    return ZOOKEEPER_SESSION_TIMEOUT.get(this);
  }

  public int getZookeeperOpsMaxAttempts() {
    return ZOOKEEPER_OPS_MAX_ATTEMPTS.get(this);
  }

  public int getZookeeperOpsRetryWaitMsecs() {
    return ZOOKEEPER_OPS_RETRY_WAIT_MSECS.get(this);
  }

  public boolean getNettyServerUseExecutionHandler() {
    return NETTY_SERVER_USE_EXECUTION_HANDLER.get(this);
  }

  public int getNettyServerThreads() {
    return NETTY_SERVER_THREADS.get(this);
  }

  public int getNettyServerExecutionThreads() {
    return NETTY_SERVER_EXECUTION_THREADS.get(this);
  }

  
  public int getNettyServerExecutionConcurrency() {
    if (getNettyServerUseExecutionHandler()) {
      return getNettyServerExecutionThreads();
    } else {
      return getNettyServerThreads();
    }
  }

  
  public ByteBufAllocator getNettyAllocator() {
    if (nettyBufferAllocator == null) {
      if (NETTY_USE_POOLED_ALLOCATOR.get(this)) { 
        nettyBufferAllocator = new PooledByteBufAllocator(
          NETTY_USE_DIRECT_MEMORY.get(this));
      } else { 
        
        nettyBufferAllocator = new UnpooledByteBufAllocator(
            NETTY_USE_DIRECT_MEMORY.get(this));
      }
    }
    return nettyBufferAllocator;
  }

  public int getZookeeperConnectionAttempts() {
    return ZOOKEEPER_CONNECTION_ATTEMPTS.get(this);
  }

  public int getZooKeeperMinSessionTimeout() {
    return ZOOKEEPER_MIN_SESSION_TIMEOUT.get(this);
  }

  public int getZooKeeperMaxSessionTimeout() {
    return ZOOKEEPER_MAX_SESSION_TIMEOUT.get(this);
  }

  public boolean getZooKeeperForceSync() {
    return ZOOKEEPER_FORCE_SYNC.get(this);
  }

  public boolean getZooKeeperSkipAcl() {
    return ZOOKEEPER_SKIP_ACL.get(this);
  }

  
  public int getMapTasks() {
    int mapTasks = getInt("mapred.map.tasks", -1);
    if (mapTasks == -1) {
      throw new IllegalStateException("getMapTasks: Failed to get the map " +
          "tasks!");
    }
    return mapTasks;
  }

  
  public boolean authenticate() {
    return AUTHENTICATE.get(this);
  }

  
  public void setNumComputeThreads(int numComputeThreads) {
    NUM_COMPUTE_THREADS.set(this, numComputeThreads);
  }

  public int getNumComputeThreads() {
    return NUM_COMPUTE_THREADS.get(this);
  }

  public int getNumOocThreads() {
    return NUM_OOC_THREADS.get(this);
  }

  
  public void setNumInputSplitsThreads(int numInputSplitsThreads) {
    NUM_INPUT_THREADS.set(this, numInputSplitsThreads);
  }

  public int getNumInputSplitsThreads() {
    return NUM_INPUT_THREADS.get(this);
  }

  public long getInputSplitMaxVertices() {
    return INPUT_SPLIT_MAX_VERTICES.get(this);
  }

  public long getInputSplitMaxEdges() {
    return INPUT_SPLIT_MAX_EDGES.get(this);
  }

  
  public void useUnsafeSerialization(boolean useUnsafeSerialization) {
    USE_UNSAFE_SERIALIZATION.set(this, useUnsafeSerialization);
  }

  
  public boolean useMessageSizeEncoding() {
    return USE_MESSAGE_SIZE_ENCODING.get(this);
  }

  
  public void setCheckpointFrequency(int checkpointFrequency) {
    CHECKPOINT_FREQUENCY.set(this, checkpointFrequency);
  }

  
  public int getCheckpointFrequency() {
    return CHECKPOINT_FREQUENCY.get(this);
  }

  
  public boolean useCheckpointing() {
    return getCheckpointFrequency() != 0;
  }

  
  public void setCheckpointSupportedChecker(
      Class<? extends CheckpointSupportedChecker> clazz) {
    GiraphConstants.CHECKPOINT_SUPPORTED_CHECKER.set(this, clazz);
  }

  
  public void setMaxTaskAttempts(int maxTaskAttempts) {
    MAX_TASK_ATTEMPTS.set(this, maxTaskAttempts);
  }

  
  public int getMaxTaskAttempts() {
    return MAX_TASK_ATTEMPTS.get(this);
  }

  
  public int getEventWaitMsecs() {
    return EVENT_WAIT_MSECS.get(this);
  }

  
  public void setEventWaitMsecs(int eventWaitMsecs) {
    EVENT_WAIT_MSECS.set(this, eventWaitMsecs);
  }

  
  public int getMaxMasterSuperstepWaitMsecs() {
    return MAX_MASTER_SUPERSTEP_WAIT_MSECS.get(this);
  }

  
  public void setMaxMasterSuperstepWaitMsecs(int maxMasterSuperstepWaitMsecs) {
    MAX_MASTER_SUPERSTEP_WAIT_MSECS.set(this, maxMasterSuperstepWaitMsecs);
  }

  
  public void configureHadoopSecurity() {
    String hadoopTokenFilePath = System.getenv("HADOOP_TOKEN_FILE_LOCATION");
    if (hadoopTokenFilePath != null) {
      set("mapreduce.job.credentials.binary", hadoopTokenFilePath);
    }
  }

  
  public boolean useInputSplitLocality() {
    return USE_INPUT_SPLIT_LOCALITY.get(this);
  }

  
  public String getLocalHostname() throws UnknownHostException {
    return DNS.getDefaultHost(
        GiraphConstants.DNS_INTERFACE.get(this),
        GiraphConstants.DNS_NAMESERVER.get(this)).toLowerCase();
  }

  
  public void setMaxNumberOfSupersteps(int maxNumberOfSupersteps) {
    MAX_NUMBER_OF_SUPERSTEPS.set(this, maxNumberOfSupersteps);
  }

  
  public int getMaxNumberOfSupersteps() {
    return MAX_NUMBER_OF_SUPERSTEPS.get(this);
  }

  
  public boolean isStaticGraph() {
    return STATIC_GRAPH.isTrue(this);
  }

  
  public String getYourKitOutputDir(Mapper.Context context) {
    final String cacheKey = "giraph.yourkit.outputDirCached";
    String outputDir = get(cacheKey);
    if (outputDir == null) {
      outputDir = getStringVars(YOURKIT_OUTPUT_DIR, YOURKIT_OUTPUT_DIR_DEFAULT,
          context);
      set(cacheKey, outputDir);
    }
    return outputDir;
  }

  
  public String getStringVars(String key, Mapper.Context context) {
    return getStringVars(key, null, context);
  }

  
  public String getStringVars(String key, String defaultValue,
                              Mapper.Context context) {
    String value = get(key);
    if (value == null) {
      if (defaultValue == null) {
        return null;
      }
      value = defaultValue;
    }
    value = value.replace("%JOB_ID%", context.getJobID().toString());
    value = value.replace("%TASK_ID%", context.getTaskAttemptID().toString());
    value = value.replace("%USER%", get("user.name", "unknown_user"));
    return value;
  }

  
  public boolean getCreateSourceVertex() {
    return CREATE_EDGE_SOURCE_VERTICES.get(this);
  }

  
  public void setCreateSourceVertex(boolean createVertex) {
    CREATE_EDGE_SOURCE_VERTICES.set(this, createVertex);
  }

  
  public int getWaitTaskDoneTimeoutMs() {
    return WAIT_TASK_DONE_TIMEOUT_MS.get(this);
  }

  
  public void setWaitTaskDoneTimeoutMs(int ms) {
    WAIT_TASK_DONE_TIMEOUT_MS.set(this, ms);
  }

  
  public boolean trackJobProgressOnClient() {
    return TRACK_JOB_PROGRESS_ON_CLIENT.get(this);
  }

  
  public int getHdfsFileCreationRetries() {
    return HDFS_FILE_CREATION_RETRIES.get(this);
  }

  
  public int getHdfsFileCreationRetryWaitMs() {
    return HDFS_FILE_CREATION_RETRY_WAIT_MS.get(this);
  }
}

<code block>

package org.apache.giraph.conf;

import static java.util.concurrent.TimeUnit.MINUTES;
import static java.util.concurrent.TimeUnit.SECONDS;

import org.apache.giraph.aggregators.AggregatorWriter;
import org.apache.giraph.aggregators.TextAggregatorWriter;
import org.apache.giraph.bsp.BspOutputFormat;
import org.apache.giraph.bsp.checkpoints.CheckpointSupportedChecker;
import org.apache.giraph.bsp.checkpoints.DefaultCheckpointSupportedChecker;
import org.apache.giraph.combiner.MessageCombiner;
import org.apache.giraph.comm.messages.InMemoryMessageStoreFactory;
import org.apache.giraph.comm.messages.MessageEncodeAndStoreType;
import org.apache.giraph.comm.messages.MessageStoreFactory;
import org.apache.giraph.edge.ByteArrayEdges;
import org.apache.giraph.edge.EdgeStoreFactory;
import org.apache.giraph.edge.InMemoryEdgeStoreFactory;
import org.apache.giraph.edge.OutEdges;
import org.apache.giraph.factories.ComputationFactory;
import org.apache.giraph.factories.DefaultComputationFactory;
import org.apache.giraph.factories.DefaultEdgeValueFactory;
import org.apache.giraph.factories.DefaultMessageValueFactory;
import org.apache.giraph.factories.DefaultVertexIdFactory;
import org.apache.giraph.factories.DefaultVertexValueFactory;
import org.apache.giraph.factories.EdgeValueFactory;
import org.apache.giraph.factories.MessageValueFactory;
import org.apache.giraph.factories.VertexIdFactory;
import org.apache.giraph.factories.VertexValueFactory;
import org.apache.giraph.graph.Computation;
import org.apache.giraph.graph.DefaultVertex;
import org.apache.giraph.graph.DefaultVertexResolver;
import org.apache.giraph.graph.DefaultVertexValueCombiner;
import org.apache.giraph.graph.Language;
import org.apache.giraph.graph.Vertex;
import org.apache.giraph.graph.VertexResolver;
import org.apache.giraph.graph.VertexValueCombiner;
import org.apache.giraph.io.EdgeInputFormat;
import org.apache.giraph.io.EdgeOutputFormat;
import org.apache.giraph.io.MappingInputFormat;
import org.apache.giraph.io.VertexInputFormat;
import org.apache.giraph.io.VertexOutputFormat;
import org.apache.giraph.io.filters.DefaultEdgeInputFilter;
import org.apache.giraph.io.filters.DefaultVertexInputFilter;
import org.apache.giraph.io.filters.EdgeInputFilter;
import org.apache.giraph.io.filters.VertexInputFilter;
import org.apache.giraph.job.DefaultGiraphJobRetryChecker;
import org.apache.giraph.job.DefaultJobObserver;
import org.apache.giraph.job.GiraphJobObserver;
import org.apache.giraph.job.GiraphJobRetryChecker;
import org.apache.giraph.job.HaltApplicationUtils;
import org.apache.giraph.mapping.MappingStore;
import org.apache.giraph.mapping.MappingStoreOps;
import org.apache.giraph.mapping.translate.TranslateEdge;
import org.apache.giraph.master.DefaultMasterCompute;
import org.apache.giraph.master.MasterCompute;
import org.apache.giraph.master.MasterObserver;
import org.apache.giraph.ooc.JVMMemoryEstimator;
import org.apache.giraph.ooc.MemoryEstimator;
import org.apache.giraph.partition.GraphPartitionerFactory;
import org.apache.giraph.partition.HashPartitionerFactory;
import org.apache.giraph.partition.Partition;
import org.apache.giraph.partition.SimplePartition;
import org.apache.giraph.worker.DefaultWorkerContext;
import org.apache.giraph.worker.WorkerContext;
import org.apache.giraph.worker.WorkerObserver;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.mapreduce.OutputFormat;



public interface GiraphConstants {
  
  int ONE_KB = 1024;
  
  int ONE_MB = 1024 * 1024;

  
  ClassConfOption<MappingStore> MAPPING_STORE_CLASS =
      ClassConfOption.create("giraph.mappingStoreClass", null,
          MappingStore.class, "MappingStore Class");

  
  ClassConfOption<MappingStoreOps> MAPPING_STORE_OPS_CLASS =
      ClassConfOption.create("giraph.mappingStoreOpsClass", null,
          MappingStoreOps.class, "MappingStoreOps class");

  
  IntConfOption LB_MAPPINGSTORE_UPPER =
      new IntConfOption("giraph.lbMappingStoreUpper", -1,
          "'upper' value used by lbmappingstore");
  
  IntConfOption LB_MAPPINGSTORE_LOWER =
      new IntConfOption("giraph.lbMappingStoreLower", -1,
          "'lower' value used by lbMappingstore");
  
  ClassConfOption EDGE_TRANSLATION_CLASS =
      ClassConfOption.create("giraph.edgeTranslationClass", null,
          TranslateEdge.class, "Class used to conduct expensive edge " +
              "translation during vertex input phase");

  
  ClassConfOption<Computation> COMPUTATION_CLASS =
      ClassConfOption.create("giraph.computationClass", null,
          Computation.class, "Computation class - required");
  
  ClassConfOption<ComputationFactory> COMPUTATION_FACTORY_CLASS =
      ClassConfOption.create("giraph.computation.factory.class",
          DefaultComputationFactory.class, ComputationFactory.class,
          "Computation factory class - optional");

  
  ClassConfOption<TypesHolder> TYPES_HOLDER_CLASS =
      ClassConfOption.create("giraph.typesHolder", null,
          TypesHolder.class,
          "TypesHolder, used if Computation not set - optional");

  
  ClassConfOption<EdgeStoreFactory> EDGE_STORE_FACTORY_CLASS =
      ClassConfOption.create("giraph.edgeStoreFactoryClass",
          InMemoryEdgeStoreFactory.class,
          EdgeStoreFactory.class,
          "Edge Store Factory class to use for creating edgeStore");

  
  ClassConfOption<MessageStoreFactory> MESSAGE_STORE_FACTORY_CLASS =
      ClassConfOption.create("giraph.messageStoreFactoryClass",
          InMemoryMessageStoreFactory.class,
          MessageStoreFactory.class,
          "Message Store Factory Class that is to be used");

  
  PerGraphTypeEnumConfOption<Language> GRAPH_TYPE_LANGUAGES =
      PerGraphTypeEnumConfOption.create("giraph.types.language",
          Language.class, Language.JAVA,
          "Language user graph types (IVEMM) are implemented in");

  
  PerGraphTypeBooleanConfOption GRAPH_TYPES_NEEDS_WRAPPERS =
      new PerGraphTypeBooleanConfOption("giraph.jython.type.wrappers",
          false, "Whether user graph types (IVEMM) need Jython wrappers");

  
  ClassConfOption<VertexIdFactory> VERTEX_ID_FACTORY_CLASS =
      ClassConfOption.create("giraph.vertexIdFactoryClass",
          DefaultVertexIdFactory.class, VertexIdFactory.class,
          "Vertex ID factory class - optional");
  
  ClassConfOption<VertexValueFactory> VERTEX_VALUE_FACTORY_CLASS =
      ClassConfOption.create("giraph.vertexValueFactoryClass",
          DefaultVertexValueFactory.class, VertexValueFactory.class,
          "Vertex value factory class - optional");
  
  ClassConfOption<EdgeValueFactory> EDGE_VALUE_FACTORY_CLASS =
      ClassConfOption.create("giraph.edgeValueFactoryClass",
          DefaultEdgeValueFactory.class, EdgeValueFactory.class,
          "Edge value factory class - optional");
  
  ClassConfOption<MessageValueFactory>
  OUTGOING_MESSAGE_VALUE_FACTORY_CLASS =
      ClassConfOption.create("giraph.outgoingMessageValueFactoryClass",
          DefaultMessageValueFactory.class, MessageValueFactory.class,
          "Outgoing message value factory class - optional");

  
  ClassConfOption<OutEdges> VERTEX_EDGES_CLASS =
      ClassConfOption.create("giraph.outEdgesClass", ByteArrayEdges.class,
          OutEdges.class, "Vertex edges class - optional");
  
  ClassConfOption<OutEdges> INPUT_VERTEX_EDGES_CLASS =
      ClassConfOption.create("giraph.inputOutEdgesClass",
          ByteArrayEdges.class, OutEdges.class,
          "Vertex edges class to be used during edge input only - optional");

  
  ClassConfOption<MasterCompute> MASTER_COMPUTE_CLASS =
      ClassConfOption.create("giraph.masterComputeClass",
          DefaultMasterCompute.class, MasterCompute.class,
          "Class for Master - optional");
  
  ClassConfOption<MasterObserver> MASTER_OBSERVER_CLASSES =
      ClassConfOption.create("giraph.master.observers",
          null, MasterObserver.class, "Classes for Master Observer - optional");
  
  ClassConfOption<WorkerObserver> WORKER_OBSERVER_CLASSES =
      ClassConfOption.create("giraph.worker.observers", null,
          WorkerObserver.class, "Classes for Worker Observer - optional");
  
  ClassConfOption<MessageCombiner> MESSAGE_COMBINER_CLASS =
      ClassConfOption.create("giraph.messageCombinerClass", null,
          MessageCombiner.class, "Message combiner class - optional");
  
  ClassConfOption<VertexResolver> VERTEX_RESOLVER_CLASS =
      ClassConfOption.create("giraph.vertexResolverClass",
          DefaultVertexResolver.class, VertexResolver.class,
          "Vertex resolver class - optional");
  
  ClassConfOption<VertexValueCombiner> VERTEX_VALUE_COMBINER_CLASS =
      ClassConfOption.create("giraph.vertexValueCombinerClass",
          DefaultVertexValueCombiner.class, VertexValueCombiner.class,
          "Vertex value combiner class - optional");

  
  EnumConfOption<Language> COMPUTATION_LANGUAGE =
      EnumConfOption.create("giraph.computation.language",
          Language.class, Language.JAVA,
          "Which language computation is implemented in");

  
  BooleanConfOption RESOLVER_CREATE_VERTEX_ON_MSGS =
      new BooleanConfOption("giraph.vertex.resolver.create.on.msgs", true,
          "Option of whether to create vertexes that were not existent " +
          "before but received messages");
  
  ClassConfOption<GraphPartitionerFactory> GRAPH_PARTITIONER_FACTORY_CLASS =
      ClassConfOption.create("giraph.graphPartitionerFactoryClass",
          HashPartitionerFactory.class, GraphPartitionerFactory.class,
          "Graph partitioner factory class - optional");

  
  ClassConfOption<GiraphJobObserver> JOB_OBSERVER_CLASS =
      ClassConfOption.create("giraph.jobObserverClass",
          DefaultJobObserver.class, GiraphJobObserver.class,
          "Observer class to watch over job status - optional");

  
  ClassConfOption<GiraphJobRetryChecker> JOB_RETRY_CHECKER_CLASS =
      ClassConfOption.create("giraph.jobRetryCheckerClass",
          DefaultGiraphJobRetryChecker.class, GiraphJobRetryChecker.class,
          "Class which decides whether a failed job should be retried - " +
              "optional");

  
  LongConfOption MAX_ALLOWED_JOB_TIME_MS =
      new LongConfOption("giraph.maxAllowedJobTimeMilliseconds", -1,
          "Maximum allowed time for job to run after getting all resources " +
              "before it will be killed, in milliseconds " +
              "(-1 if it has no limit)");

  
  
  ClassConfOption<VertexInputFormat> VERTEX_INPUT_FORMAT_CLASS =
      ClassConfOption.create("giraph.vertexInputFormatClass", null,
          VertexInputFormat.class, "VertexInputFormat class (at least " +
          "one of the input format classes is required)");
  
  ClassConfOption<EdgeInputFormat> EDGE_INPUT_FORMAT_CLASS =
      ClassConfOption.create("giraph.edgeInputFormatClass", null,
          EdgeInputFormat.class, "EdgeInputFormat class");
  
  ClassConfOption<MappingInputFormat> MAPPING_INPUT_FORMAT_CLASS =
      ClassConfOption.create("giraph.mappingInputFormatClass", null,
          MappingInputFormat.class, "MappingInputFormat class");

  
  ClassConfOption<EdgeInputFilter> EDGE_INPUT_FILTER_CLASS =
      ClassConfOption.create("giraph.edgeInputFilterClass",
          DefaultEdgeInputFilter.class, EdgeInputFilter.class,
          "EdgeInputFilter class");
  
  ClassConfOption<VertexInputFilter> VERTEX_INPUT_FILTER_CLASS =
      ClassConfOption.create("giraph.vertexInputFilterClass",
          DefaultVertexInputFilter.class, VertexInputFilter.class,
          "VertexInputFilter class");
  
  ClassConfOption<Vertex> VERTEX_CLASS =
      ClassConfOption.create("giraph.vertexClass",
          DefaultVertex.class, Vertex.class,
          "Vertex class");
  
  ClassConfOption<VertexOutputFormat> VERTEX_OUTPUT_FORMAT_CLASS =
      ClassConfOption.create("giraph.vertexOutputFormatClass", null,
          VertexOutputFormat.class, "VertexOutputFormat class");
  
  StrConfOption VERTEX_OUTPUT_FORMAT_SUBDIR =
    new StrConfOption("giraph.vertex.output.subdir", "",
                      "VertexOutputFormat sub-directory");
  
  ClassConfOption<EdgeOutputFormat> EDGE_OUTPUT_FORMAT_CLASS =
      ClassConfOption.create("giraph.edgeOutputFormatClass", null,
          EdgeOutputFormat.class, "EdgeOutputFormat class");
  
  StrConfOption EDGE_OUTPUT_FORMAT_SUBDIR =
    new StrConfOption("giraph.edge.output.subdir", "",
                      "EdgeOutputFormat sub-directory");

  
  StrConfOption GIRAPH_TEXT_OUTPUT_FORMAT_SEPARATOR =
    new StrConfOption("giraph.textoutputformat.separator", "\t",
                      "GiraphTextOuputFormat Separator");
  
  BooleanConfOption GIRAPH_TEXT_OUTPUT_FORMAT_REVERSE =
      new BooleanConfOption("giraph.textoutputformat.reverse", false,
                            "Reverse values in the output");

  
  BooleanConfOption DO_OUTPUT_DURING_COMPUTATION =
      new BooleanConfOption("giraph.doOutputDuringComputation", false,
          "If you use this option, instead of having saving vertices in the " +
          "end of application, saveVertex will be called right after each " +
          "vertex.compute() is called." +
          "NOTE: This feature doesn't work well with checkpointing - if you " +
          "restart from a checkpoint you won't have any ouptut from previous " +
          "supresteps.");
  
  BooleanConfOption VERTEX_OUTPUT_FORMAT_THREAD_SAFE =
      new BooleanConfOption("giraph.vertexOutputFormatThreadSafe", false,
          "Vertex output format thread-safe - if your VertexOutputFormat " +
          "allows several vertexWriters to be created and written to in " +
          "parallel, you should set this to true.");
  
  IntConfOption NUM_OUTPUT_THREADS =
      new IntConfOption("giraph.numOutputThreads", 1,
          "Number of threads for writing output in the end of the application");

  
  StrConfOption GIRAPH_YARN_LIBJARS =
    new StrConfOption("giraph.yarn.libjars", "",
        "conf key for comma-separated list of jars to export to YARN workers");
  
  String GIRAPH_YARN_CONF_FILE = "giraph-conf.xml";
  
  int GIRAPH_YARN_TASK_HEAP_MB_DEFAULT = 1024;
  
  IntConfOption GIRAPH_YARN_TASK_HEAP_MB = new IntConfOption(
    "giraph.yarn.task.heap.mb", GIRAPH_YARN_TASK_HEAP_MB_DEFAULT,
    "Name of Giraph property for user-configurable heap memory per worker");
  
  int GIRAPH_YARN_PRIORITY = 10;
  
  BooleanConfOption IS_PURE_YARN_JOB =
    new BooleanConfOption("giraph.pure.yarn.job", false,
        "Is this a pure YARN job (i.e. no MapReduce layer managing Giraph " +
        "tasks)");

  
  ClassConfOption<WritableComparable> VERTEX_ID_CLASS =
      ClassConfOption.create("giraph.vertexIdClass", null,
          WritableComparable.class, "Vertex index class");
  
  ClassConfOption<Writable> VERTEX_VALUE_CLASS =
      ClassConfOption.create("giraph.vertexValueClass", null, Writable.class,
          "Vertex value class");
  
  ClassConfOption<Writable> EDGE_VALUE_CLASS =
      ClassConfOption.create("giraph.edgeValueClass", null, Writable.class,
          "Edge value class");
  
  ClassConfOption<Writable> OUTGOING_MESSAGE_VALUE_CLASS =
      ClassConfOption.create("giraph.outgoingMessageValueClass", null,
          Writable.class, "Outgoing message value class");
  
  ClassConfOption<WorkerContext> WORKER_CONTEXT_CLASS =
      ClassConfOption.create("giraph.workerContextClass",
          DefaultWorkerContext.class, WorkerContext.class,
          "Worker contextclass");
  
  ClassConfOption<AggregatorWriter> AGGREGATOR_WRITER_CLASS =
      ClassConfOption.create("giraph.aggregatorWriterClass",
          TextAggregatorWriter.class, AggregatorWriter.class,
          "AggregatorWriter class - optional");

  
  ClassConfOption<Partition> PARTITION_CLASS =
      ClassConfOption.create("giraph.partitionClass", SimplePartition.class,
          Partition.class, "Partition class - optional");

  
  String MIN_WORKERS = "giraph.minWorkers";
  
  String MAX_WORKERS = "giraph.maxWorkers";

  
  BooleanConfOption SPLIT_MASTER_WORKER =
      new BooleanConfOption("giraph.SplitMasterWorker", true,
          "Separate the workers and the master tasks.  This is required to " +
          "support dynamic recovery. (boolean)");

  
  BooleanConfOption LOCAL_TEST_MODE =
      new BooleanConfOption("giraph.localTestMode", false,
          "Indicates whether this job is run in an internal unit test");

  
  StrConfOption LOG_LEVEL = new StrConfOption("giraph.logLevel", "info",
      "Override the Hadoop log level and set the desired log level.");

  
  BooleanConfOption LOG_THREAD_LAYOUT =
      new BooleanConfOption("giraph.logThreadLayout", false,
          "Use thread level debugging?");

  
  BooleanConfOption JMAP_ENABLE =
      new BooleanConfOption("giraph.jmap.histo.enable", false,
          "Configuration key to enable jmap printing");

  
  IntConfOption JMAP_SLEEP_MILLIS =
      new IntConfOption("giraph.jmap.histo.msec", SECONDS.toMillis(30),
          "Configuration key for msec to sleep between calls");

  
  IntConfOption JMAP_PRINT_LINES =
      new IntConfOption("giraph.jmap.histo.print_lines", 30,
          "Configuration key for how many lines to print");

  
  BooleanConfOption JMAP_LIVE_ONLY =
      new BooleanConfOption("giraph.jmap.histo.live", false,
          "Only print live objects in jmap?");

  
  IntConfOption MIN_FREE_MBS_ON_HEAP =
      new IntConfOption("giraph.heap.minFreeMb", 128, "Option used by " +
          "worker and master observers to check for imminent OOM exception");
  
  BooleanConfOption REACTIVE_JMAP_ENABLE =
      new BooleanConfOption("giraph.heap.enableReactiveJmapDumping", false,
          "Option to enable dumping jmap histogram reactively based on " +
              "free memory on heap");

  
  FloatConfOption MIN_PERCENT_RESPONDED =
      new FloatConfOption("giraph.minPercentResponded", 100.0f,
          "Minimum percent of the maximum number of workers that have " +
          "responded in order to continue progressing. (float)");

  
  BooleanConfOption METRICS_ENABLE =
      new BooleanConfOption("giraph.metrics.enable", false,
          "Enable the Metrics system");

  
  StrConfOption METRICS_DIRECTORY =
      new StrConfOption("giraph.metrics.directory", "",
          "Directory in HDFS to write master metrics to, instead of stderr");

  
  StrConfOption ZOOKEEPER_LIST =
      new StrConfOption("giraph.zkList", "",
          "ZooKeeper comma-separated list (if not set, will start up " +
          "ZooKeeper locally). Consider that after locally-starting " +
          "zookeeper, this parameter will updated the configuration with " +
          "the corrent configuration value.");

  
  BooleanConfOption ZOOKEEPER_IS_EXTERNAL =
    new BooleanConfOption("giraph.zkIsExternal", true,
                          "Zookeeper List will always hold a value during " +
                          "the computation while this option provides " +
                          "information regarding whether the zookeeper was " +
                          "internally started or externally provided.");

  
  IntConfOption ZOOKEEPER_SESSION_TIMEOUT =
      new IntConfOption("giraph.zkSessionMsecTimeout", MINUTES.toMillis(1),
          "ZooKeeper session millisecond timeout");

  
  IntConfOption ZOOKEEPER_SERVERLIST_POLL_MSECS =
      new IntConfOption("giraph.zkServerlistPollMsecs", SECONDS.toMillis(3),
          "Polling interval to check for the ZooKeeper server data");

  
  IntConfOption ZOOKEEPER_SERVER_COUNT =
      new IntConfOption("giraph.zkServerCount", 1,
          "Number of nodes (not tasks) to run Zookeeper on");

  
  IntConfOption ZOOKEEPER_SERVER_PORT =
      new IntConfOption("giraph.zkServerPort", 22181, "ZooKeeper port to use");

  
  String ZOOKEEPER_DIR = "giraph.zkDir";

  
  IntConfOption ZOOKEEPER_OPS_MAX_ATTEMPTS =
      new IntConfOption("giraph.zkOpsMaxAttempts", 3,
          "Max attempts for handling ZooKeeper connection loss");

  
  IntConfOption ZOOKEEPER_OPS_RETRY_WAIT_MSECS =
      new IntConfOption("giraph.zkOpsRetryWaitMsecs", SECONDS.toMillis(5),
          "Msecs to wait before retrying a failed ZooKeeper op due to " +
          "connection loss.");

  
  BooleanConfOption ZOOKEEEPER_RUNS_IN_PROCESS = new BooleanConfOption(
      "giraph.zkRunsInProcess",
      true, "If true run zookeeper in master process, if false starts " +
      "separate process for zookeeper");

  
  IntConfOption TCP_BACKLOG = new IntConfOption("giraph.tcpBacklog", 1,
      "TCP backlog (defaults to number of workers)");

  
  BooleanConfOption NETTY_USE_POOLED_ALLOCATOR = new BooleanConfOption(
      "giraph.useNettyPooledAllocator", false, "Should netty use pooled " +
      "memory allocator?");

  
  BooleanConfOption NETTY_USE_DIRECT_MEMORY = new BooleanConfOption(
      "giraph.useNettyDirectMemory", false, "Should netty use direct " +
      "memory buffers");

  
  IntConfOption NETTY_REQUEST_ENCODER_BUFFER_SIZE =
      new IntConfOption("giraph.nettyRequestEncoderBufferSize", 32 * ONE_KB,
          "How big to make the encoder buffer?");

  
  IntConfOption NETTY_CLIENT_THREADS =
      new IntConfOption("giraph.nettyClientThreads", 4, "Netty client threads");

  
  IntConfOption NETTY_SERVER_THREADS =
      new IntConfOption("giraph.nettyServerThreads", 16,
          "Netty server threads");

  
  BooleanConfOption NETTY_CLIENT_USE_EXECUTION_HANDLER =
      new BooleanConfOption("giraph.nettyClientUseExecutionHandler", true,
          "Use the execution handler in netty on the client?");

  
  IntConfOption NETTY_CLIENT_EXECUTION_THREADS =
      new IntConfOption("giraph.nettyClientExecutionThreads", 8,
          "Netty client execution threads (execution handler)");

  
  StrConfOption NETTY_CLIENT_EXECUTION_AFTER_HANDLER =
      new StrConfOption("giraph.nettyClientExecutionAfterHandler",
          "request-encoder",
          "Where to place the netty client execution handle?");

  
  BooleanConfOption NETTY_SERVER_USE_EXECUTION_HANDLER =
      new BooleanConfOption("giraph.nettyServerUseExecutionHandler", true,
          "Use the execution handler in netty on the server?");

  
  IntConfOption NETTY_SERVER_EXECUTION_THREADS =
      new IntConfOption("giraph.nettyServerExecutionThreads", 8,
          "Netty server execution threads (execution handler)");

  
  StrConfOption NETTY_SERVER_EXECUTION_AFTER_HANDLER =
      new StrConfOption("giraph.nettyServerExecutionAfterHandler",
          "requestFrameDecoder",
          "Where to place the netty server execution handle?");

  
  BooleanConfOption NETTY_SIMULATE_FIRST_REQUEST_CLOSED =
      new BooleanConfOption("giraph.nettySimulateFirstRequestClosed", false,
          "Netty simulate a first request closed");

  
  BooleanConfOption NETTY_SIMULATE_FIRST_RESPONSE_FAILED =
      new BooleanConfOption("giraph.nettySimulateFirstResponseFailed", false,
          "Netty simulate a first response failed");

  
  StrConfOption NETTY_COMPRESSION_ALGORITHM =
      new StrConfOption("giraph.nettyCompressionAlgorithm", "",
          "Which compression algorithm to use in netty");

  
  IntConfOption MAX_RESOLVE_ADDRESS_ATTEMPTS =
      new IntConfOption("giraph.maxResolveAddressAttempts", 5,
          "Max resolve address attempts");

  
  IntConfOption WAITING_REQUEST_MSECS =
      new IntConfOption("giraph.waitingRequestMsecs", SECONDS.toMillis(15),
          "Msecs to wait between waiting for all requests to finish");

  
  IntConfOption EVENT_WAIT_MSECS =
      new IntConfOption("giraph.eventWaitMsecs", SECONDS.toMillis(30),
          "Millseconds to wait for an event before continuing");

  
  IntConfOption MAX_MASTER_SUPERSTEP_WAIT_MSECS =
      new IntConfOption("giraph.maxMasterSuperstepWaitMsecs",
          MINUTES.toMillis(10),
          "Maximum milliseconds to wait before giving up trying to get the " +
          "minimum number of workers before a superstep (int).");

  
  IntConfOption MAX_REQUEST_MILLISECONDS =
      new IntConfOption("giraph.maxRequestMilliseconds", MINUTES.toMillis(10),
          "Milliseconds for a request to complete (or else resend)");

  
  IntConfOption NETTY_MAX_CONNECTION_FAILURES =
      new IntConfOption("giraph.nettyMaxConnectionFailures", 1000,
          "Netty max connection failures");

  
  IntConfOption IPC_INITIAL_PORT =
      new IntConfOption("giraph.ipcInitialPort", 30000,
          "Initial port to start using for the IPC communication");

  
  IntConfOption MAX_IPC_PORT_BIND_ATTEMPTS =
      new IntConfOption("giraph.maxIpcPortBindAttempts", 20,
          "Maximum bind attempts for different IPC ports");
  
  BooleanConfOption FAIL_FIRST_IPC_PORT_BIND_ATTEMPT =
      new BooleanConfOption("giraph.failFirstIpcPortBindAttempt", false,
          "Fail first IPC port binding attempt, simulate binding failure " +
          "on real grid testing");

  
  IntConfOption CLIENT_SEND_BUFFER_SIZE =
      new IntConfOption("giraph.clientSendBufferSize", 512 * ONE_KB,
          "Client send buffer size");

  
  IntConfOption CLIENT_RECEIVE_BUFFER_SIZE =
      new IntConfOption("giraph.clientReceiveBufferSize", 32 * ONE_KB,
          "Client receive buffer size");

  
  IntConfOption SERVER_SEND_BUFFER_SIZE =
      new IntConfOption("giraph.serverSendBufferSize", 32 * ONE_KB,
          "Server send buffer size");

  
  IntConfOption SERVER_RECEIVE_BUFFER_SIZE =
      new IntConfOption("giraph.serverReceiveBufferSize", 512 * ONE_KB,
          "Server receive buffer size");

  
  IntConfOption MAX_MSG_REQUEST_SIZE =
      new IntConfOption("giraph.msgRequestSize", 512 * ONE_KB,
          "Maximum size of messages (in bytes) per peer before flush");

  
  FloatConfOption ADDITIONAL_MSG_REQUEST_SIZE =
      new FloatConfOption("giraph.additionalMsgRequestSize", 0.2f,
          "How much bigger than the average per partition size to make " +
          "initial per partition buffers. If this value is A, message " +
          "request size is M, and a worker has P partitions, than its " +
          "initial partition buffer size will be (M / P) * (1 + A).");


  
  FloatConfOption REQUEST_SIZE_WARNING_THRESHOLD = new FloatConfOption(
      "giraph.msgRequestWarningThreshold", 2.0f,
      "If request sizes are bigger than the buffer size by this factor " +
      "warnings are printed to the log and to the command line");

  
  IntConfOption MAX_VERTEX_REQUEST_SIZE =
      new IntConfOption("giraph.vertexRequestSize", 512 * ONE_KB,
          "Maximum size of vertices (in bytes) per peer before flush");

  
  FloatConfOption ADDITIONAL_VERTEX_REQUEST_SIZE =
      new FloatConfOption("giraph.additionalVertexRequestSize", 0.2f,
          "Additional size (expressed as a ratio) of each per-partition " +
              "buffer on top of the average size.");

  
  IntConfOption MAX_EDGE_REQUEST_SIZE =
      new IntConfOption("giraph.edgeRequestSize", 512 * ONE_KB,
          "Maximum size of edges (in bytes) per peer before flush");

  
  FloatConfOption ADDITIONAL_EDGE_REQUEST_SIZE =
      new FloatConfOption("giraph.additionalEdgeRequestSize", 0.2f,
          "Additional size (expressed as a ratio) of each per-partition " +
          "buffer on top of the average size.");

  
  IntConfOption MAX_MUTATIONS_PER_REQUEST =
      new IntConfOption("giraph.maxMutationsPerRequest", 100,
          "Maximum number of mutations per partition before flush");

  
  BooleanConfOption USE_MESSAGE_SIZE_ENCODING =
      new BooleanConfOption("giraph.useMessageSizeEncoding", false,
          "Use message size encoding (typically better for complex objects, " +
          "not meant for primitive wrapped messages)");

  
  IntConfOption CHANNELS_PER_SERVER =
      new IntConfOption("giraph.channelsPerServer", 1,
          "Number of channels used per server");

  
  String MSG_NUM_FLUSH_THREADS = "giraph.msgNumFlushThreads";

  
  IntConfOption NUM_COMPUTE_THREADS =
      new IntConfOption("giraph.numComputeThreads", 1,
          "Number of threads for vertex computation");

  
  IntConfOption NUM_INPUT_THREADS =
      new IntConfOption("giraph.numInputThreads", 1,
          "Number of threads for input split loading");

  
  IntConfOption PARTITION_LONG_TAIL_MIN_PRINT =
      new IntConfOption("giraph.partitionLongTailMinPrint", 1,
          "Minimum stragglers of the superstep before printing them out");

  
  BooleanConfOption USE_SUPERSTEP_COUNTERS =
      new BooleanConfOption("giraph.useSuperstepCounters", true,
          "Use superstep counters? (boolean)");

  
  FloatConfOption INPUT_SPLIT_SAMPLE_PERCENT =
      new FloatConfOption("giraph.inputSplitSamplePercent", 100f,
          "Input split sample percent - Used only for sampling and testing, " +
          "rather than an actual job.  The idea is that to test, you might " +
          "only want a fraction of the actual input splits from your " +
          "VertexInputFormat to load (values should be [0, 100]).");

  
  LongConfOption INPUT_SPLIT_MAX_VERTICES =
      new LongConfOption("giraph.InputSplitMaxVertices", -1,
          "To limit outlier vertex input splits from producing too many " +
              "vertices or to help with testing, the number of vertices " +
              "loaded from an input split can be limited. By default, " +
              "everything is loaded.");

  
  LongConfOption INPUT_SPLIT_MAX_EDGES =
      new LongConfOption("giraph.InputSplitMaxEdges", -1,
          "To limit outlier vertex input splits from producing too many " +
              "vertices or to help with testing, the number of edges loaded " +
              "from an input split can be limited. By default, everything is " +
              "loaded.");

  
  BooleanConfOption USE_INPUT_SPLIT_LOCALITY =
      new BooleanConfOption("giraph.useInputSplitLocality", true,
          "To minimize network usage when reading input splits, each worker " +
          "can prioritize splits that reside on its host. " +
          "This, however, comes at the cost of increased load on ZooKeeper. " +
          "Hence, users with a lot of splits and input threads (or with " +
          "configurations that can't exploit locality) may want to disable " +
          "it.");

  
  FloatConfOption PARTITION_COUNT_MULTIPLIER =
      new FloatConfOption("giraph.masterPartitionCountMultiplier", 1.0f,
          "Multiplier for the current workers squared");

  
  IntConfOption USER_PARTITION_COUNT =
      new IntConfOption("giraph.userPartitionCount", -1,
          "Overrides default partition count calculation if not -1");

  
  String PARTITION_VERTEX_KEY_SPACE_SIZE = "giraph.vertexKeySpaceSize";

  
  StrConfOption ZOOKEEPER_JAVA_OPTS =
      new StrConfOption("giraph.zkJavaOpts",
          "-Xmx512m -XX:ParallelGCThreads=4 -XX:+UseConcMarkSweepGC " +
          "-XX:CMSInitiatingOccupancyFraction=70 -XX:MaxGCPauseMillis=100",
          "Java opts passed to ZooKeeper startup");

  
  IntConfOption CHECKPOINT_FREQUENCY =
      new IntConfOption("giraph.checkpointFrequency", 0,
          "How often to checkpoint (i.e. 0, means no checkpoint, 1 means " +
          "every superstep, 2 is every two supersteps, etc.).");

  
  BooleanConfOption CLEANUP_CHECKPOINTS_AFTER_SUCCESS =
      new BooleanConfOption("giraph.cleanupCheckpointsAfterSuccess", true,
          "Delete checkpoints after a successful job run?");

  
  String RESTART_SUPERSTEP = "giraph.restartSuperstep";

  
  StrConfOption RESTART_JOB_ID = new StrConfOption("giraph.restart.jobId",
      null, "Which job ID should I try to restart?");

  
  String BASE_ZNODE_KEY = "giraph.zkBaseZNode";

  
  StrConfOption ZOOKEEPER_MANAGER_DIRECTORY =
      new StrConfOption("giraph.zkManagerDirectory",
          "_bsp/_defaultZkManagerDir",
          "If ZOOKEEPER_LIST is not set, then use this directory to manage " +
          "ZooKeeper");

  
  IntConfOption ZOOKEEPER_CONNECTION_ATTEMPTS =
      new IntConfOption("giraph.zkConnectionAttempts", 10,
          "Number of ZooKeeper client connection attempts before giving up.");

  
  StrConfOption CHECKPOINT_DIRECTORY =
      new StrConfOption("giraph.checkpointDirectory", "_bsp/_checkpoints/",
          "This directory has/stores the available checkpoint files in HDFS.");

  
  StrConfOption MESSAGES_DIRECTORY =
      new StrConfOption("giraph.messagesDirectory", "_bsp/_messages/",
          "Comma-separated list of directories in the local file system for " +
          "out-of-core messages.");

  
  IntConfOption MAX_MESSAGES_IN_MEMORY =
      new IntConfOption("giraph.maxMessagesInMemory", 1000000,
          "If using out-of-core messaging, it tells how much messages do we " +
          "keep in memory.");
  
  IntConfOption MESSAGES_BUFFER_SIZE =
      new IntConfOption("giraph.messagesBufferSize", 8 * ONE_KB,
          "Size of buffer when reading and writing messages out-of-core.");

  
  StrConfOption PARTITIONS_DIRECTORY =
      new StrConfOption("giraph.partitionsDirectory", "_bsp/_partitions",
          "Comma-separated list of directories in the local filesystem for " +
          "out-of-core partitions.");

  
  BooleanConfOption USE_OUT_OF_CORE_GRAPH =
      new BooleanConfOption("giraph.useOutOfCoreGraph", false,
          "Enable out-of-core graph.");

  
  ClassConfOption<MemoryEstimator> OUT_OF_CORE_MEM_ESTIMATOR =
      ClassConfOption.create("giraph.outOfCoreMemoryEstimator",
          JVMMemoryEstimator.class, MemoryEstimator.class,
          "Memory estimator class used for out-of-core decisions");

  
  IntConfOption NUM_OOC_THREADS =
      new IntConfOption("giraph.numOutOfCoreThreads", 1,
          "Number of threads participating in swapping data to disk.");

  
  IntConfOption MAX_PARTITIONS_IN_MEMORY =
      new IntConfOption("giraph.maxPartitionsInMemory", 0,
          "Maximum number of partitions to hold in memory for each worker. By" +
              " default it is set to 0 (for adaptive out-of-core mechanism");



  
  String YOURKIT_OUTPUT_DIR = "giraph.yourkit.outputDir";
  
  String YOURKIT_OUTPUT_DIR_DEFAULT = "/tmp/giraph/%JOB_ID%/%TASK_ID%";

  
  BooleanConfOption KEEP_ZOOKEEPER_DATA =
      new BooleanConfOption("giraph.keepZooKeeperData", false,
          "Keep the zookeeper output for debugging? Default is to remove it.");

  
  int DEFAULT_ZOOKEEPER_TICK_TIME = 6000;
  
  int DEFAULT_ZOOKEEPER_INIT_LIMIT = 10;
  
  int DEFAULT_ZOOKEEPER_SYNC_LIMIT = 5;
  
  int DEFAULT_ZOOKEEPER_SNAP_COUNT = 50000;
  
  int DEFAULT_ZOOKEEPER_MAX_CLIENT_CNXNS = 10000;
  
  IntConfOption ZOOKEEPER_MIN_SESSION_TIMEOUT =
      new IntConfOption("giraph.zKMinSessionTimeout", MINUTES.toMillis(10),
          "ZooKeeper minimum session timeout");
  
  IntConfOption ZOOKEEPER_MAX_SESSION_TIMEOUT =
      new IntConfOption("giraph.zkMaxSessionTimeout", MINUTES.toMillis(15),
          "ZooKeeper maximum session timeout");
  
  BooleanConfOption ZOOKEEPER_FORCE_SYNC =
      new BooleanConfOption("giraph.zKForceSync", false,
          "ZooKeeper force sync");
  
  BooleanConfOption ZOOKEEPER_SKIP_ACL =
      new BooleanConfOption("giraph.ZkSkipAcl", true, "ZooKeeper skip ACLs");

  
  BooleanConfOption AUTHENTICATE =
      new BooleanConfOption("giraph.authenticate", false,
          "Whether to use SASL with DIGEST and Hadoop Job Tokens to " +
          "authenticate and authorize Netty BSP Clients to Servers.");

  
  BooleanConfOption USE_UNSAFE_SERIALIZATION =
      new BooleanConfOption("giraph.useUnsafeSerialization", true,
          "Use unsafe serialization?");

  
  BooleanConfOption USE_BIG_DATA_IO_FOR_MESSAGES =
      new BooleanConfOption("giraph.useBigDataIOForMessages", false,
          "Use BigDataIO for messages?");

  
  IntConfOption MAX_TASK_ATTEMPTS =
      new IntConfOption("mapred.map.max.attempts", -1,
          "Maximum number of attempts a master/worker will retry before " +
          "killing the job.  This directly maps to the number of map task " +
          "attempts in Hadoop.");

  
  StrConfOption DNS_INTERFACE =
      new StrConfOption("giraph.dns.interface", "default",
          "Interface to use for hostname resolution");
  
  StrConfOption DNS_NAMESERVER =
      new StrConfOption("giraph.dns.nameserver", "default",
          "Server for hostname resolution");

  
  IntConfOption MAX_NUMBER_OF_SUPERSTEPS =
      new IntConfOption("giraph.maxNumberOfSupersteps", 1,
          "The application will halt after this many supersteps is " +
          "completed. For instance, if it is set to 3, the application will " +
          "run at most 0, 1, and 2 supersteps and then go into the shutdown " +
          "superstep.");

  
  BooleanConfOption STATIC_GRAPH =
      new BooleanConfOption("giraph.isStaticGraph", false,
          "The application will not mutate the graph topology (the edges). " +
          "It is used to optimise out-of-core graph, by not writing back " +
          "edges every time.");

  
  EnumConfOption<MessageEncodeAndStoreType> MESSAGE_ENCODE_AND_STORE_TYPE =
      EnumConfOption.create("giraph.messageEncodeAndStoreType",
          MessageEncodeAndStoreType.class,
          MessageEncodeAndStoreType.BYTEARRAY_PER_PARTITION,
          "Select the message_encode_and_store_type to use");

  
  BooleanConfOption CREATE_EDGE_SOURCE_VERTICES =
      new BooleanConfOption("giraph.createEdgeSourceVertices", true,
          "Create a source vertex if present in edge input but not " +
          "necessarily in vertex input");

  
  String ZOOKEEPER_SERVER_PORT_COUNTER_GROUP = "Zookeeper server:port";

  
  String ZOOKEEPER_HALT_NODE_COUNTER_GROUP = "Zookeeper halt node";

  
  String ZOOKEEPER_BASE_PATH_COUNTER_GROUP = "Zookeeper base path";

  
  ClassConfOption<HaltApplicationUtils.HaltInstructionsWriter>
  HALT_INSTRUCTIONS_WRITER_CLASS = ClassConfOption.create(
      "giraph.haltInstructionsWriter",
      HaltApplicationUtils.DefaultHaltInstructionsWriter.class,
      HaltApplicationUtils.HaltInstructionsWriter.class,
      "Class used to write instructions on how to halt the application");

  
  IntConfOption WAIT_TASK_DONE_TIMEOUT_MS =
      new IntConfOption("giraph.waitTaskDoneTimeoutMs", MINUTES.toMillis(15),
          "Maximum timeout (in ms) for waiting for all all tasks to " +
              "complete");

  
  BooleanConfOption TRACK_JOB_PROGRESS_ON_CLIENT =
      new BooleanConfOption("giraph.trackJobProgressOnClient", false,
          "Whether to track job progress on client or not");

  
  IntConfOption HDFS_FILE_CREATION_RETRIES =
      new IntConfOption("giraph.hdfs.file.creation.retries", 10,
          "Retries to create an HDFS file before failing");

  
  IntConfOption HDFS_FILE_CREATION_RETRY_WAIT_MS =
      new IntConfOption("giraph.hdfs.file.creation.retry.wait.ms", 30_000,
          "Milliseconds to wait prior to retrying creation of an HDFS file");

  
  IntConfOption NUM_CHECKPOINT_IO_THREADS =
      new IntConfOption("giraph.checkpoint.io.threads", 8,
          "Number of threads for writing and reading checkpoints");

  
  StrConfOption CHECKPOINT_COMPRESSION_CODEC =
      new StrConfOption("giraph.checkpoint.compression.codec",
          ".deflate",
          "Defines compression algorithm we will be using for " +
              "storing checkpoint. Available options include but " +
              "not restricted to: .deflate, .gz, .bz2, .lzo");

  
  ClassConfOption<CheckpointSupportedChecker> CHECKPOINT_SUPPORTED_CHECKER =
      ClassConfOption.create("giraph.checkpoint.supported.checker",
          DefaultCheckpointSupportedChecker.class,
          CheckpointSupportedChecker.class,
          "This is the way to specify if checkpointing is " +
              "supported by the job");


  
  IntConfOption ASYNC_MESSAGE_STORE_THREADS_COUNT =
      new IntConfOption("giraph.async.message.store.threads", 0,
          "Number of threads to be used in async message store.");

  
  ClassConfOption<OutputFormat> HADOOP_OUTPUT_FORMAT_CLASS =
      ClassConfOption.create("giraph.hadoopOutputFormatClass",
          BspOutputFormat.class, OutputFormat.class,
          "Output format class for hadoop to use (for committing)");
}


<code block>


package org.apache.giraph.bsp;

import org.apache.giraph.comm.ServerData;
import org.apache.giraph.comm.WorkerClient;
import org.apache.giraph.graph.FinishedSuperstepStats;
import org.apache.giraph.graph.GlobalStats;
import org.apache.giraph.graph.GraphTaskManager;
import org.apache.giraph.graph.VertexEdgeCount;
import org.apache.giraph.io.superstep_output.SuperstepOutput;
import org.apache.giraph.master.MasterInfo;
import org.apache.giraph.metrics.GiraphTimerContext;
import org.apache.giraph.partition.PartitionOwner;
import org.apache.giraph.partition.PartitionStats;
import org.apache.giraph.partition.PartitionStore;
import org.apache.giraph.worker.WorkerAggregatorHandler;
import org.apache.giraph.worker.WorkerContext;
import org.apache.giraph.worker.WorkerInfo;
import org.apache.giraph.worker.WorkerObserver;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;

import java.io.IOException;
import java.util.Collection;
import java.util.List;


@SuppressWarnings("rawtypes")
public interface CentralizedServiceWorker<I extends WritableComparable,
  V extends Writable, E extends Writable>
  extends CentralizedService<I, V, E> {
  
  FinishedSuperstepStats setup();

  
  WorkerInfo getWorkerInfo();

  
  WorkerClient<I, V, E> getWorkerClient();

  
  WorkerContext getWorkerContext();

  
  WorkerObserver[] getWorkerObservers();

  
  PartitionStore<I, V, E> getPartitionStore();

  
  void storeCheckpoint() throws IOException;

  
  VertexEdgeCount loadCheckpoint(long superstep) throws IOException;

  
  Collection<? extends PartitionOwner> startSuperstep();

  
  FinishedSuperstepStats finishSuperstep(
      List<PartitionStats> partitionStatsList,
      GiraphTimerContext superstepTimerContext);

  
  int getPartitionId(I vertexId);

  
  boolean hasPartition(Integer partitionId);

  
  PartitionOwner getVertexPartitionOwner(I vertexId);

  
  Iterable<? extends PartitionOwner> getPartitionOwners();

  
  void exchangeVertexPartitions(
      Collection<? extends PartitionOwner> masterSetPartitionOwners);

  
  MasterInfo getMasterInfo();

  
  GraphTaskManager<I, V, E> getGraphTaskManager();

  
  void failureCleanup();

  
  ServerData<I, V, E> getServerData();

  
  WorkerAggregatorHandler getAggregatorHandler();

  
  void prepareSuperstep();

  
  SuperstepOutput<I, V, E> getSuperstepOutput();

  
  void cleanup(FinishedSuperstepStats finishedSuperstepStats)
    throws IOException, InterruptedException;

  
  GlobalStats getGlobalStats();

  
  int getNumPartitionsOwned();
}

<code block>


package org.apache.giraph.partition;

import com.google.common.collect.Iterables;
import com.google.common.io.Files;
import org.apache.commons.io.FileUtils;
import org.apache.giraph.bsp.BspService;
import org.apache.giraph.bsp.CentralizedServiceWorker;
import org.apache.giraph.conf.GiraphConfiguration;
import org.apache.giraph.conf.GiraphConstants;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.giraph.edge.EdgeFactory;
import org.apache.giraph.graph.BasicComputation;
import org.apache.giraph.graph.Vertex;
import org.apache.giraph.io.formats.IdWithValueTextOutputFormat;
import org.apache.giraph.io.formats.JsonLongDoubleFloatDoubleVertexInputFormat;
import org.apache.giraph.ooc.DiskBackedPartitionStore;
import org.apache.giraph.utils.InternalVertexRunner;
import org.apache.giraph.utils.NoOpComputation;
import org.apache.giraph.utils.UnsafeByteArrayInputStream;
import org.apache.giraph.utils.UnsafeByteArrayOutputStream;
import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.FloatWritable;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapreduce.Mapper;
import org.junit.Before;
import org.junit.Test;
import org.mockito.Mockito;

import java.io.File;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;
import java.util.Random;
import java.util.concurrent.ExecutorCompletionService;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.atomic.AtomicInteger;

import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertFalse;
import static org.junit.Assert.assertTrue;


@SuppressWarnings("unchecked")
public class TestPartitionStores {
  private ImmutableClassesGiraphConfiguration<IntWritable, IntWritable,
      NullWritable> conf;
  private Mapper<?, ?, ?, ?>.Context context;

  
  private static final int NUM_OF_VERTEXES_PER_PARTITION = 20;
  private static final int NUM_OF_EDGES_PER_VERTEX = 5;
  private static final int NUM_OF_THREADS = 8;
  private static final int NUM_OF_PARTITIONS = 30;
  private static final int NUM_PARTITIONS_IN_MEMORY = 12;

  public static class MyComputation extends NoOpComputation<IntWritable,
      IntWritable, NullWritable, IntWritable> { }

  private Partition<IntWritable, IntWritable, NullWritable> createPartition(
      ImmutableClassesGiraphConfiguration<IntWritable, IntWritable,
          NullWritable> conf,
      Integer id,
      Vertex<IntWritable, IntWritable, NullWritable>... vertices) {
    Partition<IntWritable, IntWritable, NullWritable> partition =
        conf.createPartition(id, context);
    for (Vertex<IntWritable, IntWritable, NullWritable> v : vertices) {
      partition.putVertex(v);
    }
    return partition;
  }

  @Before
  public void setUp() {
    GiraphConfiguration configuration = new GiraphConfiguration();
    configuration.setComputationClass(MyComputation.class);
    conf = new ImmutableClassesGiraphConfiguration<IntWritable, IntWritable,
        NullWritable>(configuration);
    context = Mockito.mock(Mapper.Context.class);
  }

  @Test
  public void testSimplePartitionStore() {
    CentralizedServiceWorker<IntWritable, IntWritable, NullWritable>
    serviceWorker = Mockito.mock(CentralizedServiceWorker.class);
    PartitionStore<IntWritable, IntWritable, NullWritable>
    partitionStore = new SimplePartitionStore<IntWritable, IntWritable,
                NullWritable>(conf, context, serviceWorker);
    testReadWrite(partitionStore, conf);
    partitionStore.shutdown();
  }

  @Test
  public void testUnsafePartitionSerializationClass() throws IOException {
    conf.setPartitionClass(ByteArrayPartition.class);
    Vertex<IntWritable, IntWritable, NullWritable> v1 =
        conf.createVertex();
    v1.initialize(new IntWritable(1), new IntWritable(1));
    Vertex<IntWritable, IntWritable, NullWritable> v2 = conf.createVertex();
    v2.initialize(new IntWritable(2), new IntWritable(2));
    Vertex<IntWritable, IntWritable, NullWritable> v3 = conf.createVertex();
    v3.initialize(new IntWritable(3), new IntWritable(3));
    Vertex<IntWritable, IntWritable, NullWritable> v4 = conf.createVertex();
    v4.initialize(new IntWritable(4), new IntWritable(4));
    Vertex<IntWritable, IntWritable, NullWritable> v5 = conf.createVertex();
    v5.initialize(new IntWritable(5), new IntWritable(5));
    Vertex<IntWritable, IntWritable, NullWritable> v6 = conf.createVertex();
    v6.initialize(new IntWritable(6), new IntWritable(6));
    Vertex<IntWritable, IntWritable, NullWritable> v7 = conf.createVertex();
    v7.initialize(new IntWritable(7), new IntWritable(7));

    Partition<IntWritable, IntWritable, NullWritable> partition =
        createPartition(conf, 3, v1, v2, v3, v4, v5, v6, v7);
    assertEquals(3, partition.getId());
    assertEquals(0, partition.getEdgeCount());
    assertEquals(7, partition.getVertexCount());
    UnsafeByteArrayOutputStream outputStream = new
        UnsafeByteArrayOutputStream();
    partition.write(outputStream);
    UnsafeByteArrayInputStream inputStream = new UnsafeByteArrayInputStream(
        outputStream.getByteArray(), 0, outputStream.getPos());
    Partition<IntWritable, IntWritable, NullWritable> deserializatedPartition =
        conf.createPartition(-1, context);
    deserializatedPartition.readFields(inputStream);

    assertEquals(3, deserializatedPartition.getId());
    assertEquals(0, deserializatedPartition.getEdgeCount());
    assertEquals(7, deserializatedPartition.getVertexCount());
  }

  @Test
  public void testDiskBackedPartitionStoreWithByteArrayPartition()
    throws IOException {
    File directory = Files.createTempDir();
    GiraphConstants.PARTITIONS_DIRECTORY.set(
        conf, new File(directory, "giraph_partitions").toString());
    GiraphConstants.USE_OUT_OF_CORE_GRAPH.set(conf, true);
    GiraphConstants.MAX_PARTITIONS_IN_MEMORY.set(conf, 1);
    conf.setPartitionClass(ByteArrayPartition.class);

    CentralizedServiceWorker<IntWritable, IntWritable, NullWritable>
      serviceWorker = Mockito.mock(CentralizedServiceWorker.class);
    Mockito.when(serviceWorker.getSuperstep()).thenReturn(
        BspService.INPUT_SUPERSTEP);

    PartitionStore<IntWritable, IntWritable, NullWritable> partitionStore =
        new DiskBackedPartitionStore<IntWritable, IntWritable, NullWritable>(
            conf, context, serviceWorker);
    testReadWrite(partitionStore, conf);
    partitionStore.shutdown();
    FileUtils.deleteDirectory(directory);
  }

  @Test
  public void testDiskBackedPartitionStore() throws IOException {
    File directory = Files.createTempDir();
    GiraphConstants.PARTITIONS_DIRECTORY.set(
        conf, new File(directory, "giraph_partitions").toString());
    GiraphConstants.USE_OUT_OF_CORE_GRAPH.set(conf, true);
    GiraphConstants.MAX_PARTITIONS_IN_MEMORY.set(conf, 1);

    CentralizedServiceWorker<IntWritable, IntWritable, NullWritable>
    serviceWorker = Mockito.mock(CentralizedServiceWorker.class);

    Mockito.when(serviceWorker.getSuperstep()).thenReturn(
        BspService.INPUT_SUPERSTEP);

    PartitionStore<IntWritable, IntWritable, NullWritable> partitionStore =
        new DiskBackedPartitionStore<IntWritable, IntWritable, NullWritable>(
            conf, context, serviceWorker);
    testReadWrite(partitionStore, conf);
    partitionStore.shutdown();

    FileUtils.deleteDirectory(directory);
  }

  @Test
  public void testDiskBackedPartitionStoreComputation() throws Exception {
    Iterable<String> results;
    String[] graph =
        {
            "[1,0,[]]", "[2,0,[]]", "[3,0,[]]", "[4,0,[]]", "[5,0,[]]",
            "[6,0,[]]", "[7,0,[]]", "[8,0,[]]", "[9,0,[]]", "[10,0,[]]"
        };
    String[] expected =
        {
            "1\t0", "2\t0", "3\t0", "4\t0", "5\t0",
            "6\t0", "7\t0", "8\t0", "9\t0", "10\t0"
        };

    GiraphConstants.USE_OUT_OF_CORE_GRAPH.set(conf, true);
    GiraphConstants.MAX_PARTITIONS_IN_MEMORY.set(conf, 1);
    GiraphConstants.USER_PARTITION_COUNT.set(conf, 10);

    File directory = Files.createTempDir();
    GiraphConstants.PARTITIONS_DIRECTORY.set(conf,
        new File(directory, "giraph_partitions").toString());

    conf.setComputationClass(EmptyComputation.class);
    conf.setVertexInputFormatClass(JsonLongDoubleFloatDoubleVertexInputFormat.class);
    conf.setVertexOutputFormatClass(IdWithValueTextOutputFormat.class);

    results = InternalVertexRunner.run(conf, graph);
    checkResults(results, expected);
    FileUtils.deleteDirectory(directory);
  }

  @Test
  public void testDiskBackedPartitionStoreWithByteArrayComputation()
    throws Exception {
    Iterable<String> results;
    String[] graph =
    {
      "[1,0,[]]", "[2,0,[]]", "[3,0,[]]", "[4,0,[]]", "[5,0,[]]",
      "[6,0,[]]", "[7,0,[]]", "[8,0,[]]", "[9,0,[]]", "[10,0,[]]"
    };
    String[] expected =
    {
      "1\t0", "2\t0", "3\t0", "4\t0", "5\t0",
      "6\t0", "7\t0", "8\t0", "9\t0", "10\t0"
    };

    GiraphConstants.USE_OUT_OF_CORE_GRAPH.set(conf, true);
    GiraphConstants.MAX_PARTITIONS_IN_MEMORY.set(conf, 1);
    GiraphConstants.USER_PARTITION_COUNT.set(conf, 10);

    File directory = Files.createTempDir();
    GiraphConstants.PARTITIONS_DIRECTORY.set(conf,
      new File(directory, "giraph_partitions").toString());

    conf.setPartitionClass(ByteArrayPartition.class);
    conf.setComputationClass(EmptyComputation.class);
    conf.setVertexInputFormatClass(JsonLongDoubleFloatDoubleVertexInputFormat.class);
    conf.setVertexOutputFormatClass(IdWithValueTextOutputFormat.class);

    results = InternalVertexRunner.run(conf, graph);
    checkResults(results, expected);
    FileUtils.deleteDirectory(directory);
  }

  @Test
  public void testDiskBackedPartitionStoreMT() throws Exception {
    GiraphConstants.MAX_PARTITIONS_IN_MEMORY.set(conf, NUM_PARTITIONS_IN_MEMORY);
    GiraphConstants.STATIC_GRAPH.set(conf, false);
    testMultiThreaded();
  }


  @Test
  public void testDiskBackedPartitionStoreMTStatic() throws Exception {
    GiraphConstants.MAX_PARTITIONS_IN_MEMORY.set(conf, NUM_PARTITIONS_IN_MEMORY);
    GiraphConstants.STATIC_GRAPH.set(conf, true);
    testMultiThreaded();
  }

  @Test
  public void testDiskBackedPartitionStoreAdaptiveOOC() throws Exception {
    GiraphConstants.STATIC_GRAPH.set(conf, true);
    testMultiThreaded();
  }

  private void testMultiThreaded() throws Exception {
    final AtomicInteger vertexCounter = new AtomicInteger(0);
    ExecutorService pool = Executors.newFixedThreadPool(NUM_OF_THREADS);
    ExecutorCompletionService<Boolean> executor =
      new ExecutorCompletionService<Boolean>(pool);

    File directory = Files.createTempDir();
    GiraphConstants.PARTITIONS_DIRECTORY.set(
        conf, new File(directory, "giraph_partitions").toString());
    GiraphConstants.USE_OUT_OF_CORE_GRAPH.set(conf, true);

    CentralizedServiceWorker<IntWritable, IntWritable, NullWritable>
    serviceWorker = Mockito.mock(CentralizedServiceWorker.class);

    Mockito.when(serviceWorker.getSuperstep()).thenReturn(
        BspService.INPUT_SUPERSTEP);

    PartitionStore<IntWritable, IntWritable, NullWritable> store =
        new DiskBackedPartitionStore<IntWritable, IntWritable, NullWritable>(
            conf, context, serviceWorker);
    store.initialize();

    
    for (int i = 0; i < NUM_OF_THREADS; ++i) {
      List<Integer> partitionIds = new ArrayList<Integer>();
      for (int id = i; id < NUM_OF_PARTITIONS; id += NUM_OF_THREADS) {
        partitionIds.add(id);
      }
      Worker worker =
        new Worker(vertexCounter, store, partitionIds, conf);
      executor.submit(worker, new Boolean(true));
    }
    for (int i = 0; i < NUM_OF_THREADS; ++i)
      executor.take();
    pool.shutdownNow();

    
    int totalVertexes = 0;
    int totalEdges = 0;
    Partition<IntWritable, IntWritable, NullWritable> partition;
    for (int i = 0; i < NUM_OF_PARTITIONS; ++i) {
      totalVertexes += store.getPartitionVertexCount(i);
      totalEdges += store.getPartitionEdgeCount(i);
    }

    assert vertexCounter.get() == NUM_OF_PARTITIONS * NUM_OF_VERTEXES_PER_PARTITION;
    assert totalVertexes == NUM_OF_PARTITIONS * NUM_OF_VERTEXES_PER_PARTITION;
    assert totalEdges == totalVertexes * NUM_OF_EDGES_PER_VERTEX;

    
    int expected = 0;
    for (int i = 0; i < NUM_OF_VERTEXES_PER_PARTITION * NUM_OF_PARTITIONS; ++i) {
      expected += i;
    }
    int totalValues = 0;
    store.startIteration();
    for (int i = 0; i < NUM_OF_PARTITIONS; ++i) {
      partition = store.getNextPartition();
      assert partition != null;
      Iterator<Vertex<IntWritable, IntWritable, NullWritable>> vertexes =
        partition.iterator();

      while (vertexes.hasNext()) {
        Vertex<IntWritable, IntWritable, NullWritable> v = vertexes.next();
        totalValues += v.getId().get();
      }
      store.putPartition(partition);
    }
    assert totalValues == expected;

    store.shutdown();
  }

  private Partition<IntWritable, IntWritable, NullWritable>
  getPartition(PartitionStore<IntWritable, IntWritable,
      NullWritable> partitionStore, int partitionId) {
    partitionStore.startIteration();
    Partition p;
    Partition result = null;
    while ((p = partitionStore.getNextPartition()) != null) {
      if (p.getId() == partitionId) {
        result = p;
      }
      partitionStore.putPartition(p);
    }
    return result;
  }

  
  public void testReadWrite(
      PartitionStore<IntWritable, IntWritable,
          NullWritable> partitionStore,
      ImmutableClassesGiraphConfiguration<IntWritable, IntWritable,
          NullWritable> conf) {
    Vertex<IntWritable, IntWritable, NullWritable> v1 = conf.createVertex();
    v1.initialize(new IntWritable(1), new IntWritable(1));
    Vertex<IntWritable, IntWritable, NullWritable> v2 = conf.createVertex();
    v2.initialize(new IntWritable(2), new IntWritable(2));
    Vertex<IntWritable, IntWritable, NullWritable> v3 = conf.createVertex();
    v3.initialize(new IntWritable(3), new IntWritable(3));
    Vertex<IntWritable, IntWritable, NullWritable> v4 = conf.createVertex();
    v4.initialize(new IntWritable(4), new IntWritable(4));
    Vertex<IntWritable, IntWritable, NullWritable> v5 = conf.createVertex();
    v5.initialize(new IntWritable(5), new IntWritable(5));
    Vertex<IntWritable, IntWritable, NullWritable> v6 = conf.createVertex();
    v6.initialize(new IntWritable(7), new IntWritable(7));
    Vertex<IntWritable, IntWritable, NullWritable> v7 = conf.createVertex();
    v7.initialize(new IntWritable(7), new IntWritable(7));
    v7.addEdge(EdgeFactory.create(new IntWritable(1)));
    v7.addEdge(EdgeFactory.create(new IntWritable(2)));

    partitionStore.addPartition(createPartition(conf, 1, v1, v2, v6));
    partitionStore.addPartition(createPartition(conf, 2, v3, v4));
    partitionStore.addPartition(createPartition(conf, 3, v5));
    partitionStore.addPartition(createPartition(conf, 4, v7));

    getPartition(partitionStore, 1);
    getPartition(partitionStore, 2);
    partitionStore.removePartition(3);
    getPartition(partitionStore, 4);

    assertEquals(3, partitionStore.getNumPartitions());
    assertEquals(3, Iterables.size(partitionStore.getPartitionIds()));
    int partitionsNumber = 0;

    partitionStore.startIteration();
    Partition<IntWritable, IntWritable, NullWritable> p;
    while ((p = partitionStore.getNextPartition()) != null) {
      partitionStore.putPartition(p);
      partitionsNumber++;
    }
    assertEquals(3, partitionsNumber);
    assertTrue(partitionStore.hasPartition(1));
    assertTrue(partitionStore.hasPartition(2));
    assertFalse(partitionStore.hasPartition(3));
    assertTrue(partitionStore.hasPartition(4));
    assertEquals(3, partitionStore.getPartitionVertexCount(1));
    assertEquals(2, partitionStore.getPartitionVertexCount(2));
    assertEquals(1, partitionStore.getPartitionVertexCount(4));
    assertEquals(2, partitionStore.getPartitionEdgeCount(4));
  }

  
  private void checkResults(Iterable<String> results, String[] expected) {
    Iterator<String> result = results.iterator();

    assert results != null;

    while(result.hasNext()) {
      String  resultStr = result.next();
      boolean found = false;

      for (int j = 0; j < expected.length; ++j) {
        if (expected[j].equals(resultStr)) {
          found = true;
        }
      }

      assert found;
    }
  }

  
  public static class EmptyComputation
    extends BasicComputation<LongWritable, DoubleWritable, FloatWritable,
      LongWritable> {

    @Override
    public void compute(
      Vertex<LongWritable, DoubleWritable,FloatWritable> vertex,
      Iterable<LongWritable> messages) throws IOException {

      vertex.voteToHalt();
    }
  }

  @Test
  public void testEdgeCombineWithSimplePartition() throws IOException {
    testEdgeCombine(SimplePartition.class);
  }

  @Test
  public void testEdgeCombineWithByteArrayPartition() throws IOException {
    testEdgeCombine(ByteArrayPartition.class);
  }

  private void testEdgeCombine(Class<? extends Partition> partitionClass)
      throws IOException {
    Vertex<IntWritable, IntWritable, NullWritable> v1 = conf.createVertex();
    v1.initialize(new IntWritable(1), new IntWritable(1));
    Vertex<IntWritable, IntWritable, NullWritable> v2 = conf.createVertex();
    v2.initialize(new IntWritable(2), new IntWritable(2));
    Vertex<IntWritable, IntWritable, NullWritable> v3 = conf.createVertex();
    v3.initialize(new IntWritable(3), new IntWritable(3));
    Vertex<IntWritable, IntWritable, NullWritable> v1e2 = conf.createVertex();
    v1e2.initialize(new IntWritable(1), new IntWritable(1));
    v1e2.addEdge(EdgeFactory.create(new IntWritable(2)));
    Vertex<IntWritable, IntWritable, NullWritable> v1e3 = conf.createVertex();
    v1e3.initialize(new IntWritable(1), new IntWritable(1));
    v1e3.addEdge(EdgeFactory.create(new IntWritable(3)));

    GiraphConfiguration newconf = new GiraphConfiguration(conf);
    newconf.setPartitionClass(partitionClass);
    Partition<IntWritable, IntWritable, NullWritable> partition =
        (new ImmutableClassesGiraphConfiguration<IntWritable, IntWritable,
            NullWritable>(newconf)).createPartition(1, context);
    assertEquals(partitionClass, partition.getClass());
    partition.putVertex(v1);
    partition.putVertex(v2);
    partition.putVertex(v3);
    assertEquals(3, partition.getVertexCount());
    assertEquals(0, partition.getEdgeCount());
    partition.putOrCombine(v1e2);
    assertEquals(3, partition.getVertexCount());
    assertEquals(1, partition.getEdgeCount());
    partition.putOrCombine(v1e3);
    assertEquals(3, partition.getVertexCount());
    assertEquals(2, partition.getEdgeCount());
    v1 = partition.getVertex(new IntWritable(1));
    assertEquals(new IntWritable(1), v1.getId());
    assertEquals(new IntWritable(1), v1.getValue());
    assertEquals(2, v1.getNumEdges());
  }

  private class Worker implements Runnable {

    private final AtomicInteger vertexCounter;
    private final PartitionStore<IntWritable, IntWritable, NullWritable>
      partitionStore;
    private final List<Integer> partitionIds;
    private final ImmutableClassesGiraphConfiguration<IntWritable, IntWritable,
            NullWritable> conf;

    public Worker(AtomicInteger vertexCounter,
        PartitionStore<IntWritable, IntWritable, NullWritable> partitionStore,
        List<Integer> partitionIds,
        ImmutableClassesGiraphConfiguration<IntWritable, IntWritable,
          NullWritable> conf) {

      this.vertexCounter = vertexCounter;
      this.partitionStore = partitionStore;
      this.partitionIds = partitionIds;
      this.conf = conf;
    }

    public void run() {
      for (int partitionId : partitionIds) {
        Partition<IntWritable, IntWritable, NullWritable> partition =
            conf.createPartition(partitionId, context);
        for (int i = 0; i < NUM_OF_VERTEXES_PER_PARTITION; ++i) {
          int id = vertexCounter.getAndIncrement();
          Vertex<IntWritable, IntWritable, NullWritable> v = conf.createVertex();
          v.initialize(new IntWritable(id), new IntWritable(id));

          Random rand = new Random(id);
          for (int j = 0; j < NUM_OF_EDGES_PER_VERTEX; ++j) {
            int dest = rand.nextInt(id + 1);
            v.addEdge(EdgeFactory.create(new IntWritable(dest)));
          }

          partition.putVertex(v);
        }
        partitionStore.addPartition(partition);
      }
    }
  }
}

<code block>


package org.apache.giraph.comm;

import org.apache.giraph.comm.netty.NettyClient;
import org.apache.giraph.comm.netty.NettyServer;
import org.apache.giraph.comm.netty.handler.WorkerRequestServerHandler;
import org.apache.giraph.comm.requests.SendPartitionMutationsRequest;
import org.apache.giraph.comm.requests.SendVertexRequest;
import org.apache.giraph.comm.requests.SendWorkerMessagesRequest;
import org.apache.giraph.comm.requests.SendWorkerOneMessageToManyRequest;
import org.apache.giraph.conf.GiraphConfiguration;
import org.apache.giraph.conf.GiraphConstants;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.giraph.edge.Edge;
import org.apache.giraph.edge.EdgeFactory;
import org.apache.giraph.factories.TestMessageValueFactory;
import org.apache.giraph.graph.Vertex;
import org.apache.giraph.graph.VertexMutations;
import org.apache.giraph.metrics.GiraphMetrics;
import org.apache.giraph.partition.Partition;
import org.apache.giraph.partition.PartitionStore;
import org.apache.giraph.utils.ByteArrayOneMessageToManyIds;
import org.apache.giraph.utils.VertexIdMessages;
import org.apache.giraph.utils.ByteArrayVertexIdMessages;
import org.apache.giraph.utils.ExtendedDataOutput;
import org.apache.giraph.utils.IntNoOpComputation;
import org.apache.giraph.utils.MockUtils;
import org.apache.giraph.utils.PairList;
import org.apache.giraph.worker.WorkerInfo;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.mapreduce.Mapper.Context;
import org.junit.Before;
import org.junit.Test;

import com.google.common.collect.Lists;
import com.google.common.collect.Maps;

import java.io.IOException;
import java.util.Map;
import java.util.Map.Entry;
import java.util.concurrent.ConcurrentMap;

import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertTrue;
import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;


@SuppressWarnings("unchecked")
public class RequestTest {
  
  private ImmutableClassesGiraphConfiguration conf;
  
  private ServerData<IntWritable, IntWritable, IntWritable> serverData;
  
  private NettyServer server;
  
  private NettyClient client;
  
  private WorkerInfo workerInfo;

  @Before
  public void setUp() throws IOException {
    
    GiraphConfiguration tmpConf = new GiraphConfiguration();
    GiraphConstants.COMPUTATION_CLASS.set(tmpConf, IntNoOpComputation.class);
    conf = new ImmutableClassesGiraphConfiguration(tmpConf);

    @SuppressWarnings("rawtypes")
    Context context = mock(Context.class);
    when(context.getConfiguration()).thenReturn(conf);

    
    serverData = MockUtils.createNewServerData(conf, context);
    serverData.prepareSuperstep();
    workerInfo = new WorkerInfo();
    server = new NettyServer(conf,
        new WorkerRequestServerHandler.Factory(serverData), workerInfo,
            context, new MockExceptionHandler());
    server.start();

    workerInfo.setInetSocketAddress(server.getMyAddress());
    client = new NettyClient(context, conf, new WorkerInfo(),
        new MockExceptionHandler());
    client.connectAllAddresses(
        Lists.<WorkerInfo>newArrayList(workerInfo));
  }

  @Test
  public void sendVertexPartition() throws IOException {
    
    int partitionId = 13;
    Partition<IntWritable, IntWritable, IntWritable> partition =
        conf.createPartition(partitionId, null);
    for (int i = 0; i < 10; ++i) {
      Vertex vertex = conf.createVertex();
      vertex.initialize(new IntWritable(i), new IntWritable(i));
      partition.putVertex(vertex);
    }

    
    SendVertexRequest<IntWritable, IntWritable, IntWritable> request =
      new SendVertexRequest<IntWritable, IntWritable, IntWritable>(partition);
    client.sendWritableRequest(workerInfo.getTaskId(), request);
    client.waitAllRequests();

    
    client.stop();
    server.stop();

    
    PartitionStore<IntWritable, IntWritable, IntWritable> partitionStore =
        serverData.getPartitionStore();
    assertTrue(partitionStore.hasPartition(partitionId));
    int total = 0;
    Partition<IntWritable, IntWritable, IntWritable> partition2 =
        partitionStore.removePartition(partitionId);
    for (Vertex<IntWritable, IntWritable, IntWritable> vertex : partition2) {
      total += vertex.getId().get();
    }
    partitionStore.putPartition(partition2);
    assertEquals(total, 45);
    partitionStore.shutdown();
  }

  @Test
  public void sendWorkerMessagesRequest() throws IOException {
    
    PairList<Integer, VertexIdMessages<IntWritable,
            IntWritable>>
        dataToSend = new PairList<>();
    dataToSend.initialize();
    int partitionId = 0;
    ByteArrayVertexIdMessages<IntWritable,
            IntWritable> vertexIdMessages =
        new ByteArrayVertexIdMessages<>(
            new TestMessageValueFactory<>(IntWritable.class));
    vertexIdMessages.setConf(conf);
    vertexIdMessages.initialize();
    dataToSend.add(partitionId, vertexIdMessages);
    for (int i = 1; i < 7; ++i) {
      IntWritable vertexId = new IntWritable(i);
      for (int j = 0; j < i; ++j) {
        vertexIdMessages.add(vertexId, new IntWritable(j));
      }
    }

    
    SendWorkerMessagesRequest<IntWritable, IntWritable> request =
      new SendWorkerMessagesRequest<>(dataToSend);
    request.setConf(conf);

    client.sendWritableRequest(workerInfo.getTaskId(), request);
    client.waitAllRequests();

    
    client.stop();
    server.stop();

    
    Iterable<IntWritable> vertices =
        serverData.getIncomingMessageStore().getPartitionDestinationVertices(0);
    int keySum = 0;
    int messageSum = 0;
    for (IntWritable vertexId : vertices) {
      keySum += vertexId.get();
      Iterable<IntWritable> messages =
          serverData.<IntWritable>getIncomingMessageStore().getVertexMessages(
              vertexId);
      synchronized (messages) {
        for (IntWritable message : messages) {
          messageSum += message.get();
        }
      }
    }
    assertEquals(21, keySum);
    assertEquals(35, messageSum);
  }

  @Test
  public void sendWorkerIndividualMessagesRequest() throws IOException {
    
    ByteArrayOneMessageToManyIds<IntWritable, IntWritable>
        dataToSend = new ByteArrayOneMessageToManyIds<>(new
        TestMessageValueFactory<>(IntWritable.class));
    dataToSend.setConf(conf);
    dataToSend.initialize();
    ExtendedDataOutput output = conf.createExtendedDataOutput();
    for (int i = 1; i <= 7; ++i) {
      IntWritable vertexId = new IntWritable(i);
      vertexId.write(output);
    }
    dataToSend.add(output.getByteArray(), output.getPos(), 7, new IntWritable(1));

    
    SendWorkerOneMessageToManyRequest<IntWritable, IntWritable> request =
      new SendWorkerOneMessageToManyRequest<>(dataToSend, conf);
    client.sendWritableRequest(workerInfo.getTaskId(), request);
    client.waitAllRequests();

    
    client.stop();
    server.stop();

    
    Iterable<IntWritable> vertices =
        serverData.getIncomingMessageStore().getPartitionDestinationVertices(0);
    int keySum = 0;
    int messageSum = 0;
    for (IntWritable vertexId : vertices) {
      keySum += vertexId.get();
      Iterable<IntWritable> messages =
          serverData.<IntWritable>getIncomingMessageStore().getVertexMessages(
              vertexId);
      synchronized (messages) {
        for (IntWritable message : messages) {
          messageSum += message.get();
        }
      }
    }
    assertEquals(28, keySum);
    assertEquals(7, messageSum);
  }

  @Test
  public void sendPartitionMutationsRequest() throws IOException {
    
    int partitionId = 19;
    Map<IntWritable, VertexMutations<IntWritable, IntWritable,
    IntWritable>> vertexIdMutations =
        Maps.newHashMap();
    for (int i = 0; i < 11; ++i) {
      VertexMutations<IntWritable, IntWritable, IntWritable> mutations =
          new VertexMutations<IntWritable, IntWritable, IntWritable>();
      for (int j = 0; j < 3; ++j) {
        Vertex vertex = conf.createVertex();
        vertex.initialize(new IntWritable(i), new IntWritable(j));
        mutations.addVertex(vertex);
      }
      for (int j = 0; j < 2; ++j) {
        mutations.removeVertex();
      }
      for (int j = 0; j < 5; ++j) {
        Edge<IntWritable, IntWritable> edge =
            EdgeFactory.create(new IntWritable(i), new IntWritable(2 * j));
        mutations.addEdge(edge);
      }
      for (int j = 0; j < 7; ++j) {
        mutations.removeEdge(new IntWritable(j));
      }
      vertexIdMutations.put(new IntWritable(i), mutations);
    }

    
    SendPartitionMutationsRequest<IntWritable, IntWritable, IntWritable>
        request = new SendPartitionMutationsRequest<IntWritable, IntWritable,
        IntWritable>(partitionId,
        vertexIdMutations);
    GiraphMetrics.init(conf);
    client.sendWritableRequest(workerInfo.getTaskId(), request);
    client.waitAllRequests();

    
    client.stop();
    server.stop();

    
    ConcurrentMap<IntWritable,
        VertexMutations<IntWritable, IntWritable, IntWritable>>
        inVertexIdMutations =
        serverData.getPartitionMutations().get(partitionId);
    int keySum = 0;
    for (Entry<IntWritable,
        VertexMutations<IntWritable, IntWritable, IntWritable>> entry :
        inVertexIdMutations
        .entrySet()) {
      synchronized (entry.getValue()) {
        keySum += entry.getKey().get();
        int vertexValueSum = 0;
        for (Vertex<IntWritable, IntWritable, IntWritable> vertex : entry
            .getValue().getAddedVertexList()) {
          vertexValueSum += vertex.getValue().get();
        }
        assertEquals(3, vertexValueSum);
        assertEquals(2, entry.getValue().getRemovedVertexCount());
        int removeEdgeValueSum = 0;
        for (Edge<IntWritable, IntWritable> edge : entry.getValue()
            .getAddedEdgeList()) {
          removeEdgeValueSum += edge.getValue().get();
        }
        assertEquals(20, removeEdgeValueSum);
      }
    }
    assertEquals(55, keySum);
  }
}
<code block>


package org.apache.giraph.comm.messages;

import java.io.IOException;
import java.util.Iterator;

import junit.framework.Assert;

import org.apache.giraph.bsp.CentralizedServiceWorker;
import org.apache.giraph.combiner.FloatSumMessageCombiner;
import org.apache.giraph.comm.messages.primitives.IntByteArrayMessageStore;
import org.apache.giraph.comm.messages.primitives.IntFloatMessageStore;
import org.apache.giraph.conf.GiraphConfiguration;
import org.apache.giraph.conf.GiraphConstants;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.giraph.factories.TestMessageValueFactory;
import org.apache.giraph.graph.BasicComputation;
import org.apache.giraph.graph.Vertex;
import org.apache.giraph.partition.Partition;
import org.apache.giraph.partition.PartitionStore;
import org.apache.giraph.utils.ByteArrayVertexIdMessages;
import org.apache.hadoop.io.FloatWritable;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Writable;
import org.junit.Before;
import org.junit.Test;
import org.mockito.Mockito;
import org.mockito.invocation.InvocationOnMock;
import org.mockito.stubbing.Answer;

import com.google.common.collect.Iterables;
import com.google.common.collect.Lists;

public class TestIntFloatPrimitiveMessageStores {
  private static final int NUM_PARTITIONS = 2;
  private static CentralizedServiceWorker<IntWritable, Writable, Writable>
    service;
  private static ImmutableClassesGiraphConfiguration<IntWritable, Writable,
      Writable> conf;

  @Before
  public void prepare() throws IOException {
    service = Mockito.mock(CentralizedServiceWorker.class);
    Mockito.when(
        service.getPartitionId(Mockito.any(IntWritable.class))).thenAnswer(
        new Answer<Integer>() {
          @Override
          public Integer answer(InvocationOnMock invocation) {
            IntWritable vertexId = (IntWritable) invocation.getArguments()[0];
            return vertexId.get() % NUM_PARTITIONS;
          }
        }
    );
    PartitionStore partitionStore = Mockito.mock(PartitionStore.class);
    Mockito.when(service.getPartitionStore()).thenReturn(partitionStore);
    Mockito.when(partitionStore.getPartitionIds()).thenReturn(
        Lists.newArrayList(0, 1));
    Partition partition = Mockito.mock(Partition.class);
    Mockito.when(partition.getVertexCount()).thenReturn(Long.valueOf(1));
    Mockito.when(partitionStore.getNextPartition()).thenReturn(partition);
    Mockito.when(partitionStore.getNextPartition()).thenReturn(partition);

    GiraphConfiguration initConf = new GiraphConfiguration();
    initConf.setComputationClass(IntFloatNoOpComputation.class);
    conf = new ImmutableClassesGiraphConfiguration(initConf);
  }

  private static class IntFloatNoOpComputation extends
      BasicComputation<IntWritable, NullWritable, NullWritable,
          FloatWritable> {
    @Override
    public void compute(Vertex<IntWritable, NullWritable, NullWritable> vertex,
        Iterable<FloatWritable> messages) throws IOException {
    }
  }

  private static ByteArrayVertexIdMessages<IntWritable, FloatWritable>
  createIntFloatMessages() {
    ByteArrayVertexIdMessages<IntWritable, FloatWritable> messages =
        new ByteArrayVertexIdMessages<IntWritable, FloatWritable>(
            new TestMessageValueFactory<FloatWritable>(FloatWritable.class));
    messages.setConf(conf);
    messages.initialize();
    return messages;
  }

  private static void insertIntFloatMessages(
      MessageStore<IntWritable, FloatWritable> messageStore) throws
      IOException {
    ByteArrayVertexIdMessages<IntWritable, FloatWritable> messages =
        createIntFloatMessages();
    messages.add(new IntWritable(0), new FloatWritable(1));
    messages.add(new IntWritable(2), new FloatWritable(3));
    messages.add(new IntWritable(0), new FloatWritable(4));
    messageStore.addPartitionMessages(0, messages);
    messages = createIntFloatMessages();
    messages.add(new IntWritable(1), new FloatWritable(1));
    messages.add(new IntWritable(1), new FloatWritable(3));
    messages.add(new IntWritable(1), new FloatWritable(4));
    messageStore.addPartitionMessages(1, messages);
    messages = createIntFloatMessages();
    messages.add(new IntWritable(0), new FloatWritable(5));
    messageStore.addPartitionMessages(0, messages);
  }

  @Test
  public void testIntFloatMessageStore() throws IOException {
    IntFloatMessageStore messageStore =
        new IntFloatMessageStore(service, new FloatSumMessageCombiner());
    insertIntFloatMessages(messageStore);

    Iterable<FloatWritable> m0 =
        messageStore.getVertexMessages(new IntWritable(0));
    Assert.assertEquals(1, Iterables.size(m0));
    Assert.assertEquals((float) 10.0, m0.iterator().next().get());
    Iterable<FloatWritable> m1 =
        messageStore.getVertexMessages(new IntWritable(1));
    Assert.assertEquals(1, Iterables.size(m1));
    Assert.assertEquals((float) 8.0, m1.iterator().next().get());
    Iterable<FloatWritable> m2 =
        messageStore.getVertexMessages(new IntWritable(2));
    Assert.assertEquals(1, Iterables.size(m2));
    Assert.assertEquals((float) 3.0, m2.iterator().next().get());
    Assert.assertTrue(
        Iterables.isEmpty(messageStore.getVertexMessages(new IntWritable(3))));
  }

  @Test
  public void testIntByteArrayMessageStore() throws IOException {
    IntByteArrayMessageStore<FloatWritable> messageStore =
        new IntByteArrayMessageStore<FloatWritable>(new
            TestMessageValueFactory<FloatWritable>(FloatWritable.class),
            service, conf);
    insertIntFloatMessages(messageStore);

    Iterable<FloatWritable> m0 =
        messageStore.getVertexMessages(new IntWritable(0));
    Assert.assertEquals(3, Iterables.size(m0));
    Iterator<FloatWritable> i0 = m0.iterator();
    Assert.assertEquals((float) 1.0, i0.next().get());
    Assert.assertEquals((float) 4.0, i0.next().get());
    Assert.assertEquals((float) 5.0, i0.next().get());
    Iterable<FloatWritable> m1 =
        messageStore.getVertexMessages(new IntWritable(1));
    Assert.assertEquals(3, Iterables.size(m1));
    Iterator<FloatWritable> i1 = m1.iterator();
    Assert.assertEquals((float) 1.0, i1.next().get());
    Assert.assertEquals((float) 3.0, i1.next().get());
    Assert.assertEquals((float) 4.0, i1.next().get());
    Iterable<FloatWritable> m2 =
        messageStore.getVertexMessages(new IntWritable(2));
    Assert.assertEquals(1, Iterables.size(m2));
    Assert.assertEquals((float) 3.0, m2.iterator().next().get());
    Assert.assertTrue(
        Iterables.isEmpty(messageStore.getVertexMessages(new IntWritable(3))));
  }

  @Test
  public void testIntByteArrayMessageStoreWithMessageEncoding() throws
      IOException {
    GiraphConstants.USE_MESSAGE_SIZE_ENCODING.set(conf, true);
    testIntByteArrayMessageStore();
    GiraphConstants.USE_MESSAGE_SIZE_ENCODING.set(conf, false);
  }
}

<code block>


package org.apache.giraph.comm.messages;

import java.io.IOException;
import java.util.Iterator;

import junit.framework.Assert;

import org.apache.giraph.bsp.CentralizedServiceWorker;
import org.apache.giraph.combiner.DoubleSumMessageCombiner;
import org.apache.giraph.comm.messages.primitives.long_id.LongByteArrayMessageStore;
import org.apache.giraph.comm.messages.primitives.LongDoubleMessageStore;
import org.apache.giraph.conf.GiraphConfiguration;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.giraph.factories.TestMessageValueFactory;
import org.apache.giraph.graph.BasicComputation;
import org.apache.giraph.graph.Vertex;
import org.apache.giraph.partition.Partition;
import org.apache.giraph.partition.PartitionStore;
import org.apache.giraph.utils.ByteArrayVertexIdMessages;
import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Writable;
import org.junit.Before;
import org.junit.Test;
import org.mockito.Mockito;
import org.mockito.invocation.InvocationOnMock;
import org.mockito.stubbing.Answer;

import com.google.common.collect.Iterables;
import com.google.common.collect.Lists;

public class TestLongDoublePrimitiveMessageStores {
  private static final int NUM_PARTITIONS = 2;
  private static CentralizedServiceWorker<LongWritable, Writable, Writable>
    service;

  @Before
  public void prepare() throws IOException {
    service = Mockito.mock(CentralizedServiceWorker.class);
    Mockito.when(
        service.getPartitionId(Mockito.any(LongWritable.class))).thenAnswer(
        new Answer<Integer>() {
          @Override
          public Integer answer(InvocationOnMock invocation) {
            LongWritable vertexId = (LongWritable) invocation.getArguments()[0];
            return (int) (vertexId.get() % NUM_PARTITIONS);
          }
        }
    );
    PartitionStore partitionStore = Mockito.mock(PartitionStore.class);
    Mockito.when(service.getPartitionStore()).thenReturn(partitionStore);
    Mockito.when(partitionStore.getPartitionIds()).thenReturn(
        Lists.newArrayList(0, 1));
    Partition partition = Mockito.mock(Partition.class);
    Mockito.when(partition.getVertexCount()).thenReturn(Long.valueOf(1));
    Mockito.when(partitionStore.getNextPartition()).thenReturn(partition);
    Mockito.when(partitionStore.getNextPartition()).thenReturn(partition);
  }

  private static class LongDoubleNoOpComputation extends
      BasicComputation<LongWritable, NullWritable, NullWritable,
          DoubleWritable> {
    @Override
    public void compute(Vertex<LongWritable, NullWritable, NullWritable> vertex,
        Iterable<DoubleWritable> messages) throws IOException {
    }
  }

  private static ImmutableClassesGiraphConfiguration<LongWritable, Writable,
    Writable> createLongDoubleConf() {

    GiraphConfiguration initConf = new GiraphConfiguration();
    initConf.setComputationClass(LongDoubleNoOpComputation.class);
    return new ImmutableClassesGiraphConfiguration(initConf);
  }

  private static ByteArrayVertexIdMessages<LongWritable, DoubleWritable>
  createLongDoubleMessages() {
    ByteArrayVertexIdMessages<LongWritable, DoubleWritable> messages =
        new ByteArrayVertexIdMessages<LongWritable, DoubleWritable>(
            new TestMessageValueFactory<DoubleWritable>(DoubleWritable.class));
    messages.setConf(createLongDoubleConf());
    messages.initialize();
    return messages;
  }

  private static void insertLongDoubleMessages(
      MessageStore<LongWritable, DoubleWritable> messageStore) throws
      IOException {
    ByteArrayVertexIdMessages<LongWritable, DoubleWritable> messages =
        createLongDoubleMessages();
    messages.add(new LongWritable(0), new DoubleWritable(1));
    messages.add(new LongWritable(2), new DoubleWritable(3));
    messages.add(new LongWritable(0), new DoubleWritable(4));
    messageStore.addPartitionMessages(0, messages);
    messages = createLongDoubleMessages();
    messages.add(new LongWritable(1), new DoubleWritable(1));
    messages.add(new LongWritable(1), new DoubleWritable(3));
    messages.add(new LongWritable(1), new DoubleWritable(4));
    messageStore.addPartitionMessages(1, messages);
    messages = createLongDoubleMessages();
    messages.add(new LongWritable(0), new DoubleWritable(5));
    messageStore.addPartitionMessages(0, messages);
  }

  @Test
  public void testLongDoubleMessageStore() throws IOException {
    LongDoubleMessageStore messageStore =
        new LongDoubleMessageStore(service, new DoubleSumMessageCombiner());
    insertLongDoubleMessages(messageStore);

    Iterable<DoubleWritable> m0 =
        messageStore.getVertexMessages(new LongWritable(0));
    Assert.assertEquals(1, Iterables.size(m0));
    Assert.assertEquals(10.0, m0.iterator().next().get());
    Iterable<DoubleWritable> m1 =
        messageStore.getVertexMessages(new LongWritable(1));
    Assert.assertEquals(1, Iterables.size(m1));
    Assert.assertEquals(8.0, m1.iterator().next().get());
    Iterable<DoubleWritable> m2 =
        messageStore.getVertexMessages(new LongWritable(2));
    Assert.assertEquals(1, Iterables.size(m2));
    Assert.assertEquals(3.0, m2.iterator().next().get());
    Assert.assertTrue(
        Iterables.isEmpty(messageStore.getVertexMessages(new LongWritable(3))));
  }

  @Test
  public void testLongByteArrayMessageStore() throws IOException {
    LongByteArrayMessageStore<DoubleWritable> messageStore =
        new LongByteArrayMessageStore<DoubleWritable>(
            new TestMessageValueFactory<DoubleWritable>(DoubleWritable.class),
            service, createLongDoubleConf());
    insertLongDoubleMessages(messageStore);

    Iterable<DoubleWritable> m0 =
        messageStore.getVertexMessages(new LongWritable(0));
    Assert.assertEquals(3, Iterables.size(m0));
    Iterator<DoubleWritable> i0 = m0.iterator();
    Assert.assertEquals(1.0, i0.next().get());
    Assert.assertEquals(4.0, i0.next().get());
    Assert.assertEquals(5.0, i0.next().get());
    Iterable<DoubleWritable> m1 =
        messageStore.getVertexMessages(new LongWritable(1));
    Assert.assertEquals(3, Iterables.size(m1));
    Iterator<DoubleWritable> i1 = m1.iterator();
    Assert.assertEquals(1.0, i1.next().get());
    Assert.assertEquals(3.0, i1.next().get());
    Assert.assertEquals(4.0, i1.next().get());
    Iterable<DoubleWritable> m2 =
        messageStore.getVertexMessages(new LongWritable(2));
    Assert.assertEquals(1, Iterables.size(m2));
    Assert.assertEquals(3.0, m2.iterator().next().get());
    Assert.assertTrue(
        Iterables.isEmpty(messageStore.getVertexMessages(new LongWritable(3))));
  }
}

<code block>


package org.apache.giraph.partition;

import static org.apache.giraph.conf.GiraphConstants.MAX_PARTITIONS_IN_MEMORY;
import static org.apache.giraph.conf.GiraphConstants.MAX_STICKY_PARTITIONS;
import static org.apache.giraph.conf.GiraphConstants.NUM_COMPUTE_THREADS;
import static org.apache.giraph.conf.GiraphConstants.NUM_INPUT_THREADS;
import static org.apache.giraph.conf.GiraphConstants.NUM_OUTPUT_THREADS;
import static org.apache.giraph.conf.GiraphConstants.PARTITIONS_DIRECTORY;

import java.io.BufferedInputStream;
import java.io.BufferedOutputStream;
import java.io.DataInput;
import java.io.DataInputStream;
import java.io.DataOutput;
import java.io.DataOutputStream;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.util.Iterator;
import java.util.Map;
import java.util.Map.Entry;
import java.util.concurrent.ConcurrentMap;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicLong;

import org.apache.giraph.bsp.CentralizedServiceWorker;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.giraph.edge.OutEdges;
import org.apache.giraph.graph.Vertex;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Mapper.Context;
import org.apache.log4j.Logger;

import com.google.common.collect.Iterables;
import com.google.common.collect.Maps;
import com.google.common.hash.HashFunction;
import com.google.common.hash.Hashing;


@SuppressWarnings("rawtypes")
public class DiskBackedPartitionStore<I extends WritableComparable,
    V extends Writable, E extends Writable>
    extends PartitionStore<I, V, E> {
  
  private static final Logger LOG =
      Logger.getLogger(DiskBackedPartitionStore.class);
  
  private enum State { INIT, ACTIVE, INACTIVE, ONDISK };

  
  private final ConcurrentMap<Integer, MetaPartition> partitions =
    Maps.newConcurrentMap();
  
  private final Map<Integer, MetaPartition> lru = Maps.newLinkedHashMap();

  
  private final
  ImmutableClassesGiraphConfiguration<I, V, E> conf;
  
  private final Context context;
  
  private final String[] basePaths;
  
  private final HashFunction hasher = Hashing.murmur3_32();
  
  private final int maxPartitionsInMem;
  
  private AtomicInteger numPartitionsInMem;
  
  private CentralizedServiceWorker<I, V, E> serviceWorker;
  
  private AtomicLong numOfStickyPartitions;
  
  private long passedThroughEdges;

  
  public DiskBackedPartitionStore(
      ImmutableClassesGiraphConfiguration<I, V, E> conf,
      Mapper<?, ?, ?, ?>.Context context,
      CentralizedServiceWorker<I, V, E> serviceWorker) {
    this.conf = conf;
    this.context = context;
    this.serviceWorker = serviceWorker;

    this.passedThroughEdges = 0;
    this.numPartitionsInMem = new AtomicInteger(0);

    
    this.maxPartitionsInMem = Math.max(MAX_PARTITIONS_IN_MEMORY.get(conf), 1);

    int numInputThreads = NUM_INPUT_THREADS.get(conf);
    int numComputeThreads = NUM_COMPUTE_THREADS.get(conf);
    int numOutputThreads = NUM_OUTPUT_THREADS.get(conf);

    int maxThreads =
      Math.max(numInputThreads,
        Math.max(numComputeThreads, numOutputThreads));

    
    long maxSticky = MAX_STICKY_PARTITIONS.get(conf);

    
    if (maxSticky > 0 && maxPartitionsInMem - maxSticky >= maxThreads) {
      this.numOfStickyPartitions = new AtomicLong(maxSticky);
    } else {
      if (maxPartitionsInMem - maxSticky >= maxThreads) {
        if (LOG.isInfoEnabled()) {
          LOG.info("giraph.maxSticky parameter unset or improperly set " +
            "resetting to automatically computed value.");
        }
      }

      if (maxPartitionsInMem == 1 || maxThreads >= maxPartitionsInMem) {
        this.numOfStickyPartitions = new AtomicLong(0);
      } else {
        this.numOfStickyPartitions =
          new AtomicLong(maxPartitionsInMem - maxThreads);
      }
    }

    
    String[] userPaths = PARTITIONS_DIRECTORY.getArray(conf);
    basePaths = new String[userPaths.length];
    int i = 0;
    for (String path : userPaths) {
      basePaths[i++] = path + "/" + conf.get("mapred.job.id", "Unknown Job");
    }
    if (LOG.isInfoEnabled()) {
      LOG.info("DiskBackedPartitionStore with maxInMemoryPartitions=" +
        maxPartitionsInMem + ", isStaticGraph=" + conf.isStaticGraph());
    }
  }

  @Override
  public Iterable<Integer> getPartitionIds() {
    return Iterables.unmodifiableIterable(partitions.keySet());
  }

  @Override
  public boolean hasPartition(final Integer id) {
    return partitions.containsKey(id);
  }

  @Override
  public int getNumPartitions() {
    return partitions.size();
  }

  @Override
  public long getPartitionVertexCount(int partitionId) {
    MetaPartition meta = partitions.get(partitionId);
    if (meta.getState() == State.ONDISK) {
      return meta.getVertexCount();
    } else {
      return meta.getPartition().getVertexCount();
    }
  }

  @Override
  public long getPartitionEdgeCount(int partitionId) {
    MetaPartition meta = partitions.get(partitionId);
    if (meta.getState() == State.ONDISK) {
      return meta.getEdgeCount();
    } else {
      return meta.getPartition().getEdgeCount();
    }
  }

  @Override
  public Partition<I, V, E> getOrCreatePartition(Integer id) {
    MetaPartition meta = new MetaPartition(id);
    MetaPartition temp;

    temp = partitions.putIfAbsent(id, meta);
    if (temp != null) {
      meta = temp;
    }

    synchronized (meta) {
      if (temp == null && numOfStickyPartitions.getAndDecrement() > 0) {
        meta.setSticky();
      }
      getPartition(meta);
      if (meta.getPartition() == null) {
        Partition<I, V, E> partition = conf.createPartition(id, context);
        meta.setPartition(partition);
        addPartition(meta, partition);
        if (meta.getState() == State.INIT) {
          LOG.warn("Partition was still INIT after ADD (not possibile).");
        }
        
        
        
        
        getPartition(meta);
        if (meta.getState() == State.INIT) {
          LOG.warn("Partition was still INIT after GET (not possibile).");
        }
      }

      if (meta.getState() == State.INIT) {
        String msg = "Getting a partition which is in INIT state is " +
                     "not allowed. " + meta;
        LOG.error(msg);
        throw new IllegalStateException(msg);
      }

      return meta.getPartition();
    }
  }

  @Override
  public void putPartition(Partition<I, V, E> partition) {
    Integer id = partition.getId();
    MetaPartition meta = partitions.get(id);
    putPartition(meta);
  }

  @Override
  public void deletePartition(Integer id) {
    if (hasPartition(id)) {
      MetaPartition meta = partitions.get(id);
      deletePartition(meta);
    }
  }

  @Override
  public Partition<I, V, E> removePartition(Integer id) {
    if (hasPartition(id)) {
      MetaPartition meta;

      meta = partitions.get(id);
      synchronized (meta) {
        getPartition(meta);
        putPartition(meta);
        deletePartition(id);
      }
      return meta.getPartition();
    }
    return null;
  }

  @Override
  public void addPartition(Partition<I, V, E> partition) {
    Integer id = partition.getId();
    MetaPartition meta = new MetaPartition(id);
    MetaPartition temp;

    meta.setPartition(partition);
    temp = partitions.putIfAbsent(id, meta);
    if (temp != null) {
      meta = temp;
    }

    synchronized (meta) {
      if (temp == null && numOfStickyPartitions.getAndDecrement() > 0) {
        meta.setSticky();
      }
      addPartition(meta, partition);
    }
  }

  @Override
  public void shutdown() {
    for (MetaPartition e : partitions.values()) {
      if (e.getState() == State.ONDISK) {
        deletePartitionFiles(e.getId());
      }
    }
  }

  @Override
  public String toString() {
    StringBuilder sb = new StringBuilder();
    for (MetaPartition e : partitions.values()) {
      sb.append(e.toString() + "\n");
    }
    return sb.toString();
  }

  
  private void writeVertexData(DataOutput output, Vertex<I, V, E> vertex)
    throws IOException {

    vertex.getId().write(output);
    vertex.getValue().write(output);
    output.writeBoolean(vertex.isHalted());
  }

  
  @SuppressWarnings("unchecked")
  private void writeOutEdges(DataOutput output, Vertex<I, V, E> vertex)
    throws IOException {

    vertex.getId().write(output);
    OutEdges<I, E> edges = (OutEdges<I, E>) vertex.getEdges();
    edges.write(output);
  }

  
  private void readVertexData(DataInput in, Vertex<I, V, E> vertex)
    throws IOException {

    I id = conf.createVertexId();
    id.readFields(in);
    V value = conf.createVertexValue();
    value.readFields(in);
    OutEdges<I, E> edges = conf.createAndInitializeOutEdges(0);
    vertex.initialize(id, value, edges);
    if (in.readBoolean()) {
      vertex.voteToHalt();
    } else {
      vertex.wakeUp();
    }
  }

  
  @SuppressWarnings("unchecked")
  private void readOutEdges(DataInput in, Partition<I, V, E> partition)
    throws IOException {

    I id = conf.createVertexId();
    id.readFields(in);
    Vertex<I, V, E> v = partition.getVertex(id);
    OutEdges<I, E> edges = (OutEdges<I, E>) v.getEdges();
    edges.readFields(in);
    partition.saveVertex(v);
  }

  
  private Partition<I, V, E> loadPartition(int id, long numVertices)
    throws IOException {

    Partition<I, V, E> partition = conf.createPartition(id, context);

    
    File file = new File(getVerticesPath(id));
    if (LOG.isDebugEnabled()) {
      LOG.debug("loadPartition: loading partition vertices " +
        partition.getId() + " from " + file.getAbsolutePath());
    }

    FileInputStream filein = new FileInputStream(file);
    BufferedInputStream bufferin = new BufferedInputStream(filein);
    DataInputStream inputStream  = new DataInputStream(bufferin);
    for (int i = 0; i < numVertices; ++i) {
      Vertex<I, V , E> vertex = conf.createVertex();
      readVertexData(inputStream, vertex);
      partition.putVertex(vertex);
    }
    inputStream.close();
    if (!file.delete()) {
      String msg = "loadPartition: failed to delete " + file.getAbsolutePath();
      LOG.error(msg);
      throw new IllegalStateException(msg);
    }

    
    file = new File(getEdgesPath(id));

    if (LOG.isDebugEnabled()) {
      LOG.debug("loadPartition: loading partition edges " +
        partition.getId() + " from " + file.getAbsolutePath());
    }

    filein = new FileInputStream(file);
    bufferin = new BufferedInputStream(filein);
    inputStream  = new DataInputStream(bufferin);
    for (int i = 0; i < numVertices; ++i) {
      readOutEdges(inputStream, partition);
    }
    inputStream.close();
    
    if (!conf.isStaticGraph() && !file.delete()) {
      String msg = "loadPartition: failed to delete " + file.getAbsolutePath();
      LOG.error(msg);
      throw new IllegalStateException(msg);
    }
    return partition;
  }

  
  private void offloadPartition(MetaPartition meta) throws IOException {

    Partition<I, V, E> partition = meta.getPartition();
    File file = new File(getVerticesPath(partition.getId()));
    File parent = file.getParentFile();
    if (!parent.exists() && !parent.mkdirs() && LOG.isDebugEnabled()) {
      LOG.debug("offloadPartition: directory " + parent.getAbsolutePath() +
        " already exists.");
    }

    if (!file.createNewFile()) {
      String msg = "offloadPartition: file " + parent.getAbsolutePath() +
        " already exists.";
      LOG.error(msg);
      throw new IllegalStateException(msg);
    }

    if (LOG.isDebugEnabled()) {
      LOG.debug("offloadPartition: writing partition vertices " +
        partition.getId() + " to " + file.getAbsolutePath());
    }

    FileOutputStream fileout = new FileOutputStream(file);
    BufferedOutputStream bufferout = new BufferedOutputStream(fileout);
    DataOutputStream outputStream  = new DataOutputStream(bufferout);
    for (Vertex<I, V, E> vertex : partition) {
      writeVertexData(outputStream, vertex);
    }
    outputStream.close();

    
    
    
    
    file = new File(getEdgesPath(partition.getId()));
    if (meta.getPrevVertexCount() != partition.getVertexCount() ||
        !conf.isStaticGraph() || !file.exists()) {

      meta.setPrevVertexCount(partition.getVertexCount());

      if (!file.createNewFile() && LOG.isDebugEnabled()) {
        LOG.debug("offloadPartition: file " + file.getAbsolutePath() +
            " already exists.");
      }

      if (LOG.isDebugEnabled()) {
        LOG.debug("offloadPartition: writing partition edges " +
          partition.getId() + " to " + file.getAbsolutePath());
      }

      fileout = new FileOutputStream(file);
      bufferout = new BufferedOutputStream(fileout);
      outputStream = new DataOutputStream(bufferout);
      for (Vertex<I, V, E> vertex : partition) {
        writeOutEdges(outputStream, vertex);
      }
      outputStream.close();
    }
  }

  
  private void addToOOCPartition(MetaPartition meta,
    Partition<I, V, E> partition) throws IOException {

    Integer id = partition.getId();
    File file = new File(getVerticesPath(id));
    DataOutputStream outputStream = null;

    FileOutputStream fileout = new FileOutputStream(file, true);
    BufferedOutputStream bufferout = new BufferedOutputStream(fileout);
    outputStream = new DataOutputStream(bufferout);
    for (Vertex<I, V, E> vertex : partition) {
      writeVertexData(outputStream, vertex);
    }
    outputStream.close();

    file = new File(getEdgesPath(id));
    fileout = new FileOutputStream(file, true);
    bufferout = new BufferedOutputStream(fileout);
    outputStream = new DataOutputStream(bufferout);
    for (Vertex<I, V, E> vertex : partition) {
      writeOutEdges(outputStream, vertex);
    }
    outputStream.close();
  }

  
  public void deletePartitionFiles(Integer id) {
    
    File file = new File(getVerticesPath(id));
    if (file.exists() && !file.delete()) {
      String msg = "deletePartitionFiles: Failed to delete file " +
        file.getAbsolutePath();
      LOG.error(msg);
      throw new IllegalStateException(msg);
    }

    
    file = new File(getEdgesPath(id));
    if (file.exists() && !file.delete()) {
      String msg = "deletePartitionFiles: Failed to delete file " +
        file.getAbsolutePath();
      LOG.error(msg);
      throw new IllegalStateException(msg);
    }
  }

  
  private String getPartitionPath(Integer partitionId) {
    int hash = hasher.hashInt(partitionId).asInt();
    int idx = Math.abs(hash % basePaths.length);
    return basePaths[idx] + "/partition-" + partitionId;
  }

  
  private String getVerticesPath(Integer partitionId) {
    return getPartitionPath(partitionId) + "_vertices";
  }

  
  private String getEdgesPath(Integer partitionId) {
    return getPartitionPath(partitionId) + "_edges";
  }

  
  private MetaPartition getLRUPartition() {
    synchronized (lru) {
      Iterator<Entry<Integer, MetaPartition>> i =
          lru.entrySet().iterator();
      Entry<Integer, MetaPartition> entry = i.next();
      i.remove();
      return entry.getValue();
    }
  }

  
  @edu.umd.cs.findbugs.annotations.SuppressWarnings(
    value = "TLW_TWO_LOCK_WAIT",
    justification = "The two locks held do not produce a deadlock")
  private void getPartition(MetaPartition meta) {
    synchronized (meta) {
      boolean isNotDone = true;

      if (meta.getState() != State.INIT) {
        State state;

        while (isNotDone) {
          state = meta.getState();
          switch (state) {
          case ONDISK:
            MetaPartition swapOutPartition = null;
            long numVertices = meta.getVertexCount();

          synchronized (lru) {
            try {
              while (numPartitionsInMem.get() >= maxPartitionsInMem &&
                     lru.isEmpty()) {
                
                
                
                
                
                
                lru.wait();
              }
            } catch (InterruptedException e) {
              LOG.error("getPartition: error while waiting on " +
                "LRU data structure: " + e.getMessage());
              throw new IllegalStateException(e);
            }

            
            
            
            
            
            
            
            
            if (numPartitionsInMem.get() >= maxPartitionsInMem &&
                !lru.isEmpty()) {
              swapOutPartition = getLRUPartition();
            } else if (numPartitionsInMem.get() < maxPartitionsInMem) {
              numPartitionsInMem.getAndIncrement();
            } else {
              String msg = "lru is empty and there is not space in memory, " +
                           "hence the partition cannot be loaded.";
              LOG.error(msg);
              throw new IllegalStateException(msg);
            }
          }

            if (swapOutPartition != null) {
              synchronized (swapOutPartition) {
                if (swapOutPartition.isSticky()) {
                  String msg = "Partition " + meta.getId() + " is sticky " +
                    " and cannot be offloaded.";
                  LOG.error(msg);
                  throw new IllegalStateException(msg);
                }
                
                if (swapOutPartition.getState() != State.INACTIVE) {
                  String msg = "Someone is holding the partition with id " +
                    swapOutPartition.getId() + " but is supposed to be " +
                    "inactive.";
                  LOG.error(msg);
                  throw new IllegalStateException(msg);
                }

                try {
                  offloadPartition(swapOutPartition);
                  Partition<I, V, E> p = swapOutPartition.getPartition();
                  swapOutPartition.setOnDisk(p);
                  
                  
                  
                  swapOutPartition.notifyAll();
                } catch (IOException e)  {
                  LOG.error("getPartition: Failed while Offloading " +
                    "New Partition: " + e.getMessage());
                  throw new IllegalStateException(e);
                }
              }
            }

            
            
            
            
            
            
            
            
            
            
            Partition<I, V, E> partition;
            try {
              partition = loadPartition(meta.getId(), numVertices);
            } catch (IOException e)  {
              LOG.error("getPartition: Failed while Loading Partition from " +
                "disk: " + e.getMessage());
              throw new IllegalStateException(e);
            }
            meta.setActive(partition);

            isNotDone = false;
            break;
          case INACTIVE:
            MetaPartition p = null;

            if (meta.isSticky()) {
              meta.setActive();
              isNotDone = false;
              break;
            }

          synchronized (lru) {
            p = lru.remove(meta.getId());
          }
            if (p == meta && p.getState() == State.INACTIVE) {
              meta.setActive();
              isNotDone = false;
            } else {
              try {
                
                
                
                
                
                
                meta.wait();
              } catch (InterruptedException e) {
                LOG.error("getPartition: error while waiting on " +
                  "previously Inactive Partition: " + e.getMessage());
                throw new IllegalStateException(e);
              }
              isNotDone = true;
            }
            break;
          case ACTIVE:
            meta.incrementReferences();
            isNotDone = false;
            break;
          default:
            throw new IllegalStateException("illegal state " + meta.getState() +
              " for partition " + meta.getId());
          }
        }
      }
    }
  }

  
  private void putPartition(MetaPartition meta) {
    synchronized (meta) {
      if (meta.getState() != State.ACTIVE) {
        String msg = "It is not possible to put back a partition which is " +
          "not ACTIVE.\n" + meta.toString();
        LOG.error(msg);
        throw new IllegalStateException(msg);
      }

      if (meta.decrementReferences() == 0) {
        meta.setState(State.INACTIVE);
        if (!meta.isSticky()) {
          synchronized (lru) {
            lru.put(meta.getId(), meta);
            lru.notifyAll();  
                              
          }
        }
        meta.notifyAll(); 
                          
                          
      }
    }
  }

  
  private void addPartition(MetaPartition meta, Partition<I, V, E> partition) {
    synchronized (meta) {
      
      
      if (meta.getState() == State.INIT) {
        
        if (partition == null) {
          String msg = "No partition was provided.";
          LOG.error(msg);
          throw new IllegalStateException(msg);
        }

        
        
        if (partition != meta.getPartition()) {
          String msg = "Partition and Meta-Partition should " +
            "contain the same data";
          LOG.error(msg);
          throw new IllegalStateException(msg);
        }

        synchronized (lru) {
          if (numPartitionsInMem.get() < maxPartitionsInMem || meta.isSticky) {
            meta.setState(State.INACTIVE);
            numPartitionsInMem.getAndIncrement();
            if (!meta.isSticky) {
              lru.put(meta.getId(), meta);
              lru.notifyAll();  
                                
            }
            return; 
                    
                    
                    
          }
        }

        
        try {
          offloadPartition(meta);
          meta.setOnDisk(partition);
        } catch (IOException e)  {
          LOG.error("addPartition: Failed while Offloading New Partition: " +
            e.getMessage());
          throw new IllegalStateException(e);
        }
      } else {
        Partition<I, V, E> existing = null;
        boolean isOOC = false;
        boolean isNotDone = true;
        State state;

        while (isNotDone) {
          state = meta.getState();
          switch (state) {
          case ONDISK:
            isOOC = true;
            isNotDone = false;
            meta.addToVertexCount(partition.getVertexCount());
            break;
          case INACTIVE:
            MetaPartition p = null;

            if (meta.isSticky()) {
              existing = meta.getPartition();
              isNotDone = false;
              break;
            }

          synchronized (lru) {
            p = lru.get(meta.getId());
          }
            
            
            
            
            
            
            
            if (p == meta) {
              existing = meta.getPartition();
              isNotDone = false;
            } else {
              try {
                
                
                
                
                
                
                meta.wait();
              } catch (InterruptedException e) {
                LOG.error("addPartition: error while waiting on " +
                  "previously inactive partition: " + e.getMessage());
                throw new IllegalStateException(e);
              }

              isNotDone = true;
            }
            break;
          case ACTIVE:
            existing = meta.getPartition();
            isNotDone = false;
            break;
          default:
            throw new IllegalStateException("illegal state " + state +
              " for partition " + meta.getId());
          }
        }

        if (isOOC) {
          try {
            addToOOCPartition(meta, partition);
          } catch (IOException e) {
            LOG.error("addPartition: Failed while Adding to OOC Partition: " +
              e.getMessage());
            throw new IllegalStateException(e);
          }
        } else {
          existing.addPartition(partition);
        }
      }
    }
  }

  
  private void deletePartition(MetaPartition meta) {
    synchronized (meta) {
      boolean isDone = false;
      int id = meta.getId();

      State state;
      while (!isDone) {
        state = meta.getState();
        switch (state) {
        case ONDISK:
          deletePartitionFiles(id);
          isDone = true;
          break;
        case INACTIVE:
          MetaPartition p;

          if (meta.isSticky()) {
            isDone = true;
            numPartitionsInMem.getAndDecrement();
            break;
          }

        synchronized (lru) {
          p = lru.remove(id);
          if (p == meta && p.getState() == State.INACTIVE) {
            isDone = true;
            numPartitionsInMem.getAndDecrement();
            lru.notifyAll(); 
                             
            
            
            
            break;
          }
        }
          try {
            
            
            
            
            
            
            meta.wait();
          } catch (InterruptedException e) {
            LOG.error("deletePartition: error while waiting on " +
              "previously inactive partition: " + e.getMessage());
            throw new IllegalStateException(e);
          }
          isDone = false;
          break;
        case ACTIVE:
          try {
            
            
            
            
            meta.wait();
          } catch (InterruptedException e) {
            LOG.error("deletePartition: error while waiting on " +
              "active partition: " + e.getMessage());
            throw new IllegalStateException(e);
          }
          break;
        default:
          throw new IllegalStateException("illegal state " + state +
            " for partition " + id);
        }
      }
      partitions.remove(id);
    }
  }

  
  private class MetaPartition {
    
    
    private int id;
    
    private State state;
    
    private int references;
    
    private long vertexCount;
    
    private long prevVertexCount;
    
    private long edgeCount;
    
    private boolean isSticky;

    
    
    private Partition<I, V, E> partition;

    
    public MetaPartition(int id) {
      this.id = id;
      this.state = State.INIT;
      this.references = 0;
      this.vertexCount = 0;
      this.prevVertexCount = 0;
      this.edgeCount = 0;
      this.isSticky = false;

      this.partition = null;
    }

    
    public int getId() {
      return id;
    }

    
    public State getState() {
      return state;
    }

    
    public void setOnDisk(Partition<I, V, E> partition) {
      this.state = State.ONDISK;
      this.partition = null;
      this.vertexCount = partition.getVertexCount();
      this.edgeCount = partition.getEdgeCount();
    }

    
    public void setActive() {
      this.setActive(null);
    }

    
    public void setActive(Partition<I, V, E> partition) {
      if (partition != null) {
        this.partition = partition;
      }
      this.state = State.ACTIVE;
      this.prevVertexCount = this.vertexCount;
      this.vertexCount = 0;
      this.incrementReferences();
    }

    
    public void setState(State state) {
      this.state = state;
    }

    
    public int decrementReferences() {
      if (references > 0) {
        references -= 1;
      }
      return references;
    }

    
    public int incrementReferences() {
      return ++references;
    }

    
    public void setPrevVertexCount(long vertexCount) {
      this.prevVertexCount = vertexCount;
    }

    
    public long getPrevVertexCount() {
      return prevVertexCount;
    }

    
    public long getVertexCount() {
      return vertexCount;
    }

    
    public long getEdgeCount() {
      return edgeCount;
    }

    
    public void addToVertexCount(long inc) {
      this.vertexCount += inc;
      this.prevVertexCount = vertexCount;
    }

    
    public Partition<I, V, E> getPartition() {
      return partition;
    }

    
    public void setPartition(Partition<I, V, E> partition) {
      this.partition = partition;
    }

    
    public void setSticky() {
      this.isSticky = true;
    }

    
    public boolean isSticky() {
      return this.isSticky;
    }

    @Override
    public String toString() {
      StringBuffer sb = new StringBuffer();

      sb.append("Meta Data: { ");
      sb.append("ID: " + id + "; ");
      sb.append("State: " + state + "; ");
      sb.append("Number of References: " + references + "; ");
      sb.append("Number of Vertices: " + vertexCount + "; ");
      sb.append("Previous number of Vertices: " + prevVertexCount + "; ");
      sb.append("Number of edges: " + edgeCount + "; ");
      sb.append("Is Sticky: " + isSticky + "; ");
      sb.append("Partition: " + partition + "; }");

      return sb.toString();
    }
  }
}

<code block>


package org.apache.giraph.partition;

import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;


public abstract class PartitionStore<I extends WritableComparable,
    V extends Writable, E extends Writable> {
  
  public abstract void addPartition(Partition<I, V, E> partition);

  
  public abstract Partition<I, V, E> getOrCreatePartition(Integer partitionId);

  
  public abstract void putPartition(Partition<I, V, E> partition);

  
  public abstract Partition<I, V, E> removePartition(Integer partitionId);

  
  public abstract void deletePartition(Integer partitionId);

  
  public abstract boolean hasPartition(Integer partitionId);

  
  public abstract Iterable<Integer> getPartitionIds();

  
  public abstract int getNumPartitions();

  
  public abstract long getPartitionVertexCount(int partitionId);

  
  public abstract long getPartitionEdgeCount(int partitionId);

  
  public boolean isEmpty() {
    return getNumPartitions() == 0;
  }

  
  public void shutdown() { }
}

<code block>


package org.apache.giraph.partition;

import com.google.common.collect.Maps;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.mapreduce.Mapper;

import java.util.concurrent.ConcurrentMap;


public class SimplePartitionStore<I extends WritableComparable,
    V extends Writable, E extends Writable>
    extends PartitionStore<I, V, E> {
  
  private final ConcurrentMap<Integer, Partition<I, V, E>> partitions =
      Maps.newConcurrentMap();
  
  private final ImmutableClassesGiraphConfiguration<I, V, E> conf;
  
  private final Mapper<?, ?, ?, ?>.Context context;

  
  public SimplePartitionStore(
      ImmutableClassesGiraphConfiguration<I, V, E> conf,
      Mapper<?, ?, ?, ?>.Context context) {
    this.conf = conf;
    this.context = context;
  }

  @Override
  public void addPartition(Partition<I, V, E> partition) {
    Partition<I, V, E> oldPartition = partitions.get(partition.getId());
    if (oldPartition == null) {
      oldPartition = partitions.putIfAbsent(partition.getId(), partition);
      if (oldPartition == null) {
        return;
      }
    }
    
    oldPartition.addPartition(partition);
  }

  @Override
  public Partition<I, V, E> getOrCreatePartition(Integer partitionId) {
    Partition<I, V, E> oldPartition = partitions.get(partitionId);
    if (oldPartition == null) {
      Partition<I, V, E> newPartition =
          conf.createPartition(partitionId, context);
      oldPartition = partitions.putIfAbsent(partitionId, newPartition);
      if (oldPartition == null) {
        return newPartition;
      }
    }
    return oldPartition;
  }

  @Override
  public Partition<I, V, E> removePartition(Integer partitionId) {
    return partitions.remove(partitionId);
  }

  @Override
  public void deletePartition(Integer partitionId) {
    partitions.remove(partitionId);
  }

  @Override
  public boolean hasPartition(Integer partitionId) {
    return partitions.containsKey(partitionId);
  }

  @Override
  public Iterable<Integer> getPartitionIds() {
    return partitions.keySet();
  }

  @Override
  public int getNumPartitions() {
    return partitions.size();
  }

  @Override
  public long getPartitionVertexCount(int partitionId) {
    Partition partition = partitions.get(partitionId);
    if (partition == null) {
      return 0;
    } else {
      return partition.getVertexCount();
    }
  }

  @Override
  public long getPartitionEdgeCount(int partitionId) {
    Partition partition = partitions.get(partitionId);
    if (partition == null) {
      return 0;
    } else {
      return partition.getEdgeCount();
    }
  }

  @Override
  public void putPartition(Partition<I, V, E> partition) { }
}

<code block>


package org.apache.giraph.graph;

import java.io.IOException;
import java.net.URL;
import java.net.URLDecoder;
import java.util.ArrayList;
import java.util.Collection;
import java.util.Enumeration;
import java.util.List;
import java.util.concurrent.ArrayBlockingQueue;
import java.util.concurrent.BlockingQueue;
import java.util.concurrent.Callable;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.TimeUnit;

import org.apache.giraph.bsp.BspService;
import org.apache.giraph.bsp.CentralizedServiceMaster;
import org.apache.giraph.bsp.CentralizedServiceWorker;
import org.apache.giraph.bsp.checkpoints.CheckpointStatus;
import org.apache.giraph.comm.messages.MessageStore;
import org.apache.giraph.conf.ClassConfOption;
import org.apache.giraph.conf.GiraphConstants;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.giraph.job.JobProgressTracker;
import org.apache.giraph.master.BspServiceMaster;
import org.apache.giraph.master.MasterThread;
import org.apache.giraph.metrics.GiraphMetrics;
import org.apache.giraph.metrics.GiraphMetricsRegistry;
import org.apache.giraph.metrics.GiraphTimer;
import org.apache.giraph.metrics.GiraphTimerContext;
import org.apache.giraph.metrics.ResetSuperstepMetricsObserver;
import org.apache.giraph.metrics.SuperstepMetricsRegistry;
import org.apache.giraph.partition.PartitionOwner;
import org.apache.giraph.partition.PartitionStats;
import org.apache.giraph.partition.PartitionStore;
import org.apache.giraph.scripting.ScriptLoader;
import org.apache.giraph.utils.CallableFactory;
import org.apache.giraph.utils.MemoryUtils;
import org.apache.giraph.utils.ProgressableUtils;
import org.apache.giraph.worker.BspServiceWorker;
import org.apache.giraph.worker.InputSplitsCallable;
import org.apache.giraph.worker.WorkerContext;
import org.apache.giraph.worker.WorkerObserver;
import org.apache.giraph.worker.WorkerProgress;
import org.apache.giraph.zk.ZooKeeperManager;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.log4j.Appender;
import org.apache.log4j.Level;
import org.apache.log4j.LogManager;
import org.apache.log4j.Logger;
import org.apache.log4j.PatternLayout;


@SuppressWarnings("rawtypes")
public class GraphTaskManager<I extends WritableComparable, V extends Writable,
  E extends Writable> implements
  ResetSuperstepMetricsObserver {

  
  public static final ClassConfOption<CheckerIfWorkerShouldFailAfterException>
  CHECKER_IF_WORKER_SHOULD_FAIL_AFTER_EXCEPTION_CLASS = ClassConfOption.create(
      "giraph.checkerIfWorkerShouldFailAfterExceptionClass",
      FailWithEveryExceptionOccurred.class,
      CheckerIfWorkerShouldFailAfterException.class,
      "Class which checks if an exception on some thread should cause worker " +
          "to fail, by default all exceptions cause failure");
  
  public static final String TIMER_SUPERSTEP_TIME = "superstep-time-ms";
  
  public static final String TIMER_COMPUTE_ALL = "compute-all-ms";
  
  public static final String TIMER_TIME_TO_FIRST_MSG =
      "time-to-first-message-ms";
  
  public static final String TIMER_COMMUNICATION_TIME = "communication-time-ms";

  
  private static final Logger LOG = Logger.getLogger(GraphTaskManager.class);
  
  private CentralizedServiceWorker<I, V, E> serviceWorker;
  
  private CentralizedServiceMaster<I, V, E> serviceMaster;
  
  private Thread masterThread = null;
  
  private boolean alreadyRun = false;
  
  private ZooKeeperManager zkManager;
  
  private ImmutableClassesGiraphConfiguration<I, V, E> conf;
  
  private boolean done = false;
  
  private GraphFunctions graphFunctions = GraphFunctions.UNKNOWN;
  
  private FinishedSuperstepStats finishedSuperstepStats =
      new FinishedSuperstepStats(0, false, 0, 0, false, CheckpointStatus.NONE);
  
  private JobProgressTrackerClient jobProgressTracker;

  
  
  private GiraphTimer wcPreAppTimer;
  
  private GiraphTimer wcPostAppTimer;

  
  
  private GiraphTimer superstepTimer;
  
  private GiraphTimer computeAll;
  
  private GiraphTimer timeToFirstMessage;
  
  private GiraphTimerContext timeToFirstMessageTimerContext;
  
  private GiraphTimer communicationTimer;
  
  private GiraphTimerContext communicationTimerContext;
  
  private GiraphTimer wcPreSuperstepTimer;
  
  private final Mapper<?, ?, ?, ?>.Context context;
  
  private boolean isMaster;

  
  public GraphTaskManager(Mapper<?, ?, ?, ?>.Context context) {
    this.context = context;
    this.isMaster = false;
  }

  
  private void checkInput() {
    if (conf.hasEdgeInputFormat()) {
      conf.createWrappedEdgeInputFormat().checkInputSpecs(conf);
    }
    if (conf.hasVertexInputFormat()) {
      conf.createWrappedVertexInputFormat().checkInputSpecs(conf);
    }
  }

  
  private void createZooKeeperCounter(String serverPortList) {
    
    context.getCounter(GiraphConstants.ZOOKEEPER_SERVER_PORT_COUNTER_GROUP,
        serverPortList);
  }

  
  public void setup(Path[] zkPathList)
    throws IOException, InterruptedException {
    context.setStatus("setup: Beginning worker setup.");
    Configuration hadoopConf = context.getConfiguration();
    conf = new ImmutableClassesGiraphConfiguration<I, V, E>(hadoopConf);
    initializeJobProgressTracker();
    
    
    
    conf.getGiraphTypes().writeIfUnset(conf);
    
    initializeAndConfigureLogging();
    
    setupAndInitializeGiraphMetrics();
    
    checkInput();
    
    ScriptLoader.loadScripts(conf);
    
    conf.createComputationFactory().initialize(conf);
    
    context.setStatus("setup: Initializing Zookeeper services.");
    String serverPortList = conf.getZookeeperList();
    if (serverPortList.isEmpty()) {
      if (startZooKeeperManager()) {
        return; 
      }
    } else {
      createZooKeeperCounter(serverPortList);
    }
    if (zkManager != null && zkManager.runsZooKeeper()) {
      if (LOG.isInfoEnabled()) {
        LOG.info("setup: Chosen to run ZooKeeper...");
      }
    }
    context
        .setStatus("setup: Connected to Zookeeper service " + serverPortList);
    this.graphFunctions = determineGraphFunctions(conf, zkManager);
    
    if (conf.getZooKeeperServerCount() > 1) {
      Thread.sleep(GiraphConstants.DEFAULT_ZOOKEEPER_INIT_LIMIT *
        GiraphConstants.DEFAULT_ZOOKEEPER_TICK_TIME);
    }
    try {
      instantiateBspService();
    } catch (IOException e) {
      LOG.error("setup: Caught exception just before end of setup", e);
      if (zkManager != null) {
        zkManager.offlineZooKeeperServers(ZooKeeperManager.State.FAILED);
      }
      throw new RuntimeException(
        "setup: Offlining servers due to exception...", e);
    }
    context.setStatus(getGraphFunctions().toString() + " starting...");
  }

  
  private void initializeJobProgressTracker() {
    if (!conf.trackJobProgressOnClient()) {
      jobProgressTracker = new JobProgressTrackerClientNoOp();
    } else {
      try {
        jobProgressTracker = new RetryableJobProgressTrackerClient(conf);
      } catch (InterruptedException | ExecutionException e) {
        LOG.warn("createJobProgressClient: Exception occurred while trying to" +
            " connect to JobProgressTracker - not reporting progress", e);
        jobProgressTracker = new JobProgressTrackerClientNoOp();
      }
    }
    jobProgressTracker.mapperStarted();
  }

  
  public void execute() throws IOException, InterruptedException {
    if (checkTaskState()) {
      return;
    }
    preLoadOnWorkerObservers();
    finishedSuperstepStats = serviceWorker.setup();
    if (collectInputSuperstepStats(finishedSuperstepStats)) {
      return;
    }
    prepareGraphStateAndWorkerContext();
    List<PartitionStats> partitionStatsList = new ArrayList<PartitionStats>();
    int numComputeThreads = conf.getNumComputeThreads();

    
    while (!finishedSuperstepStats.allVerticesHalted()) {
      final long superstep = serviceWorker.getSuperstep();
      GiraphTimerContext superstepTimerContext =
        getTimerForThisSuperstep(superstep);
      GraphState graphState = new GraphState(superstep,
          finishedSuperstepStats.getVertexCount(),
          finishedSuperstepStats.getEdgeCount(),
          context);
      Collection<? extends PartitionOwner> masterAssignedPartitionOwners =
        serviceWorker.startSuperstep();
      if (LOG.isDebugEnabled()) {
        LOG.debug("execute: " + MemoryUtils.getRuntimeMemoryStats());
      }
      context.progress();
      serviceWorker.exchangeVertexPartitions(masterAssignedPartitionOwners);
      context.progress();
      boolean hasBeenRestarted = checkSuperstepRestarted(superstep);

      GlobalStats globalStats = serviceWorker.getGlobalStats();

      if (hasBeenRestarted) {
        graphState = new GraphState(superstep,
            finishedSuperstepStats.getVertexCount(),
            finishedSuperstepStats.getEdgeCount(),
            context);
      } else if (storeCheckpoint(globalStats.getCheckpointStatus())) {
        break;
      }
      serviceWorker.getServerData().prepareResolveMutations();
      context.progress();
      prepareForSuperstep(graphState);
      context.progress();
      MessageStore<I, Writable> messageStore =
        serviceWorker.getServerData().getCurrentMessageStore();
      int numPartitions = serviceWorker.getPartitionStore().getNumPartitions();
      int numThreads = Math.min(numComputeThreads, numPartitions);
      if (LOG.isInfoEnabled()) {
        LOG.info("execute: " + numPartitions + " partitions to process with " +
          numThreads + " compute thread(s), originally " +
          numComputeThreads + " thread(s) on superstep " + superstep);
      }
      partitionStatsList.clear();
      
      if (numPartitions > 0) {
        processGraphPartitions(context, partitionStatsList, graphState,
          messageStore, numPartitions, numThreads);
      }
      finishedSuperstepStats = completeSuperstepAndCollectStats(
        partitionStatsList, superstepTimerContext);

      
    }

    if (LOG.isInfoEnabled()) {
      LOG.info("execute: BSP application done (global vertices marked done)");
    }
    updateSuperstepGraphState();
    postApplication();
  }

  
  private void postApplication() throws IOException, InterruptedException {
    GiraphTimerContext postAppTimerContext = wcPostAppTimer.time();
    serviceWorker.getWorkerContext().postApplication();
    serviceWorker.getSuperstepOutput().postApplication();
    postAppTimerContext.stop();
    context.progress();

    for (WorkerObserver obs : serviceWorker.getWorkerObservers()) {
      obs.postApplication();
      context.progress();
    }
  }

  
  public void setIsMaster(final boolean im) {
    this.isMaster = im;
  }

  
  public boolean isMaster() {
    return isMaster;
  }

  
  private GiraphTimerContext getTimerForThisSuperstep(long superstep) {
    GiraphMetrics.get().resetSuperstepMetrics(superstep);
    return superstepTimer.time();
  }

  
  private void setupAndInitializeGiraphMetrics() {
    GiraphMetrics.init(conf);
    GiraphMetrics.get().addSuperstepResetObserver(this);
    initJobMetrics();
    MemoryUtils.initMetrics();
    InputSplitsCallable.initMetrics();
  }

  
  private boolean startZooKeeperManager()
    throws IOException, InterruptedException {
    zkManager = new ZooKeeperManager(context, conf);
    context.setStatus("setup: Setting up Zookeeper manager.");
    zkManager.setup();
    if (zkManager.computationDone()) {
      done = true;
      return true;
    }
    zkManager.onlineZooKeeperServers();
    String serverPortList = zkManager.getZooKeeperServerPortString();
    conf.setZookeeperList(serverPortList);
    createZooKeeperCounter(serverPortList);
    return false;
  }

  
  private void updateSuperstepGraphState() {
    serviceWorker.getWorkerContext().setGraphState(
        new GraphState(serviceWorker.getSuperstep(),
            finishedSuperstepStats.getVertexCount(),
            finishedSuperstepStats.getEdgeCount(), context));
  }

  
  private FinishedSuperstepStats completeSuperstepAndCollectStats(
    List<PartitionStats> partitionStatsList,
    GiraphTimerContext superstepTimerContext) {

    
    
    
    finishedSuperstepStats =
      serviceWorker.finishSuperstep(partitionStatsList, superstepTimerContext);
    if (conf.metricsEnabled()) {
      GiraphMetrics.get().perSuperstep().printSummary(System.err);
    }
    return finishedSuperstepStats;
  }

  
  private void prepareForSuperstep(GraphState graphState) {
    serviceWorker.prepareSuperstep();

    serviceWorker.getWorkerContext().setGraphState(graphState);
    serviceWorker.getWorkerContext().setupSuperstep(serviceWorker);
    GiraphTimerContext preSuperstepTimer = wcPreSuperstepTimer.time();
    serviceWorker.getWorkerContext().preSuperstep();
    preSuperstepTimer.stop();
    context.progress();

    for (WorkerObserver obs : serviceWorker.getWorkerObservers()) {
      obs.preSuperstep(graphState.getSuperstep());
      context.progress();
    }
  }

  
  private void prepareGraphStateAndWorkerContext() {
    updateSuperstepGraphState();
    workerContextPreApp();
  }

  
  public GraphFunctions getGraphFunctions() {
    return graphFunctions;
  }

  public final WorkerContext getWorkerContext() {
    return serviceWorker.getWorkerContext();
  }

  public JobProgressTracker getJobProgressTracker() {
    return jobProgressTracker;
  }

  
  private static String findContainingJar(Class<?> myClass) {
    ClassLoader loader = myClass.getClassLoader();
    String classFile =
        myClass.getName().replaceAll("\\.", "/") + ".class";
    try {
      for (Enumeration<?> itr = loader.getResources(classFile);
          itr.hasMoreElements();) {
        URL url = (URL) itr.nextElement();
        if ("jar".equals(url.getProtocol())) {
          String toReturn = url.getPath();
          if (toReturn.startsWith("file:")) {
            toReturn = toReturn.substring("file:".length());
          }
          toReturn = URLDecoder.decode(toReturn, "UTF-8");
          return toReturn.replaceAll("!.*$", "");
        }
      }
    } catch (IOException e) {
      throw new RuntimeException(e);
    }
    return null;
  }

  
  private static GraphFunctions determineGraphFunctions(
      ImmutableClassesGiraphConfiguration conf,
      ZooKeeperManager zkManager) {
    boolean splitMasterWorker = conf.getSplitMasterWorker();
    int taskPartition = conf.getTaskPartition();
    boolean zkAlreadyProvided = conf.isZookeeperExternal();
    GraphFunctions functions = GraphFunctions.UNKNOWN;
    
    if (!splitMasterWorker) {
      if ((zkManager != null) && zkManager.runsZooKeeper()) {
        functions = GraphFunctions.ALL;
      } else {
        functions = GraphFunctions.ALL_EXCEPT_ZOOKEEPER;
      }
    } else {
      if (zkAlreadyProvided) {
        int masterCount = conf.getZooKeeperServerCount();
        if (taskPartition < masterCount) {
          functions = GraphFunctions.MASTER_ONLY;
        } else {
          functions = GraphFunctions.WORKER_ONLY;
        }
      } else {
        if ((zkManager != null) && zkManager.runsZooKeeper()) {
          functions = GraphFunctions.MASTER_ZOOKEEPER_ONLY;
        } else {
          functions = GraphFunctions.WORKER_ONLY;
        }
      }
    }
    return functions;
  }

  
  private void instantiateBspService()
    throws IOException, InterruptedException {
    if (graphFunctions.isMaster()) {
      if (LOG.isInfoEnabled()) {
        LOG.info("setup: Starting up BspServiceMaster " +
          "(master thread)...");
      }
      serviceMaster = new BspServiceMaster<I, V, E>(context, this);
      masterThread = new MasterThread<I, V, E>(serviceMaster, context);
      masterThread.start();
    }
    if (graphFunctions.isWorker()) {
      if (LOG.isInfoEnabled()) {
        LOG.info("setup: Starting up BspServiceWorker...");
      }
      serviceWorker = new BspServiceWorker<I, V, E>(context, this);
      if (LOG.isInfoEnabled()) {
        LOG.info("setup: Registering health of this worker...");
      }
    }
  }

  
  private void initializeAndConfigureLogging() {
    
    String logLevel = conf.getLocalLevel();
    if (!Logger.getRootLogger().getLevel().equals(Level.toLevel(logLevel))) {
      Logger.getRootLogger().setLevel(Level.toLevel(logLevel));
      if (LOG.isInfoEnabled()) {
        LOG.info("setup: Set log level to " + logLevel);
      }
    } else {
      if (LOG.isInfoEnabled()) {
        LOG.info("setup: Log level remains at " + logLevel);
      }
    }
    
    if (conf.useLogThreadLayout()) {
      PatternLayout layout =
        new PatternLayout("%-7p %d [%t] %c %x - %m%n");
      Enumeration<Appender> appenderEnum =
        Logger.getRootLogger().getAllAppenders();
      while (appenderEnum.hasMoreElements()) {
        appenderEnum.nextElement().setLayout(layout);
      }
    }
    
    
    if (conf.getLocalTestMode()) {
      LogManager.getLogger(org.apache.zookeeper.server.PrepRequestProcessor.
          class.getName()).setLevel(Level.ERROR);
    }
  }

  
  private void initJobMetrics() {
    GiraphMetricsRegistry jobMetrics = GiraphMetrics.get().perJobOptional();
    wcPreAppTimer = new GiraphTimer(jobMetrics, "worker-context-pre-app",
        TimeUnit.MILLISECONDS);
    wcPostAppTimer = new GiraphTimer(jobMetrics, "worker-context-post-app",
        TimeUnit.MILLISECONDS);
  }

  @Override
  public void newSuperstep(SuperstepMetricsRegistry superstepMetrics) {
    superstepTimer = new GiraphTimer(superstepMetrics,
        TIMER_SUPERSTEP_TIME, TimeUnit.MILLISECONDS);
    computeAll = new GiraphTimer(superstepMetrics,
        TIMER_COMPUTE_ALL, TimeUnit.MILLISECONDS);
    timeToFirstMessage = new GiraphTimer(superstepMetrics,
        TIMER_TIME_TO_FIRST_MSG, TimeUnit.MICROSECONDS);
    communicationTimer = new GiraphTimer(superstepMetrics,
        TIMER_COMMUNICATION_TIME, TimeUnit.MILLISECONDS);
    wcPreSuperstepTimer = new GiraphTimer(superstepMetrics,
        "worker-context-pre-superstep", TimeUnit.MILLISECONDS);
  }

  
  public void notifySentMessages() {
    
    
    GiraphTimerContext tmp = timeToFirstMessageTimerContext;
    if (tmp != null) {
      synchronized (timeToFirstMessage) {
        if (timeToFirstMessageTimerContext != null) {
          timeToFirstMessageTimerContext.stop();
          timeToFirstMessageTimerContext = null;
          communicationTimerContext = communicationTimer.time();
        }
      }
    }
  }

  
  public void notifyFinishedCommunication() {
    GiraphTimerContext tmp = communicationTimerContext;
    if (tmp != null) {
      synchronized (communicationTimer) {
        if (communicationTimerContext != null) {
          communicationTimerContext.stop();
          communicationTimerContext = null;
        }
      }
    }
  }

  
  private void processGraphPartitions(final Mapper<?, ?, ?, ?>.Context context,
      List<PartitionStats> partitionStatsList,
      final GraphState graphState,
      final MessageStore<I, Writable> messageStore,
      int numPartitions,
      int numThreads) {
    final BlockingQueue<Integer> computePartitionIdQueue =
      new ArrayBlockingQueue<Integer>(numPartitions);
    long verticesToCompute = 0;
    PartitionStore<I, V, E> partitionStore = serviceWorker.getPartitionStore();
    for (Integer partitionId : partitionStore.getPartitionIds()) {
      computePartitionIdQueue.add(partitionId);
      verticesToCompute += partitionStore.getPartitionVertexCount(partitionId);
    }
    WorkerProgress.get().startSuperstep(
        serviceWorker.getSuperstep(), verticesToCompute,
        serviceWorker.getPartitionStore().getNumPartitions());

    GiraphTimerContext computeAllTimerContext = computeAll.time();
    timeToFirstMessageTimerContext = timeToFirstMessage.time();

    CallableFactory<Collection<PartitionStats>> callableFactory =
      new CallableFactory<Collection<PartitionStats>>() {
        @Override
        public Callable<Collection<PartitionStats>> newCallable(
            int callableId) {
          return new ComputeCallable<I, V, E, Writable, Writable>(
              context,
              graphState,
              messageStore,
              computePartitionIdQueue,
              conf,
              serviceWorker);
        }
      };
    List<Collection<PartitionStats>> results =
        ProgressableUtils.getResultsWithNCallables(callableFactory, numThreads,
            "compute-%d", context);

    for (Collection<PartitionStats> result : results) {
      partitionStatsList.addAll(result);
    }

    computeAllTimerContext.stop();
  }

  
  private boolean checkSuperstepRestarted(long superstep) throws IOException {
    
    
    if (serviceWorker.getRestartedSuperstep() == superstep) {
      if (LOG.isInfoEnabled()) {
        LOG.info("execute: Loading from checkpoint " + superstep);
      }
      VertexEdgeCount vertexEdgeCount = serviceWorker.loadCheckpoint(
        serviceWorker.getRestartedSuperstep());
      finishedSuperstepStats = new FinishedSuperstepStats(0, false,
          vertexEdgeCount.getVertexCount(), vertexEdgeCount.getEdgeCount(),
          false, CheckpointStatus.NONE);
      return true;
    }
    return false;
  }

  
  private boolean storeCheckpoint(CheckpointStatus checkpointStatus)
    throws IOException {
    if (checkpointStatus != CheckpointStatus.NONE) {
      serviceWorker.storeCheckpoint();
    }
    return checkpointStatus == CheckpointStatus.CHECKPOINT_AND_HALT;
  }

  
  private boolean collectInputSuperstepStats(
    FinishedSuperstepStats inputSuperstepStats) {
    if (inputSuperstepStats.getVertexCount() == 0 &&
        !inputSuperstepStats.mustLoadCheckpoint()) {
      LOG.warn("map: No vertices in the graph, exiting.");
      return true;
    }
    if (conf.metricsEnabled()) {
      GiraphMetrics.get().perSuperstep().printSummary(System.err);
    }
    return false;
  }

  
  private boolean checkTaskState() {
    if (done) {
      return true;
    }
    GiraphMetrics.get().resetSuperstepMetrics(BspService.INPUT_SUPERSTEP);
    if (graphFunctions.isNotAWorker()) {
      if (LOG.isInfoEnabled()) {
        LOG.info("map: No need to do anything when not a worker");
      }
      return true;
    }
    if (alreadyRun) {
      throw new RuntimeException("map: In BSP, map should have only been" +
        " run exactly once, (already run)");
    }
    alreadyRun = true;
    return false;
  }

  
  private void workerContextPreApp() {
    GiraphTimerContext preAppTimerContext = wcPreAppTimer.time();
    try {
      serviceWorker.getWorkerContext().preApplication();
    } catch (InstantiationException e) {
      LOG.fatal("execute: preApplication failed in instantiation", e);
      throw new RuntimeException(
          "execute: preApplication failed in instantiation", e);
    } catch (IllegalAccessException e) {
      LOG.fatal("execute: preApplication failed in access", e);
      throw new RuntimeException(
          "execute: preApplication failed in access", e);
    }
    preAppTimerContext.stop();
    context.progress();

    for (WorkerObserver obs : serviceWorker.getWorkerObservers()) {
      obs.preApplication();
      context.progress();
    }
  }

  
  private void preLoadOnWorkerObservers() {
    for (WorkerObserver obs : serviceWorker.getWorkerObservers()) {
      obs.preLoad();
      context.progress();
    }
  }

  
  private void postSaveOnWorkerObservers() {
    for (WorkerObserver obs : serviceWorker.getWorkerObservers()) {
      obs.postSave();
      context.progress();
    }
  }

  
  public void cleanup()
    throws IOException, InterruptedException {
    if (LOG.isInfoEnabled()) {
      LOG.info("cleanup: Starting for " + getGraphFunctions());
    }
    jobProgressTracker.cleanup();
    if (done) {
      return;
    }

    if (serviceWorker != null) {
      serviceWorker.cleanup(finishedSuperstepStats);
      postSaveOnWorkerObservers();
    }
    try {
      if (masterThread != null) {
        masterThread.join();
        LOG.info("cleanup: Joined with master thread");
      }
    } catch (InterruptedException e) {
      
      LOG.error("cleanup: Master thread couldn't join");
    }
    if (zkManager != null) {
      LOG.info("cleanup: Offlining ZooKeeper servers");
      try {
        zkManager.offlineZooKeeperServers(ZooKeeperManager.State.FINISHED);
      
      
      
      
      
      
      
      } catch (Throwable e) {
      
        LOG.error("cleanup: Error offlining zookeeper", e);
      }
    }

    
    GiraphMetrics.get().shutdown();
  }

  
  public void zooKeeperCleanup() {
    if (graphFunctions.isZooKeeper()) {
      
      if (zkManager != null) {
        zkManager.cleanup();
      }
    }
  }

  
  public void workerFailureCleanup() {
    try {
      if (graphFunctions.isWorker()) {
        serviceWorker.failureCleanup();
      }
      
      GiraphMetrics.get().shutdown();
    
    
    
    } catch (RuntimeException e1) {
    
      LOG.error("run: Worker failure failed on another RuntimeException, " +
          "original expection will be rethrown", e1);
    }
  }

  
  public Thread.UncaughtExceptionHandler createUncaughtExceptionHandler() {
    return new OverrideExceptionHandler(
        CHECKER_IF_WORKER_SHOULD_FAIL_AFTER_EXCEPTION_CLASS.newInstance(
            getConf()));
  }

  public ImmutableClassesGiraphConfiguration<I, V, E> getConf() {
    return conf;
  }


  
  class OverrideExceptionHandler implements Thread.UncaughtExceptionHandler {
    
    private final CheckerIfWorkerShouldFailAfterException checker;

    
    public OverrideExceptionHandler(
        CheckerIfWorkerShouldFailAfterException checker) {
      this.checker = checker;
    }

    @Override
    public void uncaughtException(final Thread t, final Throwable e) {
      if (!checker.checkIfWorkerShouldFail(t, e)) {
        return;
      }
      try {
        LOG.fatal(
            "uncaughtException: OverrideExceptionHandler on thread " +
                t.getName() + ", msg = " +  e.getMessage() + ", exiting...", e);

        zooKeeperCleanup();
        workerFailureCleanup();
      } finally {
        System.exit(1);
      }
    }
  }

  
  public interface CheckerIfWorkerShouldFailAfterException {
    
    boolean checkIfWorkerShouldFail(Thread thread, Throwable exception);
  }

  
  public static class FailWithEveryExceptionOccurred
      implements CheckerIfWorkerShouldFailAfterException {
    @Override
    public boolean checkIfWorkerShouldFail(Thread thread, Throwable exception) {
      return true;
    }
  }
}

<code block>

package org.apache.giraph.graph;

import java.io.IOException;
import java.util.Collection;
import java.util.List;
import java.util.concurrent.BlockingQueue;
import java.util.concurrent.Callable;

import org.apache.giraph.bsp.CentralizedServiceWorker;
import org.apache.giraph.comm.WorkerClientRequestProcessor;
import org.apache.giraph.comm.messages.MessageStore;
import org.apache.giraph.comm.netty.NettyWorkerClientRequestProcessor;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.giraph.io.SimpleVertexWriter;
import org.apache.giraph.metrics.GiraphMetrics;
import org.apache.giraph.metrics.MetricNames;
import org.apache.giraph.metrics.SuperstepMetricsRegistry;
import org.apache.giraph.partition.Partition;
import org.apache.giraph.partition.PartitionStats;
import org.apache.giraph.time.SystemTime;
import org.apache.giraph.time.Time;
import org.apache.giraph.time.Times;
import org.apache.giraph.utils.MemoryUtils;
import org.apache.giraph.utils.TimedLogger;
import org.apache.giraph.utils.Trimmable;
import org.apache.giraph.worker.WorkerContext;
import org.apache.giraph.worker.WorkerProgress;
import org.apache.giraph.worker.WorkerThreadGlobalCommUsage;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.log4j.Logger;

import com.google.common.collect.Iterables;
import com.google.common.collect.Lists;
import com.yammer.metrics.core.Counter;
import com.yammer.metrics.core.Histogram;


public class ComputeCallable<I extends WritableComparable, V extends Writable,
    E extends Writable, M1 extends Writable, M2 extends Writable>
    implements Callable<Collection<PartitionStats>> {
  
  private static final Logger LOG  = Logger.getLogger(ComputeCallable.class);
  
  private static final Time TIME = SystemTime.get();
  
  private static final long VERTICES_TO_UPDATE_PROGRESS = 100000;
  
  private final Mapper<?, ?, ?, ?>.Context context;
  
  private final GraphState graphState;
  
  private final BlockingQueue<Integer> partitionIdQueue;
  
  private final MessageStore<I, M1> messageStore;
  
  private final ImmutableClassesGiraphConfiguration<I, V, E> configuration;
  
  private final CentralizedServiceWorker<I, V, E> serviceWorker;
  
  private final TimedLogger timedLogger = new TimedLogger(30 * 1000, LOG);
  
  private SimpleVertexWriter<I, V, E> vertexWriter;
  
  private final long startNanos = TIME.getNanoseconds();

  
  
  private final Counter messagesSentCounter;
  
  private final Counter messageBytesSentCounter;
  
  private final Histogram histogramComputePerPartition;

  
  public ComputeCallable(
      Mapper<?, ?, ?, ?>.Context context, GraphState graphState,
      MessageStore<I, M1> messageStore,
      BlockingQueue<Integer> partitionIdQueue,
      ImmutableClassesGiraphConfiguration<I, V, E> configuration,
      CentralizedServiceWorker<I, V, E> serviceWorker) {
    this.context = context;
    this.configuration = configuration;
    this.partitionIdQueue = partitionIdQueue;
    this.messageStore = messageStore;
    this.serviceWorker = serviceWorker;
    this.graphState = graphState;

    SuperstepMetricsRegistry metrics = GiraphMetrics.get().perSuperstep();
    messagesSentCounter = metrics.getCounter(MetricNames.MESSAGES_SENT);
    messageBytesSentCounter =
      metrics.getCounter(MetricNames.MESSAGE_BYTES_SENT);
    histogramComputePerPartition = metrics.getUniformHistogram(
        MetricNames.HISTOGRAM_COMPUTE_PER_PARTITION);
  }

  @Override
  public Collection<PartitionStats> call() {
    
    WorkerClientRequestProcessor<I, V, E> workerClientRequestProcessor =
        new NettyWorkerClientRequestProcessor<I, V, E>(
            context, configuration, serviceWorker,
            configuration.getOutgoingMessageEncodeAndStoreType().
              useOneMessageToManyIdsEncoding());
    WorkerThreadGlobalCommUsage aggregatorUsage =
        serviceWorker.getAggregatorHandler().newThreadAggregatorUsage();
    WorkerContext workerContext = serviceWorker.getWorkerContext();

    vertexWriter = serviceWorker.getSuperstepOutput().getVertexWriter();

    Computation<I, V, E, M1, M2> computation =
        (Computation<I, V, E, M1, M2>) configuration.createComputation();
    computation.initialize(graphState, workerClientRequestProcessor,
        serviceWorker.getGraphTaskManager(), aggregatorUsage, workerContext);
    computation.preSuperstep();

    List<PartitionStats> partitionStatsList = Lists.newArrayList();
    while (!partitionIdQueue.isEmpty()) {
      Integer partitionId = partitionIdQueue.poll();
      if (partitionId == null) {
        break;
      }

      long startTime = System.currentTimeMillis();
      Partition<I, V, E> partition =
          serviceWorker.getPartitionStore().getOrCreatePartition(partitionId);

      try {
        serviceWorker.getServerData().resolvePartitionMutation(partition);
        PartitionStats partitionStats =
            computePartition(computation, partition);
        partitionStatsList.add(partitionStats);
        long partitionMsgs = workerClientRequestProcessor.resetMessageCount();
        partitionStats.addMessagesSentCount(partitionMsgs);
        messagesSentCounter.inc(partitionMsgs);
        long partitionMsgBytes =
          workerClientRequestProcessor.resetMessageBytesCount();
        partitionStats.addMessageBytesSentCount(partitionMsgBytes);
        messageBytesSentCounter.inc(partitionMsgBytes);
        timedLogger.info("call: Completed " +
            partitionStatsList.size() + " partitions, " +
            partitionIdQueue.size() + " remaining " +
            MemoryUtils.getRuntimeMemoryStats());
      } catch (IOException e) {
        throw new IllegalStateException("call: Caught unexpected IOException," +
            " failing.", e);
      } catch (InterruptedException e) {
        throw new IllegalStateException("call: Caught unexpected " +
            "InterruptedException, failing.", e);
      } finally {
        serviceWorker.getPartitionStore().putPartition(partition);
      }

      histogramComputePerPartition.update(
          System.currentTimeMillis() - startTime);
    }

    computation.postSuperstep();

    
    serviceWorker.getSuperstepOutput().returnVertexWriter(vertexWriter);

    if (LOG.isInfoEnabled()) {
      float seconds = Times.getNanosSince(TIME, startNanos) /
          Time.NS_PER_SECOND_AS_FLOAT;
      LOG.info("call: Computation took " + seconds + " secs for "  +
          partitionStatsList.size() + " partitions on superstep " +
          graphState.getSuperstep() + ".  Flushing started");
    }
    try {
      workerClientRequestProcessor.flush();
      
      
      if (partitionStatsList.size() > 0) {
        long partitionMsgBytes =
          workerClientRequestProcessor.resetMessageBytesCount();
        partitionStatsList.get(partitionStatsList.size() - 1).
          addMessageBytesSentCount(partitionMsgBytes);
        messageBytesSentCounter.inc(partitionMsgBytes);
      }
      aggregatorUsage.finishThreadComputation();
    } catch (IOException e) {
      throw new IllegalStateException("call: Flushing failed.", e);
    }
    return partitionStatsList;
  }

  
  private PartitionStats computePartition(
      Computation<I, V, E, M1, M2> computation,
      Partition<I, V, E> partition) throws IOException, InterruptedException {
    PartitionStats partitionStats =
        new PartitionStats(partition.getId(), 0, 0, 0, 0, 0);
    long verticesComputedProgress = 0;
    
    synchronized (partition) {
      for (Vertex<I, V, E> vertex : partition) {
        Iterable<M1> messages = messageStore.getVertexMessages(vertex.getId());
        if (vertex.isHalted() && !Iterables.isEmpty(messages)) {
          vertex.wakeUp();
        }
        if (!vertex.isHalted()) {
          context.progress();
          computation.compute(vertex, messages);
          
          vertex.unwrapMutableEdges();
          
          if (vertex instanceof Trimmable) {
            ((Trimmable) vertex).trim();
          }
          
          vertexWriter.writeVertex(vertex);
          
          partition.saveVertex(vertex);
        }
        if (vertex.isHalted()) {
          partitionStats.incrFinishedVertexCount();
        }
        
        messageStore.clearVertexMessages(vertex.getId());

        
        partitionStats.incrVertexCount();
        partitionStats.addEdgeCount(vertex.getNumEdges());

        verticesComputedProgress++;
        if (verticesComputedProgress == VERTICES_TO_UPDATE_PROGRESS) {
          WorkerProgress.get().addVerticesComputed(verticesComputedProgress);
          verticesComputedProgress = 0;
        }
      }

      messageStore.clearPartition(partition.getId());
    }
    WorkerProgress.get().addVerticesComputed(verticesComputedProgress);
    WorkerProgress.get().incrementPartitionsComputed();
    return partitionStats;
  }
}


<code block>


package org.apache.giraph.edge;

import com.google.common.collect.MapMaker;
import org.apache.giraph.bsp.CentralizedServiceWorker;
import org.apache.giraph.conf.DefaultImmutableClassesGiraphConfigurable;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.giraph.graph.Vertex;
import org.apache.giraph.partition.Partition;
import org.apache.giraph.utils.CallableFactory;
import org.apache.giraph.utils.ProgressableUtils;
import org.apache.giraph.utils.Trimmable;
import org.apache.giraph.utils.VertexIdEdgeIterator;
import org.apache.giraph.utils.VertexIdEdges;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.util.Progressable;
import org.apache.log4j.Logger;

import java.util.Iterator;
import java.util.Map;
import java.util.concurrent.ArrayBlockingQueue;
import java.util.concurrent.BlockingQueue;
import java.util.concurrent.Callable;
import java.util.concurrent.ConcurrentMap;


public abstract class AbstractEdgeStore<I extends WritableComparable,
  V extends Writable, E extends Writable, K, Et>
  extends DefaultImmutableClassesGiraphConfigurable<I, V, E>
  implements EdgeStore<I, V, E> {
  
  private static final Logger LOG = Logger.getLogger(AbstractEdgeStore.class);
  
  protected CentralizedServiceWorker<I, V, E> service;
  
  protected ImmutableClassesGiraphConfiguration<I, V, E> configuration;
  
  protected Progressable progressable;
  
  protected ConcurrentMap<Integer, Map<K, OutEdges<I, E>>> transientEdges;
  
  protected boolean reuseEdgeObjects;
  
  protected boolean useInputOutEdges;

  
  public AbstractEdgeStore(
    CentralizedServiceWorker<I, V, E> service,
    ImmutableClassesGiraphConfiguration<I, V, E> configuration,
    Progressable progressable) {
    this.service = service;
    this.configuration = configuration;
    this.progressable = progressable;
    transientEdges = new MapMaker().concurrencyLevel(
      configuration.getNettyServerExecutionConcurrency()).makeMap();
    reuseEdgeObjects = configuration.reuseEdgeObjects();
    useInputOutEdges = configuration.useInputOutEdges();
  }

  
  protected abstract I getVertexId(Et entry, I representativeVertexId);

  
  protected abstract I createVertexId(Et entry);

  
  protected abstract Map<K, OutEdges<I, E>> getPartitionEdges(int partitionId);

  
  protected abstract OutEdges<I, E> getPartitionEdges(Et entry);

  
  protected abstract Iterator<Et>
  getPartitionEdgesIterator(Map<K, OutEdges<I, E>> partitionEdges);

  
  protected abstract OutEdges<I, E> getVertexOutEdges(
    VertexIdEdgeIterator<I, E> vertexIdEdgeIterator,
    Map<K, OutEdges<I, E>> partitionEdgesIn);

  @Override
  public void addPartitionEdges(
    int partitionId, VertexIdEdges<I, E> edges) {
    Map<K, OutEdges<I, E>> partitionEdges = getPartitionEdges(partitionId);

    VertexIdEdgeIterator<I, E> vertexIdEdgeIterator =
        edges.getVertexIdEdgeIterator();
    while (vertexIdEdgeIterator.hasNext()) {
      vertexIdEdgeIterator.next();
      Edge<I, E> edge = reuseEdgeObjects ?
          vertexIdEdgeIterator.getCurrentEdge() :
          vertexIdEdgeIterator.releaseCurrentEdge();
      OutEdges<I, E> outEdges = getVertexOutEdges(vertexIdEdgeIterator,
          partitionEdges);
      synchronized (outEdges) {
        outEdges.add(edge);
      }
    }
  }

  
  private OutEdges<I, E> convertInputToComputeEdges(
    OutEdges<I, E> inputEdges) {
    if (!useInputOutEdges) {
      return inputEdges;
    } else {
      return configuration.createAndInitializeOutEdges(inputEdges);
    }
  }

  @Override
  public void moveEdgesToVertices() {
    final boolean createSourceVertex = configuration.getCreateSourceVertex();
    if (transientEdges.isEmpty()) {
      if (LOG.isInfoEnabled()) {
        LOG.info("moveEdgesToVertices: No edges to move");
      }
      return;
    }

    if (LOG.isInfoEnabled()) {
      LOG.info("moveEdgesToVertices: Moving incoming edges to vertices.");
    }

    final BlockingQueue<Integer> partitionIdQueue =
        new ArrayBlockingQueue<>(transientEdges.size());
    partitionIdQueue.addAll(transientEdges.keySet());
    int numThreads = configuration.getNumInputSplitsThreads();

    CallableFactory<Void> callableFactory = new CallableFactory<Void>() {
      @Override
      public Callable<Void> newCallable(int callableId) {
        return new Callable<Void>() {
          @Override
          public Void call() throws Exception {
            Integer partitionId;
            I representativeVertexId = configuration.createVertexId();
            while ((partitionId = partitionIdQueue.poll()) != null) {
              Partition<I, V, E> partition =
                  service.getPartitionStore().getOrCreatePartition(partitionId);
              Map<K, OutEdges<I, E>> partitionEdges =
                  transientEdges.remove(partitionId);
              Iterator<Et> iterator =
                  getPartitionEdgesIterator(partitionEdges);
              
              while (iterator.hasNext()) {
                Et entry = iterator.next();
                I vertexId = getVertexId(entry, representativeVertexId);
                OutEdges<I, E> outEdges = convertInputToComputeEdges(
                  getPartitionEdges(entry));
                Vertex<I, V, E> vertex = partition.getVertex(vertexId);
                
                
                if (vertex == null) {
                  if (createSourceVertex) {
                    
                    vertex = configuration.createVertex();
                    vertex.initialize(createVertexId(entry),
                        configuration.createVertexValue(), outEdges);
                    partition.putVertex(vertex);
                  }
                } else {
                  
                  
                  if (vertex.getNumEdges() == 0) {
                    vertex.setEdges(outEdges);
                  } else {
                    for (Edge<I, E> edge : outEdges) {
                      vertex.addEdge(edge);
                    }
                  }
                  if (vertex instanceof Trimmable) {
                    ((Trimmable) vertex).trim();
                  }
                  
                  
                  partition.saveVertex(vertex);
                }
                iterator.remove();
              }
              
              
              
              service.getPartitionStore().putPartition(partition);
            }
            return null;
          }
        };
      }
    };
    ProgressableUtils.getResultsWithNCallables(callableFactory, numThreads,
        "move-edges-%d", progressable);

    
    transientEdges.clear();

    if (LOG.isInfoEnabled()) {
      LOG.info("moveEdgesToVertices: Finished moving incoming edges to " +
          "vertices.");
    }
  }
}

<code block>


package org.apache.giraph.edge;

import org.apache.giraph.utils.VertexIdEdges;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;


public interface EdgeStore<I extends WritableComparable,
   V extends Writable, E extends Writable> {
  
  void addPartitionEdges(int partitionId, VertexIdEdges<I, E> edges);

  
  void moveEdgesToVertices();
}

<code block>


package org.apache.giraph.edge;

import org.apache.giraph.bsp.CentralizedServiceWorker;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.giraph.utils.VertexIdEdgeIterator;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.util.Progressable;

import com.google.common.collect.MapMaker;

import java.util.Iterator;
import java.util.Map;
import java.util.concurrent.ConcurrentMap;


public class SimpleEdgeStore<I extends WritableComparable,
  V extends Writable, E extends Writable>
  extends AbstractEdgeStore<I, V, E, I,
  Map.Entry<I, OutEdges<I, E>>> {

  
  public SimpleEdgeStore(
    CentralizedServiceWorker<I, V, E> service,
    ImmutableClassesGiraphConfiguration<I, V, E> configuration,
    Progressable progressable) {
    super(service, configuration, progressable);
  }

  @Override
  protected I getVertexId(Map.Entry<I, OutEdges<I, E>> entry,
    I representativeVertexId) {
    return entry.getKey();
  }

  @Override
  protected I createVertexId(Map.Entry<I, OutEdges<I, E>> entry) {
    return entry.getKey();
  }

  @Override
  protected ConcurrentMap<I, OutEdges<I, E>> getPartitionEdges(
    int partitionId) {
    ConcurrentMap<I, OutEdges<I, E>> partitionEdges =
        (ConcurrentMap<I, OutEdges<I, E>>) transientEdges.get(partitionId);
    if (partitionEdges == null) {
      ConcurrentMap<I, OutEdges<I, E>> newPartitionEdges =
          new MapMaker().concurrencyLevel(
              configuration.getNettyServerExecutionConcurrency()).makeMap();
      partitionEdges = (ConcurrentMap<I, OutEdges<I, E>>)
          transientEdges.putIfAbsent(partitionId, newPartitionEdges);
      if (partitionEdges == null) {
        partitionEdges = newPartitionEdges;
      }
    }
    return partitionEdges;
  }

  @Override
  protected OutEdges<I, E> getPartitionEdges(
    Map.Entry<I, OutEdges<I, E>> entry) {
    return entry.getValue();
  }

  @Override
  protected Iterator<Map.Entry<I, OutEdges<I, E>>>
  getPartitionEdgesIterator(Map<I, OutEdges<I, E>> partitionEdges) {
    return partitionEdges.entrySet().iterator();
  }

  @Override
  protected OutEdges<I, E> getVertexOutEdges(
      VertexIdEdgeIterator<I, E> vertexIdEdgeIterator,
      Map<I, OutEdges<I, E>> partitionEdgesIn) {
    ConcurrentMap<I, OutEdges<I, E>> partitionEdges =
        (ConcurrentMap<I, OutEdges<I, E>>) partitionEdgesIn;
    I vertexId = vertexIdEdgeIterator.getCurrentVertexId();
    OutEdges<I, E> outEdges = partitionEdges.get(vertexId);
    if (outEdges == null) {
      OutEdges<I, E> newOutEdges =
          configuration.createAndInitializeInputOutEdges();
      outEdges = partitionEdges.putIfAbsent(vertexId, newOutEdges);
      if (outEdges == null) {
        outEdges = newOutEdges;
        
        
        vertexIdEdgeIterator.releaseCurrentVertexId();
      }
    }
    return outEdges;
  }
}

<code block>


package org.apache.giraph.edge.primitives;

import org.apache.giraph.bsp.CentralizedServiceWorker;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.giraph.edge.AbstractEdgeStore;
import org.apache.giraph.edge.OutEdges;
import org.apache.giraph.utils.VertexIdEdgeIterator;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.util.Progressable;

import it.unimi.dsi.fastutil.longs.Long2ObjectMap;
import it.unimi.dsi.fastutil.longs.Long2ObjectMaps;
import it.unimi.dsi.fastutil.longs.Long2ObjectOpenHashMap;

import java.util.Iterator;
import java.util.Map;


public class LongEdgeStore<V extends Writable, E extends Writable>
  extends AbstractEdgeStore<LongWritable, V, E, Long,
  Long2ObjectMap.Entry<OutEdges<LongWritable, E>>> {

  
  public LongEdgeStore(
    CentralizedServiceWorker<LongWritable, V, E> service,
    ImmutableClassesGiraphConfiguration<LongWritable, V, E> configuration,
    Progressable progressable) {
    super(service, configuration, progressable);
  }

  @Override
  protected LongWritable getVertexId(
    Long2ObjectMap.Entry<OutEdges<LongWritable, E>> entry,
    LongWritable representativeVertexId) {
    representativeVertexId.set(entry.getLongKey());
    return representativeVertexId;
  }

  @Override
  protected LongWritable createVertexId(
    Long2ObjectMap.Entry<OutEdges<LongWritable, E>> entry) {
    return new LongWritable(entry.getLongKey());
  }


  @Override
  protected OutEdges<LongWritable, E> getPartitionEdges(
    Long2ObjectMap.Entry<OutEdges<LongWritable, E>> entry) {
    return entry.getValue();
  }

  @Override
  protected Iterator<Long2ObjectMap.Entry<OutEdges<LongWritable, E>>>
  getPartitionEdgesIterator(
      Map<Long, OutEdges<LongWritable, E>> partitionEdges) {
    return ((Long2ObjectMap<OutEdges<LongWritable, E>>) partitionEdges)
        .long2ObjectEntrySet()
        .iterator();
  }

  @Override
  protected Long2ObjectMap<OutEdges<LongWritable, E>> getPartitionEdges(
    int partitionId) {
    Long2ObjectMap<OutEdges<LongWritable, E>> partitionEdges =
      (Long2ObjectMap<OutEdges<LongWritable, E>>)
        transientEdges.get(partitionId);
    if (partitionEdges == null) {
      Long2ObjectMap<OutEdges<LongWritable, E>> newPartitionEdges =
          Long2ObjectMaps.synchronize(
              new Long2ObjectOpenHashMap<OutEdges<LongWritable, E>>());
      partitionEdges = (Long2ObjectMap<OutEdges<LongWritable, E>>)
          transientEdges.putIfAbsent(partitionId,
          newPartitionEdges);
      if (partitionEdges == null) {
        partitionEdges = newPartitionEdges;
      }
    }
    return partitionEdges;
  }

  @Override
  protected OutEdges<LongWritable, E> getVertexOutEdges(
    VertexIdEdgeIterator<LongWritable, E> vertexIdEdgeIterator,
    Map<Long, OutEdges<LongWritable, E>> partitionEdgesIn) {
    Long2ObjectMap<OutEdges<LongWritable, E>> partitionEdges =
        (Long2ObjectMap<OutEdges<LongWritable, E>>) partitionEdgesIn;
    LongWritable vertexId = vertexIdEdgeIterator.getCurrentVertexId();
    OutEdges<LongWritable, E> outEdges = partitionEdges.get(vertexId.get());
    if (outEdges == null) {
      synchronized (partitionEdges) {
        outEdges = partitionEdges.get(vertexId.get());
        if (outEdges == null) {
          outEdges = configuration.createAndInitializeInputOutEdges();
          partitionEdges.put(vertexId.get(), outEdges);
        }
      }
    }
    return outEdges;
  }
}

<code block>


package org.apache.giraph.edge.primitives;

import org.apache.giraph.bsp.CentralizedServiceWorker;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.giraph.edge.AbstractEdgeStore;
import org.apache.giraph.edge.OutEdges;
import org.apache.giraph.utils.VertexIdEdgeIterator;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.util.Progressable;

import it.unimi.dsi.fastutil.ints.Int2ObjectMaps;
import it.unimi.dsi.fastutil.ints.Int2ObjectOpenHashMap;
import it.unimi.dsi.fastutil.ints.Int2ObjectMap;

import java.util.Iterator;
import java.util.Map;


public class IntEdgeStore<V extends Writable, E extends Writable>
  extends AbstractEdgeStore<IntWritable, V, E, Integer,
  Int2ObjectMap.Entry<OutEdges<IntWritable, E>>> {

  
  public IntEdgeStore(
      CentralizedServiceWorker<IntWritable, V, E> service,
      ImmutableClassesGiraphConfiguration<IntWritable, V, E> configuration,
      Progressable progressable) {
    super(service, configuration, progressable);
  }

  @Override
  protected IntWritable getVertexId(
    Int2ObjectMap.Entry<OutEdges<IntWritable, E>> entry,
    IntWritable representativeVertexId) {
    representativeVertexId.set(entry.getIntKey());
    return representativeVertexId;
  }

  @Override
  protected IntWritable createVertexId(
    Int2ObjectMap.Entry<OutEdges<IntWritable, E>> entry) {
    return new IntWritable(entry.getIntKey());
  }

  @Override
  protected OutEdges<IntWritable, E> getPartitionEdges(
    Int2ObjectMap.Entry<OutEdges<IntWritable, E>> entry) {
    return entry.getValue();
  }

  @Override
  protected Iterator<Int2ObjectMap.Entry<OutEdges<IntWritable, E>>>
  getPartitionEdgesIterator(
    Map<Integer, OutEdges<IntWritable, E>> partitionEdges) {
    return  ((Int2ObjectMap<OutEdges<IntWritable, E>>) partitionEdges)
        .int2ObjectEntrySet()
        .iterator();
  }

  @Override
  protected Int2ObjectMap<OutEdges<IntWritable, E>> getPartitionEdges(
      int partitionId) {
    Int2ObjectMap<OutEdges<IntWritable, E>> partitionEdges =
        (Int2ObjectMap<OutEdges<IntWritable, E>>)
            transientEdges.get(partitionId);
    if (partitionEdges == null) {
      Int2ObjectMap<OutEdges<IntWritable, E>> newPartitionEdges =
          Int2ObjectMaps.synchronize(
              new Int2ObjectOpenHashMap<OutEdges<IntWritable, E>>());
      partitionEdges = (Int2ObjectMap<OutEdges<IntWritable, E>>)
          transientEdges.putIfAbsent(partitionId,
              newPartitionEdges);
      if (partitionEdges == null) {
        partitionEdges = newPartitionEdges;
      }
    }
    return partitionEdges;
  }

  @Override
  protected OutEdges<IntWritable, E> getVertexOutEdges(
      VertexIdEdgeIterator<IntWritable, E> vertexIdEdgeIterator,
      Map<Integer, OutEdges<IntWritable, E>> partitionEdgesIn) {
    Int2ObjectMap<OutEdges<IntWritable, E>> partitionEdges =
        (Int2ObjectMap<OutEdges<IntWritable, E>>) partitionEdgesIn;
    IntWritable vertexId = vertexIdEdgeIterator.getCurrentVertexId();
    OutEdges<IntWritable, E> outEdges = partitionEdges.get(vertexId.get());
    if (outEdges == null) {
      synchronized (partitionEdges) {
        outEdges = partitionEdges.get(vertexId.get());
        if (outEdges == null) {
          outEdges = configuration.createAndInitializeInputOutEdges();
          partitionEdges.put(vertexId.get(), outEdges);
        }
      }
    }
    return outEdges;
  }
}

<code block>


package org.apache.giraph.comm;

import java.io.IOException;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
import java.util.Map;
import java.util.concurrent.ConcurrentMap;

import com.google.common.collect.Iterables;
import com.google.common.collect.Maps;

import org.apache.giraph.bsp.CentralizedServiceWorker;
import org.apache.giraph.comm.aggregators.AllAggregatorServerData;
import org.apache.giraph.comm.aggregators.OwnerAggregatorServerData;
import org.apache.giraph.comm.messages.MessageStore;
import org.apache.giraph.comm.messages.MessageStoreFactory;
import org.apache.giraph.comm.messages.queue.AsyncMessageStoreWrapper;
import org.apache.giraph.conf.GiraphConstants;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.giraph.edge.EdgeStore;
import org.apache.giraph.edge.EdgeStoreFactory;
import org.apache.giraph.graph.Vertex;
import org.apache.giraph.graph.VertexMutations;
import org.apache.giraph.graph.VertexResolver;
import org.apache.giraph.partition.DiskBackedPartitionStore;
import org.apache.giraph.partition.Partition;
import org.apache.giraph.partition.PartitionStore;
import org.apache.giraph.partition.SimplePartitionStore;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.log4j.Logger;


@SuppressWarnings("rawtypes")
public class ServerData<I extends WritableComparable,
    V extends Writable, E extends Writable> {
  
  private static final Logger LOG =
      Logger.getLogger(ServerData.class);
  
  private final ImmutableClassesGiraphConfiguration<I, V, E> conf;
  
  private volatile PartitionStore<I, V, E> partitionStore;
  
  private final EdgeStore<I, V, E> edgeStore;
  
  private final MessageStoreFactory<I, Writable, MessageStore<I, Writable>>
  messageStoreFactory;
  
  private volatile MessageStore<I, Writable> incomingMessageStore;
  
  private volatile MessageStore<I, Writable> currentMessageStore;
  
  private ConcurrentMap<Integer,
      ConcurrentMap<I, VertexMutations<I, V, E>>>
      oldPartitionMutations = Maps.newConcurrentMap();
  
  private ConcurrentMap<Integer,
      ConcurrentMap<I, VertexMutations<I, V, E>>>
      partitionMutations = Maps.newConcurrentMap();
  
  private final OwnerAggregatorServerData ownerAggregatorData;
  
  private final AllAggregatorServerData allAggregatorData;
  
  private final CentralizedServiceWorker<I, V, E> serviceWorker;

  
  private volatile List<Writable> currentWorkerToWorkerMessages =
      Collections.synchronizedList(new ArrayList<Writable>());
  
  private volatile List<Writable> incomingWorkerToWorkerMessages =
      Collections.synchronizedList(new ArrayList<Writable>());

  
  private final Mapper<?, ?, ?, ?>.Context context;

  
  public ServerData(
      CentralizedServiceWorker<I, V, E> service,
      ImmutableClassesGiraphConfiguration<I, V, E> conf,
      MessageStoreFactory<I, Writable, MessageStore<I, Writable>>
          messageStoreFactory,
      Mapper<?, ?, ?, ?>.Context context) {
    this.serviceWorker = service;
    this.conf = conf;
    this.messageStoreFactory = messageStoreFactory;
    if (GiraphConstants.USE_OUT_OF_CORE_GRAPH.get(conf)) {
      partitionStore =
          new DiskBackedPartitionStore<I, V, E>(conf, context,
              getServiceWorker());
    } else {
      partitionStore =
          new SimplePartitionStore<I, V, E>(conf, context);
    }
    EdgeStoreFactory<I, V, E> edgeStoreFactory = conf.createEdgeStoreFactory();
    edgeStoreFactory.initialize(service, conf, context);
    edgeStore = edgeStoreFactory.newStore();
    ownerAggregatorData = new OwnerAggregatorServerData(context);
    allAggregatorData = new AllAggregatorServerData(context, conf);
    this.context = context;
  }

  public EdgeStore<I, V, E> getEdgeStore() {
    return edgeStore;
  }

  
  public PartitionStore<I, V, E> getPartitionStore() {
    return partitionStore;
  }

  
  public <M extends Writable> MessageStore<I, M> getIncomingMessageStore() {
    return (MessageStore<I, M>) incomingMessageStore;
  }

  
  public <M extends Writable> MessageStore<I, M> getCurrentMessageStore() {
    return (MessageStore<I, M>) currentMessageStore;
  }

  
  public void resetMessageStores() throws IOException {
    if (currentMessageStore != null) {
      currentMessageStore.clearAll();
      currentMessageStore = null;
    }
    if (incomingMessageStore != null) {
      incomingMessageStore.clearAll();
      incomingMessageStore = null;
    }
    prepareSuperstep();
  }

  
  public void prepareSuperstep() {
    if (currentMessageStore != null) {
      try {
        currentMessageStore.clearAll();
      } catch (IOException e) {
        throw new IllegalStateException(
            "Failed to clear previous message store");
      }
    }
    currentMessageStore =
        incomingMessageStore != null ? incomingMessageStore :
            messageStoreFactory.newStore(conf.getIncomingMessageClasses());
    incomingMessageStore =
        messageStoreFactory.newStore(conf.getOutgoingMessageClasses());
    
    currentMessageStore.finalizeStore();

    currentWorkerToWorkerMessages = incomingWorkerToWorkerMessages;
    incomingWorkerToWorkerMessages =
        Collections.synchronizedList(new ArrayList<Writable>());
  }

  
  public void waitForComplete() {
    if (incomingMessageStore instanceof AsyncMessageStoreWrapper) {
      ((AsyncMessageStoreWrapper) incomingMessageStore).waitToComplete();
    }
  }

  
  public ConcurrentMap<Integer, ConcurrentMap<I, VertexMutations<I, V, E>>>
  getPartitionMutations() {
    return partitionMutations;
  }

  
  public OwnerAggregatorServerData getOwnerAggregatorData() {
    return ownerAggregatorData;
  }

  
  public AllAggregatorServerData getAllAggregatorData() {
    return allAggregatorData;
  }

  
  public CentralizedServiceWorker<I, V, E> getServiceWorker() {
    return this.serviceWorker;
  }

  
  public List<Writable> getAndClearCurrentWorkerToWorkerMessages() {
    List<Writable> ret = currentWorkerToWorkerMessages;
    currentWorkerToWorkerMessages = null;
    return ret;
  }

  
  public void addIncomingWorkerToWorkerMessage(Writable message) {
    incomingWorkerToWorkerMessages.add(message);
  }


  
  public List<Writable> getCurrentWorkerToWorkerMessages() {
    return currentWorkerToWorkerMessages;
  }

  
  public void prepareResolveMutations() {
    oldPartitionMutations = partitionMutations;
    partitionMutations = Maps.newConcurrentMap();
  }

  
  public void resolvePartitionMutation(Partition<I, V, E> partition) {
    Integer partitionId = partition.getId();
    VertexResolver<I, V, E> vertexResolver = conf.createVertexResolver();
    ConcurrentMap<I, VertexMutations<I, V, E>> prevPartitionMutations =
        oldPartitionMutations.get(partitionId);

    
    if (prevPartitionMutations != null) {
      for (Map.Entry<I, VertexMutations<I, V, E>> entry : prevPartitionMutations
          .entrySet()) {
        I vertexId = entry.getKey();
        Vertex<I, V, E> originalVertex = partition.getVertex(vertexId);
        VertexMutations<I, V, E> vertexMutations = entry.getValue();
        Vertex<I, V, E> vertex = vertexResolver.resolve(vertexId,
            originalVertex, vertexMutations,
            getCurrentMessageStore().hasMessagesForVertex(entry.getKey()));

        if (LOG.isDebugEnabled()) {
          LOG.debug("resolvePartitionMutations: Resolved vertex index " +
              vertexId + " in partition index " + partitionId +
              " with original vertex " + originalVertex +
              ", returned vertex " + vertex + " on superstep " +
              serviceWorker.getSuperstep() + " with mutations " +
              vertexMutations);
        }

        if (vertex != null) {
          partition.putVertex(vertex);
        } else if (originalVertex != null) {
          partition.removeVertex(vertexId);
          try {
            getCurrentMessageStore().clearVertexMessages(vertexId);
          } catch (IOException e) {
            throw new IllegalStateException("resolvePartitionMutations: " +
                "Caught IOException while clearing messages for a deleted " +
                "vertex due to a mutation");
          }
        }
        context.progress();
      }
    }

    
    
    Iterable<I> destinations = getCurrentMessageStore().
        getPartitionDestinationVertices(partitionId);
    if (!Iterables.isEmpty(destinations)) {
      for (I vertexId : destinations) {
        if (partition.getVertex(vertexId) == null) {
          Vertex<I, V, E> vertex =
              vertexResolver.resolve(vertexId, null, null, true);

          if (LOG.isDebugEnabled()) {
            LOG.debug("resolvePartitionMutations: A non-existing vertex has " +
                "message(s). Added vertex index " + vertexId +
                " in partition index " + partitionId +
                ", vertex = " + vertex + ", on superstep " +
                serviceWorker.getSuperstep());
          }

          if (vertex != null) {
            partition.putVertex(vertex);
          }
          context.progress();
        }
      }
    }
  }
}

<code block>


package org.apache.giraph.comm.messages.primitives.long_id;

import it.unimi.dsi.fastutil.ints.Int2ObjectOpenHashMap;
import it.unimi.dsi.fastutil.longs.Long2ObjectOpenHashMap;
import org.apache.giraph.bsp.CentralizedServiceWorker;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.giraph.factories.MessageValueFactory;
import org.apache.giraph.graph.Vertex;
import org.apache.giraph.partition.Partition;
import org.apache.giraph.partition.PartitionOwner;
import org.apache.giraph.utils.VertexIdIterator;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Writable;

import java.util.List;


public abstract class LongAbstractListMessageStore<M extends Writable,
  L extends List> extends LongAbstractMessageStore<M, L> {
  
  private final
  Int2ObjectOpenHashMap<Long2ObjectOpenHashMap<L>> nascentMap;

  
  public LongAbstractListMessageStore(
      MessageValueFactory<M> messageValueFactory,
      CentralizedServiceWorker<LongWritable, Writable, Writable> service,
      ImmutableClassesGiraphConfiguration<LongWritable,
          Writable, Writable> config) {
    super(messageValueFactory, service, config);
    populateMap();

    
    nascentMap = new Int2ObjectOpenHashMap<>();
    for (int partitionId : service.getPartitionStore().getPartitionIds()) {
      nascentMap.put(partitionId, new Long2ObjectOpenHashMap<L>());
    }
  }

  
  private void populateMap() { 
    
    for (int partitionId : service.getPartitionStore().getPartitionIds()) {
      Partition<LongWritable, ?, ?> partition = service.getPartitionStore()
          .getOrCreatePartition(partitionId);
      Long2ObjectOpenHashMap<L> partitionMap = map.get(partitionId);
      for (Vertex<LongWritable, ?, ?> vertex : partition) {
        partitionMap.put(vertex.getId().get(), createList());
      }
    }
  }

  
  protected abstract L createList();

  
  protected L getList(
    VertexIdIterator<LongWritable> iterator) {
    PartitionOwner owner =
        service.getVertexPartitionOwner(iterator.getCurrentVertexId());
    long vertexId = iterator.getCurrentVertexId().get();
    int partitionId = owner.getPartitionId();
    Long2ObjectOpenHashMap<L> partitionMap = map.get(partitionId);
    if (!partitionMap.containsKey(vertexId)) {
      synchronized (nascentMap) {
        
        
        Long2ObjectOpenHashMap<L> nascentPartitionMap =
          nascentMap.get(partitionId);
        if (nascentPartitionMap.get(vertexId) == null) {
          nascentPartitionMap.put(vertexId, createList());
        }
        return nascentPartitionMap.get(vertexId);
      }
    }
    return partitionMap.get(vertexId);
  }

  @Override
  public void finalizeStore() {
    for (int partitionId : nascentMap.keySet()) {
      
      map.get(partitionId).putAll(nascentMap.get(partitionId));
    }
    nascentMap.clear();
  }

  
  
}

<code block>


package org.apache.giraph.comm.requests;

import org.apache.giraph.comm.ServerData;
import org.apache.giraph.edge.Edge;
import org.apache.giraph.utils.ByteArrayVertexIdEdges;
import org.apache.giraph.utils.PairList;
import org.apache.giraph.utils.VertexIdEdges;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;


@SuppressWarnings("unchecked")
public class SendWorkerEdgesRequest<I extends WritableComparable,
    E extends Writable>
    extends SendWorkerDataRequest<I, Edge<I, E>,
    VertexIdEdges<I, E>> {
  
  public SendWorkerEdgesRequest() { }

  
  public SendWorkerEdgesRequest(
      PairList<Integer, VertexIdEdges<I, E>> partVertEdges) {
    this.partitionVertexData = partVertEdges;
  }

  @Override
  public VertexIdEdges<I, E> createVertexIdData() {
    return new ByteArrayVertexIdEdges<>();
  }

  @Override
  public RequestType getType() {
    return RequestType.SEND_WORKER_EDGES_REQUEST;
  }

  @Override
  public void doRequest(ServerData serverData) {
    PairList<Integer, VertexIdEdges<I, E>>.Iterator
        iterator = partitionVertexData.getIterator();
    while (iterator.hasNext()) {
      iterator.next();
      serverData.getEdgeStore().
          addPartitionEdges(iterator.getCurrentFirst(),
              iterator.getCurrentSecond());
    }
  }
}

<code block>


package org.apache.giraph.comm.requests;

import org.apache.giraph.comm.ServerData;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.giraph.utils.ExtendedDataOutput;
import org.apache.giraph.utils.PairList;
import org.apache.giraph.utils.VertexIterator;
import org.apache.giraph.utils.WritableUtils;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;
import org.apache.giraph.partition.PartitionStore;
import org.apache.giraph.partition.Partition;
import org.apache.log4j.Logger;
import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;


@SuppressWarnings("rawtypes")
public class SendWorkerVerticesRequest<I extends WritableComparable,
    V extends Writable, E extends Writable> extends
    WritableRequest<I, V, E> implements WorkerRequest<I, V, E> {
  
  private static final Logger LOG =
      Logger.getLogger(SendWorkerVerticesRequest.class);
  
  private PairList<Integer, ExtendedDataOutput> workerPartitions;

  
  public SendWorkerVerticesRequest() { }

  
  public SendWorkerVerticesRequest(
      ImmutableClassesGiraphConfiguration<I, V, E> conf,
      PairList<Integer, ExtendedDataOutput> workerPartitions) {
    this.workerPartitions = workerPartitions;
    setConf(conf);
  }

  @Override
  public void readFieldsRequest(DataInput input) throws IOException {
    int numPartitions = input.readInt();
    workerPartitions = new PairList<Integer, ExtendedDataOutput>();
    workerPartitions.initialize(numPartitions);
    while (numPartitions-- > 0) {
      final int partitionId = input.readInt();
      ExtendedDataOutput partitionData =
          WritableUtils.readExtendedDataOutput(input, getConf());
      workerPartitions.add(partitionId, partitionData);
    }
  }

  @Override
  public void writeRequest(DataOutput output) throws IOException {
    output.writeInt(workerPartitions.getSize());
    PairList<Integer, ExtendedDataOutput>.Iterator
        iterator = workerPartitions.getIterator();
    while (iterator.hasNext()) {
      iterator.next();
      output.writeInt(iterator.getCurrentFirst());
      WritableUtils.writeExtendedDataOutput(
          iterator.getCurrentSecond(), output);
    }
  }

  @Override
  public RequestType getType() {
    return RequestType.SEND_WORKER_VERTICES_REQUEST;
  }

  @Override
  public void doRequest(ServerData<I, V, E> serverData) {
    PairList<Integer, ExtendedDataOutput>.Iterator
        iterator = workerPartitions.getIterator();
    while (iterator.hasNext()) {
      iterator.next();
      VertexIterator<I, V, E> vertexIterator =
          new VertexIterator<I, V, E>(
          iterator.getCurrentSecond(), getConf());

      Partition<I, V, E> partition;
      PartitionStore store = serverData.getPartitionStore();
      partition = store.getOrCreatePartition(iterator.getCurrentFirst());
      partition.addPartitionVertices(vertexIterator);
      store.putPartition(partition);
    }
  }

  @Override
  public int getSerializedSize() {
    
    int size = super.getSerializedSize() + 4;
    PairList<Integer, ExtendedDataOutput>.Iterator iterator =
        workerPartitions.getIterator();
    while (iterator.hasNext()) {
      iterator.next();
      
      size += 8 + iterator.getCurrentSecond().getPos();
    }
    return size;
  }
}


<code block>


package org.apache.giraph.worker;

import java.io.DataInputStream;
import java.io.DataOutputStream;
import java.io.IOException;
import java.nio.charset.Charset;
import java.util.ArrayList;
import java.util.Collection;
import java.util.Collections;
import java.util.HashSet;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Queue;
import java.util.Set;
import java.util.concurrent.ArrayBlockingQueue;
import java.util.concurrent.Callable;
import java.util.concurrent.ConcurrentLinkedQueue;
import java.util.concurrent.TimeUnit;

import net.iharder.Base64;

import org.apache.giraph.bsp.ApplicationState;
import org.apache.giraph.bsp.BspService;
import org.apache.giraph.bsp.CentralizedServiceWorker;
import org.apache.giraph.bsp.checkpoints.CheckpointStatus;
import org.apache.giraph.comm.ServerData;
import org.apache.giraph.comm.WorkerClient;
import org.apache.giraph.comm.WorkerClientRequestProcessor;
import org.apache.giraph.comm.WorkerServer;
import org.apache.giraph.comm.aggregators.WorkerAggregatorRequestProcessor;
import org.apache.giraph.comm.messages.MessageStore;
import org.apache.giraph.comm.messages.queue.AsyncMessageStoreWrapper;
import org.apache.giraph.comm.netty.NettyWorkerAggregatorRequestProcessor;
import org.apache.giraph.comm.netty.NettyWorkerClient;
import org.apache.giraph.comm.netty.NettyWorkerClientRequestProcessor;
import org.apache.giraph.comm.netty.NettyWorkerServer;
import org.apache.giraph.conf.GiraphConstants;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.giraph.edge.Edge;
import org.apache.giraph.graph.AddressesAndPartitionsWritable;
import org.apache.giraph.graph.FinishedSuperstepStats;
import org.apache.giraph.graph.GlobalStats;
import org.apache.giraph.graph.GraphTaskManager;
import org.apache.giraph.graph.InputSplitEvents;
import org.apache.giraph.graph.InputSplitPaths;
import org.apache.giraph.graph.Vertex;
import org.apache.giraph.graph.VertexEdgeCount;
import org.apache.giraph.io.EdgeOutputFormat;
import org.apache.giraph.io.EdgeWriter;
import org.apache.giraph.io.VertexOutputFormat;
import org.apache.giraph.io.VertexWriter;
import org.apache.giraph.io.superstep_output.SuperstepOutput;
import org.apache.giraph.mapping.translate.TranslateEdge;
import org.apache.giraph.master.MasterInfo;
import org.apache.giraph.master.SuperstepClasses;
import org.apache.giraph.metrics.GiraphMetrics;
import org.apache.giraph.metrics.GiraphTimer;
import org.apache.giraph.metrics.GiraphTimerContext;
import org.apache.giraph.metrics.ResetSuperstepMetricsObserver;
import org.apache.giraph.metrics.SuperstepMetricsRegistry;
import org.apache.giraph.metrics.WorkerSuperstepMetrics;
import org.apache.giraph.partition.Partition;
import org.apache.giraph.partition.PartitionExchange;
import org.apache.giraph.partition.PartitionOwner;
import org.apache.giraph.partition.PartitionStats;
import org.apache.giraph.partition.PartitionStore;
import org.apache.giraph.partition.WorkerGraphPartitioner;
import org.apache.giraph.utils.CallableFactory;
import org.apache.giraph.utils.CheckpointingUtils;
import org.apache.giraph.utils.JMapHistoDumper;
import org.apache.giraph.utils.LoggerUtils;
import org.apache.giraph.utils.MemoryUtils;
import org.apache.giraph.utils.ProgressableUtils;
import org.apache.giraph.utils.ReactiveJMapHistoDumper;
import org.apache.giraph.utils.WritableUtils;
import org.apache.giraph.zk.BspEvent;
import org.apache.giraph.zk.PredicateLock;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.io.compress.CompressionCodec;
import org.apache.hadoop.io.compress.CompressionCodecFactory;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.OutputCommitter;
import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.WatchedEvent;
import org.apache.zookeeper.Watcher.Event.EventType;
import org.apache.zookeeper.ZooDefs.Ids;
import org.apache.zookeeper.data.Stat;
import org.json.JSONArray;
import org.json.JSONException;
import org.json.JSONObject;

import com.google.common.collect.Iterables;
import com.google.common.collect.Lists;


@SuppressWarnings("rawtypes")
public class BspServiceWorker<I extends WritableComparable,
    V extends Writable, E extends Writable>
    extends BspService<I, V, E>
    implements CentralizedServiceWorker<I, V, E>,
    ResetSuperstepMetricsObserver {
  
  public static final String TIMER_WAIT_REQUESTS = "wait-requests-us";
  
  private static final Logger LOG = Logger.getLogger(BspServiceWorker.class);
  
  private String myHealthZnode;
  
  private final WorkerInfo workerInfo;
  
  private final WorkerGraphPartitioner<I, V, E> workerGraphPartitioner;
  
  private final LocalData<I, V, E, ? extends Writable> localData;
  
  private final TranslateEdge<I, E> translateEdge;
  
  private final WorkerClient<I, V, E> workerClient;
  
  private final WorkerServer<I, V, E> workerServer;
  
  private final WorkerAggregatorRequestProcessor
  workerAggregatorRequestProcessor;
  
  private MasterInfo masterInfo = new MasterInfo();
  
  private List<WorkerInfo> workerInfoList = Lists.newArrayList();
  
  private final BspEvent partitionExchangeChildrenChanged;

  
  private final WorkerContext workerContext;

  
  private final WorkerAggregatorHandler globalCommHandler;

  
  private final SuperstepOutput<I, V, E> superstepOutput;

  
  private final WorkerObserver[] observers;
  
  private final WorkerProgressWriter workerProgressWriter;

  
  
  private GiraphTimer wcPostSuperstepTimer;
  
  private GiraphTimer waitRequestsTimer;

  
  private InputSplitsHandler vertexSplitsHandler;
  
  private InputSplitsHandler edgeSplitsHandler;

  
  public BspServiceWorker(
    Mapper<?, ?, ?, ?>.Context context,
    GraphTaskManager<I, V, E> graphTaskManager)
    throws IOException, InterruptedException {
    super(context, graphTaskManager);
    ImmutableClassesGiraphConfiguration<I, V, E> conf = getConfiguration();
    localData = new LocalData<>(conf);
    translateEdge = getConfiguration().edgeTranslationInstance();
    if (translateEdge != null) {
      translateEdge.initialize(this);
    }
    partitionExchangeChildrenChanged = new PredicateLock(context);
    registerBspEvent(partitionExchangeChildrenChanged);
    workerGraphPartitioner =
        getGraphPartitionerFactory().createWorkerGraphPartitioner();
    workerInfo = new WorkerInfo();
    workerServer = new NettyWorkerServer<I, V, E>(conf, this, context,
        graphTaskManager.createUncaughtExceptionHandler());
    workerInfo.setInetSocketAddress(workerServer.getMyAddress());
    workerInfo.setTaskId(getTaskPartition());
    workerClient = new NettyWorkerClient<I, V, E>(context, conf, this,
        graphTaskManager.createUncaughtExceptionHandler());

    workerAggregatorRequestProcessor =
        new NettyWorkerAggregatorRequestProcessor(getContext(), conf, this);

    globalCommHandler = new WorkerAggregatorHandler(this, conf, context);

    workerContext = conf.createWorkerContext();
    workerContext.setWorkerGlobalCommUsage(globalCommHandler);

    superstepOutput = conf.createSuperstepOutput(context);

    if (conf.isJMapHistogramDumpEnabled()) {
      conf.addWorkerObserverClass(JMapHistoDumper.class);
    }
    if (conf.isReactiveJmapHistogramDumpEnabled()) {
      conf.addWorkerObserverClass(ReactiveJMapHistoDumper.class);
    }
    observers = conf.createWorkerObservers();

    WorkerProgress.get().setTaskId(getTaskPartition());
    workerProgressWriter = conf.trackJobProgressOnClient() ?
        new WorkerProgressWriter(graphTaskManager.getJobProgressTracker()) :
        null;

    GiraphMetrics.get().addSuperstepResetObserver(this);
    vertexSplitsHandler = null;
    edgeSplitsHandler = null;
  }

  @Override
  public void newSuperstep(SuperstepMetricsRegistry superstepMetrics) {
    waitRequestsTimer = new GiraphTimer(superstepMetrics,
        TIMER_WAIT_REQUESTS, TimeUnit.MICROSECONDS);
    wcPostSuperstepTimer = new GiraphTimer(superstepMetrics,
        "worker-context-post-superstep", TimeUnit.MICROSECONDS);
  }

  @Override
  public WorkerContext getWorkerContext() {
    return workerContext;
  }

  @Override
  public WorkerObserver[] getWorkerObservers() {
    return observers;
  }

  @Override
  public WorkerClient<I, V, E> getWorkerClient() {
    return workerClient;
  }

  public LocalData<I, V, E, ? extends Writable> getLocalData() {
    return localData;
  }

  public TranslateEdge<I, E> getTranslateEdge() {
    return translateEdge;
  }

  
  public boolean isHealthy() {
    return true;
  }

  
  private VertexEdgeCount loadInputSplits(
      List<String> inputSplitPathList,
      CallableFactory<VertexEdgeCount> inputSplitsCallableFactory)
    throws KeeperException, InterruptedException {
    VertexEdgeCount vertexEdgeCount = new VertexEdgeCount();
    
    int maxInputSplitThreads = (inputSplitPathList.size() - 1) /
        getConfiguration().getMaxWorkers() + 1;
    int numThreads = Math.min(getConfiguration().getNumInputSplitsThreads(),
        maxInputSplitThreads);
    if (LOG.isInfoEnabled()) {
      LOG.info("loadInputSplits: Using " + numThreads + " thread(s), " +
          "originally " + getConfiguration().getNumInputSplitsThreads() +
          " threads(s) for " + inputSplitPathList.size() + " total splits.");
    }

    List<VertexEdgeCount> results =
        ProgressableUtils.getResultsWithNCallables(inputSplitsCallableFactory,
            numThreads, "load-%d", getContext());
    for (VertexEdgeCount result : results) {
      vertexEdgeCount = vertexEdgeCount.incrVertexEdgeCount(result);
    }

    workerClient.waitAllRequests();
    return vertexEdgeCount;
  }

  
  private long loadMapping() throws KeeperException,
    InterruptedException {
    List<String> inputSplitPathList =
        getZkExt().getChildrenExt(mappingInputSplitsPaths.getPath(),
        false, false, true);

    InputSplitPathOrganizer splitOrganizer =
        new InputSplitPathOrganizer(getZkExt(),
            inputSplitPathList, getWorkerInfo().getHostname(),
            getConfiguration().useInputSplitLocality());

    MappingInputSplitsCallableFactory<I, V, E, ? extends Writable>
        mappingInputSplitsCallableFactory =
        new MappingInputSplitsCallableFactory<>(
            getConfiguration().createWrappedMappingInputFormat(),
            splitOrganizer,
            getContext(),
            getConfiguration(),
            this,
            getZkExt());

    long entriesLoaded = 0;
    
    int maxInputSplitThreads = inputSplitPathList.size();
    int numThreads = Math.min(getConfiguration().getNumInputSplitsThreads(),
        maxInputSplitThreads);
    if (LOG.isInfoEnabled()) {
      LOG.info("loadInputSplits: Using " + numThreads + " thread(s), " +
          "originally " + getConfiguration().getNumInputSplitsThreads() +
          " threads(s) for " + inputSplitPathList.size() + " total splits.");
    }

    List<Integer> results =
        ProgressableUtils.getResultsWithNCallables(
            mappingInputSplitsCallableFactory,
            numThreads, "load-mapping-%d", getContext());
    for (Integer result : results) {
      entriesLoaded += result;
    }
    
    localData.getMappingStore().postFilling();
    return entriesLoaded;
  }

  
  private VertexEdgeCount loadVertices() throws KeeperException,
      InterruptedException {
    List<String> inputSplitPathList =
        getZkExt().getChildrenExt(vertexInputSplitsPaths.getPath(),
            false, false, true);

    InputSplitPathOrganizer splitOrganizer =
        new InputSplitPathOrganizer(getZkExt(),
            inputSplitPathList, getWorkerInfo().getHostname(),
            getConfiguration().useInputSplitLocality());
    vertexSplitsHandler = new InputSplitsHandler(
        splitOrganizer,
        getZkExt(),
        getContext(),
        BspService.VERTEX_INPUT_SPLIT_RESERVED_NODE,
        BspService.VERTEX_INPUT_SPLIT_FINISHED_NODE);

    VertexInputSplitsCallableFactory<I, V, E> inputSplitsCallableFactory =
        new VertexInputSplitsCallableFactory<I, V, E>(
            getConfiguration().createWrappedVertexInputFormat(),
            getContext(),
            getConfiguration(),
            this,
            vertexSplitsHandler,
            getZkExt());

    return loadInputSplits(inputSplitPathList, inputSplitsCallableFactory);
  }

  
  private long loadEdges() throws KeeperException, InterruptedException {
    List<String> inputSplitPathList =
        getZkExt().getChildrenExt(edgeInputSplitsPaths.getPath(),
            false, false, true);

    InputSplitPathOrganizer splitOrganizer =
        new InputSplitPathOrganizer(getZkExt(),
            inputSplitPathList, getWorkerInfo().getHostname(),
            getConfiguration().useInputSplitLocality());
    edgeSplitsHandler = new InputSplitsHandler(
        splitOrganizer,
        getZkExt(),
        getContext(),
        BspService.EDGE_INPUT_SPLIT_RESERVED_NODE,
        BspService.EDGE_INPUT_SPLIT_FINISHED_NODE);

    EdgeInputSplitsCallableFactory<I, V, E> inputSplitsCallableFactory =
        new EdgeInputSplitsCallableFactory<I, V, E>(
            getConfiguration().createWrappedEdgeInputFormat(),
            getContext(),
            getConfiguration(),
            this,
            edgeSplitsHandler,
            getZkExt());

    return loadInputSplits(inputSplitPathList, inputSplitsCallableFactory).
        getEdgeCount();
  }

  @Override
  public MasterInfo getMasterInfo() {
    return masterInfo;
  }

  @Override
  public List<WorkerInfo> getWorkerInfoList() {
    return workerInfoList;
  }

  
  private void ensureInputSplitsReady(InputSplitPaths inputSplitPaths,
                                      InputSplitEvents inputSplitEvents) {
    while (true) {
      Stat inputSplitsReadyStat;
      try {
        inputSplitsReadyStat = getZkExt().exists(
            inputSplitPaths.getAllReadyPath(), true);
      } catch (KeeperException e) {
        throw new IllegalStateException("ensureInputSplitsReady: " +
            "KeeperException waiting on input splits", e);
      } catch (InterruptedException e) {
        throw new IllegalStateException("ensureInputSplitsReady: " +
            "InterruptedException waiting on input splits", e);
      }
      if (inputSplitsReadyStat != null) {
        break;
      }
      inputSplitEvents.getAllReadyChanged().waitForever();
      inputSplitEvents.getAllReadyChanged().reset();
    }
  }

  
  private void markCurrentWorkerDoneThenWaitForOthers(
    InputSplitPaths inputSplitPaths,
    InputSplitEvents inputSplitEvents) {
    String workerInputSplitsDonePath =
        inputSplitPaths.getDonePath() + "/" +
            getWorkerInfo().getHostnameId();
    try {
      getZkExt().createExt(workerInputSplitsDonePath,
          null,
          Ids.OPEN_ACL_UNSAFE,
          CreateMode.PERSISTENT,
          true);
    } catch (KeeperException e) {
      throw new IllegalStateException(
          "markCurrentWorkerDoneThenWaitForOthers: " +
          "KeeperException creating worker done splits", e);
    } catch (InterruptedException e) {
      throw new IllegalStateException(
          "markCurrentWorkerDoneThenWaitForOthers: " +
          "InterruptedException creating worker done splits", e);
    }
    while (true) {
      Stat inputSplitsDoneStat;
      try {
        inputSplitsDoneStat =
            getZkExt().exists(inputSplitPaths.getAllDonePath(),
                true);
      } catch (KeeperException e) {
        throw new IllegalStateException(
            "markCurrentWorkerDoneThenWaitForOthers: " +
            "KeeperException waiting on worker done splits", e);
      } catch (InterruptedException e) {
        throw new IllegalStateException(
            "markCurrentWorkerDoneThenWaitForOthers: " +
            "InterruptedException waiting on worker done splits", e);
      }
      if (inputSplitsDoneStat != null) {
        break;
      }
      inputSplitEvents.getAllDoneChanged().waitForever();
      inputSplitEvents.getAllDoneChanged().reset();
    }
  }

  @Override
  public FinishedSuperstepStats setup() {
    
    
    
    
    
    
    
    if (getRestartedSuperstep() != UNSET_SUPERSTEP) {
      setCachedSuperstep(getRestartedSuperstep());
      return new FinishedSuperstepStats(0, false, 0, 0, true,
          CheckpointStatus.NONE);
    }

    JSONObject jobState = getJobState();
    if (jobState != null) {
      try {
        if ((ApplicationState.valueOf(jobState.getString(JSONOBJ_STATE_KEY)) ==
            ApplicationState.START_SUPERSTEP) &&
            jobState.getLong(JSONOBJ_SUPERSTEP_KEY) ==
            getSuperstep()) {
          if (LOG.isInfoEnabled()) {
            LOG.info("setup: Restarting from an automated " +
                "checkpointed superstep " +
                getSuperstep() + ", attempt " +
                getApplicationAttempt());
          }
          setRestartedSuperstep(getSuperstep());
          return new FinishedSuperstepStats(0, false, 0, 0, true,
              CheckpointStatus.NONE);
        }
      } catch (JSONException e) {
        throw new RuntimeException(
            "setup: Failed to get key-values from " +
                jobState.toString(), e);
      }
    }

    
    Collection<? extends PartitionOwner> masterSetPartitionOwners =
        startSuperstep();
    workerGraphPartitioner.updatePartitionOwners(
        getWorkerInfo(), masterSetPartitionOwners);


    workerClient.setup(getConfiguration().authenticate());


    
    
    globalCommHandler.prepareSuperstep(workerAggregatorRequestProcessor);

    VertexEdgeCount vertexEdgeCount;
    long entriesLoaded;

    if (getConfiguration().hasMappingInputFormat()) {
      
      ensureInputSplitsReady(mappingInputSplitsPaths, mappingInputSplitsEvents);
      getContext().progress();
      try {
        entriesLoaded = loadMapping();
        
        
        getGraphPartitionerFactory().initialize(localData);
      } catch (InterruptedException e) {
        throw new IllegalStateException(
            "setup: loadMapping failed with InterruptedException", e);
      } catch (KeeperException e) {
        throw new IllegalStateException(
            "setup: loadMapping failed with KeeperException", e);
      }
      getContext().progress();
      if (LOG.isInfoEnabled()) {
        LOG.info("setup: Finally loaded a total of " +
            entriesLoaded + " entries from inputSplits");
      }

      
      markCurrentWorkerDoneThenWaitForOthers(mappingInputSplitsPaths,
          mappingInputSplitsEvents);
      
      
      localData.printStats();
    }

    if (getConfiguration().hasVertexInputFormat()) {
      
      ensureInputSplitsReady(vertexInputSplitsPaths, vertexInputSplitsEvents);
      getContext().progress();
      try {
        vertexEdgeCount = loadVertices();
      } catch (InterruptedException e) {
        throw new IllegalStateException(
            "setup: loadVertices failed with InterruptedException", e);
      } catch (KeeperException e) {
        throw new IllegalStateException(
            "setup: loadVertices failed with KeeperException", e);
      }
      getContext().progress();
    } else {
      vertexEdgeCount = new VertexEdgeCount();
    }
    WorkerProgress.get().finishLoadingVertices();

    if (getConfiguration().hasEdgeInputFormat()) {
      
      ensureInputSplitsReady(edgeInputSplitsPaths, edgeInputSplitsEvents);
      getContext().progress();
      try {
        vertexEdgeCount = vertexEdgeCount.incrVertexEdgeCount(0, loadEdges());
      } catch (InterruptedException e) {
        throw new IllegalStateException(
            "setup: loadEdges failed with InterruptedException", e);
      } catch (KeeperException e) {
        throw new IllegalStateException(
            "setup: loadEdges failed with KeeperException", e);
      }
      getContext().progress();
    }
    WorkerProgress.get().finishLoadingEdges();

    if (LOG.isInfoEnabled()) {
      LOG.info("setup: Finally loaded a total of " + vertexEdgeCount);
    }

    if (getConfiguration().hasVertexInputFormat()) {
      
      markCurrentWorkerDoneThenWaitForOthers(vertexInputSplitsPaths,
          vertexInputSplitsEvents);
    }

    if (getConfiguration().hasEdgeInputFormat()) {
      
      markCurrentWorkerDoneThenWaitForOthers(edgeInputSplitsPaths,
          edgeInputSplitsEvents);
    }

    
    for (PartitionOwner partitionOwner : masterSetPartitionOwners) {
      if (partitionOwner.getWorkerInfo().equals(getWorkerInfo()) &&
          !getPartitionStore().hasPartition(
              partitionOwner.getPartitionId())) {
        Partition<I, V, E> partition =
            getConfiguration().createPartition(
                partitionOwner.getPartitionId(), getContext());
        getPartitionStore().addPartition(partition);
      }
    }

    
    localData.removeMappingStoreIfPossible();

    if (getConfiguration().hasEdgeInputFormat()) {
      
      getServerData().getEdgeStore().moveEdgesToVertices();
    }

    
    
    List<PartitionStats> partitionStatsList =
        new ArrayList<PartitionStats>();
    PartitionStore<I, V, E> partitionStore = getPartitionStore();
    for (Integer partitionId : partitionStore.getPartitionIds()) {
      PartitionStats partitionStats =
          new PartitionStats(partitionId,
              partitionStore.getPartitionVertexCount(partitionId),
              0,
              partitionStore.getPartitionEdgeCount(partitionId),
              0, 0);
      partitionStatsList.add(partitionStats);
    }
    workerGraphPartitioner.finalizePartitionStats(
        partitionStatsList, getPartitionStore());

    return finishSuperstep(partitionStatsList, null);
  }

  
  private void registerHealth(long superstep) {
    JSONArray hostnamePort = new JSONArray();
    hostnamePort.put(getHostname());

    hostnamePort.put(workerInfo.getPort());

    String myHealthPath = null;
    if (isHealthy()) {
      myHealthPath = getWorkerInfoHealthyPath(getApplicationAttempt(),
          getSuperstep());
    } else {
      myHealthPath = getWorkerInfoUnhealthyPath(getApplicationAttempt(),
          getSuperstep());
    }
    myHealthPath = myHealthPath + "/" + workerInfo.getHostnameId();
    try {
      myHealthZnode = getZkExt().createExt(
          myHealthPath,
          WritableUtils.writeToByteArray(workerInfo),
          Ids.OPEN_ACL_UNSAFE,
          CreateMode.EPHEMERAL,
          true);
    } catch (KeeperException.NodeExistsException e) {
      LOG.warn("registerHealth: myHealthPath already exists (likely " +
          "from previous failure): " + myHealthPath +
          ".  Waiting for change in attempts " +
          "to re-join the application");
      getApplicationAttemptChangedEvent().waitForever();
      if (LOG.isInfoEnabled()) {
        LOG.info("registerHealth: Got application " +
            "attempt changed event, killing self");
      }
      throw new IllegalStateException(
          "registerHealth: Trying " +
              "to get the new application attempt by killing self", e);
    } catch (KeeperException e) {
      throw new IllegalStateException("Creating " + myHealthPath +
          " failed with KeeperException", e);
    } catch (InterruptedException e) {
      throw new IllegalStateException("Creating " + myHealthPath +
          " failed with InterruptedException", e);
    }
    if (LOG.isInfoEnabled()) {
      LOG.info("registerHealth: Created my health node for attempt=" +
          getApplicationAttempt() + ", superstep=" +
          getSuperstep() + " with " + myHealthZnode +
          " and workerInfo= " + workerInfo);
    }
  }

  
  private void unregisterHealth() {
    LOG.error("unregisterHealth: Got failure, unregistering health on " +
        myHealthZnode + " on superstep " + getSuperstep());
    try {
      getZkExt().deleteExt(myHealthZnode, -1, false);
    } catch (InterruptedException e) {
      throw new IllegalStateException(
          "unregisterHealth: InterruptedException - Couldn't delete " +
              myHealthZnode, e);
    } catch (KeeperException e) {
      throw new IllegalStateException(
          "unregisterHealth: KeeperException - Couldn't delete " +
              myHealthZnode, e);
    }
  }

  @Override
  public void failureCleanup() {
    unregisterHealth();
  }

  @Override
  public Collection<? extends PartitionOwner> startSuperstep() {
    
    
    
    
    
    
    if (getSuperstep() != INPUT_SUPERSTEP) {
      workerServer.prepareSuperstep();
    }

    registerHealth(getSuperstep());

    String addressesAndPartitionsPath =
        getAddressesAndPartitionsPath(getApplicationAttempt(),
            getSuperstep());
    AddressesAndPartitionsWritable addressesAndPartitions =
        new AddressesAndPartitionsWritable(
            workerGraphPartitioner.createPartitionOwner().getClass());
    try {
      while (getZkExt().exists(addressesAndPartitionsPath, true) ==
          null) {
        getAddressesAndPartitionsReadyChangedEvent().waitForever();
        getAddressesAndPartitionsReadyChangedEvent().reset();
      }
      WritableUtils.readFieldsFromZnode(
          getZkExt(),
          addressesAndPartitionsPath,
          false,
          null,
          addressesAndPartitions);
    } catch (KeeperException e) {
      throw new IllegalStateException(
          "startSuperstep: KeeperException getting assignments", e);
    } catch (InterruptedException e) {
      throw new IllegalStateException(
          "startSuperstep: InterruptedException getting assignments", e);
    }

    workerInfoList.clear();
    workerInfoList = addressesAndPartitions.getWorkerInfos();
    masterInfo = addressesAndPartitions.getMasterInfo();

    if (LOG.isInfoEnabled()) {
      LOG.info("startSuperstep: " + masterInfo);
      LOG.info("startSuperstep: Ready for computation on superstep " +
          getSuperstep() + " since worker " +
          "selection and vertex range assignments are done in " +
          addressesAndPartitionsPath);
    }

    getContext().setStatus("startSuperstep: " +
        getGraphTaskManager().getGraphFunctions().toString() +
        " - Attempt=" + getApplicationAttempt() +
        ", Superstep=" + getSuperstep());

    if (LOG.isDebugEnabled()) {
      LOG.debug("startSuperstep: addressesAndPartitions" +
          addressesAndPartitions.getWorkerInfos());
      for (PartitionOwner partitionOwner : addressesAndPartitions
          .getPartitionOwners()) {
        LOG.debug(partitionOwner.getPartitionId() + " " +
            partitionOwner.getWorkerInfo());
      }
    }

    return addressesAndPartitions.getPartitionOwners();
  }

  @Override
  public FinishedSuperstepStats finishSuperstep(
      List<PartitionStats> partitionStatsList,
      GiraphTimerContext superstepTimerContext) {
    
    
    
    
    
    
    
    
    
    
    
    
    waitForRequestsToFinish();

    getGraphTaskManager().notifyFinishedCommunication();

    long workerSentMessages = 0;
    long workerSentMessageBytes = 0;
    long localVertices = 0;
    for (PartitionStats partitionStats : partitionStatsList) {
      workerSentMessages += partitionStats.getMessagesSentCount();
      workerSentMessageBytes += partitionStats.getMessageBytesSentCount();
      localVertices += partitionStats.getVertexCount();
    }

    if (getSuperstep() != INPUT_SUPERSTEP) {
      postSuperstepCallbacks();
    } else {
      if (getConfiguration().hasVertexInputFormat()) {
        vertexSplitsHandler.setDoneReadingGraph(true);
      }
      if (getConfiguration().hasEdgeInputFormat()) {
        edgeSplitsHandler.setDoneReadingGraph(true);
      }
    }

    globalCommHandler.finishSuperstep(workerAggregatorRequestProcessor);

    MessageStore<I, Writable> incomingMessageStore =
        getServerData().getIncomingMessageStore();
    if (incomingMessageStore instanceof AsyncMessageStoreWrapper) {
      ((AsyncMessageStoreWrapper) incomingMessageStore).waitToComplete();
    }

    if (LOG.isInfoEnabled()) {
      LOG.info("finishSuperstep: Superstep " + getSuperstep() +
          ", messages = " + workerSentMessages + " " +
          ", message bytes = " + workerSentMessageBytes + " , " +
          MemoryUtils.getRuntimeMemoryStats());
    }

    if (superstepTimerContext != null) {
      superstepTimerContext.stop();
    }
    writeFinshedSuperstepInfoToZK(partitionStatsList,
      workerSentMessages, workerSentMessageBytes);

    LoggerUtils.setStatusAndLog(getContext(), LOG, Level.INFO,
        "finishSuperstep: (waiting for rest " +
            "of workers) " +
            getGraphTaskManager().getGraphFunctions().toString() +
            " - Attempt=" + getApplicationAttempt() +
            ", Superstep=" + getSuperstep());

    String superstepFinishedNode =
        getSuperstepFinishedPath(getApplicationAttempt(), getSuperstep());

    waitForOtherWorkers(superstepFinishedNode);

    GlobalStats globalStats = new GlobalStats();
    SuperstepClasses superstepClasses = SuperstepClasses.createToRead(
        getConfiguration());
    WritableUtils.readFieldsFromZnode(
        getZkExt(), superstepFinishedNode, false, null, globalStats,
        superstepClasses);
    if (LOG.isInfoEnabled()) {
      LOG.info("finishSuperstep: Completed superstep " + getSuperstep() +
          " with global stats " + globalStats + " and classes " +
          superstepClasses);
    }
    getContext().setStatus("finishSuperstep: (all workers done) " +
        getGraphTaskManager().getGraphFunctions().toString() +
        " - Attempt=" + getApplicationAttempt() +
        ", Superstep=" + getSuperstep());
    incrCachedSuperstep();
    getConfiguration().updateSuperstepClasses(superstepClasses);

    return new FinishedSuperstepStats(
        localVertices,
        globalStats.getHaltComputation(),
        globalStats.getVertexCount(),
        globalStats.getEdgeCount(),
        false,
        globalStats.getCheckpointStatus());
  }

  
  private void postSuperstepCallbacks() {
    GiraphTimerContext timerContext = wcPostSuperstepTimer.time();
    getWorkerContext().postSuperstep();
    timerContext.stop();
    getContext().progress();

    for (WorkerObserver obs : getWorkerObservers()) {
      obs.postSuperstep(getSuperstep());
      getContext().progress();
    }
  }

  
  private void waitForRequestsToFinish() {
    if (LOG.isInfoEnabled()) {
      LOG.info("finishSuperstep: Waiting on all requests, superstep " +
          getSuperstep() + " " +
          MemoryUtils.getRuntimeMemoryStats());
    }
    GiraphTimerContext timerContext = waitRequestsTimer.time();
    workerClient.waitAllRequests();
    timerContext.stop();
  }

  
  private void waitForOtherWorkers(String superstepFinishedNode) {
    try {
      while (getZkExt().exists(superstepFinishedNode, true) == null) {
        getSuperstepFinishedEvent().waitForever();
        getSuperstepFinishedEvent().reset();
      }
    } catch (KeeperException e) {
      throw new IllegalStateException(
          "finishSuperstep: Failed while waiting for master to " +
              "signal completion of superstep " + getSuperstep(), e);
    } catch (InterruptedException e) {
      throw new IllegalStateException(
          "finishSuperstep: Failed while waiting for master to " +
              "signal completion of superstep " + getSuperstep(), e);
    }
  }

  
  private void writeFinshedSuperstepInfoToZK(
      List<PartitionStats> partitionStatsList, long workerSentMessages,
      long workerSentMessageBytes) {
    Collection<PartitionStats> finalizedPartitionStats =
        workerGraphPartitioner.finalizePartitionStats(
            partitionStatsList, getPartitionStore());
    List<PartitionStats> finalizedPartitionStatsList =
        new ArrayList<PartitionStats>(finalizedPartitionStats);
    byte[] partitionStatsBytes =
        WritableUtils.writeListToByteArray(finalizedPartitionStatsList);
    WorkerSuperstepMetrics metrics = new WorkerSuperstepMetrics();
    metrics.readFromRegistry();
    byte[] metricsBytes = WritableUtils.writeToByteArray(metrics);

    JSONObject workerFinishedInfoObj = new JSONObject();
    try {
      workerFinishedInfoObj.put(JSONOBJ_PARTITION_STATS_KEY,
          Base64.encodeBytes(partitionStatsBytes));
      workerFinishedInfoObj.put(JSONOBJ_NUM_MESSAGES_KEY, workerSentMessages);
      workerFinishedInfoObj.put(JSONOBJ_NUM_MESSAGE_BYTES_KEY,
        workerSentMessageBytes);
      workerFinishedInfoObj.put(JSONOBJ_METRICS_KEY,
          Base64.encodeBytes(metricsBytes));
    } catch (JSONException e) {
      throw new RuntimeException(e);
    }

    String finishedWorkerPath =
        getWorkerFinishedPath(getApplicationAttempt(), getSuperstep()) +
        "/" + getHostnamePartitionId();
    try {
      getZkExt().createExt(finishedWorkerPath,
          workerFinishedInfoObj.toString().getBytes(Charset.defaultCharset()),
          Ids.OPEN_ACL_UNSAFE,
          CreateMode.PERSISTENT,
          true);
    } catch (KeeperException.NodeExistsException e) {
      LOG.warn("finishSuperstep: finished worker path " +
          finishedWorkerPath + " already exists!");
    } catch (KeeperException e) {
      throw new IllegalStateException("Creating " + finishedWorkerPath +
          " failed with KeeperException", e);
    } catch (InterruptedException e) {
      throw new IllegalStateException("Creating " + finishedWorkerPath +
          " failed with InterruptedException", e);
    }
  }

  
  private void saveVertices(long numLocalVertices) throws IOException,
      InterruptedException {
    ImmutableClassesGiraphConfiguration<I, V, E>  conf = getConfiguration();

    if (conf.getVertexOutputFormatClass() == null) {
      LOG.warn("saveVertices: " +
          GiraphConstants.VERTEX_OUTPUT_FORMAT_CLASS +
          " not specified -- there will be no saved output");
      return;
    }
    if (conf.doOutputDuringComputation()) {
      if (LOG.isInfoEnabled()) {
        LOG.info("saveVertices: The option for doing output during " +
            "computation is selected, so there will be no saving of the " +
            "output in the end of application");
      }
      return;
    }

    final int numPartitions = getPartitionStore().getNumPartitions();
    int numThreads = Math.min(getConfiguration().getNumOutputThreads(),
        numPartitions);
    LoggerUtils.setStatusAndLog(getContext(), LOG, Level.INFO,
        "saveVertices: Starting to save " + numLocalVertices + " vertices " +
            "using " + numThreads + " threads");
    final VertexOutputFormat<I, V, E> vertexOutputFormat =
        getConfiguration().createWrappedVertexOutputFormat();

    final Queue<Integer> partitionIdQueue =
        (numPartitions == 0) ? new LinkedList<Integer>() :
            new ArrayBlockingQueue<Integer>(numPartitions);
    Iterables.addAll(partitionIdQueue, getPartitionStore().getPartitionIds());

    long verticesToStore = 0;
    PartitionStore<I, V, E> partitionStore = getPartitionStore();
    for (int partitionId : partitionStore.getPartitionIds()) {
      verticesToStore += partitionStore.getPartitionVertexCount(partitionId);
    }
    WorkerProgress.get().startStoring(
        verticesToStore, getPartitionStore().getNumPartitions());

    CallableFactory<Void> callableFactory = new CallableFactory<Void>() {
      @Override
      public Callable<Void> newCallable(int callableId) {
        return new Callable<Void>() {
          
          private static final long VERTICES_TO_UPDATE_PROGRESS = 100000;

          @Override
          public Void call() throws Exception {
            VertexWriter<I, V, E> vertexWriter =
                vertexOutputFormat.createVertexWriter(getContext());
            vertexWriter.setConf(getConfiguration());
            vertexWriter.initialize(getContext());
            long nextPrintVertices = 0;
            long nextUpdateProgressVertices = VERTICES_TO_UPDATE_PROGRESS;
            long nextPrintMsecs = System.currentTimeMillis() + 15000;
            int partitionIndex = 0;
            int numPartitions = getPartitionStore().getNumPartitions();
            while (!partitionIdQueue.isEmpty()) {
              Integer partitionId = partitionIdQueue.poll();
              if (partitionId == null) {
                break;
              }

              Partition<I, V, E> partition =
                  getPartitionStore().getOrCreatePartition(partitionId);
              long verticesWritten = 0;
              for (Vertex<I, V, E> vertex : partition) {
                vertexWriter.writeVertex(vertex);
                ++verticesWritten;

                
                if (verticesWritten > nextPrintVertices &&
                    System.currentTimeMillis() > nextPrintMsecs) {
                  LoggerUtils.setStatusAndLog(getContext(), LOG, Level.INFO,
                      "saveVertices: Saved " + verticesWritten + " out of " +
                          partition.getVertexCount() + " partition vertices, " +
                          "on partition " + partitionIndex +
                          " out of " + numPartitions);
                  nextPrintMsecs = System.currentTimeMillis() + 15000;
                  nextPrintVertices = verticesWritten + 250000;
                }

                if (verticesWritten >= nextUpdateProgressVertices) {
                  WorkerProgress.get().addVerticesStored(
                      VERTICES_TO_UPDATE_PROGRESS);
                  nextUpdateProgressVertices += VERTICES_TO_UPDATE_PROGRESS;
                }
              }
              getPartitionStore().putPartition(partition);
              ++partitionIndex;
              WorkerProgress.get().addVerticesStored(
                  verticesWritten % VERTICES_TO_UPDATE_PROGRESS);
              WorkerProgress.get().incrementPartitionsStored();
            }
            vertexWriter.close(getContext()); 
            return null;
          }
        };
      }
    };
    ProgressableUtils.getResultsWithNCallables(callableFactory, numThreads,
        "save-vertices-%d", getContext());

    LoggerUtils.setStatusAndLog(getContext(), LOG, Level.INFO,
      "saveVertices: Done saving vertices.");
    
    if (getConfiguration().isPureYarnJob() &&
      getConfiguration().getVertexOutputFormatClass() != null) {
      try {
        OutputCommitter outputCommitter =
          vertexOutputFormat.getOutputCommitter(getContext());
        if (outputCommitter.needsTaskCommit(getContext())) {
          LoggerUtils.setStatusAndLog(getContext(), LOG, Level.INFO,
            "OutputCommitter: committing task output.");
          
          
          outputCommitter.commitTask(getContext());
        }
      } catch (InterruptedException ie) {
        LOG.error("Interrupted while attempting to obtain " +
          "OutputCommitter.", ie);
      } catch (IOException ioe) {
        LOG.error("Master task's attempt to commit output has " +
          "FAILED.", ioe);
      }
    }
  }

  
  private void saveEdges() throws IOException, InterruptedException {
    final ImmutableClassesGiraphConfiguration<I, V, E>  conf =
      getConfiguration();

    if (conf.getEdgeOutputFormatClass() == null) {
      LOG.warn("saveEdges: " +
               GiraphConstants.EDGE_OUTPUT_FORMAT_CLASS +
               "Make sure that the EdgeOutputFormat is not required.");
      return;
    }

    final int numPartitions = getPartitionStore().getNumPartitions();
    int numThreads = Math.min(conf.getNumOutputThreads(),
        numPartitions);
    LoggerUtils.setStatusAndLog(getContext(), LOG, Level.INFO,
        "saveEdges: Starting to save the edges using " +
        numThreads + " threads");
    final EdgeOutputFormat<I, V, E> edgeOutputFormat =
        conf.createWrappedEdgeOutputFormat();

    final Queue<Integer> partitionIdQueue =
        (numPartitions == 0) ? new LinkedList<Integer>() :
            new ArrayBlockingQueue<Integer>(numPartitions);
    Iterables.addAll(partitionIdQueue, getPartitionStore().getPartitionIds());

    CallableFactory<Void> callableFactory = new CallableFactory<Void>() {
      @Override
      public Callable<Void> newCallable(int callableId) {
        return new Callable<Void>() {
          @Override
          public Void call() throws Exception {
            EdgeWriter<I, V, E>  edgeWriter =
                edgeOutputFormat.createEdgeWriter(getContext());
            edgeWriter.setConf(conf);
            edgeWriter.initialize(getContext());

            long nextPrintVertices = 0;
            long nextPrintMsecs = System.currentTimeMillis() + 15000;
            int partitionIndex = 0;
            int numPartitions = getPartitionStore().getNumPartitions();
            while (!partitionIdQueue.isEmpty()) {
              Integer partitionId = partitionIdQueue.poll();
              if (partitionId == null) {
                break;
              }

              Partition<I, V, E> partition =
                  getPartitionStore().getOrCreatePartition(partitionId);
              long vertices = 0;
              long edges = 0;
              long partitionEdgeCount = partition.getEdgeCount();
              for (Vertex<I, V, E> vertex : partition) {
                for (Edge<I, E> edge : vertex.getEdges()) {
                  edgeWriter.writeEdge(vertex.getId(), vertex.getValue(), edge);
                  ++edges;
                }
                ++vertices;

                
                if (vertices > nextPrintVertices &&
                    System.currentTimeMillis() > nextPrintMsecs) {
                  LoggerUtils.setStatusAndLog(getContext(), LOG, Level.INFO,
                      "saveEdges: Saved " + edges +
                      " edges out of " + partitionEdgeCount +
                      " partition edges, on partition " + partitionIndex +
                      " out of " + numPartitions);
                  nextPrintMsecs = System.currentTimeMillis() + 15000;
                  nextPrintVertices = vertices + 250000;
                }
              }
              getPartitionStore().putPartition(partition);
              ++partitionIndex;
            }
            edgeWriter.close(getContext()); 
            return null;
          }
        };
      }
    };
    ProgressableUtils.getResultsWithNCallables(callableFactory, numThreads,
        "save-vertices-%d", getContext());

    LoggerUtils.setStatusAndLog(getContext(), LOG, Level.INFO,
      "saveEdges: Done saving edges.");
    
    if (conf.isPureYarnJob() &&
      conf.getVertexOutputFormatClass() != null) {
      try {
        OutputCommitter outputCommitter =
          edgeOutputFormat.getOutputCommitter(getContext());
        if (outputCommitter.needsTaskCommit(getContext())) {
          LoggerUtils.setStatusAndLog(getContext(), LOG, Level.INFO,
            "OutputCommitter: committing task output.");
          
          
          outputCommitter.commitTask(getContext());
        }
      } catch (InterruptedException ie) {
        LOG.error("Interrupted while attempting to obtain " +
          "OutputCommitter.", ie);
      } catch (IOException ioe) {
        LOG.error("Master task's attempt to commit output has " +
          "FAILED.", ioe);
      }
    }
  }

  @Override
  public void cleanup(FinishedSuperstepStats finishedSuperstepStats)
    throws IOException, InterruptedException {
    workerClient.closeConnections();
    setCachedSuperstep(getSuperstep() - 1);
    if (finishedSuperstepStats.getCheckpointStatus() !=
        CheckpointStatus.CHECKPOINT_AND_HALT) {
      saveVertices(finishedSuperstepStats.getLocalVertexCount());
      saveEdges();
    }
    WorkerProgress.get().finishStoring();
    if (workerProgressWriter != null) {
      workerProgressWriter.stop();
    }
    getPartitionStore().shutdown();
    
    
    
    
    String workerCleanedUpPath = cleanedUpPath  + "/" +
        getTaskPartition() + WORKER_SUFFIX;
    try {
      String finalFinishedPath =
          getZkExt().createExt(workerCleanedUpPath,
              null,
              Ids.OPEN_ACL_UNSAFE,
              CreateMode.PERSISTENT,
              true);
      if (LOG.isInfoEnabled()) {
        LOG.info("cleanup: Notifying master its okay to cleanup with " +
            finalFinishedPath);
      }
    } catch (KeeperException.NodeExistsException e) {
      if (LOG.isInfoEnabled()) {
        LOG.info("cleanup: Couldn't create finished node '" +
            workerCleanedUpPath);
      }
    } catch (KeeperException e) {
      
      LOG.error("cleanup: Got KeeperException on notification " +
          "to master about cleanup", e);
    } catch (InterruptedException e) {
      
      LOG.error("cleanup: Got InterruptedException on notification " +
          "to master about cleanup", e);
    }
    try {
      getZkExt().close();
    } catch (InterruptedException e) {
      
      LOG.error("cleanup: Zookeeper failed to close with " + e);
    }

    if (getConfiguration().metricsEnabled()) {
      GiraphMetrics.get().dumpToStream(System.err);
    }

    
    
    
    workerServer.close();
  }

  @Override
  public void storeCheckpoint() throws IOException {
    LoggerUtils.setStatusAndLog(getContext(), LOG, Level.INFO,
        "storeCheckpoint: Starting checkpoint " +
            getGraphTaskManager().getGraphFunctions().toString() +
            " - Attempt=" + getApplicationAttempt() +
            ", Superstep=" + getSuperstep());

    
    
    Path metadataFilePath = createCheckpointFilePathSafe(
        CheckpointingUtils.CHECKPOINT_METADATA_POSTFIX);
    Path validFilePath = createCheckpointFilePathSafe(
        CheckpointingUtils.CHECKPOINT_VALID_POSTFIX);
    Path checkpointFilePath = createCheckpointFilePathSafe(
        CheckpointingUtils.CHECKPOINT_DATA_POSTFIX);


    
    
    FSDataOutputStream metadataOutputStream =
        getFs().create(metadataFilePath);
    metadataOutputStream.writeInt(getPartitionStore().getNumPartitions());

    for (Integer partitionId : getPartitionStore().getPartitionIds()) {
      metadataOutputStream.writeInt(partitionId);
    }
    metadataOutputStream.close();

    storeCheckpointVertices();

    FSDataOutputStream checkpointOutputStream =
        getFs().create(checkpointFilePath);
    workerContext.write(checkpointOutputStream);
    getContext().progress();

    for (Integer partitionId : getPartitionStore().getPartitionIds()) {
      
      checkpointOutputStream.writeInt(partitionId);
      getServerData().getCurrentMessageStore().writePartition(
          checkpointOutputStream, partitionId);
      getContext().progress();

    }

    List<Writable> w2wMessages =
        getServerData().getCurrentWorkerToWorkerMessages();
    WritableUtils.writeList(w2wMessages, checkpointOutputStream);

    checkpointOutputStream.close();

    getFs().createNewFile(validFilePath);

    
    String workerWroteCheckpoint =
        getWorkerWroteCheckpointPath(getApplicationAttempt(),
            getSuperstep()) + "/" + getHostnamePartitionId();
    try {
      getZkExt().createExt(workerWroteCheckpoint,
          new byte[0],
          Ids.OPEN_ACL_UNSAFE,
          CreateMode.PERSISTENT,
          true);
    } catch (KeeperException.NodeExistsException e) {
      LOG.warn("storeCheckpoint: wrote checkpoint worker path " +
          workerWroteCheckpoint + " already exists!");
    } catch (KeeperException e) {
      throw new IllegalStateException("Creating " + workerWroteCheckpoint +
          " failed with KeeperException", e);
    } catch (InterruptedException e) {
      throw new IllegalStateException("Creating " +
          workerWroteCheckpoint +
          " failed with InterruptedException", e);
    }
  }

  
  private Path createCheckpointFilePathSafe(String name) throws IOException {
    Path validFilePath = new Path(getCheckpointBasePath(getSuperstep()) + '.' +
        getWorkerId(workerInfo) + name);
    
    
    if (getFs().delete(validFilePath, false)) {
      LOG.warn("storeCheckpoint: Removed " + name + " file " +
          validFilePath);
    }
    return validFilePath;
  }

  
  private Path getSavedCheckpoint(long superstep, String name) {
    return new Path(getSavedCheckpointBasePath(superstep) + '.' +
        getWorkerId(workerInfo) + name);
  }

  
  private void storeCheckpointVertices() {
    final int numPartitions = getPartitionStore().getNumPartitions();
    int numThreads = Math.min(
        GiraphConstants.NUM_CHECKPOINT_IO_THREADS.get(getConfiguration()),
        numPartitions);

    final Queue<Integer> partitionIdQueue =
        (numPartitions == 0) ? new LinkedList<Integer>() :
            new ArrayBlockingQueue<Integer>(numPartitions);
    Iterables.addAll(partitionIdQueue, getPartitionStore().getPartitionIds());

    final CompressionCodec codec =
        new CompressionCodecFactory(getConfiguration())
            .getCodec(new Path(
                GiraphConstants.CHECKPOINT_COMPRESSION_CODEC
                    .get(getConfiguration())));

    long t0 = System.currentTimeMillis();

    CallableFactory<Void> callableFactory = new CallableFactory<Void>() {
      @Override
      public Callable<Void> newCallable(int callableId) {
        return new Callable<Void>() {

          @Override
          public Void call() throws Exception {
            while (!partitionIdQueue.isEmpty()) {
              Integer partitionId = partitionIdQueue.poll();
              if (partitionId == null) {
                break;
              }
              Path path =
                  createCheckpointFilePathSafe("_" + partitionId +
                      CheckpointingUtils.CHECKPOINT_VERTICES_POSTFIX);

              FSDataOutputStream uncompressedStream =
                  getFs().create(path);


              DataOutputStream stream = codec == null ? uncompressedStream :
                  new DataOutputStream(
                      codec.createOutputStream(uncompressedStream));

              Partition<I, V, E> partition =
                  getPartitionStore().getOrCreatePartition(partitionId);

              partition.write(stream);

              getPartitionStore().putPartition(partition);

              stream.close();
              uncompressedStream.close();
            }
            return null;
          }


        };
      }
    };

    ProgressableUtils.getResultsWithNCallables(callableFactory, numThreads,
        "checkpoint-vertices-%d", getContext());

    LOG.info("Save checkpoint in " + (System.currentTimeMillis() - t0) +
        " ms, using " + numThreads + " threads");
  }

  
  private void loadCheckpointVertices(final long superstep,
                                      List<Integer> partitions) {
    int numThreads = Math.min(
        GiraphConstants.NUM_CHECKPOINT_IO_THREADS.get(getConfiguration()),
        partitions.size());

    final Queue<Integer> partitionIdQueue =
        new ConcurrentLinkedQueue<>(partitions);

    final CompressionCodec codec =
        new CompressionCodecFactory(getConfiguration())
            .getCodec(new Path(
                GiraphConstants.CHECKPOINT_COMPRESSION_CODEC
                    .get(getConfiguration())));

    long t0 = System.currentTimeMillis();

    CallableFactory<Void> callableFactory = new CallableFactory<Void>() {
      @Override
      public Callable<Void> newCallable(int callableId) {
        return new Callable<Void>() {

          @Override
          public Void call() throws Exception {
            while (!partitionIdQueue.isEmpty()) {
              Integer partitionId = partitionIdQueue.poll();
              if (partitionId == null) {
                break;
              }
              Path path =
                  getSavedCheckpoint(superstep, "_" + partitionId +
                      CheckpointingUtils.CHECKPOINT_VERTICES_POSTFIX);

              FSDataInputStream compressedStream =
                  getFs().open(path);

              DataInputStream stream = codec == null ? compressedStream :
                  new DataInputStream(
                      codec.createInputStream(compressedStream));

              Partition<I, V, E> partition =
                  getConfiguration().createPartition(partitionId, getContext());

              partition.readFields(stream);

              getPartitionStore().addPartition(partition);

              stream.close();
            }
            return null;
          }

        };
      }
    };

    ProgressableUtils.getResultsWithNCallables(callableFactory, numThreads,
        "load-vertices-%d", getContext());

    LOG.info("Loaded checkpoint in " + (System.currentTimeMillis() - t0) +
        " ms, using " + numThreads + " threads");
  }

  @Override
  public VertexEdgeCount loadCheckpoint(long superstep) {
    Path metadataFilePath = getSavedCheckpoint(
        superstep, CheckpointingUtils.CHECKPOINT_METADATA_POSTFIX);

    Path checkpointFilePath = getSavedCheckpoint(
        superstep, CheckpointingUtils.CHECKPOINT_DATA_POSTFIX);
    
    
    
    
    try {
      DataInputStream metadataStream =
          getFs().open(metadataFilePath);

      int partitions = metadataStream.readInt();
      List<Integer> partitionIds = new ArrayList<>(partitions);
      for (int i = 0; i < partitions; i++) {
        int partitionId = metadataStream.readInt();
        partitionIds.add(partitionId);
      }

      loadCheckpointVertices(superstep, partitionIds);

      getContext().progress();

      metadataStream.close();

      DataInputStream checkpointStream =
          getFs().open(checkpointFilePath);
      workerContext.readFields(checkpointStream);

      
      GlobalStats globalStats = new GlobalStats();
      SuperstepClasses superstepClasses = SuperstepClasses.createToRead(
          getConfiguration());
      String finalizedCheckpointPath = getSavedCheckpointBasePath(superstep) +
          CheckpointingUtils.CHECKPOINT_FINALIZED_POSTFIX;
      DataInputStream finalizedStream =
          getFs().open(new Path(finalizedCheckpointPath));
      globalStats.readFields(finalizedStream);
      superstepClasses.readFields(finalizedStream);
      getConfiguration().updateSuperstepClasses(superstepClasses);
      getServerData().resetMessageStores();

      for (int i = 0; i < partitions; i++) {
        int partitionId = checkpointStream.readInt();
        getServerData().getCurrentMessageStore().readFieldsForPartition(
            checkpointStream, partitionId);
      }

      List<Writable> w2wMessages = (List<Writable>) WritableUtils.readList(
          checkpointStream);
      getServerData().getCurrentWorkerToWorkerMessages().addAll(w2wMessages);

      checkpointStream.close();

      if (LOG.isInfoEnabled()) {
        LOG.info("loadCheckpoint: Loaded " +
            workerGraphPartitioner.getPartitionOwners().size() +
            " total.");
      }

      
      

      workerClient.setup(getConfiguration().authenticate());

      return new VertexEdgeCount(globalStats.getVertexCount(),
          globalStats.getEdgeCount());

    } catch (IOException e) {
      throw new RuntimeException(
          "loadCheckpoint: Failed for superstep=" + superstep, e);
    }
  }

  
  private void sendWorkerPartitions(
      Map<WorkerInfo, List<Integer>> workerPartitionMap) {
    List<Entry<WorkerInfo, List<Integer>>> randomEntryList =
        new ArrayList<Entry<WorkerInfo, List<Integer>>>(
            workerPartitionMap.entrySet());
    Collections.shuffle(randomEntryList);
    WorkerClientRequestProcessor<I, V, E> workerClientRequestProcessor =
        new NettyWorkerClientRequestProcessor<I, V, E>(getContext(),
            getConfiguration(), this,
            false );
    for (Entry<WorkerInfo, List<Integer>> workerPartitionList :
      randomEntryList) {
      for (Integer partitionId : workerPartitionList.getValue()) {
        Partition<I, V, E> partition =
            getPartitionStore().removePartition(partitionId);
        if (partition == null) {
          throw new IllegalStateException(
              "sendWorkerPartitions: Couldn't find partition " +
                  partitionId + " to send to " +
                  workerPartitionList.getKey());
        }
        if (LOG.isInfoEnabled()) {
          LOG.info("sendWorkerPartitions: Sending worker " +
              workerPartitionList.getKey() + " partition " +
              partitionId);
        }
        workerClientRequestProcessor.sendPartitionRequest(
            workerPartitionList.getKey(),
            partition);
      }
    }

    try {
      workerClientRequestProcessor.flush();
      workerClient.waitAllRequests();
    } catch (IOException e) {
      throw new IllegalStateException("sendWorkerPartitions: Flush failed", e);
    }
    String myPartitionExchangeDonePath =
        getPartitionExchangeWorkerPath(
            getApplicationAttempt(), getSuperstep(), getWorkerInfo());
    try {
      getZkExt().createExt(myPartitionExchangeDonePath,
          null,
          Ids.OPEN_ACL_UNSAFE,
          CreateMode.PERSISTENT,
          true);
    } catch (KeeperException e) {
      throw new IllegalStateException(
          "sendWorkerPartitions: KeeperException to create " +
              myPartitionExchangeDonePath, e);
    } catch (InterruptedException e) {
      throw new IllegalStateException(
          "sendWorkerPartitions: InterruptedException to create " +
              myPartitionExchangeDonePath, e);
    }
    if (LOG.isInfoEnabled()) {
      LOG.info("sendWorkerPartitions: Done sending all my partitions.");
    }
  }

  @Override
  public final void exchangeVertexPartitions(
      Collection<? extends PartitionOwner> masterSetPartitionOwners) {
    
    
    
    
    
    
    PartitionExchange partitionExchange =
        workerGraphPartitioner.updatePartitionOwners(
            getWorkerInfo(), masterSetPartitionOwners);
    workerClient.openConnections();

    Map<WorkerInfo, List<Integer>> sendWorkerPartitionMap =
        partitionExchange.getSendWorkerPartitionMap();
    if (!getPartitionStore().isEmpty()) {
      sendWorkerPartitions(sendWorkerPartitionMap);
    }

    Set<WorkerInfo> myDependencyWorkerSet =
        partitionExchange.getMyDependencyWorkerSet();
    Set<String> workerIdSet = new HashSet<String>();
    for (WorkerInfo tmpWorkerInfo : myDependencyWorkerSet) {
      if (!workerIdSet.add(tmpWorkerInfo.getHostnameId())) {
        throw new IllegalStateException(
            "exchangeVertexPartitions: Duplicate entry " + tmpWorkerInfo);
      }
    }
    if (myDependencyWorkerSet.isEmpty() && getPartitionStore().isEmpty()) {
      if (LOG.isInfoEnabled()) {
        LOG.info("exchangeVertexPartitions: Nothing to exchange, " +
            "exiting early");
      }
      return;
    }

    String vertexExchangePath =
        getPartitionExchangePath(getApplicationAttempt(), getSuperstep());
    List<String> workerDoneList;
    try {
      while (true) {
        workerDoneList = getZkExt().getChildrenExt(
            vertexExchangePath, true, false, false);
        workerIdSet.removeAll(workerDoneList);
        if (workerIdSet.isEmpty()) {
          break;
        }
        if (LOG.isInfoEnabled()) {
          LOG.info("exchangeVertexPartitions: Waiting for workers " +
              workerIdSet);
        }
        getPartitionExchangeChildrenChangedEvent().waitForever();
        getPartitionExchangeChildrenChangedEvent().reset();
      }
    } catch (KeeperException | InterruptedException e) {
      throw new RuntimeException(
          "exchangeVertexPartitions: Got runtime exception", e);
    }

    if (LOG.isInfoEnabled()) {
      LOG.info("exchangeVertexPartitions: Done with exchange.");
    }
  }

  
  public final BspEvent getPartitionExchangeChildrenChangedEvent() {
    return partitionExchangeChildrenChanged;
  }

  @Override
  protected boolean processEvent(WatchedEvent event) {
    boolean foundEvent = false;
    if (event.getPath().startsWith(masterJobStatePath) &&
        (event.getType() == EventType.NodeChildrenChanged)) {
      if (LOG.isInfoEnabled()) {
        LOG.info("processEvent: Job state changed, checking " +
            "to see if it needs to restart");
      }
      JSONObject jsonObj = getJobState();
      
      
      if (getConfiguration().isPureYarnJob() && null == jsonObj) {
        LOG.error("BspServiceWorker#getJobState() came back NULL.");
        return false; 
      }
      try {
        if ((ApplicationState.valueOf(jsonObj.getString(JSONOBJ_STATE_KEY)) ==
            ApplicationState.START_SUPERSTEP) &&
            jsonObj.getLong(JSONOBJ_APPLICATION_ATTEMPT_KEY) !=
            getApplicationAttempt()) {
          LOG.fatal("processEvent: Worker will restart " +
              "from command - " + jsonObj.toString());
          System.exit(-1);
        }
      } catch (JSONException e) {
        throw new RuntimeException(
            "processEvent: Couldn't properly get job state from " +
                jsonObj.toString());
      }
      foundEvent = true;
    } else if (event.getPath().contains(PARTITION_EXCHANGE_DIR) &&
        event.getType() == EventType.NodeChildrenChanged) {
      if (LOG.isInfoEnabled()) {
        LOG.info("processEvent : partitionExchangeChildrenChanged " +
            "(at least one worker is done sending partitions)");
      }
      partitionExchangeChildrenChanged.signal();
      foundEvent = true;
    }

    return foundEvent;
  }

  @Override
  public WorkerInfo getWorkerInfo() {
    return workerInfo;
  }

  @Override
  public PartitionStore<I, V, E> getPartitionStore() {
    return getServerData().getPartitionStore();
  }

  @Override
  public PartitionOwner getVertexPartitionOwner(I vertexId) {
    return workerGraphPartitioner.getPartitionOwner(vertexId);
  }

  @Override
  public Iterable<? extends PartitionOwner> getPartitionOwners() {
    return workerGraphPartitioner.getPartitionOwners();
  }

  @Override
  public int getPartitionId(I vertexId) {
    PartitionOwner partitionOwner = getVertexPartitionOwner(vertexId);
    return partitionOwner.getPartitionId();
  }

  @Override
  public boolean hasPartition(Integer partitionId) {
    return getPartitionStore().hasPartition(partitionId);
  }

  @Override
  public ServerData<I, V, E> getServerData() {
    return workerServer.getServerData();
  }


  @Override
  public WorkerAggregatorHandler getAggregatorHandler() {
    return globalCommHandler;
  }

  @Override
  public void prepareSuperstep() {
    if (getSuperstep() != INPUT_SUPERSTEP) {
      globalCommHandler.prepareSuperstep(workerAggregatorRequestProcessor);
    }
  }

  @Override
  public SuperstepOutput<I, V, E> getSuperstepOutput() {
    return superstepOutput;
  }

  @Override
  public GlobalStats getGlobalStats() {
    GlobalStats globalStats = new GlobalStats();
    if (getSuperstep() > Math.max(INPUT_SUPERSTEP, getRestartedSuperstep())) {
      String superstepFinishedNode =
          getSuperstepFinishedPath(getApplicationAttempt(),
              getSuperstep() - 1);
      WritableUtils.readFieldsFromZnode(
          getZkExt(), superstepFinishedNode, false, null,
          globalStats);
    }
    return globalStats;
  }
}

<code block>


package org.apache.giraph.conf;

import io.netty.buffer.ByteBufAllocator;
import io.netty.buffer.PooledByteBufAllocator;
import io.netty.buffer.UnpooledByteBufAllocator;

import java.net.UnknownHostException;

import org.apache.giraph.aggregators.AggregatorWriter;
import org.apache.giraph.bsp.checkpoints.CheckpointSupportedChecker;
import org.apache.giraph.combiner.MessageCombiner;
import org.apache.giraph.edge.OutEdges;
import org.apache.giraph.edge.ReuseObjectsOutEdges;
import org.apache.giraph.factories.ComputationFactory;
import org.apache.giraph.factories.VertexValueFactory;
import org.apache.giraph.graph.Computation;
import org.apache.giraph.graph.Vertex;
import org.apache.giraph.graph.VertexResolver;
import org.apache.giraph.graph.VertexValueCombiner;
import org.apache.giraph.io.EdgeInputFormat;
import org.apache.giraph.io.EdgeOutputFormat;
import org.apache.giraph.io.MappingInputFormat;
import org.apache.giraph.io.VertexInputFormat;
import org.apache.giraph.io.VertexOutputFormat;
import org.apache.giraph.io.filters.EdgeInputFilter;
import org.apache.giraph.io.filters.VertexInputFilter;
import org.apache.giraph.job.GiraphJobObserver;
import org.apache.giraph.job.GiraphJobRetryChecker;
import org.apache.giraph.master.MasterCompute;
import org.apache.giraph.master.MasterObserver;
import org.apache.giraph.partition.GraphPartitionerFactory;
import org.apache.giraph.partition.Partition;
import org.apache.giraph.partition.ReusesObjectsPartition;
import org.apache.giraph.utils.ReflectionUtils;
import org.apache.giraph.worker.WorkerContext;
import org.apache.giraph.worker.WorkerObserver;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.net.DNS;


public class GiraphConfiguration extends Configuration
    implements GiraphConstants {
  
  private ByteBufAllocator nettyBufferAllocator = null;

  
  public GiraphConfiguration() {
    configureHadoopSecurity();
  }

  
  public GiraphConfiguration(Configuration conf) {
    super(conf);
    configureHadoopSecurity();
  }

  
  public String getComputationName() {
    ComputationFactory compFactory = ReflectionUtils.newInstance(
        getComputationFactoryClass());
    return compFactory.computationName(this);
  }

  
  public Class<? extends ComputationFactory> getComputationFactoryClass() {
    return COMPUTATION_FACTORY_CLASS.get(this);
  }

  
  public Class<? extends Computation> getComputationClass() {
    return COMPUTATION_CLASS.get(this);
  }

  
  public void setComputationClass(
      Class<? extends Computation> computationClass) {
    COMPUTATION_CLASS.set(this, computationClass);
  }

  
  public final void setVertexValueFactoryClass(
      Class<? extends VertexValueFactory> vertexValueFactoryClass) {
    VERTEX_VALUE_FACTORY_CLASS.set(this, vertexValueFactoryClass);
  }

  
  public void setEdgeInputFilterClass(
      Class<? extends EdgeInputFilter> edgeFilterClass) {
    EDGE_INPUT_FILTER_CLASS.set(this, edgeFilterClass);
  }

  
  public void setVertexInputFilterClass(
      Class<? extends VertexInputFilter> vertexFilterClass) {
    VERTEX_INPUT_FILTER_CLASS.set(this, vertexFilterClass);
  }

  
  public Class<? extends OutEdges> getOutEdgesClass() {
    return VERTEX_EDGES_CLASS.get(this);
  }

  
  public final void setOutEdgesClass(
      Class<? extends OutEdges> outEdgesClass) {
    VERTEX_EDGES_CLASS.set(this, outEdgesClass);
  }

  
  public final void setVertexClass(Class<? extends Vertex> vertexClass) {
    VERTEX_CLASS.set(this, vertexClass);
  }


  
  public final void setInputOutEdgesClass(
      Class<? extends OutEdges> inputOutEdgesClass) {
    INPUT_VERTEX_EDGES_CLASS.set(this, inputOutEdgesClass);
  }

  
  public boolean reuseEdgeObjects() {
    return ReuseObjectsOutEdges.class.isAssignableFrom(
        getOutEdgesClass());
  }

  
  public boolean reuseVertexObjects() {
    return ReusesObjectsPartition.class.isAssignableFrom(getPartitionClass());
  }

  
  public Class<? extends Partition> getPartitionClass() {
    return PARTITION_CLASS.get(this);
  }

  
  public boolean hasVertexInputFormat() {
    return VERTEX_INPUT_FORMAT_CLASS.get(this) != null;
  }

  
  public void setVertexInputFormatClass(
      Class<? extends VertexInputFormat> vertexInputFormatClass) {
    VERTEX_INPUT_FORMAT_CLASS.set(this, vertexInputFormatClass);
  }

  
  public boolean hasEdgeInputFormat() {
    return EDGE_INPUT_FORMAT_CLASS.get(this) != null;
  }

  
  public void setEdgeInputFormatClass(
      Class<? extends EdgeInputFormat> edgeInputFormatClass) {
    EDGE_INPUT_FORMAT_CLASS.set(this, edgeInputFormatClass);
  }

  
  public void setMappingInputFormatClass(
    Class<? extends MappingInputFormat> mappingInputFormatClass) {
    MAPPING_INPUT_FORMAT_CLASS.set(this, mappingInputFormatClass);
  }

  
  public final void setMasterComputeClass(
      Class<? extends MasterCompute> masterComputeClass) {
    MASTER_COMPUTE_CLASS.set(this, masterComputeClass);
  }

  
  public final void addMasterObserverClass(
      Class<? extends MasterObserver> masterObserverClass) {
    MASTER_OBSERVER_CLASSES.add(this, masterObserverClass);
  }

  
  public final void addWorkerObserverClass(
      Class<? extends WorkerObserver> workerObserverClass) {
    WORKER_OBSERVER_CLASSES.add(this, workerObserverClass);
  }

  
  public Class<? extends GiraphJobObserver> getJobObserverClass() {
    return JOB_OBSERVER_CLASS.get(this);
  }

  
  public void setJobObserverClass(Class<? extends GiraphJobObserver> klass) {
    JOB_OBSERVER_CLASS.set(this, klass);
  }

  
  public Class<? extends GiraphJobRetryChecker> getJobRetryCheckerClass() {
    return JOB_RETRY_CHECKER_CLASS.get(this);
  }

  
  public void setJobRetryCheckerClass(
      Class<? extends GiraphJobRetryChecker> klass) {
    JOB_RETRY_CHECKER_CLASS.set(this, klass);
  }

  
  public boolean isJMapHistogramDumpEnabled() {
    return JMAP_ENABLE.get(this);
  }

  
  public boolean isReactiveJmapHistogramDumpEnabled() {
    return REACTIVE_JMAP_ENABLE.get(this);
  }

  
  public final void setClasses(String name, Class<?> xface,
                               Class<?> ... klasses) {
    String[] klassNames = new String[klasses.length];
    for (int i = 0; i < klasses.length; ++i) {
      Class<?> klass = klasses[i];
      if (!xface.isAssignableFrom(klass)) {
        throw new RuntimeException(klass + " does not implement " +
            xface.getName());
      }
      klassNames[i] = klasses[i].getName();
    }
    setStrings(name, klassNames);
  }

  
  public boolean hasVertexOutputFormat() {
    return VERTEX_OUTPUT_FORMAT_CLASS.get(this) != null;
  }

  
  public final void setVertexOutputFormatClass(
      Class<? extends VertexOutputFormat> vertexOutputFormatClass) {
    VERTEX_OUTPUT_FORMAT_CLASS.set(this, vertexOutputFormatClass);
  }


  
  public boolean hasVertexOutputFormatSubdir() {
    return !VERTEX_OUTPUT_FORMAT_SUBDIR.get(this).isEmpty();
  }

  
  public final void setVertexOutputFormatSubdir(String path) {
    VERTEX_OUTPUT_FORMAT_SUBDIR.set(this, path);
  }

  
  public final boolean doOutputDuringComputation() {
    return DO_OUTPUT_DURING_COMPUTATION.get(this);
  }

  
  public final void setDoOutputDuringComputation(
      boolean doOutputDuringComputation) {
    DO_OUTPUT_DURING_COMPUTATION.set(this, doOutputDuringComputation);
  }

  
  public final boolean vertexOutputFormatThreadSafe() {
    return VERTEX_OUTPUT_FORMAT_THREAD_SAFE.get(this);
  }

  
  public final void setVertexOutputFormatThreadSafe(
      boolean vertexOutputFormatThreadSafe) {
    VERTEX_OUTPUT_FORMAT_THREAD_SAFE.set(this, vertexOutputFormatThreadSafe);
  }

  
  public boolean hasEdgeOutputFormat() {
    return EDGE_OUTPUT_FORMAT_CLASS.get(this) != null;
  }

  
  public final void setEdgeOutputFormatClass(
      Class<? extends EdgeOutputFormat> edgeOutputFormatClass) {
    EDGE_OUTPUT_FORMAT_CLASS.set(this, edgeOutputFormatClass);
  }

  
  public boolean hasEdgeOutputFormatSubdir() {
    return !EDGE_OUTPUT_FORMAT_SUBDIR.get(this).isEmpty();
  }

  
  public final void setEdgeOutputFormatSubdir(String path) {
    EDGE_OUTPUT_FORMAT_SUBDIR.set(this, path);
  }

  
  public final int getNumOutputThreads() {
    if (!vertexOutputFormatThreadSafe()) {
      return 1;
    } else {
      return NUM_OUTPUT_THREADS.get(this);
    }
  }

  
  public void setNumOutputThreads(int numOutputThreads) {
    NUM_OUTPUT_THREADS.set(this, numOutputThreads);
  }

  
  public void setMessageCombinerClass(
      Class<? extends MessageCombiner> messageCombinerClass) {
    MESSAGE_COMBINER_CLASS.set(this, messageCombinerClass);
  }

  
  public final void setGraphPartitionerFactoryClass(
      Class<? extends GraphPartitionerFactory> graphPartitionerFactoryClass) {
    GRAPH_PARTITIONER_FACTORY_CLASS.set(this, graphPartitionerFactoryClass);
  }

  
  public final void setVertexResolverClass(
      Class<? extends VertexResolver> vertexResolverClass) {
    VERTEX_RESOLVER_CLASS.set(this, vertexResolverClass);
  }

  
  public final boolean getResolverCreateVertexOnMessages() {
    return RESOLVER_CREATE_VERTEX_ON_MSGS.get(this);
  }

  
  public final void setResolverCreateVertexOnMessages(boolean v) {
    RESOLVER_CREATE_VERTEX_ON_MSGS.set(this, v);
  }

  
  public final void setVertexValueCombinerClass(
      Class<? extends VertexValueCombiner> vertexValueCombinerClass) {
    VERTEX_VALUE_COMBINER_CLASS.set(this, vertexValueCombinerClass);
  }

  
  public final void setWorkerContextClass(
      Class<? extends WorkerContext> workerContextClass) {
    WORKER_CONTEXT_CLASS.set(this, workerContextClass);
  }

  
  public final void setAggregatorWriterClass(
      Class<? extends AggregatorWriter> aggregatorWriterClass) {
    AGGREGATOR_WRITER_CLASS.set(this, aggregatorWriterClass);
  }

  
  public final void setPartitionClass(
      Class<? extends Partition> partitionClass) {
    PARTITION_CLASS.set(this, partitionClass);
  }

  
  public final void setWorkerConfiguration(int minWorkers,
                                           int maxWorkers,
                                           float minPercentResponded) {
    setInt(MIN_WORKERS, minWorkers);
    setInt(MAX_WORKERS, maxWorkers);
    MIN_PERCENT_RESPONDED.set(this, minPercentResponded);
  }

  public final int getMinWorkers() {
    return getInt(MIN_WORKERS, -1);
  }

  public final int getMaxWorkers() {
    return getInt(MAX_WORKERS, -1);
  }

  public final float getMinPercentResponded() {
    return MIN_PERCENT_RESPONDED.get(this);
  }

  
  public final void setZooKeeperConfiguration(String serverList) {
    ZOOKEEPER_LIST.set(this, serverList);
  }

  
  public final boolean getSplitMasterWorker() {
    return SPLIT_MASTER_WORKER.get(this);
  }

  
  public Class<? extends MasterObserver>[] getMasterObserverClasses() {
    return MASTER_OBSERVER_CLASSES.getArray(this);
  }

  
  public Class<? extends WorkerObserver>[] getWorkerObserverClasses() {
    return WORKER_OBSERVER_CLASSES.getArray(this);
  }

  
  public boolean metricsEnabled() {
    return METRICS_ENABLE.isTrue(this);
  }

  
  public int getTaskPartition() {
    return getInt("mapred.task.partition", -1);
  }

  
  public boolean isPureYarnJob() {
    return IS_PURE_YARN_JOB.get(this);
  }

  
  public String getYarnLibJars() {
    return GIRAPH_YARN_LIBJARS.get(this);
  }

  
  public void setYarnLibJars(String jarList) {
    GIRAPH_YARN_LIBJARS.set(this, jarList);
  }

  
  public int getYarnTaskHeapMb() {
    return GIRAPH_YARN_TASK_HEAP_MB.get(this);
  }

  
  public void setYarnTaskHeapMb(int heapMb) {
    GIRAPH_YARN_TASK_HEAP_MB.set(this, heapMb);
  }

  
  public String getZookeeperList() {
    return ZOOKEEPER_LIST.get(this);
  }

  
  public void setZookeeperList(String zkList) {
    ZOOKEEPER_LIST.set(this, zkList);
    ZOOKEEPER_IS_EXTERNAL.set(this, false);
  }

  
  public boolean isZookeeperExternal() {
    return ZOOKEEPER_IS_EXTERNAL.get(this);
  }

  public String getLocalLevel() {
    return LOG_LEVEL.get(this);
  }

  
  public boolean useLogThreadLayout() {
    return LOG_THREAD_LAYOUT.get(this);
  }

  
  public boolean getLocalTestMode() {
    return LOCAL_TEST_MODE.get(this);
  }

  
  public void setLocalTestMode(boolean flag) {
    LOCAL_TEST_MODE.set(this, flag);
  }

  
  public int getZooKeeperServerCount() {
    return ZOOKEEPER_SERVER_COUNT.get(this);
  }

  public int getZooKeeperSessionTimeout() {
    return ZOOKEEPER_SESSION_TIMEOUT.get(this);
  }

  public int getZookeeperOpsMaxAttempts() {
    return ZOOKEEPER_OPS_MAX_ATTEMPTS.get(this);
  }

  public int getZookeeperOpsRetryWaitMsecs() {
    return ZOOKEEPER_OPS_RETRY_WAIT_MSECS.get(this);
  }

  public boolean getNettyServerUseExecutionHandler() {
    return NETTY_SERVER_USE_EXECUTION_HANDLER.get(this);
  }

  public int getNettyServerThreads() {
    return NETTY_SERVER_THREADS.get(this);
  }

  public int getNettyServerExecutionThreads() {
    return NETTY_SERVER_EXECUTION_THREADS.get(this);
  }

  
  public int getNettyServerExecutionConcurrency() {
    if (getNettyServerUseExecutionHandler()) {
      return getNettyServerExecutionThreads();
    } else {
      return getNettyServerThreads();
    }
  }

  
  public ByteBufAllocator getNettyAllocator() {
    if (nettyBufferAllocator == null) {
      if (NETTY_USE_POOLED_ALLOCATOR.get(this)) { 
        nettyBufferAllocator = new PooledByteBufAllocator(
          NETTY_USE_DIRECT_MEMORY.get(this));
      } else { 
        
        nettyBufferAllocator = new UnpooledByteBufAllocator(
            NETTY_USE_DIRECT_MEMORY.get(this));
      }
    }
    return nettyBufferAllocator;
  }

  public int getZookeeperConnectionAttempts() {
    return ZOOKEEPER_CONNECTION_ATTEMPTS.get(this);
  }

  public int getZooKeeperMinSessionTimeout() {
    return ZOOKEEPER_MIN_SESSION_TIMEOUT.get(this);
  }

  public int getZooKeeperMaxSessionTimeout() {
    return ZOOKEEPER_MAX_SESSION_TIMEOUT.get(this);
  }

  public boolean getZooKeeperForceSync() {
    return ZOOKEEPER_FORCE_SYNC.get(this);
  }

  public boolean getZooKeeperSkipAcl() {
    return ZOOKEEPER_SKIP_ACL.get(this);
  }

  
  public int getMapTasks() {
    int mapTasks = getInt("mapred.map.tasks", -1);
    if (mapTasks == -1) {
      throw new IllegalStateException("getMapTasks: Failed to get the map " +
          "tasks!");
    }
    return mapTasks;
  }

  
  public boolean authenticate() {
    return AUTHENTICATE.get(this);
  }

  
  public void setNumComputeThreads(int numComputeThreads) {
    NUM_COMPUTE_THREADS.set(this, numComputeThreads);
  }

  public int getNumComputeThreads() {
    return NUM_COMPUTE_THREADS.get(this);
  }

  
  public void setNumInputSplitsThreads(int numInputSplitsThreads) {
    NUM_INPUT_THREADS.set(this, numInputSplitsThreads);
  }

  public int getNumInputSplitsThreads() {
    return NUM_INPUT_THREADS.get(this);
  }

  public long getInputSplitMaxVertices() {
    return INPUT_SPLIT_MAX_VERTICES.get(this);
  }

  public long getInputSplitMaxEdges() {
    return INPUT_SPLIT_MAX_EDGES.get(this);
  }

  
  public void useUnsafeSerialization(boolean useUnsafeSerialization) {
    USE_UNSAFE_SERIALIZATION.set(this, useUnsafeSerialization);
  }

  
  public boolean useMessageSizeEncoding() {
    return USE_MESSAGE_SIZE_ENCODING.get(this);
  }

  
  public void setCheckpointFrequency(int checkpointFrequency) {
    CHECKPOINT_FREQUENCY.set(this, checkpointFrequency);
  }

  
  public int getCheckpointFrequency() {
    return CHECKPOINT_FREQUENCY.get(this);
  }

  
  public boolean useCheckpointing() {
    return getCheckpointFrequency() != 0;
  }

  
  public void setCheckpointSupportedChecker(
      Class<? extends CheckpointSupportedChecker> clazz) {
    GiraphConstants.CHECKPOINT_SUPPORTED_CHECKER.set(this, clazz);
  }

  
  public void setMaxTaskAttempts(int maxTaskAttempts) {
    MAX_TASK_ATTEMPTS.set(this, maxTaskAttempts);
  }

  
  public int getMaxTaskAttempts() {
    return MAX_TASK_ATTEMPTS.get(this);
  }

  
  public int getEventWaitMsecs() {
    return EVENT_WAIT_MSECS.get(this);
  }

  
  public void setEventWaitMsecs(int eventWaitMsecs) {
    EVENT_WAIT_MSECS.set(this, eventWaitMsecs);
  }

  
  public int getMaxMasterSuperstepWaitMsecs() {
    return MAX_MASTER_SUPERSTEP_WAIT_MSECS.get(this);
  }

  
  public void setMaxMasterSuperstepWaitMsecs(int maxMasterSuperstepWaitMsecs) {
    MAX_MASTER_SUPERSTEP_WAIT_MSECS.set(this, maxMasterSuperstepWaitMsecs);
  }

  
  public void configureHadoopSecurity() {
    String hadoopTokenFilePath = System.getenv("HADOOP_TOKEN_FILE_LOCATION");
    if (hadoopTokenFilePath != null) {
      set("mapreduce.job.credentials.binary", hadoopTokenFilePath);
    }
  }

  
  public boolean useInputSplitLocality() {
    return USE_INPUT_SPLIT_LOCALITY.get(this);
  }

  
  public String getLocalHostname() throws UnknownHostException {
    return DNS.getDefaultHost(
        GiraphConstants.DNS_INTERFACE.get(this),
        GiraphConstants.DNS_NAMESERVER.get(this)).toLowerCase();
  }

  
  public void setMaxNumberOfSupersteps(int maxNumberOfSupersteps) {
    MAX_NUMBER_OF_SUPERSTEPS.set(this, maxNumberOfSupersteps);
  }

  
  public int getMaxNumberOfSupersteps() {
    return MAX_NUMBER_OF_SUPERSTEPS.get(this);
  }

  
  public boolean isStaticGraph() {
    return STATIC_GRAPH.isTrue(this);
  }

  
  public String getYourKitOutputDir(Mapper.Context context) {
    final String cacheKey = "giraph.yourkit.outputDirCached";
    String outputDir = get(cacheKey);
    if (outputDir == null) {
      outputDir = getStringVars(YOURKIT_OUTPUT_DIR, YOURKIT_OUTPUT_DIR_DEFAULT,
          context);
      set(cacheKey, outputDir);
    }
    return outputDir;
  }

  
  public String getStringVars(String key, Mapper.Context context) {
    return getStringVars(key, null, context);
  }

  
  public String getStringVars(String key, String defaultValue,
                              Mapper.Context context) {
    String value = get(key);
    if (value == null) {
      if (defaultValue == null) {
        return null;
      }
      value = defaultValue;
    }
    value = value.replace("%JOB_ID%", context.getJobID().toString());
    value = value.replace("%TASK_ID%", context.getTaskAttemptID().toString());
    value = value.replace("%USER%", get("user.name", "unknown_user"));
    return value;
  }

  
  public boolean getCreateSourceVertex() {
    return CREATE_EDGE_SOURCE_VERTICES.get(this);
  }

  
  public void setCreateSourceVertex(boolean createVertex) {
    CREATE_EDGE_SOURCE_VERTICES.set(this, createVertex);
  }

  
  public int getWaitTaskDoneTimeoutMs() {
    return WAIT_TASK_DONE_TIMEOUT_MS.get(this);
  }

  
  public void setWaitTaskDoneTimeoutMs(int ms) {
    WAIT_TASK_DONE_TIMEOUT_MS.set(this, ms);
  }

  
  public boolean trackJobProgressOnClient() {
    return TRACK_JOB_PROGRESS_ON_CLIENT.get(this);
  }

  
  public int getHdfsFileCreationRetries() {
    return HDFS_FILE_CREATION_RETRIES.get(this);
  }

  
  public int getHdfsFileCreationRetryWaitMs() {
    return HDFS_FILE_CREATION_RETRY_WAIT_MS.get(this);
  }
}

<code block>

package org.apache.giraph.conf;

import static java.util.concurrent.TimeUnit.MINUTES;
import static java.util.concurrent.TimeUnit.SECONDS;

import org.apache.giraph.aggregators.AggregatorWriter;
import org.apache.giraph.aggregators.TextAggregatorWriter;
import org.apache.giraph.bsp.BspOutputFormat;
import org.apache.giraph.bsp.checkpoints.CheckpointSupportedChecker;
import org.apache.giraph.bsp.checkpoints.DefaultCheckpointSupportedChecker;
import org.apache.giraph.combiner.MessageCombiner;
import org.apache.giraph.comm.messages.InMemoryMessageStoreFactory;
import org.apache.giraph.comm.messages.MessageEncodeAndStoreType;
import org.apache.giraph.comm.messages.MessageStoreFactory;
import org.apache.giraph.edge.ByteArrayEdges;
import org.apache.giraph.edge.EdgeStoreFactory;
import org.apache.giraph.edge.InMemoryEdgeStoreFactory;
import org.apache.giraph.edge.OutEdges;
import org.apache.giraph.factories.ComputationFactory;
import org.apache.giraph.factories.DefaultComputationFactory;
import org.apache.giraph.factories.DefaultEdgeValueFactory;
import org.apache.giraph.factories.DefaultMessageValueFactory;
import org.apache.giraph.factories.DefaultVertexIdFactory;
import org.apache.giraph.factories.DefaultVertexValueFactory;
import org.apache.giraph.factories.EdgeValueFactory;
import org.apache.giraph.factories.MessageValueFactory;
import org.apache.giraph.factories.VertexIdFactory;
import org.apache.giraph.factories.VertexValueFactory;
import org.apache.giraph.graph.Computation;
import org.apache.giraph.graph.DefaultVertex;
import org.apache.giraph.graph.DefaultVertexResolver;
import org.apache.giraph.graph.DefaultVertexValueCombiner;
import org.apache.giraph.graph.Language;
import org.apache.giraph.graph.Vertex;
import org.apache.giraph.graph.VertexResolver;
import org.apache.giraph.graph.VertexValueCombiner;
import org.apache.giraph.io.EdgeInputFormat;
import org.apache.giraph.io.EdgeOutputFormat;
import org.apache.giraph.io.MappingInputFormat;
import org.apache.giraph.io.VertexInputFormat;
import org.apache.giraph.io.VertexOutputFormat;
import org.apache.giraph.io.filters.DefaultEdgeInputFilter;
import org.apache.giraph.io.filters.DefaultVertexInputFilter;
import org.apache.giraph.io.filters.EdgeInputFilter;
import org.apache.giraph.io.filters.VertexInputFilter;
import org.apache.giraph.job.DefaultGiraphJobRetryChecker;
import org.apache.giraph.job.DefaultJobObserver;
import org.apache.giraph.job.GiraphJobObserver;
import org.apache.giraph.job.GiraphJobRetryChecker;
import org.apache.giraph.job.HaltApplicationUtils;
import org.apache.giraph.mapping.MappingStore;
import org.apache.giraph.mapping.MappingStoreOps;
import org.apache.giraph.mapping.translate.TranslateEdge;
import org.apache.giraph.master.DefaultMasterCompute;
import org.apache.giraph.master.MasterCompute;
import org.apache.giraph.master.MasterObserver;
import org.apache.giraph.partition.GraphPartitionerFactory;
import org.apache.giraph.partition.HashPartitionerFactory;
import org.apache.giraph.partition.Partition;
import org.apache.giraph.partition.SimplePartition;
import org.apache.giraph.worker.DefaultWorkerContext;
import org.apache.giraph.worker.WorkerContext;
import org.apache.giraph.worker.WorkerObserver;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.mapreduce.OutputFormat;



public interface GiraphConstants {
  
  int ONE_KB = 1024;

  
  ClassConfOption<MappingStore> MAPPING_STORE_CLASS =
      ClassConfOption.create("giraph.mappingStoreClass", null,
          MappingStore.class, "MappingStore Class");

  
  ClassConfOption<MappingStoreOps> MAPPING_STORE_OPS_CLASS =
      ClassConfOption.create("giraph.mappingStoreOpsClass", null,
          MappingStoreOps.class, "MappingStoreOps class");

  
  IntConfOption LB_MAPPINGSTORE_UPPER =
      new IntConfOption("giraph.lbMappingStoreUpper", -1,
          "'upper' value used by lbmappingstore");
  
  IntConfOption LB_MAPPINGSTORE_LOWER =
      new IntConfOption("giraph.lbMappingStoreLower", -1,
          "'lower' value used by lbMappingstore");
  
  ClassConfOption EDGE_TRANSLATION_CLASS =
      ClassConfOption.create("giraph.edgeTranslationClass", null,
          TranslateEdge.class, "Class used to conduct expensive edge " +
              "translation during vertex input phase");

  
  ClassConfOption<Computation> COMPUTATION_CLASS =
      ClassConfOption.create("giraph.computationClass", null,
          Computation.class, "Computation class - required");
  
  ClassConfOption<ComputationFactory> COMPUTATION_FACTORY_CLASS =
      ClassConfOption.create("giraph.computation.factory.class",
          DefaultComputationFactory.class, ComputationFactory.class,
          "Computation factory class - optional");

  
  ClassConfOption<TypesHolder> TYPES_HOLDER_CLASS =
      ClassConfOption.create("giraph.typesHolder", null,
          TypesHolder.class,
          "TypesHolder, used if Computation not set - optional");

  
  ClassConfOption<EdgeStoreFactory> EDGE_STORE_FACTORY_CLASS =
      ClassConfOption.create("giraph.edgeStoreFactoryClass",
          InMemoryEdgeStoreFactory.class,
          EdgeStoreFactory.class,
          "Edge Store Factory class to use for creating edgeStore");

  
  ClassConfOption<MessageStoreFactory> MESSAGE_STORE_FACTORY_CLASS =
      ClassConfOption.create("giraph.messageStoreFactoryClass",
          InMemoryMessageStoreFactory.class,
          MessageStoreFactory.class,
          "Message Store Factory Class that is to be used");

  
  PerGraphTypeEnumConfOption<Language> GRAPH_TYPE_LANGUAGES =
      PerGraphTypeEnumConfOption.create("giraph.types.language",
          Language.class, Language.JAVA,
          "Language user graph types (IVEMM) are implemented in");

  
  PerGraphTypeBooleanConfOption GRAPH_TYPES_NEEDS_WRAPPERS =
      new PerGraphTypeBooleanConfOption("giraph.jython.type.wrappers",
          false, "Whether user graph types (IVEMM) need Jython wrappers");

  
  ClassConfOption<VertexIdFactory> VERTEX_ID_FACTORY_CLASS =
      ClassConfOption.create("giraph.vertexIdFactoryClass",
          DefaultVertexIdFactory.class, VertexIdFactory.class,
          "Vertex ID factory class - optional");
  
  ClassConfOption<VertexValueFactory> VERTEX_VALUE_FACTORY_CLASS =
      ClassConfOption.create("giraph.vertexValueFactoryClass",
          DefaultVertexValueFactory.class, VertexValueFactory.class,
          "Vertex value factory class - optional");
  
  ClassConfOption<EdgeValueFactory> EDGE_VALUE_FACTORY_CLASS =
      ClassConfOption.create("giraph.edgeValueFactoryClass",
          DefaultEdgeValueFactory.class, EdgeValueFactory.class,
          "Edge value factory class - optional");
  
  ClassConfOption<MessageValueFactory>
  OUTGOING_MESSAGE_VALUE_FACTORY_CLASS =
      ClassConfOption.create("giraph.outgoingMessageValueFactoryClass",
          DefaultMessageValueFactory.class, MessageValueFactory.class,
          "Outgoing message value factory class - optional");

  
  ClassConfOption<OutEdges> VERTEX_EDGES_CLASS =
      ClassConfOption.create("giraph.outEdgesClass", ByteArrayEdges.class,
          OutEdges.class, "Vertex edges class - optional");
  
  ClassConfOption<OutEdges> INPUT_VERTEX_EDGES_CLASS =
      ClassConfOption.create("giraph.inputOutEdgesClass",
          ByteArrayEdges.class, OutEdges.class,
          "Vertex edges class to be used during edge input only - optional");

  
  ClassConfOption<MasterCompute> MASTER_COMPUTE_CLASS =
      ClassConfOption.create("giraph.masterComputeClass",
          DefaultMasterCompute.class, MasterCompute.class,
          "Class for Master - optional");
  
  ClassConfOption<MasterObserver> MASTER_OBSERVER_CLASSES =
      ClassConfOption.create("giraph.master.observers",
          null, MasterObserver.class, "Classes for Master Observer - optional");
  
  ClassConfOption<WorkerObserver> WORKER_OBSERVER_CLASSES =
      ClassConfOption.create("giraph.worker.observers", null,
          WorkerObserver.class, "Classes for Worker Observer - optional");
  
  ClassConfOption<MessageCombiner> MESSAGE_COMBINER_CLASS =
      ClassConfOption.create("giraph.messageCombinerClass", null,
          MessageCombiner.class, "Message combiner class - optional");
  
  ClassConfOption<VertexResolver> VERTEX_RESOLVER_CLASS =
      ClassConfOption.create("giraph.vertexResolverClass",
          DefaultVertexResolver.class, VertexResolver.class,
          "Vertex resolver class - optional");
  
  ClassConfOption<VertexValueCombiner> VERTEX_VALUE_COMBINER_CLASS =
      ClassConfOption.create("giraph.vertexValueCombinerClass",
          DefaultVertexValueCombiner.class, VertexValueCombiner.class,
          "Vertex value combiner class - optional");

  
  EnumConfOption<Language> COMPUTATION_LANGUAGE =
      EnumConfOption.create("giraph.computation.language",
          Language.class, Language.JAVA,
          "Which language computation is implemented in");

  
  BooleanConfOption RESOLVER_CREATE_VERTEX_ON_MSGS =
      new BooleanConfOption("giraph.vertex.resolver.create.on.msgs", true,
          "Option of whether to create vertexes that were not existent " +
          "before but received messages");
  
  ClassConfOption<GraphPartitionerFactory> GRAPH_PARTITIONER_FACTORY_CLASS =
      ClassConfOption.create("giraph.graphPartitionerFactoryClass",
          HashPartitionerFactory.class, GraphPartitionerFactory.class,
          "Graph partitioner factory class - optional");

  
  ClassConfOption<GiraphJobObserver> JOB_OBSERVER_CLASS =
      ClassConfOption.create("giraph.jobObserverClass",
          DefaultJobObserver.class, GiraphJobObserver.class,
          "Observer class to watch over job status - optional");

  
  ClassConfOption<GiraphJobRetryChecker> JOB_RETRY_CHECKER_CLASS =
      ClassConfOption.create("giraph.jobRetryCheckerClass",
          DefaultGiraphJobRetryChecker.class, GiraphJobRetryChecker.class,
          "Class which decides whether a failed job should be retried - " +
              "optional");

  
  LongConfOption MAX_ALLOWED_JOB_TIME_MS =
      new LongConfOption("giraph.maxAllowedJobTimeMilliseconds", -1,
          "Maximum allowed time for job to run after getting all resources " +
              "before it will be killed, in milliseconds " +
              "(-1 if it has no limit)");

  
  
  ClassConfOption<VertexInputFormat> VERTEX_INPUT_FORMAT_CLASS =
      ClassConfOption.create("giraph.vertexInputFormatClass", null,
          VertexInputFormat.class, "VertexInputFormat class (at least " +
          "one of the input format classes is required)");
  
  ClassConfOption<EdgeInputFormat> EDGE_INPUT_FORMAT_CLASS =
      ClassConfOption.create("giraph.edgeInputFormatClass", null,
          EdgeInputFormat.class, "EdgeInputFormat class");
  
  ClassConfOption<MappingInputFormat> MAPPING_INPUT_FORMAT_CLASS =
      ClassConfOption.create("giraph.mappingInputFormatClass", null,
          MappingInputFormat.class, "MappingInputFormat class");

  
  ClassConfOption<EdgeInputFilter> EDGE_INPUT_FILTER_CLASS =
      ClassConfOption.create("giraph.edgeInputFilterClass",
          DefaultEdgeInputFilter.class, EdgeInputFilter.class,
          "EdgeInputFilter class");
  
  ClassConfOption<VertexInputFilter> VERTEX_INPUT_FILTER_CLASS =
      ClassConfOption.create("giraph.vertexInputFilterClass",
          DefaultVertexInputFilter.class, VertexInputFilter.class,
          "VertexInputFilter class");
  
  ClassConfOption<Vertex> VERTEX_CLASS =
      ClassConfOption.create("giraph.vertexClass",
          DefaultVertex.class, Vertex.class,
          "Vertex class");
  
  ClassConfOption<VertexOutputFormat> VERTEX_OUTPUT_FORMAT_CLASS =
      ClassConfOption.create("giraph.vertexOutputFormatClass", null,
          VertexOutputFormat.class, "VertexOutputFormat class");
  
  StrConfOption VERTEX_OUTPUT_FORMAT_SUBDIR =
    new StrConfOption("giraph.vertex.output.subdir", "",
                      "VertexOutputFormat sub-directory");
  
  ClassConfOption<EdgeOutputFormat> EDGE_OUTPUT_FORMAT_CLASS =
      ClassConfOption.create("giraph.edgeOutputFormatClass", null,
          EdgeOutputFormat.class, "EdgeOutputFormat class");
  
  StrConfOption EDGE_OUTPUT_FORMAT_SUBDIR =
    new StrConfOption("giraph.edge.output.subdir", "",
                      "EdgeOutputFormat sub-directory");

  
  StrConfOption GIRAPH_TEXT_OUTPUT_FORMAT_SEPARATOR =
    new StrConfOption("giraph.textoutputformat.separator", "\t",
                      "GiraphTextOuputFormat Separator");
  
  BooleanConfOption GIRAPH_TEXT_OUTPUT_FORMAT_REVERSE =
      new BooleanConfOption("giraph.textoutputformat.reverse", false,
                            "Reverse values in the output");

  
  BooleanConfOption DO_OUTPUT_DURING_COMPUTATION =
      new BooleanConfOption("giraph.doOutputDuringComputation", false,
          "If you use this option, instead of having saving vertices in the " +
          "end of application, saveVertex will be called right after each " +
          "vertex.compute() is called." +
          "NOTE: This feature doesn't work well with checkpointing - if you " +
          "restart from a checkpoint you won't have any ouptut from previous " +
          "supresteps.");
  
  BooleanConfOption VERTEX_OUTPUT_FORMAT_THREAD_SAFE =
      new BooleanConfOption("giraph.vertexOutputFormatThreadSafe", false,
          "Vertex output format thread-safe - if your VertexOutputFormat " +
          "allows several vertexWriters to be created and written to in " +
          "parallel, you should set this to true.");
  
  IntConfOption NUM_OUTPUT_THREADS =
      new IntConfOption("giraph.numOutputThreads", 1,
          "Number of threads for writing output in the end of the application");

  
  StrConfOption GIRAPH_YARN_LIBJARS =
    new StrConfOption("giraph.yarn.libjars", "",
        "conf key for comma-separated list of jars to export to YARN workers");
  
  String GIRAPH_YARN_CONF_FILE = "giraph-conf.xml";
  
  int GIRAPH_YARN_TASK_HEAP_MB_DEFAULT = 1024;
  
  IntConfOption GIRAPH_YARN_TASK_HEAP_MB = new IntConfOption(
    "giraph.yarn.task.heap.mb", GIRAPH_YARN_TASK_HEAP_MB_DEFAULT,
    "Name of Giraph property for user-configurable heap memory per worker");
  
  int GIRAPH_YARN_PRIORITY = 10;
  
  BooleanConfOption IS_PURE_YARN_JOB =
    new BooleanConfOption("giraph.pure.yarn.job", false,
        "Is this a pure YARN job (i.e. no MapReduce layer managing Giraph " +
        "tasks)");

  
  ClassConfOption<WritableComparable> VERTEX_ID_CLASS =
      ClassConfOption.create("giraph.vertexIdClass", null,
          WritableComparable.class, "Vertex index class");
  
  ClassConfOption<Writable> VERTEX_VALUE_CLASS =
      ClassConfOption.create("giraph.vertexValueClass", null, Writable.class,
          "Vertex value class");
  
  ClassConfOption<Writable> EDGE_VALUE_CLASS =
      ClassConfOption.create("giraph.edgeValueClass", null, Writable.class,
          "Edge value class");
  
  ClassConfOption<Writable> OUTGOING_MESSAGE_VALUE_CLASS =
      ClassConfOption.create("giraph.outgoingMessageValueClass", null,
          Writable.class, "Outgoing message value class");
  
  ClassConfOption<WorkerContext> WORKER_CONTEXT_CLASS =
      ClassConfOption.create("giraph.workerContextClass",
          DefaultWorkerContext.class, WorkerContext.class,
          "Worker contextclass");
  
  ClassConfOption<AggregatorWriter> AGGREGATOR_WRITER_CLASS =
      ClassConfOption.create("giraph.aggregatorWriterClass",
          TextAggregatorWriter.class, AggregatorWriter.class,
          "AggregatorWriter class - optional");

  
  ClassConfOption<Partition> PARTITION_CLASS =
      ClassConfOption.create("giraph.partitionClass", SimplePartition.class,
          Partition.class, "Partition class - optional");

  
  String MIN_WORKERS = "giraph.minWorkers";
  
  String MAX_WORKERS = "giraph.maxWorkers";

  
  BooleanConfOption SPLIT_MASTER_WORKER =
      new BooleanConfOption("giraph.SplitMasterWorker", true,
          "Separate the workers and the master tasks.  This is required to " +
          "support dynamic recovery. (boolean)");

  
  BooleanConfOption LOCAL_TEST_MODE =
      new BooleanConfOption("giraph.localTestMode", false,
          "Indicates whether this job is run in an internal unit test");

  
  StrConfOption LOG_LEVEL = new StrConfOption("giraph.logLevel", "info",
      "Override the Hadoop log level and set the desired log level.");

  
  BooleanConfOption LOG_THREAD_LAYOUT =
      new BooleanConfOption("giraph.logThreadLayout", false,
          "Use thread level debugging?");

  
  BooleanConfOption JMAP_ENABLE =
      new BooleanConfOption("giraph.jmap.histo.enable", false,
          "Configuration key to enable jmap printing");

  
  IntConfOption JMAP_SLEEP_MILLIS =
      new IntConfOption("giraph.jmap.histo.msec", SECONDS.toMillis(30),
          "Configuration key for msec to sleep between calls");

  
  IntConfOption JMAP_PRINT_LINES =
      new IntConfOption("giraph.jmap.histo.print_lines", 30,
          "Configuration key for how many lines to print");

  
  BooleanConfOption JMAP_LIVE_ONLY =
      new BooleanConfOption("giraph.jmap.histo.live", false,
          "Only print live objects in jmap?");

  
  IntConfOption MIN_FREE_MBS_ON_HEAP =
      new IntConfOption("giraph.heap.minFreeMb", 128, "Option used by " +
          "worker and master observers to check for imminent OOM exception");
  
  BooleanConfOption REACTIVE_JMAP_ENABLE =
      new BooleanConfOption("giraph.heap.enableReactiveJmapDumping", false,
          "Option to enable dumping jmap histogram reactively based on " +
              "free memory on heap");

  
  FloatConfOption MIN_PERCENT_RESPONDED =
      new FloatConfOption("giraph.minPercentResponded", 100.0f,
          "Minimum percent of the maximum number of workers that have " +
          "responded in order to continue progressing. (float)");

  
  BooleanConfOption METRICS_ENABLE =
      new BooleanConfOption("giraph.metrics.enable", false,
          "Enable the Metrics system");

  
  StrConfOption METRICS_DIRECTORY =
      new StrConfOption("giraph.metrics.directory", "",
          "Directory in HDFS to write master metrics to, instead of stderr");

  
  StrConfOption ZOOKEEPER_LIST =
      new StrConfOption("giraph.zkList", "",
          "ZooKeeper comma-separated list (if not set, will start up " +
          "ZooKeeper locally). Consider that after locally-starting " +
          "zookeeper, this parameter will updated the configuration with " +
          "the corrent configuration value.");

  
  BooleanConfOption ZOOKEEPER_IS_EXTERNAL =
    new BooleanConfOption("giraph.zkIsExternal", true,
                          "Zookeeper List will always hold a value during " +
                          "the computation while this option provides " +
                          "information regarding whether the zookeeper was " +
                          "internally started or externally provided.");

  
  IntConfOption ZOOKEEPER_SESSION_TIMEOUT =
      new IntConfOption("giraph.zkSessionMsecTimeout", MINUTES.toMillis(1),
          "ZooKeeper session millisecond timeout");

  
  IntConfOption ZOOKEEPER_SERVERLIST_POLL_MSECS =
      new IntConfOption("giraph.zkServerlistPollMsecs", SECONDS.toMillis(3),
          "Polling interval to check for the ZooKeeper server data");

  
  IntConfOption ZOOKEEPER_SERVER_COUNT =
      new IntConfOption("giraph.zkServerCount", 1,
          "Number of nodes (not tasks) to run Zookeeper on");

  
  IntConfOption ZOOKEEPER_SERVER_PORT =
      new IntConfOption("giraph.zkServerPort", 22181, "ZooKeeper port to use");

  
  String ZOOKEEPER_DIR = "giraph.zkDir";

  
  IntConfOption ZOOKEEPER_OPS_MAX_ATTEMPTS =
      new IntConfOption("giraph.zkOpsMaxAttempts", 3,
          "Max attempts for handling ZooKeeper connection loss");

  
  IntConfOption ZOOKEEPER_OPS_RETRY_WAIT_MSECS =
      new IntConfOption("giraph.zkOpsRetryWaitMsecs", SECONDS.toMillis(5),
          "Msecs to wait before retrying a failed ZooKeeper op due to " +
          "connection loss.");

  
  BooleanConfOption ZOOKEEEPER_RUNS_IN_PROCESS = new BooleanConfOption(
      "giraph.zkRunsInProcess",
      true, "If true run zookeeper in master process, if false starts " +
      "separate process for zookeeper");

  
  IntConfOption TCP_BACKLOG = new IntConfOption("giraph.tcpBacklog", 1,
      "TCP backlog (defaults to number of workers)");

  
  BooleanConfOption NETTY_USE_POOLED_ALLOCATOR = new BooleanConfOption(
      "giraph.useNettyPooledAllocator", false, "Should netty use pooled " +
      "memory allocator?");

  
  BooleanConfOption NETTY_USE_DIRECT_MEMORY = new BooleanConfOption(
      "giraph.useNettyDirectMemory", false, "Should netty use direct " +
      "memory buffers");

  
  IntConfOption NETTY_REQUEST_ENCODER_BUFFER_SIZE =
      new IntConfOption("giraph.nettyRequestEncoderBufferSize", 32 * ONE_KB,
          "How big to make the encoder buffer?");

  
  IntConfOption NETTY_CLIENT_THREADS =
      new IntConfOption("giraph.nettyClientThreads", 4, "Netty client threads");

  
  IntConfOption NETTY_SERVER_THREADS =
      new IntConfOption("giraph.nettyServerThreads", 16,
          "Netty server threads");

  
  BooleanConfOption NETTY_CLIENT_USE_EXECUTION_HANDLER =
      new BooleanConfOption("giraph.nettyClientUseExecutionHandler", true,
          "Use the execution handler in netty on the client?");

  
  IntConfOption NETTY_CLIENT_EXECUTION_THREADS =
      new IntConfOption("giraph.nettyClientExecutionThreads", 8,
          "Netty client execution threads (execution handler)");

  
  StrConfOption NETTY_CLIENT_EXECUTION_AFTER_HANDLER =
      new StrConfOption("giraph.nettyClientExecutionAfterHandler",
          "request-encoder",
          "Where to place the netty client execution handle?");

  
  BooleanConfOption NETTY_SERVER_USE_EXECUTION_HANDLER =
      new BooleanConfOption("giraph.nettyServerUseExecutionHandler", true,
          "Use the execution handler in netty on the server?");

  
  IntConfOption NETTY_SERVER_EXECUTION_THREADS =
      new IntConfOption("giraph.nettyServerExecutionThreads", 8,
          "Netty server execution threads (execution handler)");

  
  StrConfOption NETTY_SERVER_EXECUTION_AFTER_HANDLER =
      new StrConfOption("giraph.nettyServerExecutionAfterHandler",
          "requestFrameDecoder",
          "Where to place the netty server execution handle?");

  
  BooleanConfOption NETTY_SIMULATE_FIRST_REQUEST_CLOSED =
      new BooleanConfOption("giraph.nettySimulateFirstRequestClosed", false,
          "Netty simulate a first request closed");

  
  BooleanConfOption NETTY_SIMULATE_FIRST_RESPONSE_FAILED =
      new BooleanConfOption("giraph.nettySimulateFirstResponseFailed", false,
          "Netty simulate a first response failed");

  
  StrConfOption NETTY_COMPRESSION_ALGORITHM =
      new StrConfOption("giraph.nettyCompressionAlgorithm", "",
          "Which compression algorithm to use in netty");

  
  IntConfOption MAX_RESOLVE_ADDRESS_ATTEMPTS =
      new IntConfOption("giraph.maxResolveAddressAttempts", 5,
          "Max resolve address attempts");

  
  IntConfOption WAITING_REQUEST_MSECS =
      new IntConfOption("giraph.waitingRequestMsecs", SECONDS.toMillis(15),
          "Msecs to wait between waiting for all requests to finish");

  
  IntConfOption EVENT_WAIT_MSECS =
      new IntConfOption("giraph.eventWaitMsecs", SECONDS.toMillis(30),
          "Millseconds to wait for an event before continuing");

  
  IntConfOption MAX_MASTER_SUPERSTEP_WAIT_MSECS =
      new IntConfOption("giraph.maxMasterSuperstepWaitMsecs",
          MINUTES.toMillis(10),
          "Maximum milliseconds to wait before giving up trying to get the " +
          "minimum number of workers before a superstep (int).");

  
  IntConfOption MAX_REQUEST_MILLISECONDS =
      new IntConfOption("giraph.maxRequestMilliseconds", MINUTES.toMillis(10),
          "Milliseconds for a request to complete (or else resend)");

  
  IntConfOption NETTY_MAX_CONNECTION_FAILURES =
      new IntConfOption("giraph.nettyMaxConnectionFailures", 1000,
          "Netty max connection failures");

  
  IntConfOption IPC_INITIAL_PORT =
      new IntConfOption("giraph.ipcInitialPort", 30000,
          "Initial port to start using for the IPC communication");

  
  IntConfOption MAX_IPC_PORT_BIND_ATTEMPTS =
      new IntConfOption("giraph.maxIpcPortBindAttempts", 20,
          "Maximum bind attempts for different IPC ports");
  
  BooleanConfOption FAIL_FIRST_IPC_PORT_BIND_ATTEMPT =
      new BooleanConfOption("giraph.failFirstIpcPortBindAttempt", false,
          "Fail first IPC port binding attempt, simulate binding failure " +
          "on real grid testing");

  
  IntConfOption CLIENT_SEND_BUFFER_SIZE =
      new IntConfOption("giraph.clientSendBufferSize", 512 * ONE_KB,
          "Client send buffer size");

  
  IntConfOption CLIENT_RECEIVE_BUFFER_SIZE =
      new IntConfOption("giraph.clientReceiveBufferSize", 32 * ONE_KB,
          "Client receive buffer size");

  
  IntConfOption SERVER_SEND_BUFFER_SIZE =
      new IntConfOption("giraph.serverSendBufferSize", 32 * ONE_KB,
          "Server send buffer size");

  
  IntConfOption SERVER_RECEIVE_BUFFER_SIZE =
      new IntConfOption("giraph.serverReceiveBufferSize", 512 * ONE_KB,
          "Server receive buffer size");

  
  IntConfOption MAX_MSG_REQUEST_SIZE =
      new IntConfOption("giraph.msgRequestSize", 512 * ONE_KB,
          "Maximum size of messages (in bytes) per peer before flush");

  
  FloatConfOption ADDITIONAL_MSG_REQUEST_SIZE =
      new FloatConfOption("giraph.additionalMsgRequestSize", 0.2f,
          "How much bigger than the average per partition size to make " +
          "initial per partition buffers. If this value is A, message " +
          "request size is M, and a worker has P partitions, than its " +
          "initial partition buffer size will be (M / P) * (1 + A).");


  
  FloatConfOption REQUEST_SIZE_WARNING_THRESHOLD = new FloatConfOption(
      "giraph.msgRequestWarningThreshold", 2.0f,
      "If request sizes are bigger than the buffer size by this factor " +
      "warnings are printed to the log and to the command line");

  
  IntConfOption MAX_VERTEX_REQUEST_SIZE =
      new IntConfOption("giraph.vertexRequestSize", 512 * ONE_KB,
          "Maximum size of vertices (in bytes) per peer before flush");

  
  FloatConfOption ADDITIONAL_VERTEX_REQUEST_SIZE =
      new FloatConfOption("giraph.additionalVertexRequestSize", 0.2f,
          "Additional size (expressed as a ratio) of each per-partition " +
              "buffer on top of the average size.");

  
  IntConfOption MAX_EDGE_REQUEST_SIZE =
      new IntConfOption("giraph.edgeRequestSize", 512 * ONE_KB,
          "Maximum size of edges (in bytes) per peer before flush");

  
  FloatConfOption ADDITIONAL_EDGE_REQUEST_SIZE =
      new FloatConfOption("giraph.additionalEdgeRequestSize", 0.2f,
          "Additional size (expressed as a ratio) of each per-partition " +
          "buffer on top of the average size.");

  
  IntConfOption MAX_MUTATIONS_PER_REQUEST =
      new IntConfOption("giraph.maxMutationsPerRequest", 100,
          "Maximum number of mutations per partition before flush");

  
  BooleanConfOption USE_MESSAGE_SIZE_ENCODING =
      new BooleanConfOption("giraph.useMessageSizeEncoding", false,
          "Use message size encoding (typically better for complex objects, " +
          "not meant for primitive wrapped messages)");

  
  IntConfOption CHANNELS_PER_SERVER =
      new IntConfOption("giraph.channelsPerServer", 1,
          "Number of channels used per server");

  
  String MSG_NUM_FLUSH_THREADS = "giraph.msgNumFlushThreads";

  
  IntConfOption NUM_COMPUTE_THREADS =
      new IntConfOption("giraph.numComputeThreads", 1,
          "Number of threads for vertex computation");

  
  IntConfOption NUM_INPUT_THREADS =
      new IntConfOption("giraph.numInputThreads", 1,
          "Number of threads for input split loading");

  
  IntConfOption PARTITION_LONG_TAIL_MIN_PRINT =
      new IntConfOption("giraph.partitionLongTailMinPrint", 1,
          "Minimum stragglers of the superstep before printing them out");

  
  BooleanConfOption USE_SUPERSTEP_COUNTERS =
      new BooleanConfOption("giraph.useSuperstepCounters", true,
          "Use superstep counters? (boolean)");

  
  FloatConfOption INPUT_SPLIT_SAMPLE_PERCENT =
      new FloatConfOption("giraph.inputSplitSamplePercent", 100f,
          "Input split sample percent - Used only for sampling and testing, " +
          "rather than an actual job.  The idea is that to test, you might " +
          "only want a fraction of the actual input splits from your " +
          "VertexInputFormat to load (values should be [0, 100]).");

  
  LongConfOption INPUT_SPLIT_MAX_VERTICES =
      new LongConfOption("giraph.InputSplitMaxVertices", -1,
          "To limit outlier vertex input splits from producing too many " +
              "vertices or to help with testing, the number of vertices " +
              "loaded from an input split can be limited. By default, " +
              "everything is loaded.");

  
  LongConfOption INPUT_SPLIT_MAX_EDGES =
      new LongConfOption("giraph.InputSplitMaxEdges", -1,
          "To limit outlier vertex input splits from producing too many " +
              "vertices or to help with testing, the number of edges loaded " +
              "from an input split can be limited. By default, everything is " +
              "loaded.");

  
  BooleanConfOption USE_INPUT_SPLIT_LOCALITY =
      new BooleanConfOption("giraph.useInputSplitLocality", true,
          "To minimize network usage when reading input splits, each worker " +
          "can prioritize splits that reside on its host. " +
          "This, however, comes at the cost of increased load on ZooKeeper. " +
          "Hence, users with a lot of splits and input threads (or with " +
          "configurations that can't exploit locality) may want to disable " +
          "it.");

  
  FloatConfOption PARTITION_COUNT_MULTIPLIER =
      new FloatConfOption("giraph.masterPartitionCountMultiplier", 1.0f,
          "Multiplier for the current workers squared");

  
  IntConfOption USER_PARTITION_COUNT =
      new IntConfOption("giraph.userPartitionCount", -1,
          "Overrides default partition count calculation if not -1");

  
  String PARTITION_VERTEX_KEY_SPACE_SIZE = "giraph.vertexKeySpaceSize";

  
  StrConfOption ZOOKEEPER_JAVA_OPTS =
      new StrConfOption("giraph.zkJavaOpts",
          "-Xmx512m -XX:ParallelGCThreads=4 -XX:+UseConcMarkSweepGC " +
          "-XX:CMSInitiatingOccupancyFraction=70 -XX:MaxGCPauseMillis=100",
          "Java opts passed to ZooKeeper startup");

  
  IntConfOption CHECKPOINT_FREQUENCY =
      new IntConfOption("giraph.checkpointFrequency", 0,
          "How often to checkpoint (i.e. 0, means no checkpoint, 1 means " +
          "every superstep, 2 is every two supersteps, etc.).");

  
  BooleanConfOption CLEANUP_CHECKPOINTS_AFTER_SUCCESS =
      new BooleanConfOption("giraph.cleanupCheckpointsAfterSuccess", true,
          "Delete checkpoints after a successful job run?");

  
  String RESTART_SUPERSTEP = "giraph.restartSuperstep";

  
  StrConfOption RESTART_JOB_ID = new StrConfOption("giraph.restart.jobId",
      null, "Which job ID should I try to restart?");

  
  String BASE_ZNODE_KEY = "giraph.zkBaseZNode";

  
  StrConfOption ZOOKEEPER_MANAGER_DIRECTORY =
      new StrConfOption("giraph.zkManagerDirectory",
          "_bsp/_defaultZkManagerDir",
          "If ZOOKEEPER_LIST is not set, then use this directory to manage " +
          "ZooKeeper");

  
  IntConfOption ZOOKEEPER_CONNECTION_ATTEMPTS =
      new IntConfOption("giraph.zkConnectionAttempts", 10,
          "Number of ZooKeeper client connection attempts before giving up.");

  
  StrConfOption CHECKPOINT_DIRECTORY =
      new StrConfOption("giraph.checkpointDirectory", "_bsp/_checkpoints/",
          "This directory has/stores the available checkpoint files in HDFS.");

  
  StrConfOption MESSAGES_DIRECTORY =
      new StrConfOption("giraph.messagesDirectory", "_bsp/_messages/",
          "Comma-separated list of directories in the local file system for " +
          "out-of-core messages.");

  
  IntConfOption MAX_MESSAGES_IN_MEMORY =
      new IntConfOption("giraph.maxMessagesInMemory", 1000000,
          "If using out-of-core messaging, it tells how much messages do we " +
          "keep in memory.");
  
  IntConfOption MESSAGES_BUFFER_SIZE =
      new IntConfOption("giraph.messagesBufferSize", 8 * ONE_KB,
          "Size of buffer when reading and writing messages out-of-core.");

  
  StrConfOption PARTITIONS_DIRECTORY =
      new StrConfOption("giraph.partitionsDirectory", "_bsp/_partitions",
          "Comma-separated list of directories in the local filesystem for " +
          "out-of-core partitions.");

  
  BooleanConfOption USE_OUT_OF_CORE_GRAPH =
      new BooleanConfOption("giraph.useOutOfCoreGraph", false,
          "Enable out-of-core graph.");

  
  String YOURKIT_OUTPUT_DIR = "giraph.yourkit.outputDir";
  
  String YOURKIT_OUTPUT_DIR_DEFAULT = "/tmp/giraph/%JOB_ID%/%TASK_ID%";

  
  IntConfOption MAX_PARTITIONS_IN_MEMORY =
      new IntConfOption("giraph.maxPartitionsInMemory", 10,
          "Maximum number of partitions to hold in memory for each worker.");

  
  IntConfOption MAX_STICKY_PARTITIONS =
      new IntConfOption("giraph.stickyPartitions", 0,
          "Set number of sticky partitions if sticky mode is enabled.");

  
  BooleanConfOption KEEP_ZOOKEEPER_DATA =
      new BooleanConfOption("giraph.keepZooKeeperData", false,
          "Keep the zookeeper output for debugging? Default is to remove it.");

  
  int DEFAULT_ZOOKEEPER_TICK_TIME = 6000;
  
  int DEFAULT_ZOOKEEPER_INIT_LIMIT = 10;
  
  int DEFAULT_ZOOKEEPER_SYNC_LIMIT = 5;
  
  int DEFAULT_ZOOKEEPER_SNAP_COUNT = 50000;
  
  int DEFAULT_ZOOKEEPER_MAX_CLIENT_CNXNS = 10000;
  
  IntConfOption ZOOKEEPER_MIN_SESSION_TIMEOUT =
      new IntConfOption("giraph.zKMinSessionTimeout", MINUTES.toMillis(10),
          "ZooKeeper minimum session timeout");
  
  IntConfOption ZOOKEEPER_MAX_SESSION_TIMEOUT =
      new IntConfOption("giraph.zkMaxSessionTimeout", MINUTES.toMillis(15),
          "ZooKeeper maximum session timeout");
  
  BooleanConfOption ZOOKEEPER_FORCE_SYNC =
      new BooleanConfOption("giraph.zKForceSync", false,
          "ZooKeeper force sync");
  
  BooleanConfOption ZOOKEEPER_SKIP_ACL =
      new BooleanConfOption("giraph.ZkSkipAcl", true, "ZooKeeper skip ACLs");

  
  BooleanConfOption AUTHENTICATE =
      new BooleanConfOption("giraph.authenticate", false,
          "Whether to use SASL with DIGEST and Hadoop Job Tokens to " +
          "authenticate and authorize Netty BSP Clients to Servers.");

  
  BooleanConfOption USE_UNSAFE_SERIALIZATION =
      new BooleanConfOption("giraph.useUnsafeSerialization", true,
          "Use unsafe serialization?");

  
  BooleanConfOption USE_BIG_DATA_IO_FOR_MESSAGES =
      new BooleanConfOption("giraph.useBigDataIOForMessages", false,
          "Use BigDataIO for messages?");

  
  IntConfOption MAX_TASK_ATTEMPTS =
      new IntConfOption("mapred.map.max.attempts", -1,
          "Maximum number of attempts a master/worker will retry before " +
          "killing the job.  This directly maps to the number of map task " +
          "attempts in Hadoop.");

  
  StrConfOption DNS_INTERFACE =
      new StrConfOption("giraph.dns.interface", "default",
          "Interface to use for hostname resolution");
  
  StrConfOption DNS_NAMESERVER =
      new StrConfOption("giraph.dns.nameserver", "default",
          "Server for hostname resolution");

  
  IntConfOption MAX_NUMBER_OF_SUPERSTEPS =
      new IntConfOption("giraph.maxNumberOfSupersteps", 1,
          "The application will halt after this many supersteps is " +
          "completed. For instance, if it is set to 3, the application will " +
          "run at most 0, 1, and 2 supersteps and then go into the shutdown " +
          "superstep.");

  
  BooleanConfOption STATIC_GRAPH =
      new BooleanConfOption("giraph.isStaticGraph", false,
          "The application will not mutate the graph topology (the edges). " +
          "It is used to optimise out-of-core graph, by not writing back " +
          "edges every time.");

  
  EnumConfOption<MessageEncodeAndStoreType> MESSAGE_ENCODE_AND_STORE_TYPE =
      EnumConfOption.create("giraph.messageEncodeAndStoreType",
          MessageEncodeAndStoreType.class,
          MessageEncodeAndStoreType.BYTEARRAY_PER_PARTITION,
          "Select the message_encode_and_store_type to use");

  
  BooleanConfOption CREATE_EDGE_SOURCE_VERTICES =
      new BooleanConfOption("giraph.createEdgeSourceVertices", true,
          "Create a source vertex if present in edge input but not " +
          "necessarily in vertex input");

  
  String ZOOKEEPER_SERVER_PORT_COUNTER_GROUP = "Zookeeper server:port";

  
  String ZOOKEEPER_HALT_NODE_COUNTER_GROUP = "Zookeeper halt node";

  
  String ZOOKEEPER_BASE_PATH_COUNTER_GROUP = "Zookeeper base path";

  
  ClassConfOption<HaltApplicationUtils.HaltInstructionsWriter>
  HALT_INSTRUCTIONS_WRITER_CLASS = ClassConfOption.create(
      "giraph.haltInstructionsWriter",
      HaltApplicationUtils.DefaultHaltInstructionsWriter.class,
      HaltApplicationUtils.HaltInstructionsWriter.class,
      "Class used to write instructions on how to halt the application");

  
  IntConfOption WAIT_TASK_DONE_TIMEOUT_MS =
      new IntConfOption("giraph.waitTaskDoneTimeoutMs", MINUTES.toMillis(15),
          "Maximum timeout (in ms) for waiting for all all tasks to " +
              "complete");

  
  BooleanConfOption TRACK_JOB_PROGRESS_ON_CLIENT =
      new BooleanConfOption("giraph.trackJobProgressOnClient", false,
          "Whether to track job progress on client or not");

  
  IntConfOption HDFS_FILE_CREATION_RETRIES =
      new IntConfOption("giraph.hdfs.file.creation.retries", 10,
          "Retries to create an HDFS file before failing");

  
  IntConfOption HDFS_FILE_CREATION_RETRY_WAIT_MS =
      new IntConfOption("giraph.hdfs.file.creation.retry.wait.ms", 30_000,
          "Milliseconds to wait prior to retrying creation of an HDFS file");

  
  IntConfOption NUM_CHECKPOINT_IO_THREADS =
      new IntConfOption("giraph.checkpoint.io.threads", 8,
          "Number of threads for writing and reading checkpoints");

  
  StrConfOption CHECKPOINT_COMPRESSION_CODEC =
      new StrConfOption("giraph.checkpoint.compression.codec",
          ".deflate",
          "Defines compression algorithm we will be using for " +
              "storing checkpoint. Available options include but " +
              "not restricted to: .deflate, .gz, .bz2, .lzo");

  
  ClassConfOption<CheckpointSupportedChecker> CHECKPOINT_SUPPORTED_CHECKER =
      ClassConfOption.create("giraph.checkpoint.supported.checker",
          DefaultCheckpointSupportedChecker.class,
          CheckpointSupportedChecker.class,
          "This is the way to specify if checkpointing is " +
              "supported by the job");


  
  IntConfOption ASYNC_MESSAGE_STORE_THREADS_COUNT =
      new IntConfOption("giraph.async.message.store.threads", 0,
          "Number of threads to be used in async message store.");

  
  ClassConfOption<OutputFormat> HADOOP_OUTPUT_FORMAT_CLASS =
      ClassConfOption.create("giraph.hadoopOutputFormatClass",
          BspOutputFormat.class, OutputFormat.class,
          "Output format class for hadoop to use (for committing)");
}


<code block>


package org.apache.giraph.bsp;

import org.apache.giraph.comm.ServerData;
import org.apache.giraph.comm.WorkerClient;
import org.apache.giraph.graph.FinishedSuperstepStats;
import org.apache.giraph.graph.GlobalStats;
import org.apache.giraph.graph.GraphTaskManager;
import org.apache.giraph.graph.VertexEdgeCount;
import org.apache.giraph.io.superstep_output.SuperstepOutput;
import org.apache.giraph.master.MasterInfo;
import org.apache.giraph.metrics.GiraphTimerContext;
import org.apache.giraph.partition.PartitionOwner;
import org.apache.giraph.partition.PartitionStats;
import org.apache.giraph.partition.PartitionStore;
import org.apache.giraph.worker.WorkerAggregatorHandler;
import org.apache.giraph.worker.WorkerContext;
import org.apache.giraph.worker.WorkerInfo;
import org.apache.giraph.worker.WorkerObserver;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;

import java.io.IOException;
import java.util.Collection;
import java.util.List;


@SuppressWarnings("rawtypes")
public interface CentralizedServiceWorker<I extends WritableComparable,
  V extends Writable, E extends Writable>
  extends CentralizedService<I, V, E> {
  
  FinishedSuperstepStats setup();

  
  WorkerInfo getWorkerInfo();

  
  WorkerClient<I, V, E> getWorkerClient();

  
  WorkerContext getWorkerContext();

  
  WorkerObserver[] getWorkerObservers();

  
  PartitionStore<I, V, E> getPartitionStore();

  
  void storeCheckpoint() throws IOException;

  
  VertexEdgeCount loadCheckpoint(long superstep) throws IOException;

  
  Collection<? extends PartitionOwner> startSuperstep();

  
  FinishedSuperstepStats finishSuperstep(
      List<PartitionStats> partitionStatsList,
      GiraphTimerContext superstepTimerContext);

  
  int getPartitionId(I vertexId);

  
  boolean hasPartition(Integer partitionId);

  
  PartitionOwner getVertexPartitionOwner(I vertexId);

  
  Iterable<? extends PartitionOwner> getPartitionOwners();

  
  void exchangeVertexPartitions(
      Collection<? extends PartitionOwner> masterSetPartitionOwners);

  
  MasterInfo getMasterInfo();

  
  GraphTaskManager<I, V, E> getGraphTaskManager();

  
  void failureCleanup();

  
  ServerData<I, V, E> getServerData();

  
  WorkerAggregatorHandler getAggregatorHandler();

  
  void prepareSuperstep();

  
  SuperstepOutput<I, V, E> getSuperstepOutput();

  
  void cleanup(FinishedSuperstepStats finishedSuperstepStats)
    throws IOException, InterruptedException;

  
  GlobalStats getGlobalStats();
}

<code block>


package org.apache.giraph.partition;

import static org.apache.giraph.conf.GiraphConstants.MAX_PARTITIONS_IN_MEMORY;
import static org.apache.giraph.conf.GiraphConstants.PARTITIONS_DIRECTORY;
import static org.apache.giraph.conf.GiraphConstants.USER_PARTITION_COUNT;
import static org.apache.giraph.conf.GiraphConstants.USE_OUT_OF_CORE_GRAPH;
import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertFalse;
import static org.junit.Assert.assertTrue;

import java.io.File;
import java.io.IOException;
import java.util.Iterator;
import java.util.Random;
import java.util.concurrent.ExecutorCompletionService;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.atomic.AtomicInteger;

import org.apache.commons.io.FileUtils;
import org.apache.giraph.bsp.BspService;
import org.apache.giraph.bsp.CentralizedServiceWorker;
import org.apache.giraph.conf.GiraphConfiguration;
import org.apache.giraph.conf.GiraphConstants;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.giraph.edge.EdgeFactory;
import org.apache.giraph.graph.BasicComputation;
import org.apache.giraph.graph.Vertex;
import org.apache.giraph.io.formats.IdWithValueTextOutputFormat;
import org.apache.giraph.io.formats.JsonLongDoubleFloatDoubleVertexInputFormat;
import org.apache.giraph.utils.InternalVertexRunner;
import org.apache.giraph.utils.NoOpComputation;
import org.apache.giraph.utils.UnsafeByteArrayInputStream;
import org.apache.giraph.utils.UnsafeByteArrayOutputStream;
import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.FloatWritable;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapreduce.Mapper;
import org.junit.Before;
import org.junit.Test;
import org.mockito.Mockito;

import com.google.common.collect.Iterables;
import com.google.common.io.Files;


@SuppressWarnings("unchecked")
public class TestPartitionStores {
  private ImmutableClassesGiraphConfiguration<IntWritable, IntWritable,
      NullWritable> conf;
  private Mapper<?, ?, ?, ?>.Context context;

  
  private static final int NUM_OF_VERTEXES_PER_THREAD = 10;
  private static final int NUM_OF_EDGES_PER_VERTEX = 5;
  private static final int NUM_OF_THREADS = 10;
  private static final int NUM_OF_PARTITIONS = 3;

  public static class MyComputation extends NoOpComputation<IntWritable,
      IntWritable, NullWritable, IntWritable> { }

  private Partition<IntWritable, IntWritable, NullWritable> createPartition(
      ImmutableClassesGiraphConfiguration<IntWritable, IntWritable,
          NullWritable> conf,
      Integer id,
      Vertex<IntWritable, IntWritable, NullWritable>... vertices) {
    Partition<IntWritable, IntWritable, NullWritable> partition =
        conf.createPartition(id, context);
    for (Vertex<IntWritable, IntWritable, NullWritable> v : vertices) {
      partition.putVertex(v);
    }
    return partition;
  }

  @Before
  public void setUp() {
    GiraphConfiguration configuration = new GiraphConfiguration();
    configuration.setComputationClass(MyComputation.class);
    conf = new ImmutableClassesGiraphConfiguration<IntWritable, IntWritable,
        NullWritable>(configuration);
    context = Mockito.mock(Mapper.Context.class);
  }

  @Test
  public void testSimplePartitionStore() {
    PartitionStore<IntWritable, IntWritable, NullWritable>
        partitionStore = new SimplePartitionStore<IntWritable, IntWritable,
                NullWritable>(conf, context);
    testReadWrite(partitionStore, conf);
    partitionStore.shutdown();
  }

  @Test
  public void testUnsafePartitionSerializationClass() throws IOException {
    conf.setPartitionClass(ByteArrayPartition.class);
    Vertex<IntWritable, IntWritable, NullWritable> v1 =
        conf.createVertex();
    v1.initialize(new IntWritable(1), new IntWritable(1));
    Vertex<IntWritable, IntWritable, NullWritable> v2 = conf.createVertex();
    v2.initialize(new IntWritable(2), new IntWritable(2));
    Vertex<IntWritable, IntWritable, NullWritable> v3 = conf.createVertex();
    v3.initialize(new IntWritable(3), new IntWritable(3));
    Vertex<IntWritable, IntWritable, NullWritable> v4 = conf.createVertex();
    v4.initialize(new IntWritable(4), new IntWritable(4));
    Vertex<IntWritable, IntWritable, NullWritable> v5 = conf.createVertex();
    v5.initialize(new IntWritable(5), new IntWritable(5));
    Vertex<IntWritable, IntWritable, NullWritable> v6 = conf.createVertex();
    v6.initialize(new IntWritable(6), new IntWritable(6));
    Vertex<IntWritable, IntWritable, NullWritable> v7 = conf.createVertex();
    v7.initialize(new IntWritable(7), new IntWritable(7));

    Partition<IntWritable, IntWritable, NullWritable> partition =
        createPartition(conf, 3, v1, v2, v3, v4, v5, v6, v7);
    assertEquals(3, partition.getId());
    assertEquals(0, partition.getEdgeCount());
    assertEquals(7, partition.getVertexCount());
    UnsafeByteArrayOutputStream outputStream = new
        UnsafeByteArrayOutputStream();
    partition.write(outputStream);
    UnsafeByteArrayInputStream inputStream = new UnsafeByteArrayInputStream(
        outputStream.getByteArray(), 0, outputStream.getPos());
    Partition<IntWritable, IntWritable, NullWritable> deserializatedPartition =
        conf.createPartition(-1, context);
    deserializatedPartition.readFields(inputStream);

    assertEquals(3, deserializatedPartition.getId());
    assertEquals(0, deserializatedPartition.getEdgeCount());
    assertEquals(7, deserializatedPartition.getVertexCount());
  }
  
  @Test
  public void testDiskBackedPartitionStoreWithByteArrayPartition()
    throws IOException {

    File directory = Files.createTempDir();
    GiraphConstants.PARTITIONS_DIRECTORY.set(
        conf, new File(directory, "giraph_partitions").toString());
    GiraphConstants.USE_OUT_OF_CORE_GRAPH.set(conf, true);
    GiraphConstants.MAX_PARTITIONS_IN_MEMORY.set(conf, 1);
    conf.setPartitionClass(ByteArrayPartition.class);

    CentralizedServiceWorker<IntWritable, IntWritable, NullWritable>
      serviceWorker = Mockito.mock(CentralizedServiceWorker.class);
    Mockito.when(serviceWorker.getSuperstep()).thenReturn(
      BspService.INPUT_SUPERSTEP);

    PartitionStore<IntWritable, IntWritable, NullWritable> partitionStore =
        new DiskBackedPartitionStore<IntWritable, IntWritable, NullWritable>(
            conf, context, serviceWorker);
    testReadWrite(partitionStore, conf);
    partitionStore.shutdown();
    FileUtils.deleteDirectory(directory);
  }

  @Test
  public void testDiskBackedPartitionStore() throws IOException {
    File directory = Files.createTempDir();
    GiraphConstants.PARTITIONS_DIRECTORY.set(
        conf, new File(directory, "giraph_partitions").toString());
    GiraphConstants.USE_OUT_OF_CORE_GRAPH.set(conf, true);
    GiraphConstants.MAX_PARTITIONS_IN_MEMORY.set(conf, 1);

    CentralizedServiceWorker<IntWritable, IntWritable, NullWritable>
    serviceWorker = Mockito.mock(CentralizedServiceWorker.class);

    Mockito.when(serviceWorker.getSuperstep()).thenReturn(
      BspService.INPUT_SUPERSTEP);

    PartitionStore<IntWritable, IntWritable, NullWritable> partitionStore =
        new DiskBackedPartitionStore<IntWritable, IntWritable, NullWritable>(
            conf, context, serviceWorker);
    testReadWrite(partitionStore, conf);
    partitionStore.shutdown();

    GiraphConstants.MAX_PARTITIONS_IN_MEMORY.set(conf, 2);
    partitionStore = new DiskBackedPartitionStore<IntWritable,
            IntWritable, NullWritable>(conf, context, serviceWorker);
    testReadWrite(partitionStore, conf);
    partitionStore.shutdown();
    FileUtils.deleteDirectory(directory);
  }

  @Test
  public void testDiskBackedPartitionStoreWithByteArrayComputation()
    throws Exception {

    Iterable<String> results;
    String[] graph =
    {
      "[1,0,[]]", "[2,0,[]]", "[3,0,[]]", "[4,0,[]]", "[5,0,[]]",
      "[6,0,[]]", "[7,0,[]]", "[8,0,[]]", "[9,0,[]]", "[10,0,[]]"
    };
    String[] expected =
    {
      "1\t0", "2\t0", "3\t0", "4\t0", "5\t0",
      "6\t0", "7\t0", "8\t0", "9\t0", "10\t0"
    };

    USE_OUT_OF_CORE_GRAPH.set(conf, true);
    MAX_PARTITIONS_IN_MEMORY.set(conf, 1);
    USER_PARTITION_COUNT.set(conf, 10);

    File directory = Files.createTempDir();
    PARTITIONS_DIRECTORY.set(conf,
      new File(directory, "giraph_partitions").toString());

    conf.setPartitionClass(ByteArrayPartition.class);
    conf.setComputationClass(EmptyComputation.class);
    conf.setVertexInputFormatClass(JsonLongDoubleFloatDoubleVertexInputFormat.class);
    conf.setVertexOutputFormatClass(IdWithValueTextOutputFormat.class);

    results = InternalVertexRunner.run(conf, graph);
    checkResults(results, expected);
    FileUtils.deleteDirectory(directory);
  }

  @Test
  public void testDiskBackedPartitionStoreMT() throws Exception {
    GiraphConstants.STATIC_GRAPH.set(conf, false);
    testMultiThreaded();
  }

  

  private void testMultiThreaded() throws Exception {
    final AtomicInteger vertexCounter = new AtomicInteger(0);
    ExecutorService pool = Executors.newFixedThreadPool(NUM_OF_THREADS);
    ExecutorCompletionService<Boolean> executor =
      new ExecutorCompletionService<Boolean>(pool);

    File directory = Files.createTempDir();
    GiraphConstants.PARTITIONS_DIRECTORY.set(
        conf, new File(directory, "giraph_partitions").toString());
    GiraphConstants.USE_OUT_OF_CORE_GRAPH.set(conf, true);
    GiraphConstants.MAX_PARTITIONS_IN_MEMORY.set(conf, 1);

    CentralizedServiceWorker<IntWritable, IntWritable, NullWritable>
    serviceWorker = Mockito.mock(CentralizedServiceWorker.class);

    Mockito.when(serviceWorker.getSuperstep()).thenReturn(
      BspService.INPUT_SUPERSTEP);

    PartitionStore<IntWritable, IntWritable, NullWritable> store =
        new DiskBackedPartitionStore<IntWritable, IntWritable, NullWritable>(
            conf, context, serviceWorker);

    
    for (int i = 0; i < NUM_OF_THREADS; ++i) {
      int partitionId = i % NUM_OF_PARTITIONS;
      Worker worker =
        new Worker(vertexCounter, store, partitionId, conf);
      executor.submit(worker, new Boolean(true));
    }
    for (int i = 0; i < NUM_OF_THREADS; ++i)
      executor.take();
    pool.shutdownNow();

    
    int totalVertexes = 0;
    int totalEdges = 0;
    Partition<IntWritable, IntWritable, NullWritable> partition;
    for (int i = 0; i < NUM_OF_PARTITIONS; ++i) {
      totalVertexes += store.getPartitionVertexCount(i);
      totalEdges += store.getPartitionEdgeCount(i);
    }
    assert vertexCounter.get() == NUM_OF_THREADS * NUM_OF_VERTEXES_PER_THREAD;
    assert totalVertexes == NUM_OF_THREADS * NUM_OF_VERTEXES_PER_THREAD;
    assert totalEdges == totalVertexes * NUM_OF_EDGES_PER_VERTEX;

    
    int expected = 0;
    for (int i = 0; i < NUM_OF_VERTEXES_PER_THREAD * NUM_OF_VERTEXES_PER_THREAD; ++i) {
      expected += i;
    }
    int totalValues = 0;
    for (int i = 0; i < NUM_OF_PARTITIONS; ++i) {
      partition = store.getOrCreatePartition(i);
      Iterator<Vertex<IntWritable, IntWritable, NullWritable>> vertexes = 
        partition.iterator();

      while (vertexes.hasNext()) {
        Vertex<IntWritable, IntWritable, NullWritable> v = vertexes.next();
        totalValues += v.getId().get();
      }
      store.putPartition(partition);
    }
    assert totalValues == expected;
    
    store.shutdown();
  }

  @Test
  public void testDiskBackedPartitionStoreComputation() throws Exception {
    Iterable<String> results;
    String[] graph =
    {
      "[1,0,[]]", "[2,0,[]]", "[3,0,[]]", "[4,0,[]]", "[5,0,[]]",
      "[6,0,[]]", "[7,0,[]]", "[8,0,[]]", "[9,0,[]]", "[10,0,[]]"
    };
    String[] expected =
    {
      "1\t0", "2\t0", "3\t0", "4\t0", "5\t0",
      "6\t0", "7\t0", "8\t0", "9\t0", "10\t0"
    };

    USE_OUT_OF_CORE_GRAPH.set(conf, true);
    MAX_PARTITIONS_IN_MEMORY.set(conf, 1);
    USER_PARTITION_COUNT.set(conf, 10);

    File directory = Files.createTempDir();
    PARTITIONS_DIRECTORY.set(conf,
      new File(directory, "giraph_partitions").toString());

    conf.setComputationClass(EmptyComputation.class);
    conf.setVertexInputFormatClass(JsonLongDoubleFloatDoubleVertexInputFormat.class);
    conf.setVertexOutputFormatClass(IdWithValueTextOutputFormat.class);

    results = InternalVertexRunner.run(conf, graph);
    checkResults(results, expected);
    FileUtils.deleteDirectory(directory);
  }

  
  public void testReadWrite(
      PartitionStore<IntWritable, IntWritable,
          NullWritable> partitionStore,
      ImmutableClassesGiraphConfiguration<IntWritable, IntWritable,
          NullWritable> conf) {
    Vertex<IntWritable, IntWritable, NullWritable> v1 = conf.createVertex();
    v1.initialize(new IntWritable(1), new IntWritable(1));
    Vertex<IntWritable, IntWritable, NullWritable> v2 = conf.createVertex();
    v2.initialize(new IntWritable(2), new IntWritable(2));
    Vertex<IntWritable, IntWritable, NullWritable> v3 = conf.createVertex();
    v3.initialize(new IntWritable(3), new IntWritable(3));
    Vertex<IntWritable, IntWritable, NullWritable> v4 = conf.createVertex();
    v4.initialize(new IntWritable(4), new IntWritable(4));
    Vertex<IntWritable, IntWritable, NullWritable> v5 = conf.createVertex();
    v5.initialize(new IntWritable(5), new IntWritable(5));
    Vertex<IntWritable, IntWritable, NullWritable> v6 = conf.createVertex();
    v6.initialize(new IntWritable(7), new IntWritable(7));
    Vertex<IntWritable, IntWritable, NullWritable> v7 = conf.createVertex();
    v7.initialize(new IntWritable(7), new IntWritable(7));
    v7.addEdge(EdgeFactory.create(new IntWritable(1)));
    v7.addEdge(EdgeFactory.create(new IntWritable(2)));

    partitionStore.addPartition(createPartition(conf, 1, v1, v2));
    partitionStore.addPartition(createPartition(conf, 2, v3));
    partitionStore.addPartition(createPartition(conf, 2, v4));
    partitionStore.addPartition(createPartition(conf, 3, v5));
    partitionStore.addPartition(createPartition(conf, 1, v6));
    partitionStore.addPartition(createPartition(conf, 4, v7));

    Partition<IntWritable, IntWritable, NullWritable> partition1 =
        partitionStore.getOrCreatePartition(1);
    partitionStore.putPartition(partition1);
    Partition<IntWritable, IntWritable, NullWritable> partition2 =
        partitionStore.getOrCreatePartition(2);
    partitionStore.putPartition(partition2);
    Partition<IntWritable, IntWritable, NullWritable> partition3 =
        partitionStore.removePartition(3);
    Partition<IntWritable, IntWritable, NullWritable> partition4 =
        partitionStore.getOrCreatePartition(4);
    partitionStore.putPartition(partition4);

    assertEquals(3, partitionStore.getNumPartitions());
    assertEquals(3, Iterables.size(partitionStore.getPartitionIds()));
    int partitionsNumber = 0;
    for (Integer partitionId : partitionStore.getPartitionIds()) {
      Partition<IntWritable, IntWritable, NullWritable> p =
          partitionStore.getOrCreatePartition(partitionId);
      partitionStore.putPartition(p);
      partitionsNumber++;
    }
    Partition<IntWritable, IntWritable, NullWritable> partition;
    assertEquals(3, partitionsNumber);
    assertTrue(partitionStore.hasPartition(1));
    assertTrue(partitionStore.hasPartition(2));
    assertFalse(partitionStore.hasPartition(3));
    assertTrue(partitionStore.hasPartition(4));
    assertEquals(3, partitionStore.getPartitionVertexCount(1));
    assertEquals(2, partitionStore.getPartitionVertexCount(2));
    assertEquals(1, partitionStore.getPartitionVertexCount(4));
    assertEquals(2, partitionStore.getPartitionEdgeCount(4));
    partitionStore.deletePartition(2);
    assertEquals(2, partitionStore.getNumPartitions());
  }

  
  private void checkResults(Iterable<String> results, String[] expected) {
    Iterator<String> result = results.iterator();

    assert results != null;

    while(result.hasNext()) {
      String  resultStr = result.next();
      boolean found = false;

      for (int j = 0; j < expected.length; ++j) {
        if (expected[j].equals(resultStr)) {
          found = true;
        }
      }

      assert found;
    }
  }

  
  public static class EmptyComputation
    extends BasicComputation<LongWritable, DoubleWritable, FloatWritable,
      LongWritable> {

    @Override
    public void compute(
      Vertex<LongWritable, DoubleWritable,FloatWritable> vertex,
      Iterable<LongWritable> messages) throws IOException {

      vertex.voteToHalt();
    }
  }

  @Test
  public void testEdgeCombineWithSimplePartition() throws IOException {
    testEdgeCombine(SimplePartition.class);
  }
 
  @Test
  public void testEdgeCombineWithByteArrayPartition() throws IOException {
    testEdgeCombine(ByteArrayPartition.class);
  }
 
  private void testEdgeCombine(Class<? extends Partition> partitionClass)
      throws IOException {
    Vertex<IntWritable, IntWritable, NullWritable> v1 = conf.createVertex();
    v1.initialize(new IntWritable(1), new IntWritable(1));
    Vertex<IntWritable, IntWritable, NullWritable> v2 = conf.createVertex();
    v2.initialize(new IntWritable(2), new IntWritable(2));
    Vertex<IntWritable, IntWritable, NullWritable> v3 = conf.createVertex();
    v3.initialize(new IntWritable(3), new IntWritable(3));
    Vertex<IntWritable, IntWritable, NullWritable> v1e2 = conf.createVertex();
    v1e2.initialize(new IntWritable(1), new IntWritable(1));
    v1e2.addEdge(EdgeFactory.create(new IntWritable(2)));
    Vertex<IntWritable, IntWritable, NullWritable> v1e3 = conf.createVertex();
    v1e3.initialize(new IntWritable(1), new IntWritable(1));
    v1e3.addEdge(EdgeFactory.create(new IntWritable(3)));

    GiraphConfiguration newconf = new GiraphConfiguration(conf);
    newconf.setPartitionClass(partitionClass);
    Partition<IntWritable, IntWritable, NullWritable> partition =
        (new ImmutableClassesGiraphConfiguration<IntWritable, IntWritable,
            NullWritable>(newconf)).createPartition(1, context);
    assertEquals(partitionClass, partition.getClass());
    partition.putVertex(v1);
    partition.putVertex(v2);
    partition.putVertex(v3);
    assertEquals(3, partition.getVertexCount());
    assertEquals(0, partition.getEdgeCount());
    partition.putOrCombine(v1e2);
    assertEquals(3, partition.getVertexCount());
    assertEquals(1, partition.getEdgeCount());
    partition.putOrCombine(v1e3);
    assertEquals(3, partition.getVertexCount());
    assertEquals(2, partition.getEdgeCount());
    v1 = partition.getVertex(new IntWritable(1));
    assertEquals(new IntWritable(1), v1.getId());
    assertEquals(new IntWritable(1), v1.getValue());
    assertEquals(2, v1.getNumEdges());
  }

  private class Worker implements Runnable {

    private final AtomicInteger vertexCounter;
    private final PartitionStore<IntWritable, IntWritable, NullWritable>
      partitionStore;
    private final int partitionId;
    private final ImmutableClassesGiraphConfiguration<IntWritable, IntWritable,
            NullWritable> conf;

    public Worker(AtomicInteger vertexCounter,
        PartitionStore<IntWritable, IntWritable, NullWritable> partitionStore,
        int partitionId,
        ImmutableClassesGiraphConfiguration<IntWritable, IntWritable,
          NullWritable> conf) {

      this.vertexCounter = vertexCounter;
      this.partitionStore = partitionStore;
      this.partitionId = partitionId;
      this.conf = conf;
    }

    public void run() {
      for (int i = 0; i < NUM_OF_VERTEXES_PER_THREAD; ++i) {
        int id = vertexCounter.getAndIncrement();
        Vertex<IntWritable, IntWritable, NullWritable> v = conf.createVertex();
        v.initialize(new IntWritable(id), new IntWritable(id));

        Partition<IntWritable, IntWritable, NullWritable> partition =
          partitionStore.getOrCreatePartition(partitionId);

        Random rand = new Random(id);
        for (int j = 0; j < NUM_OF_EDGES_PER_VERTEX; ++j) {
          int dest = rand.nextInt(id + 1);
          v.addEdge(EdgeFactory.create(new IntWritable(dest)));
        }

        partition.putVertex(v);
        partitionStore.putPartition(partition);
      }
    }
  }
}

<code block>


package org.apache.giraph.comm;

import org.apache.giraph.comm.netty.NettyClient;
import org.apache.giraph.comm.netty.NettyServer;
import org.apache.giraph.comm.netty.handler.WorkerRequestServerHandler;
import org.apache.giraph.comm.requests.SendPartitionMutationsRequest;
import org.apache.giraph.comm.requests.SendVertexRequest;
import org.apache.giraph.comm.requests.SendWorkerMessagesRequest;
import org.apache.giraph.comm.requests.SendWorkerOneMessageToManyRequest;
import org.apache.giraph.conf.GiraphConfiguration;
import org.apache.giraph.conf.GiraphConstants;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.giraph.edge.Edge;
import org.apache.giraph.edge.EdgeFactory;
import org.apache.giraph.factories.TestMessageValueFactory;
import org.apache.giraph.graph.Vertex;
import org.apache.giraph.graph.VertexMutations;
import org.apache.giraph.metrics.GiraphMetrics;
import org.apache.giraph.partition.Partition;
import org.apache.giraph.partition.PartitionStore;
import org.apache.giraph.utils.ByteArrayOneMessageToManyIds;
import org.apache.giraph.utils.VertexIdMessages;
import org.apache.giraph.utils.ByteArrayVertexIdMessages;
import org.apache.giraph.utils.ExtendedDataOutput;
import org.apache.giraph.utils.IntNoOpComputation;
import org.apache.giraph.utils.MockUtils;
import org.apache.giraph.utils.PairList;
import org.apache.giraph.worker.WorkerInfo;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.mapreduce.Mapper.Context;
import org.junit.Before;
import org.junit.Test;

import com.google.common.collect.Lists;
import com.google.common.collect.Maps;

import java.io.IOException;
import java.util.Map;
import java.util.Map.Entry;
import java.util.concurrent.ConcurrentMap;

import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertTrue;
import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;


@SuppressWarnings("unchecked")
public class RequestTest {
  
  private ImmutableClassesGiraphConfiguration conf;
  
  private ServerData<IntWritable, IntWritable, IntWritable> serverData;
  
  private NettyServer server;
  
  private NettyClient client;
  
  private WorkerInfo workerInfo;

  @Before
  public void setUp() throws IOException {
    
    GiraphConfiguration tmpConf = new GiraphConfiguration();
    GiraphConstants.COMPUTATION_CLASS.set(tmpConf, IntNoOpComputation.class);
    conf = new ImmutableClassesGiraphConfiguration(tmpConf);

    @SuppressWarnings("rawtypes")
    Context context = mock(Context.class);
    when(context.getConfiguration()).thenReturn(conf);

    
    serverData = MockUtils.createNewServerData(conf, context);
    serverData.prepareSuperstep();
    workerInfo = new WorkerInfo();
    server = new NettyServer(conf,
        new WorkerRequestServerHandler.Factory(serverData), workerInfo,
            context, new MockExceptionHandler());
    server.start();

    workerInfo.setInetSocketAddress(server.getMyAddress());
    client = new NettyClient(context, conf, new WorkerInfo(),
        new MockExceptionHandler());
    client.connectAllAddresses(
        Lists.<WorkerInfo>newArrayList(workerInfo));
  }

  @Test
  public void sendVertexPartition() throws IOException {
    
    int partitionId = 13;
    Partition<IntWritable, IntWritable, IntWritable> partition =
        conf.createPartition(partitionId, null);
    for (int i = 0; i < 10; ++i) {
      Vertex vertex = conf.createVertex();
      vertex.initialize(new IntWritable(i), new IntWritable(i));
      partition.putVertex(vertex);
    }

    
    SendVertexRequest<IntWritable, IntWritable, IntWritable> request =
      new SendVertexRequest<IntWritable, IntWritable, IntWritable>(partition);
    client.sendWritableRequest(workerInfo.getTaskId(), request);
    client.waitAllRequests();

    
    client.stop();
    server.stop();

    
    PartitionStore<IntWritable, IntWritable, IntWritable> partitionStore =
        serverData.getPartitionStore();
    assertTrue(partitionStore.hasPartition(partitionId));
    int total = 0;
    Partition<IntWritable, IntWritable, IntWritable> partition2 =
        partitionStore.getOrCreatePartition(partitionId);
    for (Vertex<IntWritable, IntWritable, IntWritable> vertex : partition2) {
      total += vertex.getId().get();
    }
    partitionStore.putPartition(partition2);
    assertEquals(total, 45);
    partitionStore.shutdown();
  }

  @Test
  public void sendWorkerMessagesRequest() throws IOException {
    
    PairList<Integer, VertexIdMessages<IntWritable,
            IntWritable>>
        dataToSend = new PairList<>();
    dataToSend.initialize();
    int partitionId = 0;
    ByteArrayVertexIdMessages<IntWritable,
            IntWritable> vertexIdMessages =
        new ByteArrayVertexIdMessages<>(
            new TestMessageValueFactory<>(IntWritable.class));
    vertexIdMessages.setConf(conf);
    vertexIdMessages.initialize();
    dataToSend.add(partitionId, vertexIdMessages);
    for (int i = 1; i < 7; ++i) {
      IntWritable vertexId = new IntWritable(i);
      for (int j = 0; j < i; ++j) {
        vertexIdMessages.add(vertexId, new IntWritable(j));
      }
    }

    
    SendWorkerMessagesRequest<IntWritable, IntWritable> request =
      new SendWorkerMessagesRequest<>(dataToSend);
    request.setConf(conf);

    client.sendWritableRequest(workerInfo.getTaskId(), request);
    client.waitAllRequests();

    
    client.stop();
    server.stop();

    
    Iterable<IntWritable> vertices =
        serverData.getIncomingMessageStore().getPartitionDestinationVertices(0);
    int keySum = 0;
    int messageSum = 0;
    for (IntWritable vertexId : vertices) {
      keySum += vertexId.get();
      Iterable<IntWritable> messages =
          serverData.<IntWritable>getIncomingMessageStore().getVertexMessages(
              vertexId);
      synchronized (messages) {
        for (IntWritable message : messages) {
          messageSum += message.get();
        }
      }
    }
    assertEquals(21, keySum);
    assertEquals(35, messageSum);
  }

  @Test
  public void sendWorkerIndividualMessagesRequest() throws IOException {
    
    ByteArrayOneMessageToManyIds<IntWritable, IntWritable>
        dataToSend = new ByteArrayOneMessageToManyIds<>(new
        TestMessageValueFactory<>(IntWritable.class));
    dataToSend.setConf(conf);
    dataToSend.initialize();
    ExtendedDataOutput output = conf.createExtendedDataOutput();
    for (int i = 1; i <= 7; ++i) {
      IntWritable vertexId = new IntWritable(i);
      vertexId.write(output);
    }
    dataToSend.add(output.getByteArray(), output.getPos(), 7, new IntWritable(1));

    
    SendWorkerOneMessageToManyRequest<IntWritable, IntWritable> request =
      new SendWorkerOneMessageToManyRequest<>(dataToSend, conf);
    client.sendWritableRequest(workerInfo.getTaskId(), request);
    client.waitAllRequests();

    
    client.stop();
    server.stop();

    
    Iterable<IntWritable> vertices =
        serverData.getIncomingMessageStore().getPartitionDestinationVertices(0);
    int keySum = 0;
    int messageSum = 0;
    for (IntWritable vertexId : vertices) {
      keySum += vertexId.get();
      Iterable<IntWritable> messages =
          serverData.<IntWritable>getIncomingMessageStore().getVertexMessages(
              vertexId);
      synchronized (messages) {
        for (IntWritable message : messages) {
          messageSum += message.get();
        }
      }
    }
    assertEquals(28, keySum);
    assertEquals(7, messageSum);
  }

  @Test
  public void sendPartitionMutationsRequest() throws IOException {
    
    int partitionId = 19;
    Map<IntWritable, VertexMutations<IntWritable, IntWritable,
    IntWritable>> vertexIdMutations =
        Maps.newHashMap();
    for (int i = 0; i < 11; ++i) {
      VertexMutations<IntWritable, IntWritable, IntWritable> mutations =
          new VertexMutations<IntWritable, IntWritable, IntWritable>();
      for (int j = 0; j < 3; ++j) {
        Vertex vertex = conf.createVertex();
        vertex.initialize(new IntWritable(i), new IntWritable(j));
        mutations.addVertex(vertex);
      }
      for (int j = 0; j < 2; ++j) {
        mutations.removeVertex();
      }
      for (int j = 0; j < 5; ++j) {
        Edge<IntWritable, IntWritable> edge =
            EdgeFactory.create(new IntWritable(i), new IntWritable(2 * j));
        mutations.addEdge(edge);
      }
      for (int j = 0; j < 7; ++j) {
        mutations.removeEdge(new IntWritable(j));
      }
      vertexIdMutations.put(new IntWritable(i), mutations);
    }

    
    SendPartitionMutationsRequest<IntWritable, IntWritable, IntWritable>
        request = new SendPartitionMutationsRequest<IntWritable, IntWritable,
        IntWritable>(partitionId,
        vertexIdMutations);
    GiraphMetrics.init(conf);
    client.sendWritableRequest(workerInfo.getTaskId(), request);
    client.waitAllRequests();

    
    client.stop();
    server.stop();

    
    ConcurrentMap<IntWritable,
        VertexMutations<IntWritable, IntWritable, IntWritable>>
        inVertexIdMutations =
        serverData.getPartitionMutations().get(partitionId);
    int keySum = 0;
    for (Entry<IntWritable,
        VertexMutations<IntWritable, IntWritable, IntWritable>> entry :
        inVertexIdMutations
        .entrySet()) {
      synchronized (entry.getValue()) {
        keySum += entry.getKey().get();
        int vertexValueSum = 0;
        for (Vertex<IntWritable, IntWritable, IntWritable> vertex : entry
            .getValue().getAddedVertexList()) {
          vertexValueSum += vertex.getValue().get();
        }
        assertEquals(3, vertexValueSum);
        assertEquals(2, entry.getValue().getRemovedVertexCount());
        int removeEdgeValueSum = 0;
        for (Edge<IntWritable, IntWritable> edge : entry.getValue()
            .getAddedEdgeList()) {
          removeEdgeValueSum += edge.getValue().get();
        }
        assertEquals(20, removeEdgeValueSum);
      }
    }
    assertEquals(55, keySum);
  }
}
<code block>


package org.apache.giraph.comm.messages;

import java.io.IOException;
import java.util.Iterator;

import junit.framework.Assert;

import org.apache.giraph.bsp.CentralizedServiceWorker;
import org.apache.giraph.combiner.FloatSumMessageCombiner;
import org.apache.giraph.comm.messages.primitives.IntByteArrayMessageStore;
import org.apache.giraph.comm.messages.primitives.IntFloatMessageStore;
import org.apache.giraph.conf.GiraphConfiguration;
import org.apache.giraph.conf.GiraphConstants;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.giraph.factories.TestMessageValueFactory;
import org.apache.giraph.graph.BasicComputation;
import org.apache.giraph.graph.Vertex;
import org.apache.giraph.partition.Partition;
import org.apache.giraph.partition.PartitionStore;
import org.apache.giraph.utils.ByteArrayVertexIdMessages;
import org.apache.hadoop.io.FloatWritable;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Writable;
import org.junit.Before;
import org.junit.Test;
import org.mockito.Mockito;
import org.mockito.invocation.InvocationOnMock;
import org.mockito.stubbing.Answer;

import com.google.common.collect.Iterables;
import com.google.common.collect.Lists;

public class TestIntFloatPrimitiveMessageStores {
  private static final int NUM_PARTITIONS = 2;
  private static CentralizedServiceWorker<IntWritable, Writable, Writable>
    service;
  private static ImmutableClassesGiraphConfiguration<IntWritable, Writable,
      Writable> conf;

  @Before
  public void prepare() throws IOException {
    service = Mockito.mock(CentralizedServiceWorker.class);
    Mockito.when(
        service.getPartitionId(Mockito.any(IntWritable.class))).thenAnswer(
        new Answer<Integer>() {
          @Override
          public Integer answer(InvocationOnMock invocation) {
            IntWritable vertexId = (IntWritable) invocation.getArguments()[0];
            return vertexId.get() % NUM_PARTITIONS;
          }
        }
    );
    PartitionStore partitionStore = Mockito.mock(PartitionStore.class);
    Mockito.when(service.getPartitionStore()).thenReturn(partitionStore);
    Mockito.when(partitionStore.getPartitionIds()).thenReturn(
        Lists.newArrayList(0, 1));
    Partition partition = Mockito.mock(Partition.class);
    Mockito.when(partition.getVertexCount()).thenReturn(Long.valueOf(1));
    Mockito.when(partitionStore.getOrCreatePartition(0)).thenReturn(partition);
    Mockito.when(partitionStore.getOrCreatePartition(1)).thenReturn(partition);

    GiraphConfiguration initConf = new GiraphConfiguration();
    initConf.setComputationClass(IntFloatNoOpComputation.class);
    conf = new ImmutableClassesGiraphConfiguration(initConf);
  }

  private static class IntFloatNoOpComputation extends
      BasicComputation<IntWritable, NullWritable, NullWritable,
          FloatWritable> {
    @Override
    public void compute(Vertex<IntWritable, NullWritable, NullWritable> vertex,
        Iterable<FloatWritable> messages) throws IOException {
    }
  }

  private static ByteArrayVertexIdMessages<IntWritable, FloatWritable>
  createIntFloatMessages() {
    ByteArrayVertexIdMessages<IntWritable, FloatWritable> messages =
        new ByteArrayVertexIdMessages<IntWritable, FloatWritable>(
            new TestMessageValueFactory<FloatWritable>(FloatWritable.class));
    messages.setConf(conf);
    messages.initialize();
    return messages;
  }

  private static void insertIntFloatMessages(
      MessageStore<IntWritable, FloatWritable> messageStore) throws
      IOException {
    ByteArrayVertexIdMessages<IntWritable, FloatWritable> messages =
        createIntFloatMessages();
    messages.add(new IntWritable(0), new FloatWritable(1));
    messages.add(new IntWritable(2), new FloatWritable(3));
    messages.add(new IntWritable(0), new FloatWritable(4));
    messageStore.addPartitionMessages(0, messages);
    messages = createIntFloatMessages();
    messages.add(new IntWritable(1), new FloatWritable(1));
    messages.add(new IntWritable(1), new FloatWritable(3));
    messages.add(new IntWritable(1), new FloatWritable(4));
    messageStore.addPartitionMessages(1, messages);
    messages = createIntFloatMessages();
    messages.add(new IntWritable(0), new FloatWritable(5));
    messageStore.addPartitionMessages(0, messages);
  }

  @Test
  public void testIntFloatMessageStore() throws IOException {
    IntFloatMessageStore messageStore =
        new IntFloatMessageStore(service, new FloatSumMessageCombiner());
    insertIntFloatMessages(messageStore);

    Iterable<FloatWritable> m0 =
        messageStore.getVertexMessages(new IntWritable(0));
    Assert.assertEquals(1, Iterables.size(m0));
    Assert.assertEquals((float) 10.0, m0.iterator().next().get());
    Iterable<FloatWritable> m1 =
        messageStore.getVertexMessages(new IntWritable(1));
    Assert.assertEquals(1, Iterables.size(m1));
    Assert.assertEquals((float) 8.0, m1.iterator().next().get());
    Iterable<FloatWritable> m2 =
        messageStore.getVertexMessages(new IntWritable(2));
    Assert.assertEquals(1, Iterables.size(m2));
    Assert.assertEquals((float) 3.0, m2.iterator().next().get());
    Assert.assertTrue(
        Iterables.isEmpty(messageStore.getVertexMessages(new IntWritable(3))));
  }

  @Test
  public void testIntByteArrayMessageStore() throws IOException {
    IntByteArrayMessageStore<FloatWritable> messageStore =
        new IntByteArrayMessageStore<FloatWritable>(new
            TestMessageValueFactory<FloatWritable>(FloatWritable.class),
            service, conf);
    insertIntFloatMessages(messageStore);

    Iterable<FloatWritable> m0 =
        messageStore.getVertexMessages(new IntWritable(0));
    Assert.assertEquals(3, Iterables.size(m0));
    Iterator<FloatWritable> i0 = m0.iterator();
    Assert.assertEquals((float) 1.0, i0.next().get());
    Assert.assertEquals((float) 4.0, i0.next().get());
    Assert.assertEquals((float) 5.0, i0.next().get());
    Iterable<FloatWritable> m1 =
        messageStore.getVertexMessages(new IntWritable(1));
    Assert.assertEquals(3, Iterables.size(m1));
    Iterator<FloatWritable> i1 = m1.iterator();
    Assert.assertEquals((float) 1.0, i1.next().get());
    Assert.assertEquals((float) 3.0, i1.next().get());
    Assert.assertEquals((float) 4.0, i1.next().get());
    Iterable<FloatWritable> m2 =
        messageStore.getVertexMessages(new IntWritable(2));
    Assert.assertEquals(1, Iterables.size(m2));
    Assert.assertEquals((float) 3.0, m2.iterator().next().get());
    Assert.assertTrue(
        Iterables.isEmpty(messageStore.getVertexMessages(new IntWritable(3))));
  }

  @Test
  public void testIntByteArrayMessageStoreWithMessageEncoding() throws
      IOException {
    GiraphConstants.USE_MESSAGE_SIZE_ENCODING.set(conf, true);
    testIntByteArrayMessageStore();
    GiraphConstants.USE_MESSAGE_SIZE_ENCODING.set(conf, false);
  }
}

<code block>


package org.apache.giraph.comm.messages;

import java.io.IOException;
import java.util.Iterator;

import junit.framework.Assert;

import org.apache.giraph.bsp.CentralizedServiceWorker;
import org.apache.giraph.combiner.DoubleSumMessageCombiner;
import org.apache.giraph.comm.messages.primitives.long_id.LongByteArrayMessageStore;
import org.apache.giraph.comm.messages.primitives.LongDoubleMessageStore;
import org.apache.giraph.conf.GiraphConfiguration;
import org.apache.giraph.conf.ImmutableClassesGiraphConfiguration;
import org.apache.giraph.factories.TestMessageValueFactory;
import org.apache.giraph.graph.BasicComputation;
import org.apache.giraph.graph.Vertex;
import org.apache.giraph.partition.Partition;
import org.apache.giraph.partition.PartitionStore;
import org.apache.giraph.utils.ByteArrayVertexIdMessages;
import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Writable;
import org.junit.Before;
import org.junit.Test;
import org.mockito.Mockito;
import org.mockito.invocation.InvocationOnMock;
import org.mockito.stubbing.Answer;

import com.google.common.collect.Iterables;
import com.google.common.collect.Lists;

public class TestLongDoublePrimitiveMessageStores {
  private static final int NUM_PARTITIONS = 2;
  private static CentralizedServiceWorker<LongWritable, Writable, Writable>
    service;

  @Before
  public void prepare() throws IOException {
    service = Mockito.mock(CentralizedServiceWorker.class);
    Mockito.when(
        service.getPartitionId(Mockito.any(LongWritable.class))).thenAnswer(
        new Answer<Integer>() {
          @Override
          public Integer answer(InvocationOnMock invocation) {
            LongWritable vertexId = (LongWritable) invocation.getArguments()[0];
            return (int) (vertexId.get() % NUM_PARTITIONS);
          }
        }
    );
    PartitionStore partitionStore = Mockito.mock(PartitionStore.class);
    Mockito.when(service.getPartitionStore()).thenReturn(partitionStore);
    Mockito.when(partitionStore.getPartitionIds()).thenReturn(
        Lists.newArrayList(0, 1));
    Partition partition = Mockito.mock(Partition.class);
    Mockito.when(partition.getVertexCount()).thenReturn(Long.valueOf(1));
    Mockito.when(partitionStore.getOrCreatePartition(0)).thenReturn(partition);
    Mockito.when(partitionStore.getOrCreatePartition(1)).thenReturn(partition);
  }

  private static class LongDoubleNoOpComputation extends
      BasicComputation<LongWritable, NullWritable, NullWritable,
          DoubleWritable> {
    @Override
    public void compute(Vertex<LongWritable, NullWritable, NullWritable> vertex,
        Iterable<DoubleWritable> messages) throws IOException {
    }
  }

  private static ImmutableClassesGiraphConfiguration<LongWritable, Writable,
    Writable> createLongDoubleConf() {

    GiraphConfiguration initConf = new GiraphConfiguration();
    initConf.setComputationClass(LongDoubleNoOpComputation.class);
    return new ImmutableClassesGiraphConfiguration(initConf);
  }

  private static ByteArrayVertexIdMessages<LongWritable, DoubleWritable>
  createLongDoubleMessages() {
    ByteArrayVertexIdMessages<LongWritable, DoubleWritable> messages =
        new ByteArrayVertexIdMessages<LongWritable, DoubleWritable>(
            new TestMessageValueFactory<DoubleWritable>(DoubleWritable.class));
    messages.setConf(createLongDoubleConf());
    messages.initialize();
    return messages;
  }

  private static void insertLongDoubleMessages(
      MessageStore<LongWritable, DoubleWritable> messageStore) throws
      IOException {
    ByteArrayVertexIdMessages<LongWritable, DoubleWritable> messages =
        createLongDoubleMessages();
    messages.add(new LongWritable(0), new DoubleWritable(1));
    messages.add(new LongWritable(2), new DoubleWritable(3));
    messages.add(new LongWritable(0), new DoubleWritable(4));
    messageStore.addPartitionMessages(0, messages);
    messages = createLongDoubleMessages();
    messages.add(new LongWritable(1), new DoubleWritable(1));
    messages.add(new LongWritable(1), new DoubleWritable(3));
    messages.add(new LongWritable(1), new DoubleWritable(4));
    messageStore.addPartitionMessages(1, messages);
    messages = createLongDoubleMessages();
    messages.add(new LongWritable(0), new DoubleWritable(5));
    messageStore.addPartitionMessages(0, messages);
  }

  @Test
  public void testLongDoubleMessageStore() throws IOException {
    LongDoubleMessageStore messageStore =
        new LongDoubleMessageStore(service, new DoubleSumMessageCombiner());
    insertLongDoubleMessages(messageStore);

    Iterable<DoubleWritable> m0 =
        messageStore.getVertexMessages(new LongWritable(0));
    Assert.assertEquals(1, Iterables.size(m0));
    Assert.assertEquals(10.0, m0.iterator().next().get());
    Iterable<DoubleWritable> m1 =
        messageStore.getVertexMessages(new LongWritable(1));
    Assert.assertEquals(1, Iterables.size(m1));
    Assert.assertEquals(8.0, m1.iterator().next().get());
    Iterable<DoubleWritable> m2 =
        messageStore.getVertexMessages(new LongWritable(2));
    Assert.assertEquals(1, Iterables.size(m2));
    Assert.assertEquals(3.0, m2.iterator().next().get());
    Assert.assertTrue(
        Iterables.isEmpty(messageStore.getVertexMessages(new LongWritable(3))));
  }

  @Test
  public void testLongByteArrayMessageStore() throws IOException {
    LongByteArrayMessageStore<DoubleWritable> messageStore =
        new LongByteArrayMessageStore<DoubleWritable>(
            new TestMessageValueFactory<DoubleWritable>(DoubleWritable.class),
            service, createLongDoubleConf());
    insertLongDoubleMessages(messageStore);

    Iterable<DoubleWritable> m0 =
        messageStore.getVertexMessages(new LongWritable(0));
    Assert.assertEquals(3, Iterables.size(m0));
    Iterator<DoubleWritable> i0 = m0.iterator();
    Assert.assertEquals(1.0, i0.next().get());
    Assert.assertEquals(4.0, i0.next().get());
    Assert.assertEquals(5.0, i0.next().get());
    Iterable<DoubleWritable> m1 =
        messageStore.getVertexMessages(new LongWritable(1));
    Assert.assertEquals(3, Iterables.size(m1));
    Iterator<DoubleWritable> i1 = m1.iterator();
    Assert.assertEquals(1.0, i1.next().get());
    Assert.assertEquals(3.0, i1.next().get());
    Assert.assertEquals(4.0, i1.next().get());
    Iterable<DoubleWritable> m2 =
        messageStore.getVertexMessages(new LongWritable(2));
    Assert.assertEquals(1, Iterables.size(m2));
    Assert.assertEquals(3.0, m2.iterator().next().get());
    Assert.assertTrue(
        Iterables.isEmpty(messageStore.getVertexMessages(new LongWritable(3))));
  }
}

<code block>

package org.apache.giraph.writable.kryo;

import java.io.DataInput;
import java.io.DataOutput;
import java.util.Arrays;
import java.util.Collections;
import java.util.LinkedHashMap;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Random;

import org.apache.giraph.conf.GiraphConfigurationSettable;
import org.apache.giraph.types.ops.collections.Basic2ObjectMap;
import org.apache.giraph.types.ops.collections.BasicArrayList;
import org.apache.giraph.types.ops.collections.BasicSet;
import org.apache.giraph.writable.kryo.markers.KryoIgnoreWritable;
import org.apache.giraph.writable.kryo.markers.NonKryoWritable;
import org.apache.giraph.writable.kryo.serializers.ArraysAsListSerializer;
import org.apache.giraph.writable.kryo.serializers.CollectionsNCopiesSerializer;
import org.apache.giraph.writable.kryo.serializers.DirectWritableSerializer;
import org.apache.giraph.writable.kryo.serializers.FastUtilSerializer;
import org.apache.giraph.writable.kryo.serializers.ImmutableMapSerializer;
import org.apache.giraph.writable.kryo.serializers.ReusableFieldSerializer;
import org.apache.hadoop.conf.Configurable;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.io.Writable;
import org.objenesis.strategy.StdInstantiatorStrategy;

import com.esotericsoftware.kryo.Kryo;
import com.esotericsoftware.kryo.Serializer;
import com.esotericsoftware.kryo.factories.SerializerFactory;
import com.esotericsoftware.kryo.io.Input;
import com.esotericsoftware.kryo.io.InputChunked;
import com.esotericsoftware.kryo.io.Output;
import com.esotericsoftware.kryo.io.OutputChunked;
import com.esotericsoftware.kryo.pool.KryoCallback;
import com.esotericsoftware.kryo.pool.KryoFactory;
import com.esotericsoftware.kryo.pool.KryoPool;
import com.esotericsoftware.kryo.serializers.ClosureSerializer;
import com.esotericsoftware.kryo.serializers.FieldSerializer;
import com.esotericsoftware.kryo.util.ObjectMap;
import com.google.common.base.Preconditions;

import de.javakaffee.kryoserializers.guava.ImmutableListSerializer;


public class HadoopKryo extends Kryo {
  
  private static final KryoPool KRYO_POOL = new KryoPool.Builder(
      new KryoFactory() {
        @Override
        public Kryo create() {
          return createKryo();
        }
      }).build();

  
  private static final Map<Class<?>, String> NON_SERIALIZABLE;

  static {
    NON_SERIALIZABLE = new LinkedHashMap<>();
    NON_SERIALIZABLE.put(
        NonKryoWritable.class,
        "it is marked to not allow serialization, " +
        "look at the class for more details");
    NON_SERIALIZABLE.put(
        KryoWritableWrapper.class, "recursion is dissallowed");
    NON_SERIALIZABLE.put(
        Configuration.class,
        "it cannot be supported since it contains ClassLoader");
    NON_SERIALIZABLE.put(
        GiraphConfigurationSettable.class, "configuration cannot be set");
    NON_SERIALIZABLE.put(
        Configurable.class, "configuration cannot be set");
    NON_SERIALIZABLE.put(
        Random.class,
        "it should be rarely serialized, since it would create same stream " +
        "of numbers everywhere, use TransientRandom instead");
  }

  
  
  
  private final InputChunked input = new InputChunked(4096);
  
  private final OutputChunked output = new OutputChunked(4096);

  
  private final DataInputWrapperStream dataInputWrapperStream =
      new DataInputWrapperStream();
  
  private final DataOutputWrapperStream dataOutputWrapperStream =
      new DataOutputWrapperStream();

  
  private final ObjectMap<Class<?>, ReusableFieldSerializer<Object>>
  classToIntoSerializer = new ObjectMap<>();

  
  private HadoopKryo() {
  }

  

  
  public static void writeClassAndObject(
      final DataOutput out, final Object object) {
    writeInternal(out, object, false);
  }

  
  public static <T> T readClassAndObject(DataInput in) {
    return readInternal(in, null, false);
  }

  
  public static void writeOutOfObject(
      final DataOutput out, final Object object) {
    writeInternal(out, object, true);
  }

  
  public static void readIntoObject(DataInput in, Object object) {
    readInternal(in, object, true);
  }

  
  public static <T> T createCopy(final T object) {
    return KRYO_POOL.run(new KryoCallback<T>() {
      @Override
      public T execute(Kryo kryo) {
        return kryo.copy(object);
      }
    });
  }

  

  
  private static HadoopKryo createKryo() {
    HadoopKryo kryo = new HadoopKryo();

    String version = System.getProperty("java.version");
    char minor = version.charAt(2);
    if (minor >= '8') {
      try {
        kryo.register(Class.forName("java.lang.invoke.SerializedLambda"));
        kryo.register(Class.forName("com.esotericsoftware.kryo.Kryo$Closure"),
            new ClosureSerializer());
      } catch (ClassNotFoundException e) {
        throw new IllegalStateException(
            "Trying to use Kryo on >= Java 8 (" + version +
            "), but unable to find needed classes", e);
      }
    }

    kryo.register(Arrays.asList().getClass(), new ArraysAsListSerializer());
    kryo.register(Collections.nCopies(1, new Object()).getClass(),
        new CollectionsNCopiesSerializer());

    ImmutableListSerializer.registerSerializers(kryo);

    registerSerializer(kryo, "com.google.common.collect.RegularImmutableMap",
        new ImmutableMapSerializer());
    registerSerializer(kryo,
        "com.google.common.collect.SingletonImmutableBiMap",
        new ImmutableMapSerializer());

    
    
    FastUtilSerializer.registerAll(kryo);

    kryo.setInstantiatorStrategy(new DefaultInstantiatorStrategy(
        new StdInstantiatorStrategy()));

    kryo.setDefaultSerializer(new SerializerFactory() {
      @SuppressWarnings("rawtypes")
      @Override
      public Serializer makeSerializer(Kryo kryo, final Class<?> type) {
        for (final Entry<Class<?>, String> entry :
            NON_SERIALIZABLE.entrySet()) {
          if (entry.getKey().isAssignableFrom(type)) {
            
            return new Serializer() {
              @Override
              public Object read(Kryo kryo, Input input, Class type) {
                throw new RuntimeException("Cannot serialize " + type +
                    ". Objects being serialized cannot capture " +
                    entry.getKey() + " because " + entry.getValue() +
                    ". Either remove field in question" +
                    ", or make it transient (so that it isn't serialized)");
              }

              @Override
              public void write(Kryo kryo, Output output, Object object) {
                throw new RuntimeException("Cannot serialize " + type +
                    ". Objects being serialized cannot capture " +
                    entry.getKey() + " because " + entry.getValue() +
                    ". Either remove field in question" +
                    ", or make it transient (so that it isn't serialized)");
              }
            };
          }
        }

        if (Writable.class.isAssignableFrom(type) &&
            !KryoIgnoreWritable.class.isAssignableFrom(type) &&
            
            
            !BasicSet.class.isAssignableFrom(type) &&
            !BasicArrayList.class.isAssignableFrom(type) &&
            !Basic2ObjectMap.class.isAssignableFrom(type)) {
          
          DirectWritableSerializer serializer = new DirectWritableSerializer();
          return serializer;
        } else {
          FieldSerializer serializer = new FieldSerializer<>(kryo, type);
          serializer.setIgnoreSyntheticFields(false);
          return serializer;
        }
      }
    });

    return kryo;
  }

  
  private static void registerSerializer(HadoopKryo kryo, String className,
      Serializer serializer) {
    try {
      kryo.register(Class.forName(className), serializer);
    } catch (ClassNotFoundException e) {
      throw new IllegalStateException("Class " + className + " is missing", e);
    }
  }

  
  private void setDataInput(DataInput in) {
    dataInputWrapperStream.setDataInput(in);
    input.setInputStream(dataInputWrapperStream);
  }

  
  private void setDataOutput(DataOutput out) {
    dataOutputWrapperStream.setDataOutput(out);
    output.setOutputStream(dataOutputWrapperStream);
  }

  
  private ReusableFieldSerializer<Object> getOrCreateReusableSerializer(
      Class<?> type) {
    ReusableFieldSerializer<Object> serializer =
        classToIntoSerializer.get(type);
    if (serializer == null) {
      serializer = new ReusableFieldSerializer<>(this, type);
      classToIntoSerializer.put(type, serializer);
    }
    return serializer;
  }

  
  private static void writeInternal(
      final DataOutput out, final Object object, final boolean outOf) {
    KRYO_POOL.run(new KryoCallback<Void>() {
      @Override
      public Void execute(Kryo kryo) {
        HadoopKryo hkryo = (HadoopKryo) kryo;
        hkryo.setDataOutput(out);

        if (outOf) {
          hkryo.writeOutOfObject(hkryo.output, object);
        } else {
          hkryo.writeClassAndObject(hkryo.output, object);
        }

        hkryo.output.endChunks();
        hkryo.output.close();

        return null;
      }
    });
  }

  
  @SuppressWarnings("unchecked")
  private static <T> T readInternal(
      final DataInput in, final T outObject, final boolean into) {
    return KRYO_POOL.run(new KryoCallback<T>() {
      @Override
      public T execute(Kryo kryo) {
        HadoopKryo hkryo = (HadoopKryo) kryo;
        hkryo.setDataInput(in);

        T object;
        if (into) {
          hkryo.readIntoObject(hkryo.input, outObject);
          object = outObject;
        } else {
          object = (T) hkryo.readClassAndObject(hkryo.input);
        }
        hkryo.input.nextChunks();

        hkryo.input.close();
        return object;
      }
    });
  }

  
  private void readIntoObject(Input input, Object object) {
    Preconditions.checkNotNull(object);

    Class<?> type = object.getClass();
    ReusableFieldSerializer<Object> serializer =
        getOrCreateReusableSerializer(type);

    serializer.setReadIntoObject(object);
    Object result = readObject(input, type, serializer);

    Preconditions.checkState(result == object);
  }

  
  private void writeOutOfObject(Output output, Object object) {
    ReusableFieldSerializer<Object> serializer =
        getOrCreateReusableSerializer(object.getClass());
    writeObject(output, object, serializer);
  }

}

<code block>


package org.apache.giraph.writable.kryo.serializers;


import com.esotericsoftware.kryo.Kryo;
import com.esotericsoftware.kryo.io.Input;
import com.esotericsoftware.kryo.serializers.MapSerializer;
import com.google.common.collect.ImmutableMap;

import java.util.HashMap;
import java.util.Map;


public class ImmutableMapSerializer extends MapSerializer {
  @Override
  public Map read(Kryo kryo, Input input, Class<Map> type) {
    Map map = super.read(kryo, input,
        (Class<Map>) ((Object) HashMap.class));
    return ImmutableMap.copyOf(map);
  }
}

<code block>

package org.apache.giraph.writable.kryo;

import static org.junit.Assert.assertEquals;

import it.unimi.dsi.fastutil.longs.Long2IntOpenHashMap;
import it.unimi.dsi.fastutil.longs.LongArrayList;

import java.util.Arrays;
import java.util.List;

import org.apache.giraph.types.ops.collections.BasicArrayList.BasicLongArrayList;
import org.apache.giraph.utils.WritableUtils;
import org.apache.hadoop.io.LongWritable;
import org.junit.Assert;
import org.junit.Test;

import com.google.common.collect.ImmutableMap;



public class KryoWritableTest {
  public static class TestClassA extends KryoWritable {
    final String testObject;
    final List list;
    final int something;

    public TestClassA(String testObject, List list, int something) {
      this.testObject = testObject;
      this.list = list;
      this.something = something;
    }

    public TestClassA() {
      this.testObject = null;
      this.list = null;
      this.something = -1;
    }
  }

  @Test
  public void testTestClassA() throws Exception {
    String testObject = "Hello World!";
    TestClassA res = new TestClassA();
    WritableUtils.copyInto(
        new TestClassA(testObject, Arrays.asList(1, 2, 3), 5), res, true);

    assertEquals(testObject, res.testObject);

    assertEquals(3, res.list.size());
    assertEquals(1, res.list.get(0));
    assertEquals(2, res.list.get(1));
    assertEquals(3, res.list.get(2));

    assertEquals(5, res.something);
  }

  public static class LongKryoWritable extends KryoWritable {
    private long value;

    public LongKryoWritable(long value) {
      this.value = value;
    }

    public long get() {
      return value;
    }

    public void set(long value) {
      this.value = value;
    }
  }


  int multiplier = 5000; 
  int longTestTimes = 1000 * multiplier;

  @Test
  public void testLongKryoWritable() throws Exception {
    LongKryoWritable from = new LongKryoWritable(0);
    LongKryoWritable to = new LongKryoWritable(0);

    for (int i = 0; i < longTestTimes; i++) {
      from.set(i);
      WritableUtils.copyInto(from, to, true);
      assertEquals(i, to.get());
    }
  }

  @Test
  public void testLongWritable() throws Exception {
    LongWritable from = new LongWritable(0);
    LongWritable to = new LongWritable(0);

    for (int i = 0; i < longTestTimes; i++) {
      from.set(i);
      WritableUtils.copyInto(from, to, true);
      assertEquals(i, to.get());
    }
  }

  public static class LongListKryoWritable extends KryoWritable {
    public LongArrayList value;

    public LongListKryoWritable(LongArrayList value) {
      this.value = value;
    }
  }

  int longListTestTimes = 1 * multiplier;
  int longListTestSize = 100000;

  @Test
  public void testLongListKryoWritable() throws Exception {
    LongArrayList list = new LongArrayList(longListTestSize);
    for (int i = 0; i < longListTestSize; i++) {
      list.add(i);
    }

    LongListKryoWritable from = new LongListKryoWritable(list);
    LongListKryoWritable to = new LongListKryoWritable(null);

    for (int i = 0; i < longListTestTimes; i++) {
      from.value.set((2 * i) % longListTestSize, 0);
      WritableUtils.copyInto(from, to, true);
    }
  }

  @Test
  public void testLongListWritable() throws Exception {
    BasicLongArrayList from = new BasicLongArrayList(longListTestSize);
    LongWritable value = new LongWritable();
    for (int i = 0; i < longListTestSize; i++) {
      value.set(i);
      from.add(value);
    }

    BasicLongArrayList to = new BasicLongArrayList(longListTestSize);
    value.set(0);

    for (int i = 0; i < longListTestTimes; i++) {
      from.set((2 * i) % longListTestSize, value);
      WritableUtils.copyInto(from, to, true);
    }
  }

  public static class NestedKryoWritable<T> extends KryoWritable {
    public LongKryoWritable value1;
    public T value2;

    public NestedKryoWritable(LongKryoWritable value1, T value2) {
      this.value1 = value1;
      this.value2 = value2;
    }
  }

  @Test
  public void testNestedKryoWritable() throws Exception {
    LongKryoWritable inner = new LongKryoWritable(5);
    NestedKryoWritable<LongKryoWritable> res = new NestedKryoWritable<>(null, null);
    WritableUtils.copyInto(
        new NestedKryoWritable<>(inner, inner), res, true);

    assertEquals(5, res.value1.get());
    Assert.assertTrue(res.value1 == res.value2);
  }

  @Test
  public void testRecursiveKryoWritable() throws Exception {
    LongKryoWritable inner = new LongKryoWritable(5);
    NestedKryoWritable wanted = new NestedKryoWritable<>(inner, null);
    wanted.value2 = wanted;

    NestedKryoWritable res = new NestedKryoWritable<>(null, null);
    WritableUtils.copyInto(wanted, res, true);

    assertEquals(5, res.value1.get());
    Assert.assertTrue(res == res.value2);
  }

  @Test
  public void testKryoImmutableMap() throws Exception {
    Long2IntOpenHashMap map = new Long2IntOpenHashMap();
    map.put(1, 2);
    map.put(10, 20);
    ImmutableMap<Long, Integer> copy =
        WritableUtils.createCopy(
            new KryoWritableWrapper<>(ImmutableMap.copyOf(map))).get();
    Assert.assertEquals(2, copy.size());
    Assert.assertEquals(2, copy.get(1L).intValue());
    Assert.assertEquals(20, copy.get(10L).intValue());
  }
}

<code block>

package org.apache.giraph.writable.kryo;

import java.io.DataInput;
import java.io.DataOutput;
import java.util.Arrays;
import java.util.Collections;
import java.util.LinkedHashMap;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Random;

import org.apache.giraph.conf.GiraphConfigurationSettable;
import org.apache.giraph.types.ops.collections.Basic2ObjectMap;
import org.apache.giraph.types.ops.collections.BasicArrayList;
import org.apache.giraph.types.ops.collections.BasicSet;
import org.apache.giraph.writable.kryo.markers.KryoIgnoreWritable;
import org.apache.giraph.writable.kryo.markers.NonKryoWritable;
import org.apache.giraph.writable.kryo.serializers.ArraysAsListSerializer;
import org.apache.giraph.writable.kryo.serializers.CollectionsNCopiesSerializer;
import org.apache.giraph.writable.kryo.serializers.DirectWritableSerializer;
import org.apache.giraph.writable.kryo.serializers.FastUtilSerializer;
import org.apache.giraph.writable.kryo.serializers.ReusableFieldSerializer;
import org.apache.hadoop.conf.Configurable;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.io.Writable;
import org.objenesis.strategy.StdInstantiatorStrategy;

import com.esotericsoftware.kryo.Kryo;
import com.esotericsoftware.kryo.Serializer;
import com.esotericsoftware.kryo.factories.SerializerFactory;
import com.esotericsoftware.kryo.io.Input;
import com.esotericsoftware.kryo.io.InputChunked;
import com.esotericsoftware.kryo.io.Output;
import com.esotericsoftware.kryo.io.OutputChunked;
import com.esotericsoftware.kryo.pool.KryoCallback;
import com.esotericsoftware.kryo.pool.KryoFactory;
import com.esotericsoftware.kryo.pool.KryoPool;
import com.esotericsoftware.kryo.serializers.ClosureSerializer;
import com.esotericsoftware.kryo.serializers.FieldSerializer;
import com.esotericsoftware.kryo.util.ObjectMap;
import com.google.common.base.Preconditions;

import de.javakaffee.kryoserializers.guava.ImmutableListSerializer;


public class HadoopKryo extends Kryo {
  
  private static final KryoPool KRYO_POOL = new KryoPool.Builder(
      new KryoFactory() {
        @Override
        public Kryo create() {
          return createKryo();
        }
      }).build();

  
  private static final Map<Class<?>, String> NON_SERIALIZABLE;

  static {
    NON_SERIALIZABLE = new LinkedHashMap<>();
    NON_SERIALIZABLE.put(
        NonKryoWritable.class,
        "it is marked to not allow serialization, " +
        "look at the class for more details");
    NON_SERIALIZABLE.put(
        KryoWritableWrapper.class, "recursion is dissallowed");
    NON_SERIALIZABLE.put(
        Configuration.class,
        "it cannot be supported since it contains ClassLoader");
    NON_SERIALIZABLE.put(
        GiraphConfigurationSettable.class, "configuration cannot be set");
    NON_SERIALIZABLE.put(
        Configurable.class, "configuration cannot be set");
    NON_SERIALIZABLE.put(
        Random.class,
        "it should be rarely serialized, since it would create same stream " +
        "of numbers everywhere, use TransientRandom instead");
  }

  
  
  
  private final InputChunked input = new InputChunked(4096);
  
  private final OutputChunked output = new OutputChunked(4096);

  
  private final DataInputWrapperStream dataInputWrapperStream =
      new DataInputWrapperStream();
  
  private final DataOutputWrapperStream dataOutputWrapperStream =
      new DataOutputWrapperStream();

  
  private final ObjectMap<Class<?>, ReusableFieldSerializer<Object>>
  classToIntoSerializer = new ObjectMap<>();

  
  private HadoopKryo() {
  }

  

  
  public static void writeClassAndObject(
      final DataOutput out, final Object object) {
    writeInternal(out, object, false);
  }

  
  public static <T> T readClassAndObject(DataInput in) {
    return readInternal(in, null, false);
  }

  
  public static void writeOutOfObject(
      final DataOutput out, final Object object) {
    writeInternal(out, object, true);
  }

  
  public static void readIntoObject(DataInput in, Object object) {
    readInternal(in, object, true);
  }

  
  public static <T> T createCopy(final T object) {
    return KRYO_POOL.run(new KryoCallback<T>() {
      @Override
      public T execute(Kryo kryo) {
        return kryo.copy(object);
      }
    });
  }

  

  
  private static HadoopKryo createKryo() {
    HadoopKryo kryo = new HadoopKryo();

    String version = System.getProperty("java.version");
    char minor = version.charAt(2);
    if (minor >= '8') {
      try {
        kryo.register(Class.forName("java.lang.invoke.SerializedLambda"));
        kryo.register(Class.forName("com.esotericsoftware.kryo.Kryo$Closure"),
            new ClosureSerializer());
      } catch (ClassNotFoundException e) {
        throw new IllegalStateException(
            "Trying to use Kryo on >= Java 8 (" + version +
            "), but unable to find needed classes", e);
      }
    }

    kryo.register(Arrays.asList().getClass(), new ArraysAsListSerializer());
    kryo.register(Collections.nCopies(1, new Object()).getClass(),
        new CollectionsNCopiesSerializer());

    ImmutableListSerializer.registerSerializers(kryo);

    
    try {
      kryo.register(
          Class.forName("com.google.common.collect.RegularImmutableList"),
          new ImmutableListSerializer());
    } catch (ClassNotFoundException e) {
      throw new IllegalStateException(
          "Guava has RegularImmutableList missing", e);
    }

    
    
    FastUtilSerializer.registerAll(kryo);

    kryo.setInstantiatorStrategy(new DefaultInstantiatorStrategy(
        new StdInstantiatorStrategy()));

    kryo.setDefaultSerializer(new SerializerFactory() {
      @SuppressWarnings("rawtypes")
      @Override
      public Serializer makeSerializer(Kryo kryo, final Class<?> type) {
        for (final Entry<Class<?>, String> entry :
            NON_SERIALIZABLE.entrySet()) {
          if (entry.getKey().isAssignableFrom(type)) {
            
            return new Serializer() {
              @Override
              public Object read(Kryo kryo, Input input, Class type) {
                throw new RuntimeException("Cannot serialize " + type +
                    ". Objects being serialized cannot capture " +
                    entry.getKey() + " because " + entry.getValue() +
                    ". Either remove field in question" +
                    ", or make it transient (so that it isn't serialized)");
              }

              @Override
              public void write(Kryo kryo, Output output, Object object) {
                throw new RuntimeException("Cannot serialize " + type +
                    ". Objects being serialized cannot capture " +
                    entry.getKey() + " because " + entry.getValue() +
                    ". Either remove field in question" +
                    ", or make it transient (so that it isn't serialized)");
              }
            };
          }
        }

        if (Writable.class.isAssignableFrom(type) &&
            !KryoIgnoreWritable.class.isAssignableFrom(type) &&
            
            
            !BasicSet.class.isAssignableFrom(type) &&
            !BasicArrayList.class.isAssignableFrom(type) &&
            !Basic2ObjectMap.class.isAssignableFrom(type)) {
          
          DirectWritableSerializer serializer = new DirectWritableSerializer();
          return serializer;
        } else {
          FieldSerializer serializer = new FieldSerializer<>(kryo, type);
          serializer.setIgnoreSyntheticFields(false);
          return serializer;
        }
      }
    });

    return kryo;
  }

  
  private void setDataInput(DataInput in) {
    dataInputWrapperStream.setDataInput(in);
    input.setInputStream(dataInputWrapperStream);
  }

  
  private void setDataOutput(DataOutput out) {
    dataOutputWrapperStream.setDataOutput(out);
    output.setOutputStream(dataOutputWrapperStream);
  }

  
  private ReusableFieldSerializer<Object> getOrCreateReusableSerializer(
      Class<?> type) {
    ReusableFieldSerializer<Object> serializer =
        classToIntoSerializer.get(type);
    if (serializer == null) {
      serializer = new ReusableFieldSerializer<>(this, type);
      classToIntoSerializer.put(type, serializer);
    }
    return serializer;
  }

  
  private static void writeInternal(
      final DataOutput out, final Object object, final boolean outOf) {
    KRYO_POOL.run(new KryoCallback<Void>() {
      @Override
      public Void execute(Kryo kryo) {
        HadoopKryo hkryo = (HadoopKryo) kryo;
        hkryo.setDataOutput(out);

        if (outOf) {
          hkryo.writeOutOfObject(hkryo.output, object);
        } else {
          hkryo.writeClassAndObject(hkryo.output, object);
        }

        hkryo.output.endChunks();
        hkryo.output.close();

        return null;
      }
    });
  }

  
  @SuppressWarnings("unchecked")
  private static <T> T readInternal(
      final DataInput in, final T outObject, final boolean into) {
    return KRYO_POOL.run(new KryoCallback<T>() {
      @Override
      public T execute(Kryo kryo) {
        HadoopKryo hkryo = (HadoopKryo) kryo;
        hkryo.setDataInput(in);

        T object;
        if (into) {
          hkryo.readIntoObject(hkryo.input, outObject);
          object = outObject;
        } else {
          object = (T) hkryo.readClassAndObject(hkryo.input);
        }
        hkryo.input.nextChunks();

        hkryo.input.close();
        return object;
      }
    });
  }

  
  private void readIntoObject(Input input, Object object) {
    Preconditions.checkNotNull(object);

    Class<?> type = object.getClass();
    ReusableFieldSerializer<Object> serializer =
        getOrCreateReusableSerializer(type);

    serializer.setReadIntoObject(object);
    Object result = readObject(input, type, serializer);

    Preconditions.checkState(result == object);
  }

  
  private void writeOutOfObject(Output output, Object object) {
    ReusableFieldSerializer<Object> serializer =
        getOrCreateReusableSerializer(object.getClass());
    writeObject(output, object, serializer);
  }

}

<code block>

package org.apache.giraph.writable.kryo;

import static org.junit.Assert.assertEquals;
import it.unimi.dsi.fastutil.longs.LongArrayList;

import java.util.Arrays;
import java.util.List;

import org.apache.giraph.types.ops.collections.BasicArrayList.BasicLongArrayList;
import org.apache.giraph.utils.WritableUtils;
import org.apache.hadoop.io.LongWritable;
import org.junit.Assert;
import org.junit.Test;




public class KryoWritableTest {
  public static class TestClassA extends KryoWritable {
    final String testObject;
    final List list;
    final int something;

    public TestClassA(String testObject, List list, int something) {
      this.testObject = testObject;
      this.list = list;
      this.something = something;
    }

    public TestClassA() {
      this.testObject = null;
      this.list = null;
      this.something = -1;
    }
  }

  @Test
  public void testTestClassA() throws Exception {
    String testObject = "Hello World!";
    TestClassA res = new TestClassA();
    WritableUtils.copyInto(
        new TestClassA(testObject, Arrays.asList(1, 2, 3), 5), res, true);

    assertEquals(testObject, res.testObject);

    assertEquals(3, res.list.size());
    assertEquals(1, res.list.get(0));
    assertEquals(2, res.list.get(1));
    assertEquals(3, res.list.get(2));

    assertEquals(5, res.something);
  }

  public static class LongKryoWritable extends KryoWritable {
    private long value;

    public LongKryoWritable(long value) {
      this.value = value;
    }

    public long get() {
      return value;
    }

    public void set(long value) {
      this.value = value;
    }
  }


  int multiplier = 5000; 
  int longTestTimes = 1000 * multiplier;

  @Test
  public void testLongKryoWritable() throws Exception {
    LongKryoWritable from = new LongKryoWritable(0);
    LongKryoWritable to = new LongKryoWritable(0);

    for (int i = 0; i < longTestTimes; i++) {
      from.set(i);
      WritableUtils.copyInto(from, to, true);
      assertEquals(i, to.get());
    }
  }

  @Test
  public void testLongWritable() throws Exception {
    LongWritable from = new LongWritable(0);
    LongWritable to = new LongWritable(0);

    for (int i = 0; i < longTestTimes; i++) {
      from.set(i);
      WritableUtils.copyInto(from, to, true);
      assertEquals(i, to.get());
    }
  }

  public static class LongListKryoWritable extends KryoWritable {
    public LongArrayList value;

    public LongListKryoWritable(LongArrayList value) {
      this.value = value;
    }
  }

  int longListTestTimes = 1 * multiplier;
  int longListTestSize = 100000;

  @Test
  public void testLongListKryoWritable() throws Exception {
    LongArrayList list = new LongArrayList(longListTestSize);
    for (int i = 0; i < longListTestSize; i++) {
      list.add(i);
    }

    LongListKryoWritable from = new LongListKryoWritable(list);
    LongListKryoWritable to = new LongListKryoWritable(null);

    for (int i = 0; i < longListTestTimes; i++) {
      from.value.set((2 * i) % longListTestSize, 0);
      WritableUtils.copyInto(from, to, true);
    }
  }

  @Test
  public void testLongListWritable() throws Exception {
    BasicLongArrayList from = new BasicLongArrayList(longListTestSize);
    LongWritable value = new LongWritable();
    for (int i = 0; i < longListTestSize; i++) {
      value.set(i);
      from.add(value);
    }

    BasicLongArrayList to = new BasicLongArrayList(longListTestSize);
    value.set(0);

    for (int i = 0; i < longListTestTimes; i++) {
      from.set((2 * i) % longListTestSize, value);
      WritableUtils.copyInto(from, to, true);
    }
  }

  public static class NestedKryoWritable<T> extends KryoWritable {
    public LongKryoWritable value1;
    public T value2;

    public NestedKryoWritable(LongKryoWritable value1, T value2) {
      this.value1 = value1;
      this.value2 = value2;
    }
  }

  @Test
  public void testNestedKryoWritable() throws Exception {
    LongKryoWritable inner = new LongKryoWritable(5);
    NestedKryoWritable<LongKryoWritable> res = new NestedKryoWritable<>(null, null);
    WritableUtils.copyInto(
        new NestedKryoWritable<>(inner, inner), res, true);

    assertEquals(5, res.value1.get());
    Assert.assertTrue(res.value1 == res.value2);
  }

  @Test
  public void testRecursiveKryoWritable() throws Exception {
    LongKryoWritable inner = new LongKryoWritable(5);
    NestedKryoWritable wanted = new NestedKryoWritable<>(inner, null);
    wanted.value2 = wanted;

    NestedKryoWritable res = new NestedKryoWritable<>(null, null);
    WritableUtils.copyInto(wanted, res, true);

    assertEquals(5, res.value1.get());
    Assert.assertTrue(res == res.value2);
  }
}
