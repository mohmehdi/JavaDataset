package org.opentripplanner.analyst.qbroker;

import gnu.trove.map.TIntObjectMap;
import gnu.trove.map.hash.TIntObjectHashMap;

import java.util.ArrayDeque;
import java.util.List;
import java.util.Queue;


public class Job {

    private int nTasks = 0;


    String graphId;

    public final String jobId;


    Queue<Task> visibleTasks = new ArrayDeque<>();


    TIntObjectMap<Task> invisibleTasks = new TIntObjectHashMap<>();

    public Job (String jobId) {
        this.jobId = jobId;
    }


    public int addTask (String taskBody) {
        Task task = new Task();
        task.taskId = nTasks++;
        task.payload = taskBody;
        visibleTasks.add(task);
        return task.taskId;
    }

    public void markTasksDelivered(List<Task> tasks) {
        long deliveryTime = System.currentTimeMillis();
        for (Task task : tasks) {
            task.invisibleUntil = deliveryTime + 60000;
            invisibleTasks.put(task.taskId, task);
        }
    }

}

<code block>
package org.opentripplanner.analyst.qbroker;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import org.glassfish.grizzly.http.Method;
import org.glassfish.grizzly.http.server.HttpHandler;
import org.glassfish.grizzly.http.server.Request;
import org.glassfish.grizzly.http.server.Response;
import org.glassfish.grizzly.http.util.HttpStatus;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;


class BrokerHttpHandler extends HttpHandler {

    private ObjectMapper mapper = new ObjectMapper();

    private Broker broker;

    public BrokerHttpHandler(Broker broker) {
        this.broker = broker;
    }

    @Override
    public void service(Request request, Response response) throws Exception {




        response.setContentType("application/json");


        QueuePath queuePath = new QueuePath(request.getPathInfo());



        try {
            if (request.getMethod() == Method.HEAD) {

                mapper.readTree(request.getInputStream());
                response.setStatus(HttpStatus.OK_200);
                return;
            } else if (request.getMethod() == Method.GET) {

                request.getRequest().getConnection().addCloseListener((closeable, iCloseType) -> {
                    broker.removeSuspendedResponse(queuePath.graphId, response);
                });
                response.suspend(); 
                broker.registerSuspendedResponse(queuePath.graphId, response);
            } else if (request.getMethod() == Method.POST) {


                JsonNode rootNode = mapper.readTree(request.getInputStream());
                if (!rootNode.isArray()) {
                    response.setStatus(HttpStatus.BAD_REQUEST_400);
                    response.setDetailMessage("Expecting a JSON array of cluster request objects.");
                    return;
                } else {
                    for (JsonNode node : rootNode) {
                        if (!node.isObject()) {
                            response.setStatus(HttpStatus.BAD_REQUEST_400);
                            response.setDetailMessage("Expecting a JSON array of cluster request objects.");
                            return;
                        }
                    }
                }
                List<String> taskBodies = new ArrayList<>();
                for (JsonNode node : rootNode) {
                    taskBodies.add(mapper.writeValueAsString(node));
                }
                broker.enqueueTasks(queuePath, taskBodies);
                response.setStatus(HttpStatus.ACCEPTED_202);
            } else if (request.getMethod() == Method.DELETE) {

                if (broker.deleteTask(queuePath)) {
                    response.setStatus(HttpStatus.OK_200);
                } else {
                    response.setStatus(HttpStatus.NOT_FOUND_404);
                }
            } else {
                response.setStatus(HttpStatus.BAD_REQUEST_400);
                response.setDetailMessage("Unrecognized HTTP method.");
            }
        } catch (JsonProcessingException jpex) {
            response.setStatus(HttpStatus.BAD_REQUEST_400);
            response.setDetailMessage("Could not decode/encode JSON payload. " + jpex.getMessage());
            jpex.printStackTrace();
        } catch (Exception ex) {
            response.setStatus(HttpStatus.INTERNAL_SERVER_ERROR_500);
            response.setDetailMessage(ex.toString());
            ex.printStackTrace();
        }
    }

    public void writeJson (Response response, Object object) throws IOException {
        mapper.writeValue(response.getOutputStream(), object);
    }

}

<code block>
package org.opentripplanner.analyst.qbroker;

import org.glassfish.grizzly.http.server.Response;
import org.glassfish.grizzly.http.util.HttpStatus;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;
import java.io.OutputStream;
import java.util.ArrayDeque;
import java.util.ArrayList;
import java.util.Collection;
import java.util.Deque;
import java.util.HashMap;
import java.util.List;
import java.util.Map;


public class Broker implements Runnable {



    private static final Logger LOG = LoggerFactory.getLogger(Broker.class);

    private CircularList<User> users = new CircularList<>();

    private int nUndeliveredTasks = 0;

    private int nWaitingConsumers = 0; 


    Map<String, Deque<Response>> connectionsForGraph = new HashMap<>();



    public synchronized void enqueueTasks (QueuePath queuePath, Collection<String> taskBodies) {
        User user = findUser(queuePath.userId, true);
        Job job = user.findJob(queuePath.jobId, true);
        if (job.graphId == null) {

            job.graphId = queuePath.graphId;
        } else {

            if (!job.graphId.equals(queuePath.graphId)) {
                LOG.warn("Job associated with a different graphId");
            }
        }
        LOG.debug("Queue {}", queuePath);
        for (String taskBody : taskBodies) {
            int taskId = job.addTask(taskBody);
            nUndeliveredTasks += 1;
            LOG.debug("Enqueued task id {} with body {}", taskId, taskBody);
        }


        notify();
    }


    public synchronized void registerSuspendedResponse(String graphId, Response response) {

        Deque<Response> deque = connectionsForGraph.get(graphId);
        if (deque == null) {
            deque = new ArrayDeque<>();
            connectionsForGraph.put(graphId, deque);
        }
        deque.addLast(response);
        nWaitingConsumers += 1;


        notify();
    }


    public synchronized boolean removeSuspendedResponse(String graphId, Response response) {
        Deque<Response> deque = connectionsForGraph.get(graphId);
        if (deque == null) {
            return false;
        }
        if (deque.remove(response)) {
            nWaitingConsumers -= 1;
            LOG.debug("Removed closed connection from queue.");
            logQueueStatus();
            return true;
        }
        return false;
    }

    private void logQueueStatus() {
        LOG.info("Status {} undelivered, {} consumers waiting.", nUndeliveredTasks, nWaitingConsumers);
    }


    public synchronized void deliverTasksForOneJob () throws InterruptedException {


        while (nUndeliveredTasks == 0) {
            LOG.debug("Task delivery thread is going to sleep, there are no tasks waiting for delivery.");
            logQueueStatus();
            wait();
        }
        LOG.debug("Task delivery thread is awake and there are some undelivered tasks.");
        logQueueStatus();


        Job job = null;
        while (job == null) {
            User user = users.advance();
            if (user == null) {
                LOG.error("There should always be at least one user here, because there is an undelivered task.");
            }
            job = user.jobs.advanceToElement(e -> e.visibleTasks.size() > 0);
        }



        LOG.debug("Task delivery thread has found undelivered tasks in job {}.", job.jobId);
        while (true) {
            while (nWaitingConsumers == 0) {
                LOG.debug("Task delivery thread is going to sleep, there are no consumers waiting.");

                wait();
            }
            LOG.debug("Task delivery thread is awake, and some consumers are waiting.");
            logQueueStatus();



            LOG.debug("Looking for an eligible consumer, respecting graph affinity.");
            Deque<Response> deque = connectionsForGraph.get(job.graphId);
            while (deque != null && !deque.isEmpty()) {
                Response response = deque.pop();
                nWaitingConsumers -= 1;
                if (deliver(job, response)) {
                    return;
                }
            }


            LOG.debug("No consumers with the right affinity. Looking for any consumer.");
            List<Deque<Response>> deques = new ArrayList<>(connectionsForGraph.values());
            deques.sort((d1, d2) -> Integer.compare(d2.size(), d1.size()));
            for (Deque<Response> d : deques) {
                while (!d.isEmpty()) {
                    Response response = d.pop();
                    nWaitingConsumers -= 1;
                    if (deliver(job, response)) {
                        return;
                    }
                }
            }


            LOG.debug("No consumer was available. They all must have closed their connections.");
            if (nWaitingConsumers != 0) {
                throw new AssertionError("There should be no waiting consumers here, something is wrong.");
            }
        }

    }


    public synchronized boolean deliver (Job job, Response response) {


        if (!response.getRequest().getRequest().getConnection().isOpen()) {
            LOG.debug("Consumer connection was closed. It will be removed.");
            return false;
        }


        List<Task> tasks = new ArrayList<>();
        while (tasks.size() < 4 && !job.visibleTasks.isEmpty()) {
            tasks.add(job.visibleTasks.poll());
        }

        try {
            response.setStatus(HttpStatus.OK_200);
            OutputStream out = response.getOutputStream();

            int n = 0;
            out.write('{');
            for (Task task : tasks) {


                if (n++ > 0) {
                    out.write(',');
                    out.write('\n');
                }
                out.write('"');
                out.write(Integer.toString(task.taskId).getBytes());
                out.write('"');
                out.write(':');
                out.write(task.payload.getBytes());
            }
            out.write('\n');
            out.write('}');
            response.resume();
        } catch (IOException e) {

            LOG.debug("Consumer connection caused IO error, it will be removed.");
            response.setStatus(HttpStatus.INTERNAL_SERVER_ERROR_500);
            response.resume();

            job.visibleTasks.addAll(tasks);
            return false;
        }

        LOG.debug("Delivery of {} tasks succeeded.", tasks.size());
        nUndeliveredTasks -= tasks.size();
        job.markTasksDelivered(tasks);
        return true;
    }


    public synchronized boolean deleteTask (QueuePath queuePath) {
        User user = findUser(queuePath.userId, false);
        if (user == null) {
            return false;
        }
        Job job = user.findJob(queuePath.jobId, false);
        if (job == null) {
            return false;
        }



        return job.invisibleTasks.remove(queuePath.taskId) != null;
    }



    @Override
    public void run() {
        while (true) {
            try {
                deliverTasksForOneJob();
            } catch (InterruptedException e) {
                LOG.warn("Task pump thread was interrupted.");
                return;
            }
        }
    }


    public User findUser (String userId, boolean create) {
        for (User user : users) {
            if (user.userId.equals(userId)) {
                return user;
            }
        }
        if (create) {
            User user = new User(userId);
            users.insertAtTail(user);
            return user;
        }
        return null;
    }


}

<code block>
package org.opentripplanner.analyst.qbroker;

public class QueuePath {

    public String queueType;
    public String userId;
    public String graphId;
    public String jobId; 
    public int taskId = -1;


    public QueuePath (String uri) {
        String[] pathComponents = uri.split("/");

        if (pathComponents.length > 1 && !pathComponents[1].isEmpty()) {
            queueType = pathComponents[1];
        }
        if (pathComponents.length > 2 && !pathComponents[2].isEmpty()) {
            userId = pathComponents[2];
        }
        if (pathComponents.length > 3 && !pathComponents[3].isEmpty()) {
            graphId = pathComponents[3];
        }
        if (pathComponents.length > 4 && !pathComponents[4].isEmpty()) {
            jobId = pathComponents[4];
        }
        if (pathComponents.length > 5 && !pathComponents[5].isEmpty()) {
            taskId = Integer.parseInt(pathComponents[5]);
        }
    }

    @Override
    public boolean equals(Object o) {
        if (this == o) return true;
        if (o == null || getClass() != o.getClass()) return false;

        QueuePath queuePath = (QueuePath) o;

        if (taskId != queuePath.taskId) return false;
        if (graphId != null ? !graphId.equals(queuePath.graphId) : queuePath.graphId != null) return false;
        if (jobId != null ? !jobId.equals(queuePath.jobId) : queuePath.jobId != null) return false;
        if (queueType != null ? !queueType.equals(queuePath.queueType) : queuePath.queueType != null) return false;
        if (userId != null ? !userId.equals(queuePath.userId) : queuePath.userId != null) return false;

        return true;
    }

    @Override
    public int hashCode() {
        int result = queueType != null ? queueType.hashCode() : 0;
        result = 31 * result + (userId != null ? userId.hashCode() : 0);
        result = 31 * result + (graphId != null ? graphId.hashCode() : 0);
        result = 31 * result + (jobId != null ? jobId.hashCode() : 0);
        result = 31 * result + taskId;
        return result;
    }

    @Override
    public String toString() {
        return "QueuePath{" +
                "queueType='" + queueType + '\'' +
                ", userId='" + userId + '\'' +
                ", graphId='" + graphId + '\'' +
                ", jobId='" + jobId + '\'' +
                ", taskId=" + taskId +
                '}';
    }
}

<code block>
package org.opentripplanner.analyst.qbroker;

import java.util.Iterator;
import java.util.Spliterator;
import java.util.function.Consumer;
import java.util.function.Predicate;


public class CircularList<T> implements Iterable<T> {

    Node<T> head = null;

    public class CircularListIterator implements Iterator<T> {

        Node<T> curr = head;

        @Override
        public boolean hasNext() {
            return curr != null;
        }

        @Override
        public T next() {
            T currElement = curr.element;
            curr = curr.next;
            if (curr == head) {
                curr = null; 
            }
            return currElement;
        }
    }

    @Override
    public Iterator<T> iterator() {
        return new CircularListIterator();
    }

    @Override
    public void forEach(Consumer<? super T> action) {
        Node<T> curr = head;
        do {
            action.accept(curr.element);
            curr = curr.next;
        } while (curr != head);
    }

    @Override
    public Spliterator<T> spliterator() {
        throw new UnsupportedOperationException();
    }

    private static class Node<T> {
        Node prev;
        Node next;
        T element;
    }


    public void insertAtTail (T element) {
        Node<T> newNode = new Node<>();
        newNode.element = element;
        if (head == null) {
            newNode.next = newNode;
            newNode.prev = newNode;
            head = newNode;
        } else {
            newNode.prev = head.prev;
            newNode.next = head;
            head.prev.next = newNode;
            head.prev = newNode;
        }
    }



    public void insertAtHead (T element) {

        insertAtTail(element);
        head = head.prev;
    }


    public T peek () {
        return head.element;
    }


    public T pop () {
        if (head == null) {
            return null;
        }
        T element = head.element;
        if (head == head.next) {

            head = null;
        } else {
            head.prev.next = head.next;
            head.next.prev = head.prev;
            head = head.next;
        }
        return element;
    }


    public T advance() {
        if (head == null) {
            return null;
        }
        T headElement = head.element;
        head = head.next;
        return headElement;
    }


    public T advanceToElement (Predicate<T> predicate) {
        Node<T> start = head;
        do {
            T currElement = advance();
            if (predicate.test(currElement)) {
                return currElement;
            }
        } while (head != start);
        return null;
    }

}

<code block>
package org.opentripplanner.analyst.qbroker;

import org.glassfish.grizzly.http.server.Request;
import org.glassfish.grizzly.http.server.Response;


public class LongPollConnection {

    Task affinity;
    Request request;
    Response response;

    public LongPollConnection(Request request, Response response) {
        this.request = request;
        this.response = response;
    }


}

<code block>
package org.opentripplanner.analyst.qbroker;


public class Task {

    public int taskId;
    public String payload; 
    public long invisibleUntil; 

}

<code block>
package org.opentripplanner.analyst.qbroker;


public class User {

    public final String userId;
    public String region; 
    public final CircularList<Job> jobs = new CircularList<>();

    public User(String userId) {
        this.userId = userId;
    }

    public Job findJob (String jobId, boolean create) {
        for (Job job : jobs) {
            if (job.jobId.equals(jobId)) {
                return job;
            }
        }
        if (create) {
            Job job = new Job(jobId);
            jobs.insertAtTail(job);
            return job;
        }
        return null;
    }


}

<code block>
package org.opentripplanner.analyst.qbroker;


public class QueueType {

    String typeId;


}

<code block>
package org.opentripplanner.analyst.qbroker;

import org.glassfish.grizzly.http.server.HttpServer;
import org.glassfish.grizzly.http.server.NetworkListener;
import org.glassfish.grizzly.strategies.SameThreadIOStrategy;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;
import java.net.BindException;



public class BrokerMain {

    private static final Logger LOG = LoggerFactory.getLogger(BrokerMain.class);

    private static final int PORT = 9001;

    private static final String BIND_ADDRESS = "0.0.0.0";

    public static void main(String[] args) {

        LOG.info("Starting qbroker on port {} of interface {}", PORT, BIND_ADDRESS);
        HttpServer httpServer = new HttpServer();
        NetworkListener networkListener = new NetworkListener("qbroker", BIND_ADDRESS, PORT);
        networkListener.getTransport().setIOStrategy(SameThreadIOStrategy.getInstance()); 
        httpServer.addListener(networkListener);


        Broker broker = new Broker();
        httpServer.getServerConfiguration().addHttpHandler(new BrokerHttpHandler(broker), "/*");
        try {
            httpServer.start();
            LOG.info("Broker running.");
            broker.run(); 
            Thread.currentThread().join();
        } catch (BindException be) {
            LOG.error("Cannot bind to port {}. Is it already in use?", PORT);
        } catch (IOException ioe) {
            LOG.error("IO exception while starting server.");
        } catch (InterruptedException ie) {
            LOG.info("Interrupted, shutting down.");
        }
        httpServer.shutdown();

    }


}

<code block>
package org.opentripplanner.analyst.cluster;

import com.amazonaws.regions.Region;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3Client;
import com.fasterxml.jackson.core.JsonParser;
import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.DeserializationFeature;
import com.fasterxml.jackson.databind.ObjectMapper;
import org.apache.http.HttpEntity;
import org.apache.http.HttpResponse;
import org.apache.http.client.methods.HttpDelete;
import org.apache.http.client.methods.HttpGet;
import org.apache.http.conn.HttpHostConnectException;
import org.apache.http.impl.client.DefaultHttpClient;
import org.apache.http.params.BasicHttpParams;
import org.apache.http.params.HttpConnectionParams;
import org.apache.http.params.HttpParams;
import org.apache.http.util.EntityUtils;
import org.opentripplanner.analyst.PointSet;
import org.opentripplanner.analyst.ResultSet;
import org.opentripplanner.analyst.SampleSet;
import org.opentripplanner.api.model.AgencyAndIdSerializer;
import org.opentripplanner.profile.RepeatedRaptorProfileRouter;
import org.opentripplanner.routing.core.RoutingRequest;
import org.opentripplanner.routing.graph.Graph;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;
import java.io.OutputStream;
import java.io.PipedInputStream;
import java.io.PipedOutputStream;
import java.net.SocketTimeoutException;
import java.util.Map;
import java.util.Random;
import java.util.zip.GZIPOutputStream;


public class AnalystWorker implements Runnable {

    private static final Logger LOG = LoggerFactory.getLogger(AnalystWorker.class);

    public static final int POLL_TIMEOUT = 30000;

    public static final Random random = new Random();

    ObjectMapper objectMapper;

    String s3Prefix = "analyst-dev";

    DefaultHttpClient httpClient = new DefaultHttpClient();


    ClusterGraphBuilder clusterGraphBuilder;


    PointSetDatastore pointSetDatastore;


    AmazonS3 s3;

    String graphId = null;
    long startupTime;


    Region awsRegion = Region.getRegion(Regions.US_EAST_1);

    boolean isSinglePoint = false;

    public AnalystWorker () {

        startupTime = System.currentTimeMillis() / 1000; 





        s3 = new AmazonS3Client();
        s3.setRegion(awsRegion);


        objectMapper = new ObjectMapper();
        objectMapper.configure(JsonParser.Feature.ALLOW_COMMENTS, true);
        objectMapper.configure(JsonParser.Feature.ALLOW_UNQUOTED_FIELD_NAMES, true);
        objectMapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false); 


        objectMapper.registerModule(AgencyAndIdSerializer.makeModule());


        clusterGraphBuilder = new ClusterGraphBuilder(s3Prefix + "-graphs");
        pointSetDatastore = new PointSetDatastore(10, null, false, s3Prefix + "-pointsets");


        HttpParams httpParams = new BasicHttpParams();
        HttpConnectionParams.setConnectionTimeout(httpParams, POLL_TIMEOUT);
        HttpConnectionParams.setSoTimeout(httpParams, POLL_TIMEOUT);
        HttpConnectionParams.setSoKeepalive(httpParams, true);
        httpClient.setParams(httpParams);

    }

    @Override
    public void run() {

        while (true) {
            LOG.info("Long-polling for work ({} second timeout).", POLL_TIMEOUT/1000.0);


            Map<Integer, AnalystClusterRequest> requests = getSomeWork();
            if (requests == null) {
                LOG.info("Didn't get any work. Retrying.");
                continue;
            }
            requests.values().parallelStream().forEach(this::handleOneRequest);

            LOG.info("Removing requests from broker queue.");
            for (int taskId : requests.keySet()) {
                boolean success = deleteRequest(taskId, requests.get(taskId));
                LOG.info("deleted task {}: {}", taskId, success ? "SUCCESS" : "FAIL");
            }
        }
    }

    private void handleOneRequest(AnalystClusterRequest clusterRequest) {
        try {
            LOG.info("Handling message {}", clusterRequest.toString());




            Graph graph = clusterGraphBuilder.getGraph(clusterRequest.graphId);
            graphId = clusterRequest.graphId; 


            ResultEnvelope envelope = new ResultEnvelope();
            if (clusterRequest.profileRequest != null) {

                SampleSet sampleSet = null;
                if (clusterRequest.destinationPointsetId != null) {


                    PointSet pointSet = pointSetDatastore.get(clusterRequest.destinationPointsetId);
                    sampleSet = pointSet.getSampleSet(graph);
                }

                RepeatedRaptorProfileRouter router =
                        new RepeatedRaptorProfileRouter(graph, clusterRequest.profileRequest, sampleSet);
                router.route();
                ResultSet.RangeSet results = router.makeResults(clusterRequest.includeTimes);

                envelope.bestCase  = results.min;
                envelope.avgCase   = results.avg;
                envelope.worstCase = results.max;
            } else {

                RoutingRequest routingRequest = clusterRequest.routingRequest;

            }

            if (clusterRequest.outputQueue != null) {

            }
            if (clusterRequest.outputLocation != null) {


                try {
                    String s3key = String.join("/", clusterRequest.jobId, clusterRequest.id + ".json.gz");
                    PipedInputStream inPipe = new PipedInputStream();
                    PipedOutputStream outPipe = new PipedOutputStream(inPipe);
                    new Thread(() -> {
                        s3.putObject(clusterRequest.outputLocation, s3key, inPipe, null);
                    }).start();
                    OutputStream gzipOutputStream = new GZIPOutputStream(outPipe);
                    objectMapper.writeValue(gzipOutputStream, envelope);
                } catch (IOException e) {
                    e.printStackTrace();
                }
            }

        } catch (Exception ex) {
            LOG.error("An error occurred while routing: " + ex.getMessage());
            ex.printStackTrace();
        }

    }

    public Map<Integer, AnalystClusterRequest> getSomeWork() {


        String url = "http://localhost:9001";
        url += "/jobs/userA/graphA/jobA";
        HttpGet httpGet = new HttpGet(url);
        HttpResponse response = null;
        try {
            response = httpClient.execute(httpGet);
            if (response.getStatusLine().getStatusCode() != 200) {
                return null;
            }
            HttpEntity entity = response.getEntity();
            if (entity == null) {
                return null;
            }
            return objectMapper.readValue(entity.getContent(), new TypeReference<Map<Integer, AnalystClusterRequest>>(){});
        } catch (JsonProcessingException e) {
            LOG.error("JSON processing exception while getting work: {}", e.getMessage());
        } catch (SocketTimeoutException stex) {
            LOG.error("Socket timeout while waiting to receive work.");
        } catch (HttpHostConnectException ce) {
            LOG.error("Broker refused connection. Sleeping before retry.");
            try {
                Thread.currentThread().sleep(5000);
            } catch (InterruptedException e) { }
        } catch (IOException e) {
            LOG.error("IO exception while getting work.");
            e.printStackTrace();
        }
        return null;

    }


    public boolean deleteRequest (int taskId, AnalystClusterRequest clusterRequest) {
        String url = "http://localhost:9001";
        url += String.format("/jobs/%s/%s/%s/%s", clusterRequest.userId, clusterRequest.graphId, clusterRequest.jobId, taskId);
        HttpDelete httpDelete = new HttpDelete(url);
        try {

            HttpResponse response = httpClient.execute(httpDelete);

            EntityUtils.consumeQuietly(response.getEntity());
            return (response.getStatusLine().getStatusCode() == 200);
        } catch (Exception e) {
            e.printStackTrace();
            return false;
        }
    }

}
<code block>
package org.opentripplanner.analyst.cluster;

import org.opentripplanner.profile.ProfileRequest;
import org.opentripplanner.routing.core.RoutingRequest;

import java.io.Serializable;


public class AnalystClusterRequest implements Serializable {


	public String destinationPointsetId;


	public String userId;


	public String graphId;


	public String jobId;


	public String id;


	public String directOutputUrl;


	public String outputQueue;


	public String outputLocation;


	public ProfileRequest profileRequest;


	public RoutingRequest routingRequest;


	public boolean includeTimes = false;
	
	private AnalystClusterRequest(String destinationPointsetId, String graphId) {
		this.destinationPointsetId = destinationPointsetId;
		this.graphId = graphId;
	}


	public AnalystClusterRequest(String destinationPointsetId, String graphId, ProfileRequest req) {
		this(destinationPointsetId, graphId);
		routingRequest = null;
		try {
			profileRequest = req.clone();
		} catch (CloneNotSupportedException e) {
			throw new AssertionError();
		}
		profileRequest.analyst = true;
		profileRequest.toLat = profileRequest.fromLat;
		profileRequest.toLon = profileRequest.fromLon;
	}


	public AnalystClusterRequest(String destinationPointsetId, String graphId, RoutingRequest req) {
		this(destinationPointsetId, graphId);
		profileRequest = null;
		routingRequest = req.clone();
		routingRequest.batch = true;
		routingRequest.rctx = null;
	}


	public AnalystClusterRequest () {  }
}

<code block>
package org.opentripplanner.analyst.cluster;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.auth.AWSCredentials;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3Client;
import com.amazonaws.services.s3.model.S3Object;
import com.google.common.collect.Maps;
import org.apache.commons.io.FileUtils;
import org.apache.commons.io.IOUtils;
import org.opentripplanner.graph_builder.GraphBuilder;
import org.opentripplanner.routing.graph.Graph;
import org.opentripplanner.routing.impl.DefaultStreetVertexIndexFactory;
import org.opentripplanner.routing.services.GraphService;
import org.opentripplanner.routing.services.GraphSource;
import org.opentripplanner.routing.services.GraphSource.Factory;
import org.opentripplanner.standalone.CommandLineParameters;
import org.opentripplanner.standalone.Router;

import java.io.*;
import java.util.Collection;
import java.util.Enumeration;
import java.util.Map;
import java.util.zip.ZipEntry;
import java.util.zip.ZipException;
import java.util.zip.ZipFile;
import java.util.zip.ZipOutputStream;


public class ClusterGraphService extends GraphService { 

	static File GRAPH_DIR = new File("cache", "graphs");
	
	private String graphBucket;
	
	private Boolean workOffline = false;
	private AmazonS3Client s3;


	private Map<String,Router> graphMap = Maps.newConcurrentMap();
	
	@Override
	public synchronized Router getRouter(String graphId) {
		
		GRAPH_DIR.mkdirs();
		
		if(!graphMap.containsKey(graphId)) {
			
			try {
				if (!bucketCached(graphId)) {
					if(!workOffline) {
						downloadGraphSourceFiles(graphId, GRAPH_DIR);
					}
				}
			} catch (IOException e) {
				e.printStackTrace();
			}
			
			CommandLineParameters params = new CommandLineParameters();
			params.build = new File(GRAPH_DIR, graphId);
			params.inMemory = true;
			GraphBuilder gbt = GraphBuilder.forDirectory(params, params.build);
			gbt.run();
			
			Graph g = gbt.getGraph();
			
			g.routerId = graphId;
			
			g.index(new DefaultStreetVertexIndexFactory());

			g.index.clusterStopsAsNeeded();
			
			Router r = new Router(graphId, g);
			



			
			return r;
					
		}
		else {
			return graphMap.get(graphId);
		}
	}

	public ClusterGraphService(String s3CredentialsFilename, Boolean workOffline, String bucket) {
		
		if(!workOffline) {
			if (s3CredentialsFilename != null) {
				AWSCredentials creds = new ProfileCredentialsProvider(s3CredentialsFilename, "default").getCredentials();
				s3 = new AmazonS3Client(creds);
			}
			else {


				s3 = new AmazonS3Client();
			}
			
			this.graphBucket = bucket;
		}
		
		this.workOffline = workOffline;
	}
	

	public void addGraphFile(File graphFile) throws IOException {
		
		String graphId = graphFile.getName();
		
		if(graphId.endsWith(".zip"))
			graphId = graphId.substring(0, graphId.length() - 4);
		
		File graphDir = new File(GRAPH_DIR, graphId);
		
		if (graphDir.exists()) {
			if (graphDir.list().length == 0) {
				graphDir.delete();
			}
			else {
				return;
			}
		}
		

		graphDir.mkdirs();
		
		File graphDataZip = new File(GRAPH_DIR, graphId + ".zip");
				


		if(graphFile.isDirectory()) {
			FileUtils.copyDirectory(graphFile, graphDir);
			
			zipGraphDir(graphDir, graphDataZip);
		}
		else if(graphFile.getName().endsWith(".zip")) {
			FileUtils.copyFile(graphFile, graphDataZip);
			unpackGraphZip(graphDataZip, graphDir, false);
		}
		else {
			graphDataZip = null;
		}
			
		if(!workOffline && graphDataZip != null) {

			try {
				s3.getObject(graphBucket, graphId + ".zip");
			} catch (AmazonServiceException e) {
				s3.putObject(graphBucket, graphId+".zip", graphDataZip);
			}
		}
		
		graphDataZip.delete();
		
	}
	
	public synchronized File getZippedGraph(String graphId) throws IOException {
		
		File graphDataDir = new File(GRAPH_DIR, graphId);
		
		File graphZipFile = new File(GRAPH_DIR, graphId + ".zip");
		
		if(!graphDataDir.exists() && graphDataDir.isDirectory()) {
			
			FileOutputStream fileOutputStream = new FileOutputStream(graphZipFile);
			ZipOutputStream zipOutputStream = new ZipOutputStream(fileOutputStream);
			
			byte[] buffer = new byte[1024];
			
			for(File f : graphDataDir.listFiles()) {
				ZipEntry zipEntry = new ZipEntry(f.getName());
				zipOutputStream.putNextEntry(zipEntry);
	    		FileInputStream fileInput = new FileInputStream(f);

	    		int len;
	    		while ((len = fileInput.read(buffer)) > 0) {
	    			zipOutputStream.write(buffer, 0, len);
	    		}
	 
	    		fileInput.close();
	    		zipOutputStream.closeEntry();
			}
			
			zipOutputStream.close();
			
			return graphZipFile;
					
		}
		
		return null;
		
	}
	
	private static boolean bucketCached(String graphId) throws IOException {
		File graphData = new File(GRAPH_DIR, graphId);
		

		if(!graphData.exists()) {
			File graphDataZip = new File(GRAPH_DIR, graphId + ".zip");
			
			if(graphDataZip.exists()) {
				zipGraphDir(graphData, graphDataZip);
			}
		}
		
		
		return graphData.exists() && graphData.isDirectory();
	}

	private void downloadGraphSourceFiles(String graphId, File dir) throws IOException {

		File graphCacheDir = dir;
		if (!graphCacheDir.exists())
			graphCacheDir.mkdirs();

		File graphZipFile = new File(graphCacheDir, graphId + ".zip");

		File extractedGraphDir = new File(graphCacheDir, graphId);

		if (extractedGraphDir.exists()) {
			FileUtils.deleteDirectory(extractedGraphDir);
		}

		extractedGraphDir.mkdirs();

		S3Object graphZip = s3.getObject(graphBucket, graphId+".zip");

		InputStream zipFileIn = graphZip.getObjectContent();

		OutputStream zipFileOut = new FileOutputStream(graphZipFile);

		IOUtils.copy(zipFileIn, zipFileOut);
		IOUtils.closeQuietly(zipFileIn);
		IOUtils.closeQuietly(zipFileOut);

		unpackGraphZip(graphZipFile, extractedGraphDir);
	}

	private static void unpackGraphZip(File graphZipFile, File extractedGraphDir) throws ZipException, IOException {

		unpackGraphZip(graphZipFile, extractedGraphDir, true);
	}
	
	private static void unpackGraphZip(File graphZipFile, File extractedGraphDir, boolean delete) throws ZipException, IOException {
		
		ZipFile zipFile = new ZipFile(graphZipFile);
		
		Enumeration<? extends ZipEntry> entries = zipFile.entries();

		while (entries.hasMoreElements()) {

			ZipEntry entry = entries.nextElement();
			File entryDestination = new File(extractedGraphDir, entry.getName());

			entryDestination.getParentFile().mkdirs();

			if (entry.isDirectory())
				entryDestination.mkdirs();
			else {
				InputStream entryFileIn = zipFile.getInputStream(entry);
				OutputStream entryFileOut = new FileOutputStream(entryDestination);
				IOUtils.copy(entryFileIn, entryFileOut);
				IOUtils.closeQuietly(entryFileIn);
				IOUtils.closeQuietly(entryFileOut);
			}
		}

		zipFile.close();

		if (delete) {
			graphZipFile.delete();
		}
	}
	
	private static void zipGraphDir(File graphDirectory, File zipGraphFile) throws IOException {
		
		FileOutputStream fileOutputStream = new FileOutputStream(zipGraphFile);
		ZipOutputStream zipOutputStream = new ZipOutputStream(fileOutputStream);
		
		byte[] buffer = new byte[1024];
		
		for(File f : graphDirectory.listFiles()) {
			if (f.isDirectory())
				continue;
			
			ZipEntry zipEntry = new ZipEntry(f.getName());
			zipOutputStream.putNextEntry(zipEntry);
    		FileInputStream fileInput = new FileInputStream(f);

    		int len;
    		while ((len = fileInput.read(buffer)) > 0) {
    			zipOutputStream.write(buffer, 0, len);
    		}
 
    		fileInput.close();
    		zipOutputStream.closeEntry();
		}
		
		zipOutputStream.close();
	}
	

	@Override
	public int evictAll() {
		graphMap.clear();
		return 0;
	}

	@Override
	public Collection<String> getRouterIds() {
		return graphMap.keySet();
	}

	@Override
	public Factory getGraphSourceFactory() {
		return null;
	}

	@Override
	public boolean registerGraph(String arg0, GraphSource arg1) {
		return false;
	}

	@Override
	public void setDefaultRouterId(String arg0) {	
	}
}

<code block>
package org.opentripplanner.analyst.broker;

import gnu.trove.map.TIntLongMap;
import gnu.trove.map.TIntObjectMap;
import gnu.trove.map.hash.TIntLongHashMap;
import gnu.trove.map.hash.TIntObjectHashMap;
import org.opentripplanner.analyst.cluster.AnalystClusterRequest;

import java.util.ArrayDeque;
import java.util.List;
import java.util.Queue;


public class Job {

    private int nTasks = 0;


    String graphId;

    public final String jobId;


    Queue<AnalystClusterRequest> visibleTasks = new ArrayDeque<>();


    TIntObjectMap<AnalystClusterRequest> invisibleTasks = new TIntObjectHashMap<>();

    TIntLongMap invisibleUntil = new TIntLongHashMap();

    public Job (String jobId) {
        this.jobId = jobId;
    }


    public void addTask (AnalystClusterRequest task) {
        nTasks++;
        visibleTasks.add(task);
    }

    public void markTasksDelivered(List<AnalystClusterRequest> tasks) {
        long deliveryTime = System.currentTimeMillis();
        long visibleAt = deliveryTime + 60000; 
        for (AnalystClusterRequest task : tasks) {
            invisibleUntil.put(task.taskId, visibleAt);
            invisibleTasks.put(task.taskId, task);
        }
    }

}

<code block>
package org.opentripplanner.analyst.broker;

import com.fasterxml.jackson.annotation.JsonInclude;
import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.ObjectMapper;
import org.glassfish.grizzly.http.Method;
import org.glassfish.grizzly.http.server.HttpHandler;
import org.glassfish.grizzly.http.server.Request;
import org.glassfish.grizzly.http.server.Response;
import org.glassfish.grizzly.http.util.HttpStatus;
import org.opentripplanner.analyst.cluster.AnalystClusterRequest;
import org.opentripplanner.api.model.AgencyAndIdSerializer;

import java.io.IOException;
import java.util.List;


class BrokerHttpHandler extends HttpHandler {


    private ObjectMapper mapper = new ObjectMapper()
            .registerModule(AgencyAndIdSerializer.makeModule())
            .setSerializationInclusion(JsonInclude.Include.NON_NULL);;

    private Broker broker;

    public BrokerHttpHandler(Broker broker) {
        this.broker = broker;
    }

    @Override
    public void service(Request request, Response response) throws Exception {




        response.setContentType("application/json");


        QueuePath queuePath = new QueuePath(request.getPathInfo());



        try {
            if (request.getMethod() == Method.HEAD) {

                mapper.readTree(request.getInputStream());
                response.setStatus(HttpStatus.OK_200);
                return;
            } else if (request.getMethod() == Method.GET) {

                request.getRequest().getConnection().addCloseListener((closeable, iCloseType) -> {
                    broker.removeSuspendedResponse(queuePath.graphId, response);
                });
                response.suspend(); 
                broker.registerSuspendedResponse(queuePath.graphId, response);
            } else if (request.getMethod() == Method.POST) {


                List<AnalystClusterRequest> tasks =
                    mapper.readValue(request.getInputStream(), new TypeReference<List<AnalystClusterRequest>>(){});
                for (AnalystClusterRequest task : tasks) {
                    if (!task.graphId.equals(queuePath.graphId)
                            || !task.userId.equals(queuePath.userId)
                            || !task.jobId.equals(queuePath.jobId)) {
                        response.setStatus(HttpStatus.BAD_REQUEST_400);
                        response.setDetailMessage("Task graph/user/job ID does not match POST path.");
                        return;
                    }
                }
                broker.enqueueTasks(queuePath, tasks);
                response.setStatus(HttpStatus.ACCEPTED_202);
            } else if (request.getMethod() == Method.DELETE) {

                if (broker.deleteTask(queuePath)) {
                    response.setStatus(HttpStatus.OK_200);
                } else {
                    response.setStatus(HttpStatus.NOT_FOUND_404);
                }
            } else {
                response.setStatus(HttpStatus.BAD_REQUEST_400);
                response.setDetailMessage("Unrecognized HTTP method.");
            }
        } catch (JsonProcessingException jpex) {
            response.setStatus(HttpStatus.BAD_REQUEST_400);
            response.setDetailMessage("Could not decode/encode JSON payload. " + jpex.getMessage());
            jpex.printStackTrace();
        } catch (Exception ex) {
            response.setStatus(HttpStatus.INTERNAL_SERVER_ERROR_500);
            response.setDetailMessage(ex.toString());
            ex.printStackTrace();
        }
    }

    public void writeJson (Response response, Object object) throws IOException {
        mapper.writeValue(response.getOutputStream(), object);
    }

}

<code block>
package org.opentripplanner.analyst.broker;

import com.fasterxml.jackson.databind.ObjectMapper;
import org.glassfish.grizzly.http.server.Response;
import org.glassfish.grizzly.http.util.HttpStatus;
import org.opentripplanner.analyst.cluster.AnalystClusterRequest;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;
import java.io.OutputStream;
import java.util.ArrayDeque;
import java.util.ArrayList;
import java.util.Collection;
import java.util.Deque;
import java.util.HashMap;
import java.util.List;
import java.util.Map;


public class Broker implements Runnable {



    private static final Logger LOG = LoggerFactory.getLogger(Broker.class);

    private CircularList<User> users = new CircularList<>();

    private int nUndeliveredTasks = 0;

    private int nWaitingConsumers = 0; 

    private int nextTaskId = 0;

    private ObjectMapper mapper = new ObjectMapper();


    Map<String, Deque<Response>> connectionsForGraph = new HashMap<>();



    public synchronized void enqueueTasks (QueuePath queuePath, Collection<AnalystClusterRequest> tasks) {
        LOG.debug("Queue {}", queuePath);

        User user = findUser(queuePath.userId, true);
        Job job = user.findJob(queuePath.jobId, true);
        for (AnalystClusterRequest task : tasks) {
            task.taskId = nextTaskId++;
            job.addTask(task);
            nUndeliveredTasks += 1;
            LOG.debug("Enqueued task id {} in job {}", task.taskId, job.jobId);
        }


        notify();
    }


    public synchronized void registerSuspendedResponse(String graphId, Response response) {

        Deque<Response> deque = connectionsForGraph.get(graphId);
        if (deque == null) {
            deque = new ArrayDeque<>();
            connectionsForGraph.put(graphId, deque);
        }
        deque.addLast(response);
        nWaitingConsumers += 1;


        notify();
    }


    public synchronized boolean removeSuspendedResponse(String graphId, Response response) {
        Deque<Response> deque = connectionsForGraph.get(graphId);
        if (deque == null) {
            return false;
        }
        if (deque.remove(response)) {
            nWaitingConsumers -= 1;
            LOG.debug("Removed closed connection from queue.");
            logQueueStatus();
            return true;
        }
        return false;
    }

    private void logQueueStatus() {
        LOG.info("Status {} undelivered, {} consumers waiting.", nUndeliveredTasks, nWaitingConsumers);
    }


    public synchronized void deliverTasksForOneJob () throws InterruptedException {


        while (nUndeliveredTasks == 0) {
            LOG.debug("Task delivery thread is going to sleep, there are no tasks waiting for delivery.");
            logQueueStatus();
            wait();
        }
        LOG.debug("Task delivery thread is awake and there are some undelivered tasks.");
        logQueueStatus();


        Job job = null;
        while (job == null) {
            User user = users.advance();
            if (user == null) {
                LOG.error("There should always be at least one user here, because there is an undelivered task.");
            }
            job = user.jobs.advanceToElement(e -> e.visibleTasks.size() > 0);
        }



        LOG.debug("Task delivery thread has found undelivered tasks in job {}.", job.jobId);
        while (true) {
            while (nWaitingConsumers == 0) {
                LOG.debug("Task delivery thread is going to sleep, there are no consumers waiting.");

                wait();
            }
            LOG.debug("Task delivery thread is awake, and some consumers are waiting.");
            logQueueStatus();



            LOG.debug("Looking for an eligible consumer, respecting graph affinity.");
            Deque<Response> deque = connectionsForGraph.get(job.graphId);
            while (deque != null && !deque.isEmpty()) {
                Response response = deque.pop();
                nWaitingConsumers -= 1;
                if (deliver(job, response)) {
                    return;
                }
            }


            LOG.debug("No consumers with the right affinity. Looking for any consumer.");
            List<Deque<Response>> deques = new ArrayList<>(connectionsForGraph.values());
            deques.sort((d1, d2) -> Integer.compare(d2.size(), d1.size()));
            for (Deque<Response> d : deques) {
                while (!d.isEmpty()) {
                    Response response = d.pop();
                    nWaitingConsumers -= 1;
                    if (deliver(job, response)) {
                        return;
                    }
                }
            }


            LOG.debug("No consumer was available. They all must have closed their connections.");
            if (nWaitingConsumers != 0) {
                throw new AssertionError("There should be no waiting consumers here, something is wrong.");
            }
        }

    }


    public synchronized boolean deliver (Job job, Response response) {


        if (!response.getRequest().getRequest().getConnection().isOpen()) {
            LOG.debug("Consumer connection was closed. It will be removed.");
            return false;
        }


        List<AnalystClusterRequest> tasks = new ArrayList<>();
        while (tasks.size() < 4 && !job.visibleTasks.isEmpty()) {
            tasks.add(job.visibleTasks.poll());
        }


        try {
            response.setStatus(HttpStatus.OK_200);
            OutputStream out = response.getOutputStream();
            mapper.writeValue(out, tasks);
            response.resume();
        } catch (IOException e) {

            LOG.debug("Consumer connection caused IO error, it will be removed.");
            response.setStatus(HttpStatus.INTERNAL_SERVER_ERROR_500);
            response.resume();

            job.visibleTasks.addAll(tasks);
            return false;
        }


        LOG.debug("Delivery of {} tasks succeeded.", tasks.size());
        nUndeliveredTasks -= tasks.size();
        job.markTasksDelivered(tasks);
        return true;

    }


    public synchronized boolean deleteTask (QueuePath queuePath) {

        User user = findUser(queuePath.userId, false);
        if (user == null) {
            return false;
        }

        Job job = user.findJob(queuePath.jobId, false);
        if (job == null) {
            return false;
        }




        return job.invisibleTasks.remove(queuePath.taskId) != null;

    }



    @Override
    public void run() {
        while (true) {
            try {
                deliverTasksForOneJob();
            } catch (InterruptedException e) {
                LOG.warn("Task pump thread was interrupted.");
                return;
            }
        }
    }


    public User findUser (String userId, boolean create) {
        for (User user : users) {
            if (user.userId.equals(userId)) {
                return user;
            }
        }
        if (create) {
            User user = new User(userId);
            users.insertAtTail(user);
            return user;
        }
        return null;
    }


}

<code block>
package org.opentripplanner.analyst.broker;

public class QueuePath {

    public String queueType;
    public String userId;
    public String graphId;
    public String jobId; 
    public int taskId = -1;


    public QueuePath (String uri) {
        String[] pathComponents = uri.split("/");

        if (pathComponents.length > 1 && !pathComponents[1].isEmpty()) {
            queueType = pathComponents[1];
        }
        if (pathComponents.length > 2 && !pathComponents[2].isEmpty()) {
            userId = pathComponents[2];
        }
        if (pathComponents.length > 3 && !pathComponents[3].isEmpty()) {
            graphId = pathComponents[3];
        }
        if (pathComponents.length > 4 && !pathComponents[4].isEmpty()) {
            jobId = pathComponents[4];
        }
        if (pathComponents.length > 5 && !pathComponents[5].isEmpty()) {
            taskId = Integer.parseInt(pathComponents[5]);
        }
    }

    @Override
    public boolean equals(Object o) {
        if (this == o) return true;
        if (o == null || getClass() != o.getClass()) return false;

        QueuePath queuePath = (QueuePath) o;

        if (taskId != queuePath.taskId) return false;
        if (graphId != null ? !graphId.equals(queuePath.graphId) : queuePath.graphId != null) return false;
        if (jobId != null ? !jobId.equals(queuePath.jobId) : queuePath.jobId != null) return false;
        if (queueType != null ? !queueType.equals(queuePath.queueType) : queuePath.queueType != null) return false;
        if (userId != null ? !userId.equals(queuePath.userId) : queuePath.userId != null) return false;

        return true;
    }

    @Override
    public int hashCode() {
        int result = queueType != null ? queueType.hashCode() : 0;
        result = 31 * result + (userId != null ? userId.hashCode() : 0);
        result = 31 * result + (graphId != null ? graphId.hashCode() : 0);
        result = 31 * result + (jobId != null ? jobId.hashCode() : 0);
        result = 31 * result + taskId;
        return result;
    }

    @Override
    public String toString() {
        return "QueuePath{" +
                "queueType='" + queueType + '\'' +
                ", userId='" + userId + '\'' +
                ", graphId='" + graphId + '\'' +
                ", jobId='" + jobId + '\'' +
                ", taskId=" + taskId +
                '}';
    }
}

<code block>
package org.opentripplanner.analyst.broker;

import java.util.Iterator;
import java.util.Spliterator;
import java.util.function.Consumer;
import java.util.function.Predicate;


public class CircularList<T> implements Iterable<T> {

    Node<T> head = null;

    public class CircularListIterator implements Iterator<T> {

        Node<T> curr = head;

        @Override
        public boolean hasNext() {
            return curr != null;
        }

        @Override
        public T next() {
            T currElement = curr.element;
            curr = curr.next;
            if (curr == head) {
                curr = null; 
            }
            return currElement;
        }
    }

    @Override
    public Iterator<T> iterator() {
        return new CircularListIterator();
    }

    @Override
    public void forEach(Consumer<? super T> action) {
        Node<T> curr = head;
        do {
            action.accept(curr.element);
            curr = curr.next;
        } while (curr != head);
    }

    @Override
    public Spliterator<T> spliterator() {
        throw new UnsupportedOperationException();
    }

    private static class Node<T> {
        Node prev;
        Node next;
        T element;
    }


    public void insertAtTail (T element) {
        Node<T> newNode = new Node<>();
        newNode.element = element;
        if (head == null) {
            newNode.next = newNode;
            newNode.prev = newNode;
            head = newNode;
        } else {
            newNode.prev = head.prev;
            newNode.next = head;
            head.prev.next = newNode;
            head.prev = newNode;
        }
    }



    public void insertAtHead (T element) {

        insertAtTail(element);
        head = head.prev;
    }


    public T peek () {
        return head.element;
    }


    public T pop () {
        if (head == null) {
            return null;
        }
        T element = head.element;
        if (head == head.next) {

            head = null;
        } else {
            head.prev.next = head.next;
            head.next.prev = head.prev;
            head = head.next;
        }
        return element;
    }


    public T advance() {
        if (head == null) {
            return null;
        }
        T headElement = head.element;
        head = head.next;
        return headElement;
    }


    public T advanceToElement (Predicate<T> predicate) {
        Node<T> start = head;
        do {
            T currElement = advance();
            if (predicate.test(currElement)) {
                return currElement;
            }
        } while (head != start);
        return null;
    }

}

<code block>
package org.opentripplanner.analyst.broker;


public class User {

    public final String userId;
    public String region; 
    public final CircularList<Job> jobs = new CircularList<>();

    public User(String userId) {
        this.userId = userId;
    }

    public Job findJob (String jobId, boolean create) {
        for (Job job : jobs) {
            if (job.jobId.equals(jobId)) {
                return job;
            }
        }
        if (create) {
            Job job = new Job(jobId);
            jobs.insertAtTail(job);
            return job;
        }
        return null;
    }


}

<code block>
package org.opentripplanner.analyst.broker;


public class QueueType {

    String typeId;


}

<code block>
package org.opentripplanner.analyst.broker;

import org.glassfish.grizzly.http.server.HttpServer;
import org.glassfish.grizzly.http.server.NetworkListener;
import org.glassfish.grizzly.strategies.SameThreadIOStrategy;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;
import java.net.BindException;



public class BrokerMain {

    private static final Logger LOG = LoggerFactory.getLogger(BrokerMain.class);

    private static final int PORT = 9001;

    private static final String BIND_ADDRESS = "0.0.0.0";

    public static void main(String[] args) {

        LOG.info("Starting qbroker on port {} of interface {}", PORT, BIND_ADDRESS);
        HttpServer httpServer = new HttpServer();
        NetworkListener networkListener = new NetworkListener("qbroker", BIND_ADDRESS, PORT);
        networkListener.getTransport().setIOStrategy(SameThreadIOStrategy.getInstance()); 
        httpServer.addListener(networkListener);


        Broker broker = new Broker();
        httpServer.getServerConfiguration().addHttpHandler(new BrokerHttpHandler(broker), "/*");
        try {
            httpServer.start();
            LOG.info("Broker running.");
            broker.run(); 
            Thread.currentThread().join();
        } catch (BindException be) {
            LOG.error("Cannot bind to port {}. Is it already in use?", PORT);
        } catch (IOException ioe) {
            LOG.error("IO exception while starting server.");
        } catch (InterruptedException ie) {
            LOG.info("Interrupted, shutting down.");
        }
        httpServer.shutdown();

    }


}

<code block>
package org.opentripplanner.analyst.cluster;

import com.amazonaws.regions.Region;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3Client;
import com.fasterxml.jackson.core.JsonParser;
import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.DeserializationFeature;
import com.fasterxml.jackson.databind.ObjectMapper;
import org.apache.http.HttpEntity;
import org.apache.http.HttpResponse;
import org.apache.http.client.methods.HttpDelete;
import org.apache.http.client.methods.HttpGet;
import org.apache.http.conn.HttpHostConnectException;
import org.apache.http.impl.client.DefaultHttpClient;
import org.apache.http.params.BasicHttpParams;
import org.apache.http.params.HttpConnectionParams;
import org.apache.http.params.HttpParams;
import org.apache.http.util.EntityUtils;
import org.opentripplanner.analyst.PointSet;
import org.opentripplanner.analyst.ResultSet;
import org.opentripplanner.analyst.SampleSet;
import org.opentripplanner.api.model.AgencyAndIdSerializer;
import org.opentripplanner.profile.RepeatedRaptorProfileRouter;
import org.opentripplanner.routing.core.RoutingRequest;
import org.opentripplanner.routing.graph.Graph;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;
import java.io.OutputStream;
import java.io.PipedInputStream;
import java.io.PipedOutputStream;
import java.net.SocketTimeoutException;
import java.util.List;
import java.util.Random;
import java.util.zip.GZIPOutputStream;


public class AnalystWorker implements Runnable {

    private static final Logger LOG = LoggerFactory.getLogger(AnalystWorker.class);

    public static final int POLL_TIMEOUT = 10000;

    public static final Random random = new Random();

    ObjectMapper objectMapper;

    String BROKER_BASE_URL = "http://localhost:9001";

    String s3Prefix = "analyst-dev";

    DefaultHttpClient httpClient = new DefaultHttpClient();


    ClusterGraphBuilder clusterGraphBuilder;


    PointSetDatastore pointSetDatastore;


    AmazonS3 s3;

    String graphId = null;
    long startupTime;


    Region awsRegion = Region.getRegion(Regions.US_EAST_1);

    boolean isSinglePoint = false;

    public AnalystWorker () {

        startupTime = System.currentTimeMillis() / 1000; 





        s3 = new AmazonS3Client();
        s3.setRegion(awsRegion);


        objectMapper = new ObjectMapper();
        objectMapper.configure(JsonParser.Feature.ALLOW_COMMENTS, true);
        objectMapper.configure(JsonParser.Feature.ALLOW_UNQUOTED_FIELD_NAMES, true);
        objectMapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false); 


        objectMapper.registerModule(AgencyAndIdSerializer.makeModule());


        clusterGraphBuilder = new ClusterGraphBuilder(s3Prefix + "-graphs");
        pointSetDatastore = new PointSetDatastore(10, null, false, s3Prefix + "-pointsets");


        HttpParams httpParams = new BasicHttpParams();
        HttpConnectionParams.setConnectionTimeout(httpParams, POLL_TIMEOUT);
        HttpConnectionParams.setSoTimeout(httpParams, POLL_TIMEOUT);
        HttpConnectionParams.setSoKeepalive(httpParams, true);
        httpClient.setParams(httpParams);

    }

    @Override
    public void run() {

        while (true) {
            LOG.info("Long-polling for work ({} second timeout).", POLL_TIMEOUT/1000.0);


            List<AnalystClusterRequest> tasks = getSomeWork();
            if (tasks == null) {
                LOG.info("Didn't get any work. Retrying.");
                continue;
            }
            tasks.parallelStream().forEach(this::handleOneRequest);

            LOG.info("Removing requests from broker queue.");
            for (AnalystClusterRequest task : tasks) {
                boolean success = deleteRequest(task);
                LOG.info("deleted task {}: {}", task.taskId, success ? "SUCCESS" : "FAIL");
            }
        }
    }

    private void handleOneRequest(AnalystClusterRequest clusterRequest) {
        try {
            LOG.info("Handling message {}", clusterRequest.toString());




            Graph graph = clusterGraphBuilder.getGraph(clusterRequest.graphId);
            graphId = clusterRequest.graphId; 


            ResultEnvelope envelope = new ResultEnvelope();
            if (clusterRequest.profileRequest != null) {

                SampleSet sampleSet = null;
                if (clusterRequest.destinationPointsetId != null) {


                    PointSet pointSet = pointSetDatastore.get(clusterRequest.destinationPointsetId);
                    sampleSet = pointSet.getSampleSet(graph);
                }

                RepeatedRaptorProfileRouter router =
                        new RepeatedRaptorProfileRouter(graph, clusterRequest.profileRequest, sampleSet);
                router.route();
                ResultSet.RangeSet results = router.makeResults(clusterRequest.includeTimes);

                envelope.bestCase  = results.min;
                envelope.avgCase   = results.avg;
                envelope.worstCase = results.max;
            } else {

                RoutingRequest routingRequest = clusterRequest.routingRequest;

            }

            if (clusterRequest.outputQueue != null) {

            }
            if (clusterRequest.outputLocation != null) {


                try {
                    String s3key = String.join("/", clusterRequest.jobId, clusterRequest.id + ".json.gz");
                    PipedInputStream inPipe = new PipedInputStream();
                    PipedOutputStream outPipe = new PipedOutputStream(inPipe);
                    new Thread(() -> {
                        s3.putObject(clusterRequest.outputLocation, s3key, inPipe, null);
                    }).start();
                    OutputStream gzipOutputStream = new GZIPOutputStream(outPipe);
                    objectMapper.writeValue(gzipOutputStream, envelope);
                } catch (IOException e) {
                    e.printStackTrace();
                }
            }

        } catch (Exception ex) {
            LOG.error("An error occurred while routing: " + ex.getMessage());
            ex.printStackTrace();
        }

    }

    public List<AnalystClusterRequest> getSomeWork() {


        String url = BROKER_BASE_URL + "/jobs/userA/graphA/jobA";
        HttpGet httpGet = new HttpGet(url);
        HttpResponse response = null;
        try {
            response = httpClient.execute(httpGet);
            if (response.getStatusLine().getStatusCode() != 200) {
                return null;
            }
            HttpEntity entity = response.getEntity();
            if (entity == null) {
                return null;
            }
            return objectMapper.readValue(entity.getContent(), new TypeReference<List<AnalystClusterRequest>>(){});
        } catch (JsonProcessingException e) {
            LOG.error("JSON processing exception while getting work: {}", e.getMessage());
        } catch (SocketTimeoutException stex) {
            LOG.error("Socket timeout while waiting to receive work.");
        } catch (HttpHostConnectException ce) {
            LOG.error("Broker refused connection. Sleeping before retry.");
            try {
                Thread.currentThread().sleep(5000);
            } catch (InterruptedException e) { }
        } catch (IOException e) {
            LOG.error("IO exception while getting work.");
            e.printStackTrace();
        }
        return null;

    }


    public boolean deleteRequest (AnalystClusterRequest clusterRequest) {
        String url = BROKER_BASE_URL + String.format("/jobs/%s/%s/%s/%s", clusterRequest.userId, clusterRequest.graphId, clusterRequest.jobId, clusterRequest.taskId);
        HttpDelete httpDelete = new HttpDelete(url);
        try {

            HttpResponse response = httpClient.execute(httpDelete);

            EntityUtils.consumeQuietly(response.getEntity());
            return (response.getStatusLine().getStatusCode() == 200);
        } catch (Exception e) {
            e.printStackTrace();
            return false;
        }
    }

}
<code block>
package org.opentripplanner.analyst.cluster;

import org.opentripplanner.profile.ProfileRequest;
import org.opentripplanner.routing.core.RoutingRequest;

import java.io.Serializable;


public class AnalystClusterRequest implements Serializable {


	public String destinationPointsetId;


	public String userId;


	public String graphId;


	public String jobId;


	public String id;


	public String directOutputUrl;


	public int taskId;


	public String outputQueue;


	public String outputLocation;


	public ProfileRequest profileRequest;


	public RoutingRequest routingRequest;


	public boolean includeTimes = false;
	
	private AnalystClusterRequest(String destinationPointsetId, String graphId) {
		this.destinationPointsetId = destinationPointsetId;
		this.graphId = graphId;
	}


	public AnalystClusterRequest(String destinationPointsetId, String graphId, ProfileRequest req) {
		this(destinationPointsetId, graphId);
		routingRequest = null;
		try {
			profileRequest = req.clone();
		} catch (CloneNotSupportedException e) {
			throw new AssertionError();
		}
		profileRequest.analyst = true;
		profileRequest.toLat = profileRequest.fromLat;
		profileRequest.toLon = profileRequest.fromLon;
	}


	public AnalystClusterRequest(String destinationPointsetId, String graphId, RoutingRequest req) {
		this(destinationPointsetId, graphId);
		profileRequest = null;
		routingRequest = req.clone();
		routingRequest.batch = true;
		routingRequest.rctx = null;
	}


	public AnalystClusterRequest () {  }
}

<code block>
package org.opentripplanner.analyst.cluster;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.auth.AWSCredentials;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3Client;
import com.amazonaws.services.s3.model.S3Object;
import com.google.common.collect.Maps;
import org.apache.commons.io.FileUtils;
import org.apache.commons.io.IOUtils;
import org.opentripplanner.graph_builder.GraphBuilder;
import org.opentripplanner.routing.graph.Graph;
import org.opentripplanner.routing.impl.DefaultStreetVertexIndexFactory;
import org.opentripplanner.routing.services.GraphService;
import org.opentripplanner.routing.services.GraphSource;
import org.opentripplanner.routing.services.GraphSource.Factory;
import org.opentripplanner.standalone.CommandLineParameters;
import org.opentripplanner.standalone.Router;

import java.io.*;
import java.util.Collection;
import java.util.Enumeration;
import java.util.Map;
import java.util.zip.ZipEntry;
import java.util.zip.ZipException;
import java.util.zip.ZipFile;
import java.util.zip.ZipOutputStream;


public class ClusterGraphService extends GraphService { 

	static File GRAPH_DIR = new File("cache", "graphs");
	
	private String graphBucket;
	
	private Boolean workOffline = false;
	private AmazonS3Client s3;


	private Map<String,Router> graphMap = Maps.newConcurrentMap();
	
	@Override
	public synchronized Router getRouter(String graphId) {
		
		GRAPH_DIR.mkdirs();
		
		if(!graphMap.containsKey(graphId)) {
			
			try {
				if (!bucketCached(graphId)) {
					if(!workOffline) {
						downloadGraphSourceFiles(graphId, GRAPH_DIR);
					}
				}
			} catch (IOException e) {
				e.printStackTrace();
			}
			
			CommandLineParameters params = new CommandLineParameters();
			params.build = new File(GRAPH_DIR, graphId);
			params.inMemory = true;
			GraphBuilder gbt = GraphBuilder.forDirectory(params, params.build);
			gbt.run();
			
			Graph g = gbt.getGraph();
			
			g.routerId = graphId;
			
			g.index(new DefaultStreetVertexIndexFactory());

			g.index.clusterStopsAsNeeded();
			
			Router r = new Router(graphId, g);
			



			
			return r;
					
		}
		else {
			return graphMap.get(graphId);
		}
	}

	public ClusterGraphService(String s3CredentialsFilename, Boolean workOffline, String bucket) {
		
		if(!workOffline) {
			if (s3CredentialsFilename != null) {
				AWSCredentials creds = new ProfileCredentialsProvider(s3CredentialsFilename, "default").getCredentials();
				s3 = new AmazonS3Client(creds);
			}
			else {


				s3 = new AmazonS3Client();
			}
			
			this.graphBucket = bucket;
		}
		
		this.workOffline = workOffline;
	}
	

	public void addGraphFile(File graphFile) throws IOException {
		
		String graphId = graphFile.getName();
		
		if(graphId.endsWith(".zip"))
			graphId = graphId.substring(0, graphId.length() - 4);
		
		File graphDir = new File(GRAPH_DIR, graphId);
		
		if (graphDir.exists()) {
			if (graphDir.list().length == 0) {
				graphDir.delete();
			}
			else {
				return;
			}
		}
		

		graphDir.mkdirs();
		
		File graphDataZip = new File(GRAPH_DIR, graphId + ".zip");
				


		if(graphFile.isDirectory()) {
			FileUtils.copyDirectory(graphFile, graphDir);
			
			zipGraphDir(graphDir, graphDataZip);
		}
		else if(graphFile.getName().endsWith(".zip")) {
			FileUtils.copyFile(graphFile, graphDataZip);
			unpackGraphZip(graphDataZip, graphDir, false);
		}
		else {
			graphDataZip = null;
		}
			
		if(!workOffline && graphDataZip != null) {

			try {
				s3.getObject(graphBucket, graphId + ".zip");
			} catch (AmazonServiceException e) {
				s3.putObject(graphBucket, graphId+".zip", graphDataZip);
			}
		}
		
		graphDataZip.delete();
		
	}
	
	public synchronized File getZippedGraph(String graphId) throws IOException {
		
		File graphDataDir = new File(GRAPH_DIR, graphId);
		
		File graphZipFile = new File(GRAPH_DIR, graphId + ".zip");
		
		if(!graphDataDir.exists() && graphDataDir.isDirectory()) {
			
			FileOutputStream fileOutputStream = new FileOutputStream(graphZipFile);
			ZipOutputStream zipOutputStream = new ZipOutputStream(fileOutputStream);
			
			byte[] buffer = new byte[1024];
			
			for(File f : graphDataDir.listFiles()) {
				ZipEntry zipEntry = new ZipEntry(f.getName());
				zipOutputStream.putNextEntry(zipEntry);
	    		FileInputStream fileInput = new FileInputStream(f);

	    		int len;
	    		while ((len = fileInput.read(buffer)) > 0) {
	    			zipOutputStream.write(buffer, 0, len);
	    		}
	 
	    		fileInput.close();
	    		zipOutputStream.closeEntry();
			}
			
			zipOutputStream.close();
			
			return graphZipFile;
					
		}
		
		return null;
		
	}
	
	private static boolean bucketCached(String graphId) throws IOException {
		File graphData = new File(GRAPH_DIR, graphId);
		

		if(!graphData.exists()) {
			File graphDataZip = new File(GRAPH_DIR, graphId + ".zip");
			
			if(graphDataZip.exists()) {
				zipGraphDir(graphData, graphDataZip);
			}
		}
		
		
		return graphData.exists() && graphData.isDirectory();
	}

	private void downloadGraphSourceFiles(String graphId, File dir) throws IOException {

		File graphCacheDir = dir;
		if (!graphCacheDir.exists())
			graphCacheDir.mkdirs();

		File graphZipFile = new File(graphCacheDir, graphId + ".zip");

		File extractedGraphDir = new File(graphCacheDir, graphId);

		if (extractedGraphDir.exists()) {
			FileUtils.deleteDirectory(extractedGraphDir);
		}

		extractedGraphDir.mkdirs();

		S3Object graphZip = s3.getObject(graphBucket, graphId+".zip");

		InputStream zipFileIn = graphZip.getObjectContent();

		OutputStream zipFileOut = new FileOutputStream(graphZipFile);

		IOUtils.copy(zipFileIn, zipFileOut);
		IOUtils.closeQuietly(zipFileIn);
		IOUtils.closeQuietly(zipFileOut);

		unpackGraphZip(graphZipFile, extractedGraphDir);
	}

	private static void unpackGraphZip(File graphZipFile, File extractedGraphDir) throws ZipException, IOException {

		unpackGraphZip(graphZipFile, extractedGraphDir, true);
	}
	
	private static void unpackGraphZip(File graphZipFile, File extractedGraphDir, boolean delete) throws ZipException, IOException {
		
		ZipFile zipFile = new ZipFile(graphZipFile);
		
		Enumeration<? extends ZipEntry> entries = zipFile.entries();

		while (entries.hasMoreElements()) {

			ZipEntry entry = entries.nextElement();
			File entryDestination = new File(extractedGraphDir, entry.getName());

			entryDestination.getParentFile().mkdirs();

			if (entry.isDirectory())
				entryDestination.mkdirs();
			else {
				InputStream entryFileIn = zipFile.getInputStream(entry);
				OutputStream entryFileOut = new FileOutputStream(entryDestination);
				IOUtils.copy(entryFileIn, entryFileOut);
				IOUtils.closeQuietly(entryFileIn);
				IOUtils.closeQuietly(entryFileOut);
			}
		}

		zipFile.close();

		if (delete) {
			graphZipFile.delete();
		}
	}
	
	private static void zipGraphDir(File graphDirectory, File zipGraphFile) throws IOException {
		
		FileOutputStream fileOutputStream = new FileOutputStream(zipGraphFile);
		ZipOutputStream zipOutputStream = new ZipOutputStream(fileOutputStream);
		
		byte[] buffer = new byte[1024];
		
		for(File f : graphDirectory.listFiles()) {
			if (f.isDirectory())
				continue;
			
			ZipEntry zipEntry = new ZipEntry(f.getName());
			zipOutputStream.putNextEntry(zipEntry);
    		FileInputStream fileInput = new FileInputStream(f);

    		int len;
    		while ((len = fileInput.read(buffer)) > 0) {
    			zipOutputStream.write(buffer, 0, len);
    		}
 
    		fileInput.close();
    		zipOutputStream.closeEntry();
		}
		
		zipOutputStream.close();
	}
	

	@Override
	public int evictAll() {
		graphMap.clear();
		return 0;
	}

	@Override
	public Collection<String> getRouterIds() {
		return graphMap.keySet();
	}

	@Override
	public Factory getGraphSourceFactory() {
		return null;
	}

	@Override
	public boolean registerGraph(String arg0, GraphSource arg1) {
		return false;
	}

	@Override
	public void setDefaultRouterId(String arg0) {	
	}
}

<code block>
package org.opentripplanner.analyst.cluster;

import ch.qos.logback.core.PropertyDefinerBase;
import com.amazonaws.regions.Region;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3Client;
import com.conveyal.geojson.GeoJsonModule;
import com.fasterxml.jackson.core.JsonParser;
import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.DeserializationFeature;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.google.common.cache.Cache;
import com.google.common.cache.CacheBuilder;
import org.apache.http.HttpEntity;
import org.apache.http.HttpResponse;
import org.apache.http.client.HttpClient;
import org.apache.http.client.config.RequestConfig;
import org.apache.http.client.methods.HttpDelete;
import org.apache.http.client.methods.HttpGet;
import org.apache.http.client.methods.HttpPost;
import org.apache.http.config.SocketConfig;
import org.apache.http.conn.HttpHostConnectException;
import org.apache.http.entity.ByteArrayEntity;
import org.apache.http.impl.client.HttpClients;
import org.apache.http.impl.conn.PoolingHttpClientConnectionManager;
import org.apache.http.message.BasicHeader;
import org.apache.http.util.EntityUtils;
import org.opentripplanner.analyst.PointSet;
import org.opentripplanner.analyst.ResultSet;
import org.opentripplanner.analyst.SampleSet;
import org.opentripplanner.api.model.AgencyAndIdSerializer;
import org.opentripplanner.api.model.JodaLocalDateSerializer;
import org.opentripplanner.api.model.QualifiedModeSetSerializer;
import org.opentripplanner.api.model.TraverseModeSetSerializer;
import org.opentripplanner.profile.RaptorWorkerData;
import org.opentripplanner.profile.RepeatedRaptorProfileRouter;
import org.opentripplanner.routing.core.RoutingRequest;
import org.opentripplanner.routing.graph.Graph;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.*;
import java.net.SocketTimeoutException;
import java.net.URI;
import java.util.List;
import java.util.Properties;
import java.util.Random;
import java.util.UUID;
import java.util.zip.GZIPOutputStream;


public class AnalystWorker implements Runnable {

    private static final Logger LOG = LoggerFactory.getLogger(AnalystWorker.class);

    public static final String WORKER_ID_HEADER = "X-Worker-Id";

    public static final int POLL_TIMEOUT = 10 * 1000;


    public final boolean autoShutdown;

    public static final Random random = new Random();

    private TaskStatisticsStore statsStore;

    ObjectMapper objectMapper;

    String BROKER_BASE_URL = "http://localhost:9001";

    String s3Prefix = "analyst-dev";

    static final HttpClient httpClient;


    private Cache<String, RaptorWorkerData> workerDataCache = CacheBuilder.newBuilder()
            .maximumSize(200)
            .build();

    static {
        PoolingHttpClientConnectionManager mgr = new PoolingHttpClientConnectionManager();
        mgr.setDefaultMaxPerRoute(20);

        int timeout = 10 * 1000;
        SocketConfig cfg = SocketConfig.custom()
                .setSoTimeout(timeout)
                .build();
        mgr.setDefaultSocketConfig(cfg);

        httpClient = HttpClients.custom()
                .setConnectionManager(mgr)
                .build();
    }

    private final String workerId = UUID.randomUUID().toString().replace("-", ""); 



    ClusterGraphBuilder clusterGraphBuilder;


    PointSetDatastore pointSetDatastore;


    AmazonS3 s3;

    String graphId = null;
    long startupTime, nextShutdownCheckTime;


    Region awsRegion = Region.getRegion(Regions.US_EAST_1);


    private String instanceType;


    public static final String machineId = UUID.randomUUID().toString().replaceAll("-", "");

    boolean isSinglePoint = false;

    public AnalystWorker(Properties config) {



        String statsQueue = config.getProperty("statistics-queue");
        if (statsQueue != null)
            this.statsStore = new SQSTaskStatisticsStore(statsQueue);
        else

            this.statsStore = s -> {};

        String addr = config.getProperty("broker-address");
        String port = config.getProperty("broker-port");

        if (addr != null) {
            if (port != null)
                this.BROKER_BASE_URL = String.format("http://%s:%s", addr, port);
            else
                this.BROKER_BASE_URL = String.format("http://%s", addr);
        }





        this.graphId = config.getProperty("initial-graph-id");

        this.pointSetDatastore = new PointSetDatastore(10, null, false, config.getProperty("pointsets-bucket"));
        this.clusterGraphBuilder = new ClusterGraphBuilder(config.getProperty("graphs-bucket"));

        Boolean autoShutdown = Boolean.parseBoolean(config.getProperty("auto-shutdown"));
        this.autoShutdown = autoShutdown == null ? false : autoShutdown;


        startupTime = System.currentTimeMillis();
        nextShutdownCheckTime = startupTime + 55 * 60 * 1000;





        s3 = new AmazonS3Client();
        s3.setRegion(awsRegion);


        objectMapper = new ObjectMapper();
        objectMapper.configure(JsonParser.Feature.ALLOW_COMMENTS, true);
        objectMapper.configure(JsonParser.Feature.ALLOW_UNQUOTED_FIELD_NAMES, true);
        objectMapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false); 


        objectMapper.registerModule(AgencyAndIdSerializer.makeModule());


        objectMapper.registerModule(QualifiedModeSetSerializer.makeModule());


        objectMapper.registerModule(JodaLocalDateSerializer.makeModule());


        objectMapper.registerModule(TraverseModeSetSerializer.makeModule());

        objectMapper.registerModule(new GeoJsonModule());

        instanceType = getInstanceType();
    }

    @Override
    public void run() {

        boolean idle = false;
        while (true) {

            if (System.currentTimeMillis() > nextShutdownCheckTime && autoShutdown) {
                if (idle) {
                    try {
                        Process process = new ProcessBuilder("sudo", "/sbin/shutdown", "-h", "now").start();
                        process.waitFor();
                    } catch (Exception ex) {
                        ex.printStackTrace();
                    } finally {
                        System.exit(0);
                    }
                }
                nextShutdownCheckTime += 60 * 60 * 1000;
            }
            LOG.info("Long-polling for work ({} second timeout).", POLL_TIMEOUT / 1000.0);


            List<AnalystClusterRequest> tasks = getSomeWork();
            if (tasks == null) {
                LOG.info("Didn't get any work. Retrying.");
                idle = true;
                continue;
            }
            tasks.parallelStream().forEach(this::handleOneRequest);
            idle = false;
        }
    }

    private void handleOneRequest(AnalystClusterRequest clusterRequest) {
        try {
            long startTime = System.currentTimeMillis();
            LOG.info("Handling message {}", clusterRequest.toString());

            TaskStatistics ts = new TaskStatistics();
            ts.pointsetId = clusterRequest.destinationPointsetId;
            ts.graphId = clusterRequest.graphId;
            ts.awsInstanceType = instanceType;
            ts.jobId = clusterRequest.jobId;
            ts.workerId = machineId;
            ts.single = clusterRequest.outputLocation == null;

            long graphStartTime = System.currentTimeMillis();




            Graph graph = clusterGraphBuilder.getGraph(clusterRequest.graphId);
            graphId = clusterRequest.graphId; 

            ts.graphBuild = (int) (System.currentTimeMillis() - graphStartTime);

            ts.graphTripCount = graph.index.patternForTrip.size();
            ts.graphStopCount = graph.index.stopForId.size();


            ResultEnvelope envelope = new ResultEnvelope();
            if (clusterRequest.profileRequest != null) {
                ts.lon = clusterRequest.profileRequest.fromLon;
                ts.lat = clusterRequest.profileRequest.fromLat;


                RepeatedRaptorProfileRouter router;

                boolean isochrone = clusterRequest.destinationPointsetId == null;
                ts.isochrone = isochrone;
                if (!isochrone) {


                    PointSet pointSet = pointSetDatastore.get(clusterRequest.destinationPointsetId);

                    SampleSet sampleSet = pointSet.getOrCreateSampleSet(graph);
                    router =
                            new RepeatedRaptorProfileRouter(graph, clusterRequest.profileRequest, sampleSet);


                    if (clusterRequest.outputLocation != null) {
                        router.raptorWorkerData = workerDataCache.get(clusterRequest.jobId, () -> {
                            return RepeatedRaptorProfileRouter
                                    .getRaptorWorkerData(clusterRequest.profileRequest, graph,
                                            sampleSet, ts);
                        });
                    }
                } else {
                    router = new RepeatedRaptorProfileRouter(graph, clusterRequest.profileRequest);

                    if (clusterRequest.outputLocation == null) {
                        router.raptorWorkerData = workerDataCache.get(clusterRequest.jobId, () -> {
                            return RepeatedRaptorProfileRouter
                                    .getRaptorWorkerData(clusterRequest.profileRequest, graph, null,
                                            ts);
                        });
                    }
                }

                try {
                    router.route(ts);
                    long resultSetStart = System.currentTimeMillis();

                    if (isochrone) {
                        envelope.worstCase = new ResultSet(router.timeSurfaceRangeSet.max);
                        envelope.bestCase = new ResultSet(router.timeSurfaceRangeSet.min);
                        envelope.avgCase = new ResultSet(router.timeSurfaceRangeSet.avg);
                    } else {
                        ResultSet.RangeSet results = router
                                .makeResults(clusterRequest.includeTimes, !isochrone, isochrone);

                        envelope.bestCase = results.min;
                        envelope.avgCase = results.avg;
                        envelope.worstCase = results.max;
                    }
                    envelope.id = clusterRequest.id;
                    envelope.destinationPointsetId = clusterRequest.destinationPointsetId;

                    ts.resultSets = (int) (System.currentTimeMillis() - resultSetStart);
                    ts.success = true;
                } catch (Exception ex) {

                    ts.success = false;
                }
            } else {

                RoutingRequest routingRequest = clusterRequest.routingRequest;

            }

            if (clusterRequest.outputLocation != null) {


                String s3key = String.join("/", clusterRequest.jobId, clusterRequest.id + ".json.gz");
                PipedInputStream inPipe = new PipedInputStream();
                PipedOutputStream outPipe = new PipedOutputStream(inPipe);
                new Thread(() -> {
                    s3.putObject(clusterRequest.outputLocation, s3key, inPipe, null);
                }).start();
                OutputStream gzipOutputStream = new GZIPOutputStream(outPipe);


                objectMapper.writeValue(gzipOutputStream, envelope);
                gzipOutputStream.close();

                deleteRequest(clusterRequest);
            } else {

                finishPriorityTask(clusterRequest, envelope);
            }

            ts.total = (int) (System.currentTimeMillis() - startTime);
            statsStore.store(ts);
        } catch (Exception ex) {
            LOG.error("An error occurred while routing: " + ex.getMessage());
            ex.printStackTrace();
        }

    }

    public List<AnalystClusterRequest> getSomeWork() {


        String url = BROKER_BASE_URL + "/dequeue/" + graphId;
        HttpPost httpPost = new HttpPost(url);
        httpPost.setHeader(new BasicHeader(WORKER_ID_HEADER, workerId));
        HttpResponse response = null;
        try {
            response = httpClient.execute(httpPost);
            HttpEntity entity = response.getEntity();
            if (entity == null) {
                return null;
            }
            if (response.getStatusLine().getStatusCode() != 200) {
                EntityUtils.consumeQuietly(entity);
                return null;
            }
            return objectMapper.readValue(entity.getContent(), new TypeReference<List<AnalystClusterRequest>>() {
            });
        } catch (JsonProcessingException e) {
            LOG.error("JSON processing exception while getting work: {}", e.getMessage());
        } catch (SocketTimeoutException stex) {
            LOG.error("Socket timeout while waiting to receive work.");
        } catch (HttpHostConnectException ce) {
            LOG.error("Broker refused connection. Sleeping before retry.");
            try {
                Thread.currentThread().sleep(5000);
            } catch (InterruptedException e) {
            }
        } catch (IOException e) {
            LOG.error("IO exception while getting work.");
            e.printStackTrace();
        }
        return null;

    }


    public void finishPriorityTask(AnalystClusterRequest clusterRequest, Object result) {
        String url = BROKER_BASE_URL + String.format("/complete/priority/%s", clusterRequest.taskId);
        HttpPost httpPost = new HttpPost(url);
        try {


            byte[] serializedResult = objectMapper.writeValueAsBytes(result);
            httpPost.setEntity(new ByteArrayEntity(serializedResult));
            HttpResponse response = httpClient.execute(httpPost);

            EntityUtils.consumeQuietly(response.getEntity());
            if (response.getStatusLine().getStatusCode() == 200) {
                LOG.info("Successfully marked task {} as completed.", clusterRequest.taskId);
            } else if (response.getStatusLine().getStatusCode() == 404) {
                LOG.info("Task {} was not marked as completed because it doesn't exist.", clusterRequest.taskId);
            } else {
                LOG.info("Failed to mark task {} as completed, ({}).", clusterRequest.taskId,
                        response.getStatusLine());
            }
        } catch (Exception e) {
            e.printStackTrace();
            LOG.info("Failed to mark task {} as completed.", clusterRequest.taskId);
        }
    }


    public void deleteRequest(AnalystClusterRequest clusterRequest) {
        String url = BROKER_BASE_URL + String.format("/tasks/%s", clusterRequest.taskId);
        HttpDelete httpDelete = new HttpDelete(url);
        try {
            HttpResponse response = httpClient.execute(httpDelete);

            EntityUtils.consumeQuietly(response.getEntity());
            if (response.getStatusLine().getStatusCode() == 200) {
                LOG.info("Successfully deleted task {}.", clusterRequest.taskId);
            } else {
                LOG.info("Failed to delete task {} ({}).", clusterRequest.taskId, response.getStatusLine());
            }
        } catch (Exception e) {
            e.printStackTrace();
            LOG.info("Failed to delete task {}", clusterRequest.taskId);
        }
    }


    public String getInstanceType () {
        try {
            HttpGet get = new HttpGet();



            get.setURI(new URI("http://169.254.169.254/latest/meta-data/instance-type"));
            get.setConfig(RequestConfig.custom()
                    .setConnectTimeout(2000)
                    .setSocketTimeout(2000)
                    .build()
            );

            HttpResponse res = httpClient.execute(get);

            InputStream is = res.getEntity().getContent();
            BufferedReader reader = new BufferedReader(new InputStreamReader(is));
            String type = reader.readLine().trim();
            reader.close();
            return type;
        } catch (Exception e) {
            LOG.info("could not retrieve EC2 instance type, you may be running outside of EC2.");
            return null;
        }
    }


    public static void main(String[] args) {
        Properties config = new Properties();

        try {
            File cfg;
            if (args.length > 0)
                cfg = new File(args[0]);
            else
                cfg = new File("worker.conf");

            InputStream cfgis = new FileInputStream(cfg);
            config.load(cfgis);
            cfgis.close();
        } catch (Exception e) {
            LOG.info("Error loading worker configuration", e);
            return;
        }

        new AnalystWorker(config).run();
    }


    public static class WorkerIdDefiner extends PropertyDefinerBase {
        @Override public String getPropertyValue() {
            return AnalystWorker.machineId;
        }
    }
}
<code block>
package org.opentripplanner.analyst.cluster;

import com.amazonaws.regions.Region;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3Client;
import com.conveyal.geojson.GeoJsonModule;
import com.fasterxml.jackson.core.JsonParser;
import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.DeserializationFeature;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.google.common.cache.Cache;
import com.google.common.cache.CacheBuilder;
import org.apache.http.HttpEntity;
import org.apache.http.HttpResponse;
import org.apache.http.client.HttpClient;
import org.apache.http.client.config.RequestConfig;
import org.apache.http.client.methods.HttpDelete;
import org.apache.http.client.methods.HttpGet;
import org.apache.http.client.methods.HttpPost;
import org.apache.http.config.SocketConfig;
import org.apache.http.conn.HttpHostConnectException;
import org.apache.http.entity.ByteArrayEntity;
import org.apache.http.impl.client.HttpClients;
import org.apache.http.impl.conn.PoolingHttpClientConnectionManager;
import org.apache.http.message.BasicHeader;
import org.apache.http.util.EntityUtils;
import org.opentripplanner.analyst.PointSet;
import org.opentripplanner.analyst.ResultSet;
import org.opentripplanner.analyst.SampleSet;
import org.opentripplanner.api.model.AgencyAndIdSerializer;
import org.opentripplanner.api.model.JodaLocalDateSerializer;
import org.opentripplanner.api.model.QualifiedModeSetSerializer;
import org.opentripplanner.api.model.TraverseModeSetSerializer;
import org.opentripplanner.profile.RaptorWorkerData;
import org.opentripplanner.profile.RepeatedRaptorProfileRouter;
import org.opentripplanner.routing.core.RoutingRequest;
import org.opentripplanner.routing.graph.Graph;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.*;
import java.net.SocketTimeoutException;
import java.net.URI;
import java.util.List;
import java.util.Properties;
import java.util.Random;
import java.util.UUID;
import java.util.zip.GZIPOutputStream;


public class AnalystWorker implements Runnable {

    public static final String machineId = UUID.randomUUID().toString().replaceAll("-", "");

    private static final Logger LOG = LoggerFactory.getLogger(AnalystWorker.class);

    public static final String WORKER_ID_HEADER = "X-Worker-Id";

    public static final int POLL_TIMEOUT = 10 * 1000;


    public final boolean autoShutdown;

    public static final Random random = new Random();

    private TaskStatisticsStore statsStore;

    ObjectMapper objectMapper;

    String BROKER_BASE_URL = "http://localhost:9001";

    static final HttpClient httpClient;


    private Cache<String, RaptorWorkerData> workerDataCache = CacheBuilder.newBuilder()
            .maximumSize(200)
            .build();

    static {
        PoolingHttpClientConnectionManager mgr = new PoolingHttpClientConnectionManager();
        mgr.setDefaultMaxPerRoute(20);

        int timeout = 10 * 1000;
        SocketConfig cfg = SocketConfig.custom()
                .setSoTimeout(timeout)
                .build();
        mgr.setDefaultSocketConfig(cfg);

        httpClient = HttpClients.custom()
                .setConnectionManager(mgr)
                .build();
    }


    ClusterGraphBuilder clusterGraphBuilder;


    PointSetDatastore pointSetDatastore;


    AmazonS3 s3;

    String graphId = null;
    long startupTime, nextShutdownCheckTime;


    Region awsRegion = Region.getRegion(Regions.US_EAST_1);


    private String instanceType;

    boolean isSinglePoint = false;

    public AnalystWorker(Properties config) {



        String statsQueue = config.getProperty("statistics-queue");
        if (statsQueue != null)
            this.statsStore = new SQSTaskStatisticsStore(statsQueue);
        else

            this.statsStore = s -> {};

        String addr = config.getProperty("broker-address");
        String port = config.getProperty("broker-port");

        if (addr != null) {
            if (port != null)
                this.BROKER_BASE_URL = String.format("http://%s:%s", addr, port);
            else
                this.BROKER_BASE_URL = String.format("http://%s", addr);
        }





        this.graphId = config.getProperty("initial-graph-id");

        this.pointSetDatastore = new PointSetDatastore(10, null, false, config.getProperty("pointsets-bucket"));
        this.clusterGraphBuilder = new ClusterGraphBuilder(config.getProperty("graphs-bucket"));

        Boolean autoShutdown = Boolean.parseBoolean(config.getProperty("auto-shutdown"));
        this.autoShutdown = autoShutdown == null ? false : autoShutdown;


        startupTime = System.currentTimeMillis();
        nextShutdownCheckTime = startupTime + 55 * 60 * 1000;





        s3 = new AmazonS3Client();
        s3.setRegion(awsRegion);


        objectMapper = new ObjectMapper();
        objectMapper.configure(JsonParser.Feature.ALLOW_COMMENTS, true);
        objectMapper.configure(JsonParser.Feature.ALLOW_UNQUOTED_FIELD_NAMES, true);
        objectMapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false); 


        objectMapper.registerModule(AgencyAndIdSerializer.makeModule());


        objectMapper.registerModule(QualifiedModeSetSerializer.makeModule());


        objectMapper.registerModule(JodaLocalDateSerializer.makeModule());


        objectMapper.registerModule(TraverseModeSetSerializer.makeModule());

        objectMapper.registerModule(new GeoJsonModule());

        instanceType = getInstanceType();
    }

    @Override
    public void run() {

        boolean idle = false;
        while (true) {

            if (System.currentTimeMillis() > nextShutdownCheckTime && autoShutdown) {
                if (idle) {
                    try {
                        Process process = new ProcessBuilder("sudo", "/sbin/shutdown", "-h", "now").start();
                        process.waitFor();
                    } catch (Exception ex) {
                        ex.printStackTrace();
                    } finally {
                        System.exit(0);
                    }
                }
                nextShutdownCheckTime += 60 * 60 * 1000;
            }
            LOG.info("Long-polling for work ({} second timeout).", POLL_TIMEOUT / 1000.0);


            List<AnalystClusterRequest> tasks = getSomeWork();
            if (tasks == null) {
                LOG.info("Didn't get any work. Retrying.");
                idle = true;
                continue;
            }
            tasks.parallelStream().forEach(this::handleOneRequest);
            idle = false;
        }
    }

    private void handleOneRequest(AnalystClusterRequest clusterRequest) {
        try {
            long startTime = System.currentTimeMillis();
            LOG.info("Handling message {}", clusterRequest.toString());

            TaskStatistics ts = new TaskStatistics();
            ts.pointsetId = clusterRequest.destinationPointsetId;
            ts.graphId = clusterRequest.graphId;
            ts.awsInstanceType = instanceType;
            ts.jobId = clusterRequest.jobId;
            ts.workerId = machineId;
            ts.single = clusterRequest.outputLocation == null;

            long graphStartTime = System.currentTimeMillis();




            Graph graph = clusterGraphBuilder.getGraph(clusterRequest.graphId);
            graphId = clusterRequest.graphId; 

            ts.graphBuild = (int) (System.currentTimeMillis() - graphStartTime);

            ts.graphTripCount = graph.index.patternForTrip.size();
            ts.graphStopCount = graph.index.stopForId.size();


            ResultEnvelope envelope = new ResultEnvelope();
            if (clusterRequest.profileRequest != null) {
                ts.lon = clusterRequest.profileRequest.fromLon;
                ts.lat = clusterRequest.profileRequest.fromLat;


                RepeatedRaptorProfileRouter router;

                boolean isochrone = clusterRequest.destinationPointsetId == null;
                ts.isochrone = isochrone;
                if (!isochrone) {


                    PointSet pointSet = pointSetDatastore.get(clusterRequest.destinationPointsetId);

                    SampleSet sampleSet = pointSet.getOrCreateSampleSet(graph);
                    router =
                            new RepeatedRaptorProfileRouter(graph, clusterRequest.profileRequest, sampleSet);


                    if (clusterRequest.outputLocation != null) {
                        router.raptorWorkerData = workerDataCache.get(clusterRequest.jobId, () -> {
                            return RepeatedRaptorProfileRouter
                                    .getRaptorWorkerData(clusterRequest.profileRequest, graph,
                                            sampleSet, ts);
                        });
                    }
                } else {
                    router = new RepeatedRaptorProfileRouter(graph, clusterRequest.profileRequest);

                    if (clusterRequest.outputLocation == null) {
                        router.raptorWorkerData = workerDataCache.get(clusterRequest.jobId, () -> {
                            return RepeatedRaptorProfileRouter
                                    .getRaptorWorkerData(clusterRequest.profileRequest, graph, null,
                                            ts);
                        });
                    }
                }

                try {
                    router.route(ts);
                    long resultSetStart = System.currentTimeMillis();

                    if (isochrone) {
                        envelope.worstCase = new ResultSet(router.timeSurfaceRangeSet.max);
                        envelope.bestCase = new ResultSet(router.timeSurfaceRangeSet.min);
                        envelope.avgCase = new ResultSet(router.timeSurfaceRangeSet.avg);
                    } else {
                        ResultSet.RangeSet results = router
                                .makeResults(clusterRequest.includeTimes, !isochrone, isochrone);

                        envelope.bestCase = results.min;
                        envelope.avgCase = results.avg;
                        envelope.worstCase = results.max;
                    }
                    envelope.id = clusterRequest.id;
                    envelope.destinationPointsetId = clusterRequest.destinationPointsetId;

                    ts.resultSets = (int) (System.currentTimeMillis() - resultSetStart);
                    ts.success = true;
                } catch (Exception ex) {

                    ts.success = false;
                }
            } else {

                RoutingRequest routingRequest = clusterRequest.routingRequest;

            }

            if (clusterRequest.outputLocation != null) {


                String s3key = String.join("/", clusterRequest.jobId, clusterRequest.id + ".json.gz");
                PipedInputStream inPipe = new PipedInputStream();
                PipedOutputStream outPipe = new PipedOutputStream(inPipe);
                new Thread(() -> {
                    s3.putObject(clusterRequest.outputLocation, s3key, inPipe, null);
                }).start();
                OutputStream gzipOutputStream = new GZIPOutputStream(outPipe);


                objectMapper.writeValue(gzipOutputStream, envelope);
                gzipOutputStream.close();

                deleteRequest(clusterRequest);
            } else {

                finishPriorityTask(clusterRequest, envelope);
            }

            ts.total = (int) (System.currentTimeMillis() - startTime);
            statsStore.store(ts);
        } catch (Exception ex) {
            LOG.error("An error occurred while routing: " + ex.getMessage());
            ex.printStackTrace();
        }

    }

    public List<AnalystClusterRequest> getSomeWork() {


        String url = BROKER_BASE_URL + "/dequeue/" + graphId;
        HttpPost httpPost = new HttpPost(url);
        httpPost.setHeader(new BasicHeader(WORKER_ID_HEADER, machineId));
        HttpResponse response = null;
        try {
            response = httpClient.execute(httpPost);
            HttpEntity entity = response.getEntity();
            if (entity == null) {
                return null;
            }
            if (response.getStatusLine().getStatusCode() != 200) {
                EntityUtils.consumeQuietly(entity);
                return null;
            }
            return objectMapper.readValue(entity.getContent(), new TypeReference<List<AnalystClusterRequest>>() {
            });
        } catch (JsonProcessingException e) {
            LOG.error("JSON processing exception while getting work: {}", e.getMessage());
        } catch (SocketTimeoutException stex) {
            LOG.error("Socket timeout while waiting to receive work.");
        } catch (HttpHostConnectException ce) {
            LOG.error("Broker refused connection. Sleeping before retry.");
            try {
                Thread.currentThread().sleep(5000);
            } catch (InterruptedException e) {
            }
        } catch (IOException e) {
            LOG.error("IO exception while getting work.");
            e.printStackTrace();
        }
        return null;

    }


    public void finishPriorityTask(AnalystClusterRequest clusterRequest, Object result) {
        String url = BROKER_BASE_URL + String.format("/complete/priority/%s", clusterRequest.taskId);
        HttpPost httpPost = new HttpPost(url);
        try {


            byte[] serializedResult = objectMapper.writeValueAsBytes(result);
            httpPost.setEntity(new ByteArrayEntity(serializedResult));
            HttpResponse response = httpClient.execute(httpPost);

            EntityUtils.consumeQuietly(response.getEntity());
            if (response.getStatusLine().getStatusCode() == 200) {
                LOG.info("Successfully marked task {} as completed.", clusterRequest.taskId);
            } else if (response.getStatusLine().getStatusCode() == 404) {
                LOG.info("Task {} was not marked as completed because it doesn't exist.", clusterRequest.taskId);
            } else {
                LOG.info("Failed to mark task {} as completed, ({}).", clusterRequest.taskId,
                        response.getStatusLine());
            }
        } catch (Exception e) {
            e.printStackTrace();
            LOG.info("Failed to mark task {} as completed.", clusterRequest.taskId);
        }
    }


    public void deleteRequest(AnalystClusterRequest clusterRequest) {
        String url = BROKER_BASE_URL + String.format("/tasks/%s", clusterRequest.taskId);
        HttpDelete httpDelete = new HttpDelete(url);
        try {
            HttpResponse response = httpClient.execute(httpDelete);

            EntityUtils.consumeQuietly(response.getEntity());
            if (response.getStatusLine().getStatusCode() == 200) {
                LOG.info("Successfully deleted task {}.", clusterRequest.taskId);
            } else {
                LOG.info("Failed to delete task {} ({}).", clusterRequest.taskId, response.getStatusLine());
            }
        } catch (Exception e) {
            e.printStackTrace();
            LOG.info("Failed to delete task {}", clusterRequest.taskId);
        }
    }


    public String getInstanceType () {
        try {
            HttpGet get = new HttpGet();



            get.setURI(new URI("http://169.254.169.254/latest/meta-data/instance-type"));
            get.setConfig(RequestConfig.custom()
                    .setConnectTimeout(2000)
                    .setSocketTimeout(2000)
                    .build()
            );

            HttpResponse res = httpClient.execute(get);

            InputStream is = res.getEntity().getContent();
            BufferedReader reader = new BufferedReader(new InputStreamReader(is));
            String type = reader.readLine().trim();
            reader.close();
            return type;
        } catch (Exception e) {
            LOG.info("could not retrieve EC2 instance type, you may be running outside of EC2.");
            return null;
        }
    }


    public static void main(String[] args) {
        Properties config = new Properties();

        try {
            File cfg;
            if (args.length > 0)
                cfg = new File(args[0]);
            else
                cfg = new File("worker.conf");

            InputStream cfgis = new FileInputStream(cfg);
            config.load(cfgis);
            cfgis.close();
        } catch (Exception e) {
            LOG.info("Error loading worker configuration", e);
            return;
        }

        new AnalystWorker(config).run();
    }
}
<code block>
package org.opentripplanner.analyst.cluster;

import ch.qos.logback.core.PropertyDefinerBase;


public class WorkerIdDefiner extends PropertyDefinerBase {
    @Override public String getPropertyValue() {
        return AnalystWorker.machineId;
    }
}